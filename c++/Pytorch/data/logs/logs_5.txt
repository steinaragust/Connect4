Training set samples: 7265
Batch size: 32
[Epoch: 1, batch: 45/228] total loss per batch: 1.102
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.4771, 0.0107, 0.0115, 0.4535, 0.0245, 0.0080, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.000

[Epoch: 1, batch: 90/228] total loss per batch: 0.989
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([4.9633e-02, 2.7200e-09, 4.4818e-02, 6.7434e-09, 3.8725e-01, 5.1830e-01,
        4.7417e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 -0.352

[Epoch: 1, batch: 135/228] total loss per batch: 0.992
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.0145, 0.0492, 0.1133, 0.7518, 0.0127, 0.0495],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.013

[Epoch: 1, batch: 180/228] total loss per batch: 0.954
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0360, 0.0081, 0.0132, 0.0170, 0.8705, 0.0155, 0.0397],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 1, batch: 225/228] total loss per batch: 0.985
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0081, 0.0056, 0.0182, 0.9107, 0.0219, 0.0180, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 2, batch: 45/228] total loss per batch: 0.753
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9376, 0.0010, 0.0020, 0.0503, 0.0060, 0.0012, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 2, batch: 90/228] total loss per batch: 0.685
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([3.9278e-02, 6.9574e-10, 3.6465e-02, 3.6413e-09, 7.9687e-02, 8.4457e-01,
        2.0285e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 -0.439

[Epoch: 2, batch: 135/228] total loss per batch: 0.678
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0242, 0.0502, 0.1568, 0.3850, 0.0965, 0.0271, 0.2600],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 2, batch: 180/228] total loss per batch: 0.681
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0165, 0.0040, 0.0059, 0.0130, 0.9342, 0.0044, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 2, batch: 225/228] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0676, 0.0192, 0.0716, 0.7419, 0.0177, 0.0559, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 3, batch: 45/228] total loss per batch: 0.626
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([9.6825e-01, 5.4948e-04, 8.8323e-04, 2.6063e-02, 2.3286e-03, 4.4053e-04,
        1.4900e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.010

[Epoch: 3, batch: 90/228] total loss per batch: 0.573
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([3.7976e-02, 4.1395e-10, 2.2537e-02, 3.4587e-09, 2.8083e-01, 6.5866e-01,
        9.2945e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 -0.008

[Epoch: 3, batch: 135/228] total loss per batch: 0.569
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0270, 0.0340, 0.1631, 0.4567, 0.1606, 0.0167, 0.1419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 3, batch: 180/228] total loss per batch: 0.579
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0133, 0.0045, 0.0025, 0.0058, 0.9449, 0.0063, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 3, batch: 225/228] total loss per batch: 0.601
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0070, 0.0047, 0.0138, 0.9533, 0.0056, 0.0093, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 4, batch: 45/228] total loss per batch: 0.578
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([9.4232e-01, 6.1823e-04, 1.6059e-03, 4.9056e-02, 3.3024e-03, 1.0123e-03,
        2.0898e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.026

[Epoch: 4, batch: 90/228] total loss per batch: 0.534
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.5416e-02, 3.6798e-10, 4.4990e-02, 4.5564e-09, 2.5099e-01, 6.1860e-01,
        1.8400e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.224

[Epoch: 4, batch: 135/228] total loss per batch: 0.543
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0189, 0.0198, 0.0701, 0.3293, 0.5240, 0.0162, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 4, batch: 180/228] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0116, 0.0048, 0.0029, 0.0261, 0.9202, 0.0114, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 4, batch: 225/228] total loss per batch: 0.566
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0033, 0.0042, 0.0197, 0.9447, 0.0049, 0.0111, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.029

[Epoch: 5, batch: 45/228] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([9.3099e-01, 7.6285e-04, 1.7026e-03, 5.8945e-02, 4.4147e-03, 9.0339e-04,
        2.2836e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.020

[Epoch: 5, batch: 90/228] total loss per batch: 0.516
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.5764e-02, 4.2692e-10, 6.8657e-02, 4.0284e-09, 2.9189e-01, 5.5369e-01,
        4.3830e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.706

[Epoch: 5, batch: 135/228] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0155, 0.0199, 0.1402, 0.6901, 0.1120, 0.0085, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 5, batch: 180/228] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0138, 0.0054, 0.0027, 0.0092, 0.9358, 0.0064, 0.0266],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 5, batch: 225/228] total loss per batch: 0.546
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0073, 0.0139, 0.9466, 0.0062, 0.0113, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.049

[Epoch: 6, batch: 45/228] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9711, 0.0013, 0.0020, 0.0161, 0.0051, 0.0014, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.012

[Epoch: 6, batch: 90/228] total loss per batch: 0.509
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.5450e-01, 1.5281e-09, 9.6079e-02, 4.4238e-09, 1.2439e-01, 6.2503e-01,
        9.4011e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.764

[Epoch: 6, batch: 135/228] total loss per batch: 0.520
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0143, 0.3404, 0.2187, 0.3808, 0.0138, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 6, batch: 180/228] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0037, 0.0029, 0.0050, 0.9627, 0.0047, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 6, batch: 225/228] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0042, 0.0067, 0.0209, 0.9441, 0.0055, 0.0107, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.030

[Epoch: 7, batch: 45/228] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9491, 0.0021, 0.0020, 0.0353, 0.0044, 0.0020, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.013

[Epoch: 7, batch: 90/228] total loss per batch: 0.501
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4721e-01, 1.5345e-09, 9.7415e-02, 9.7338e-09, 2.3987e-01, 5.1550e-01,
        1.2588e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.782

[Epoch: 7, batch: 135/228] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0149, 0.0120, 0.0442, 0.6604, 0.2472, 0.0098, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 7, batch: 180/228] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0043, 0.0022, 0.0094, 0.9434, 0.0073, 0.0238],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 7, batch: 225/228] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0074, 0.0180, 0.9353, 0.0072, 0.0163, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.029

[Epoch: 8, batch: 45/228] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9335, 0.0038, 0.0048, 0.0380, 0.0089, 0.0032, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 8, batch: 90/228] total loss per batch: 0.495
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2264e-01, 2.5488e-09, 8.2944e-02, 4.5894e-09, 1.2673e-01, 6.6769e-01,
        1.0106e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.823

[Epoch: 8, batch: 135/228] total loss per batch: 0.508
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0147, 0.0107, 0.1709, 0.3642, 0.4032, 0.0181, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 8, batch: 180/228] total loss per batch: 0.508
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0035, 0.0027, 0.0036, 0.9649, 0.0056, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 8, batch: 225/228] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0075, 0.0244, 0.9345, 0.0051, 0.0141, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 9, batch: 45/228] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9639, 0.0028, 0.0030, 0.0194, 0.0050, 0.0021, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 9, batch: 90/228] total loss per batch: 0.489
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3243e-01, 1.3572e-09, 1.0901e-01, 4.6522e-09, 3.0758e-01, 4.5097e-01,
        4.9615e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.818

[Epoch: 9, batch: 135/228] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0197, 0.0111, 0.1783, 0.5828, 0.1823, 0.0119, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 9, batch: 180/228] total loss per batch: 0.505
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0086, 0.0039, 0.0029, 0.0048, 0.9682, 0.0039, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 9, batch: 225/228] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0102, 0.0107, 0.0177, 0.9155, 0.0064, 0.0253, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.028

[Epoch: 10, batch: 45/228] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9607, 0.0028, 0.0029, 0.0171, 0.0063, 0.0035, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 10, batch: 90/228] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.4789e-02, 3.8596e-09, 1.0890e-01, 3.8932e-09, 1.0866e-01, 6.8764e-01,
        8.6105e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.814

[Epoch: 10, batch: 135/228] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0127, 0.0173, 0.1494, 0.4193, 0.3675, 0.0169, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 10, batch: 180/228] total loss per batch: 0.504
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0045, 0.0021, 0.0054, 0.9639, 0.0036, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 10, batch: 225/228] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0062, 0.0104, 0.0190, 0.9351, 0.0087, 0.0133, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.022

[Epoch: 11, batch: 45/228] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9564, 0.0025, 0.0026, 0.0186, 0.0083, 0.0043, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 11, batch: 90/228] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1928e-01, 5.6763e-09, 1.3691e-01, 2.0928e-08, 2.3627e-01, 5.0753e-01,
        9.6009e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.850

[Epoch: 11, batch: 135/228] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0247, 0.0127, 0.2321, 0.4140, 0.2799, 0.0197, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.011

[Epoch: 11, batch: 180/228] total loss per batch: 0.503
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0060, 0.0030, 0.0042, 0.9646, 0.0045, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 11, batch: 225/228] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0066, 0.0087, 0.0221, 0.9389, 0.0035, 0.0069, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.028

[Epoch: 12, batch: 45/228] total loss per batch: 0.508
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9518, 0.0027, 0.0032, 0.0262, 0.0059, 0.0033, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 12, batch: 90/228] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0426e-01, 6.5273e-09, 1.1265e-01, 1.4310e-08, 1.7341e-01, 6.0968e-01,
        9.5642e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.815

[Epoch: 12, batch: 135/228] total loss per batch: 0.500
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0187, 0.0125, 0.1080, 0.5907, 0.2205, 0.0194, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 12, batch: 180/228] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.0027, 0.0056, 0.9663, 0.0054, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 12, batch: 225/228] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0041, 0.0089, 0.0177, 0.9443, 0.0080, 0.0087, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.028

[Epoch: 13, batch: 45/228] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9416, 0.0035, 0.0060, 0.0225, 0.0105, 0.0056, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 13, batch: 90/228] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1356e-01, 7.7666e-09, 1.0389e-01, 1.6181e-08, 2.4595e-01, 5.3660e-01,
        6.0814e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.837

[Epoch: 13, batch: 135/228] total loss per batch: 0.500
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0132, 0.0121, 0.1990, 0.3140, 0.4377, 0.0118, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 13, batch: 180/228] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0044, 0.0037, 0.0034, 0.9708, 0.0052, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 13, batch: 225/228] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0084, 0.0170, 0.9412, 0.0051, 0.0112, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.025

[Epoch: 14, batch: 45/228] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9603, 0.0041, 0.0047, 0.0131, 0.0057, 0.0046, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 14, batch: 90/228] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0155e-01, 1.5068e-08, 9.9917e-02, 1.4424e-08, 1.1928e-01, 6.7925e-01,
        5.5428e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.772

[Epoch: 14, batch: 135/228] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0193, 0.0158, 0.1445, 0.6981, 0.0928, 0.0130, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.007

[Epoch: 14, batch: 180/228] total loss per batch: 0.501
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0065, 0.0033, 0.0039, 0.9659, 0.0059, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 14, batch: 225/228] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0075, 0.0103, 0.0136, 0.9428, 0.0072, 0.0119, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.019

[Epoch: 15, batch: 45/228] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9247, 0.0031, 0.0052, 0.0448, 0.0069, 0.0058, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.010

[Epoch: 15, batch: 90/228] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1819e-01, 1.8474e-08, 1.0866e-01, 1.7693e-08, 2.8064e-01, 4.9251e-01,
        5.2837e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.828

[Epoch: 15, batch: 135/228] total loss per batch: 0.499
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0113, 0.0089, 0.1438, 0.1841, 0.6149, 0.0127, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.007

[Epoch: 15, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0043, 0.0039, 0.0044, 0.9677, 0.0049, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 15, batch: 225/228] total loss per batch: 0.503
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0108, 0.0177, 0.9391, 0.0055, 0.0075, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.040

[Epoch: 16, batch: 45/228] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9678, 0.0042, 0.0025, 0.0105, 0.0034, 0.0049, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 16, batch: 90/228] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0838e-01, 4.6052e-08, 1.1818e-01, 5.8443e-08, 1.4142e-01, 6.3202e-01,
        1.0072e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.836

[Epoch: 16, batch: 135/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.0100, 0.1624, 0.7095, 0.0802, 0.0132, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.005

[Epoch: 16, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0066, 0.0031, 0.0047, 0.9673, 0.0044, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 16, batch: 225/228] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0081, 0.0083, 0.0206, 0.9305, 0.0073, 0.0159, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 17, batch: 45/228] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9602, 0.0028, 0.0046, 0.0129, 0.0061, 0.0043, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 17, batch: 90/228] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2648e-01, 1.1698e-08, 1.1709e-01, 1.3225e-08, 2.1976e-01, 5.3668e-01,
        8.4399e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.845

[Epoch: 17, batch: 135/228] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0183, 0.0093, 0.2262, 0.4090, 0.3092, 0.0156, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.009

[Epoch: 17, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0033, 0.0027, 0.0049, 0.9700, 0.0048, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 17, batch: 225/228] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0093, 0.0143, 0.9456, 0.0082, 0.0074, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.036

[Epoch: 18, batch: 45/228] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9539, 0.0050, 0.0032, 0.0155, 0.0081, 0.0060, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 18, batch: 90/228] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2685e-01, 2.2697e-08, 8.6566e-02, 8.5833e-08, 1.5597e-01, 6.3062e-01,
        8.2213e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.836

[Epoch: 18, batch: 135/228] total loss per batch: 0.500
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0170, 0.0177, 0.1311, 0.4278, 0.3723, 0.0211, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 18, batch: 180/228] total loss per batch: 0.501
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0056, 0.0061, 0.0061, 0.9629, 0.0042, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 18, batch: 225/228] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0067, 0.0092, 0.0230, 0.9054, 0.0186, 0.0215, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.031

[Epoch: 19, batch: 45/228] total loss per batch: 0.516
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9643, 0.0055, 0.0035, 0.0117, 0.0070, 0.0031, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 19, batch: 90/228] total loss per batch: 0.513
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.2625e-02, 4.1012e-09, 1.2740e-01, 2.9640e-08, 2.8410e-01, 4.9588e-01,
        6.1178e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.852

[Epoch: 19, batch: 135/228] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0185, 0.0104, 0.1131, 0.4813, 0.3477, 0.0170, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 19, batch: 180/228] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0035, 0.0033, 0.0083, 0.9667, 0.0022, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 19, batch: 225/228] total loss per batch: 0.567
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0074, 0.0090, 0.9501, 0.0080, 0.0107, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.029

[Epoch: 20, batch: 45/228] total loss per batch: 0.577
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.8727, 0.0028, 0.0023, 0.1038, 0.0122, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.014

[Epoch: 20, batch: 90/228] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.0464e-02, 5.1743e-09, 6.8465e-02, 1.2469e-08, 8.2796e-02, 7.6828e-01,
        4.7012e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.662

[Epoch: 20, batch: 135/228] total loss per batch: 0.565
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.0235, 0.0997, 0.5827, 0.1924, 0.0335, 0.0545],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 20, batch: 180/228] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0020, 0.0025, 0.0018, 0.0034, 0.9856, 0.0019, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.011

[Epoch: 20, batch: 225/228] total loss per batch: 0.566
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0030, 0.0046, 0.0096, 0.9632, 0.0040, 0.0115, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.022

[Epoch: 21, batch: 45/228] total loss per batch: 0.566
Policy (actual, predicted): 0 4
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.3198, 0.0038, 0.0087, 0.0325, 0.6125, 0.0060, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 21, batch: 90/228] total loss per batch: 0.531
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.8052e-01, 2.4504e-08, 1.3762e-01, 1.2282e-07, 2.8946e-01, 3.9241e-01,
        1.2216e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.813

[Epoch: 21, batch: 135/228] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0159, 0.0174, 0.2697, 0.3616, 0.2679, 0.0419, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 21, batch: 180/228] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0087, 0.0024, 0.0038, 0.9622, 0.0032, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 21, batch: 225/228] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0077, 0.0148, 0.0154, 0.9376, 0.0066, 0.0114, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.024

[Epoch: 22, batch: 45/228] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9009, 0.0114, 0.0063, 0.0217, 0.0053, 0.0125, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 22, batch: 90/228] total loss per batch: 0.495
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3211e-01, 2.1479e-07, 1.4940e-01, 5.5532e-07, 2.0653e-01, 5.1196e-01,
        2.6271e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.707

[Epoch: 22, batch: 135/228] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0236, 0.0149, 0.0825, 0.4985, 0.3494, 0.0206, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 22, batch: 180/228] total loss per batch: 0.506
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.0058, 0.0043, 0.0049, 0.9648, 0.0048, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 22, batch: 225/228] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0125, 0.0125, 0.0199, 0.9264, 0.0096, 0.0117, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.033

[Epoch: 23, batch: 45/228] total loss per batch: 0.508
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9209, 0.0124, 0.0046, 0.0223, 0.0035, 0.0162, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 23, batch: 90/228] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.6703e-01, 6.7265e-08, 1.6256e-01, 3.6307e-07, 2.4031e-01, 4.3010e-01,
        1.3037e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.928

[Epoch: 23, batch: 135/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0174, 0.0122, 0.2743, 0.4544, 0.2065, 0.0204, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.008

[Epoch: 23, batch: 180/228] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0059, 0.0046, 0.0051, 0.9645, 0.0062, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 23, batch: 225/228] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0102, 0.0103, 0.0139, 0.9402, 0.0094, 0.0095, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.030

[Epoch: 24, batch: 45/228] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9448, 0.0098, 0.0041, 0.0151, 0.0024, 0.0100, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 24, batch: 90/228] total loss per batch: 0.480
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2610e-01, 1.8258e-07, 1.1212e-01, 4.5150e-07, 2.1393e-01, 5.4786e-01,
        4.0619e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.911

[Epoch: 24, batch: 135/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0194, 0.0119, 0.1090, 0.4404, 0.3893, 0.0192, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 24, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0057, 0.0044, 0.0044, 0.9681, 0.0051, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 24, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0101, 0.0119, 0.0164, 0.9354, 0.0100, 0.0098, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.035

[Epoch: 25, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9412, 0.0102, 0.0043, 0.0158, 0.0027, 0.0112, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 25, batch: 90/228] total loss per batch: 0.478
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1980e-01, 9.9314e-08, 1.1702e-01, 3.6696e-07, 1.9615e-01, 5.6703e-01,
        7.3057e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.898

[Epoch: 25, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0183, 0.0104, 0.1825, 0.5574, 0.2034, 0.0170, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 25, batch: 180/228] total loss per batch: 0.492
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0068, 0.0055, 0.0060, 0.9623, 0.0063, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 25, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0090, 0.0111, 0.0161, 0.9361, 0.0105, 0.0093, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.032

[Epoch: 26, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9467, 0.0101, 0.0043, 0.0146, 0.0025, 0.0106, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 26, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2499e-01, 1.3962e-07, 1.1183e-01, 3.5086e-07, 1.9288e-01, 5.7030e-01,
        7.6196e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.906

[Epoch: 26, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0181, 0.0105, 0.1580, 0.4405, 0.3438, 0.0171, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.011

[Epoch: 26, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0062, 0.0040, 0.0038, 0.9701, 0.0047, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 26, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0090, 0.0127, 0.0157, 0.9368, 0.0107, 0.0090, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.034

[Epoch: 27, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9470, 0.0096, 0.0045, 0.0134, 0.0025, 0.0099, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 27, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0571e-01, 7.0881e-08, 1.0652e-01, 2.9959e-07, 1.9327e-01, 5.9450e-01,
        1.0242e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.882

[Epoch: 27, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0168, 0.0095, 0.2028, 0.4956, 0.2489, 0.0146, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 27, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0063, 0.0057, 0.0063, 0.9638, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 27, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0080, 0.0128, 0.0166, 0.9362, 0.0105, 0.0093, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.029

[Epoch: 28, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9455, 0.0095, 0.0047, 0.0170, 0.0028, 0.0096, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 28, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1495e-01, 7.7216e-08, 1.0818e-01, 2.4137e-07, 1.9105e-01, 5.8582e-01,
        1.1164e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.871

[Epoch: 28, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0187, 0.0100, 0.1368, 0.4576, 0.3498, 0.0150, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 28, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0069, 0.0044, 0.0040, 0.9685, 0.0048, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 28, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0073, 0.0107, 0.0155, 0.9386, 0.0104, 0.0114, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.026

[Epoch: 29, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9504, 0.0097, 0.0039, 0.0113, 0.0025, 0.0102, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 29, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1284e-01, 7.0992e-08, 1.0411e-01, 2.3937e-07, 1.8522e-01, 5.9782e-01,
        9.9594e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.884

[Epoch: 29, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0139, 0.0091, 0.2044, 0.5132, 0.2361, 0.0118, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.013

[Epoch: 29, batch: 180/228] total loss per batch: 0.492
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0060, 0.0056, 0.0051, 0.9667, 0.0053, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 29, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0067, 0.0106, 0.0123, 0.9463, 0.0099, 0.0084, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.027

[Epoch: 30, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9457, 0.0085, 0.0051, 0.0182, 0.0027, 0.0096, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 30, batch: 90/228] total loss per batch: 0.478
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1056e-01, 1.0843e-07, 1.2293e-01, 2.9811e-07, 2.2551e-01, 5.4100e-01,
        2.6407e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 30, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0197, 0.0088, 0.1348, 0.4039, 0.4075, 0.0154, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 30, batch: 180/228] total loss per batch: 0.492
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0062, 0.0056, 0.0042, 0.9690, 0.0043, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 30, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0074, 0.0113, 0.0142, 0.9422, 0.0090, 0.0098, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.025

[Epoch: 31, batch: 45/228] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9512, 0.0096, 0.0041, 0.0137, 0.0028, 0.0091, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 31, batch: 90/228] total loss per batch: 0.479
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0135e-01, 8.9742e-08, 9.8982e-02, 2.6067e-07, 1.7515e-01, 6.2451e-01,
        1.1542e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.853

[Epoch: 31, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.0107, 0.2347, 0.5669, 0.1454, 0.0122, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 31, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0030, 0.0050, 0.0034, 0.0033, 0.9752, 0.0046, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 31, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0079, 0.0109, 0.0129, 0.9392, 0.0121, 0.0105, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.028

[Epoch: 32, batch: 45/228] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9537, 0.0073, 0.0037, 0.0159, 0.0026, 0.0080, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 32, batch: 90/228] total loss per batch: 0.479
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4514e-01, 1.8531e-07, 1.4088e-01, 3.1111e-07, 2.1272e-01, 5.0127e-01,
        5.2269e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 32, batch: 135/228] total loss per batch: 0.492
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0156, 0.0115, 0.1394, 0.3311, 0.4756, 0.0152, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.015

[Epoch: 32, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.0052, 0.0070, 0.0043, 0.9653, 0.0065, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 32, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0090, 0.0104, 0.0130, 0.9445, 0.0093, 0.0085, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 33, batch: 45/228] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9351, 0.0097, 0.0054, 0.0198, 0.0044, 0.0113, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 33, batch: 90/228] total loss per batch: 0.480
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.2935e-02, 1.0829e-07, 9.1048e-02, 2.2872e-07, 1.6643e-01, 6.4958e-01,
        1.1968e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.831

[Epoch: 33, batch: 135/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0150, 0.0106, 0.1469, 0.6624, 0.1374, 0.0134, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.013

[Epoch: 33, batch: 180/228] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0050, 0.0069, 0.0043, 0.9659, 0.0052, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 33, batch: 225/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0065, 0.0121, 0.0125, 0.9425, 0.0092, 0.0102, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.022

[Epoch: 34, batch: 45/228] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9527, 0.0076, 0.0050, 0.0138, 0.0029, 0.0087, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 34, batch: 90/228] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2377e-01, 2.8483e-07, 1.1215e-01, 9.6372e-07, 2.7054e-01, 4.9355e-01,
        7.7833e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.897

[Epoch: 34, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0145, 0.0107, 0.2185, 0.3429, 0.3893, 0.0114, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 34, batch: 180/228] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0056, 0.0060, 0.0051, 0.9684, 0.0057, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 34, batch: 225/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0072, 0.0116, 0.0111, 0.9351, 0.0098, 0.0173, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.028

[Epoch: 35, batch: 45/228] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9429, 0.0121, 0.0050, 0.0170, 0.0041, 0.0097, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.025

[Epoch: 35, batch: 90/228] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1323e-01, 1.5395e-07, 1.1729e-01, 2.1319e-07, 1.5704e-01, 6.1244e-01,
        9.1571e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.828

[Epoch: 35, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0141, 0.0097, 0.1552, 0.5996, 0.2002, 0.0084, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 35, batch: 180/228] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0036, 0.0049, 0.0035, 0.9742, 0.0044, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 35, batch: 225/228] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0108, 0.0171, 0.9427, 0.0112, 0.0078, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 36, batch: 45/228] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9496, 0.0113, 0.0062, 0.0111, 0.0028, 0.0093, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 36, batch: 90/228] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1464e-01, 1.9812e-07, 9.5267e-02, 3.8364e-07, 2.1708e-01, 5.7301e-01,
        7.5632e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.830

[Epoch: 36, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0164, 0.0143, 0.1828, 0.4089, 0.3522, 0.0124, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 36, batch: 180/228] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0111, 0.0058, 0.0065, 0.9556, 0.0064, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 36, batch: 225/228] total loss per batch: 0.500
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0059, 0.0119, 0.0175, 0.9328, 0.0093, 0.0116, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 37, batch: 45/228] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9451, 0.0075, 0.0043, 0.0192, 0.0043, 0.0100, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.020

[Epoch: 37, batch: 90/228] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1965e-01, 1.0881e-07, 1.3165e-01, 2.6484e-07, 1.6746e-01, 5.8124e-01,
        6.3441e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.836

[Epoch: 37, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0164, 0.0089, 0.1241, 0.6237, 0.1966, 0.0151, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.011

[Epoch: 37, batch: 180/228] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0082, 0.0062, 0.0153, 0.9617, 0.0022, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.023

[Epoch: 37, batch: 225/228] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0060, 0.0140, 0.9516, 0.0070, 0.0070, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.009

[Epoch: 38, batch: 45/228] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9199, 0.0117, 0.0087, 0.0298, 0.0070, 0.0109, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.030

[Epoch: 38, batch: 90/228] total loss per batch: 0.501
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([2.3122e-01, 3.8573e-08, 7.3740e-02, 1.3385e-07, 2.0243e-01, 4.9261e-01,
        7.5911e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.892

[Epoch: 38, batch: 135/228] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0199, 0.0073, 0.3515, 0.3896, 0.1993, 0.0067, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 38, batch: 180/228] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0016, 0.0029, 0.0018, 0.0129, 0.9658, 0.0039, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 38, batch: 225/228] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0037, 0.0220, 0.0215, 0.9371, 0.0041, 0.0081, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.026

[Epoch: 39, batch: 45/228] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9266, 0.0046, 0.0070, 0.0351, 0.0042, 0.0079, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.028

[Epoch: 39, batch: 90/228] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([3.3701e-02, 6.5795e-09, 8.2994e-02, 2.0611e-07, 1.0530e-01, 7.7801e-01,
        1.6914e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.441

[Epoch: 39, batch: 135/228] total loss per batch: 0.530
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0064, 0.0080, 0.0776, 0.2154, 0.6681, 0.0123, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 39, batch: 180/228] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0062, 0.0019, 0.0130, 0.9616, 0.0041, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 39, batch: 225/228] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0036, 0.0183, 0.0218, 0.9294, 0.0153, 0.0046, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 40, batch: 45/228] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9288, 0.0057, 0.0070, 0.0225, 0.0058, 0.0154, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 40, batch: 90/228] total loss per batch: 0.510
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2254e-01, 1.0741e-07, 8.6697e-02, 2.2868e-07, 3.5577e-01, 4.3500e-01,
        1.1390e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.949

[Epoch: 40, batch: 135/228] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0211, 0.0102, 0.1041, 0.6749, 0.1654, 0.0088, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.003

[Epoch: 40, batch: 180/228] total loss per batch: 0.508
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0080, 0.0044, 0.0206, 0.9481, 0.0049, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 40, batch: 225/228] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0027, 0.0117, 0.0130, 0.9468, 0.0093, 0.0105, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 41, batch: 45/228] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9464, 0.0054, 0.0069, 0.0179, 0.0033, 0.0113, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 41, batch: 90/228] total loss per batch: 0.488
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3177e-01, 1.4119e-07, 1.0010e-01, 7.0528e-07, 2.8335e-01, 4.8478e-01,
        5.6976e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.928

[Epoch: 41, batch: 135/228] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0178, 0.0063, 0.1460, 0.5632, 0.2419, 0.0143, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.008

[Epoch: 41, batch: 180/228] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0024, 0.0031, 0.0024, 0.0074, 0.9762, 0.0037, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 41, batch: 225/228] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0033, 0.0285, 0.0144, 0.9251, 0.0117, 0.0084, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 42, batch: 45/228] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9426, 0.0067, 0.0039, 0.0185, 0.0040, 0.0147, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 42, batch: 90/228] total loss per batch: 0.480
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3209e-01, 1.5472e-07, 9.5099e-02, 5.7627e-07, 2.6608e-01, 5.0672e-01,
        5.7318e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.941

[Epoch: 42, batch: 135/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0190, 0.0085, 0.2119, 0.4845, 0.2445, 0.0177, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.015

[Epoch: 42, batch: 180/228] total loss per batch: 0.493
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0023, 0.0020, 0.0085, 0.9736, 0.0038, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 42, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0033, 0.0145, 0.0138, 0.9448, 0.0089, 0.0078, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 43, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9442, 0.0058, 0.0047, 0.0193, 0.0038, 0.0115, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 43, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1830e-01, 8.4595e-08, 9.4773e-02, 3.8067e-07, 2.3330e-01, 5.5363e-01,
        5.6732e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.927

[Epoch: 43, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0191, 0.0079, 0.1510, 0.4585, 0.3333, 0.0179, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.011

[Epoch: 43, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.0029, 0.0022, 0.0085, 0.9739, 0.0035, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 43, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0037, 0.0159, 0.0125, 0.9408, 0.0105, 0.0087, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 44, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9514, 0.0056, 0.0040, 0.0138, 0.0038, 0.0111, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 44, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1437e-01, 6.1584e-08, 9.5717e-02, 3.4487e-07, 2.1899e-01, 5.7092e-01,
        5.4751e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.910

[Epoch: 44, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.0087, 0.1893, 0.4995, 0.2581, 0.0162, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 44, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0035, 0.0031, 0.0085, 0.9702, 0.0044, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 44, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0033, 0.0124, 0.0128, 0.9458, 0.0095, 0.0089, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 45, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9506, 0.0051, 0.0044, 0.0161, 0.0038, 0.0100, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 45, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0321e-01, 6.0799e-08, 1.0016e-01, 3.3908e-07, 2.0785e-01, 5.8878e-01,
        4.2174e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.894

[Epoch: 45, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0146, 0.0078, 0.1629, 0.5025, 0.2868, 0.0144, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 45, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0036, 0.0030, 0.0075, 0.9712, 0.0042, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 45, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0035, 0.0126, 0.0151, 0.9408, 0.0113, 0.0093, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 46, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9487, 0.0053, 0.0047, 0.0157, 0.0041, 0.0106, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 46, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2042e-01, 5.9118e-08, 1.0609e-01, 3.3606e-07, 1.9793e-01, 5.7555e-01,
        4.8090e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.895

[Epoch: 46, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0176, 0.0096, 0.1727, 0.4483, 0.3248, 0.0139, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 46, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0040, 0.0033, 0.0086, 0.9689, 0.0050, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 46, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0032, 0.0122, 0.0137, 0.9453, 0.0095, 0.0091, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 47, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9517, 0.0050, 0.0042, 0.0156, 0.0042, 0.0101, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 47, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1119e-01, 5.0215e-08, 1.0718e-01, 3.4183e-07, 1.9481e-01, 5.8682e-01,
        3.1155e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.877

[Epoch: 47, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0126, 0.0079, 0.1836, 0.5372, 0.2350, 0.0132, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 47, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0037, 0.0033, 0.0063, 0.9731, 0.0036, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 47, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0041, 0.0121, 0.0166, 0.9408, 0.0107, 0.0090, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 48, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9532, 0.0052, 0.0051, 0.0125, 0.0043, 0.0096, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 48, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1439e-01, 4.9829e-08, 9.8209e-02, 3.3468e-07, 1.9562e-01, 5.9178e-01,
        3.8151e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.871

[Epoch: 48, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0170, 0.0093, 0.1604, 0.4124, 0.3757, 0.0123, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 48, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0056, 0.0046, 0.0081, 0.9668, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 48, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0039, 0.0113, 0.0129, 0.9439, 0.0090, 0.0117, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 49, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9463, 0.0053, 0.0039, 0.0191, 0.0043, 0.0106, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 49, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1416e-01, 6.3611e-08, 1.2844e-01, 3.4808e-07, 1.9856e-01, 5.5885e-01,
        4.7678e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.867

[Epoch: 49, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0146, 0.0094, 0.1970, 0.5303, 0.2239, 0.0131, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 49, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0032, 0.0032, 0.0068, 0.9731, 0.0040, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 49, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0039, 0.0118, 0.0124, 0.9483, 0.0104, 0.0076, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 50, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9539, 0.0053, 0.0049, 0.0125, 0.0042, 0.0098, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 50, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1348e-01, 6.7016e-08, 9.5489e-02, 3.4775e-07, 1.9614e-01, 5.9490e-01,
        4.0671e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.866

[Epoch: 50, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0121, 0.0095, 0.1437, 0.4653, 0.3499, 0.0096, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 50, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0061, 0.0058, 0.0078, 0.9621, 0.0049, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 50, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0106, 0.0199, 0.9329, 0.0107, 0.0128, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 51, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9511, 0.0057, 0.0042, 0.0174, 0.0043, 0.0091, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 51, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.8376e-02, 8.7425e-08, 1.0397e-01, 3.3560e-07, 1.8723e-01, 6.1042e-01,
        4.7111e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.867

[Epoch: 51, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0134, 0.0107, 0.2377, 0.4708, 0.2400, 0.0152, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 51, batch: 180/228] total loss per batch: 0.492
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0046, 0.0037, 0.0066, 0.9705, 0.0050, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 51, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0037, 0.0105, 0.0089, 0.9535, 0.0093, 0.0082, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 52, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9508, 0.0057, 0.0052, 0.0144, 0.0043, 0.0101, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 52, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3318e-01, 9.1816e-08, 1.3500e-01, 4.3636e-07, 1.9663e-01, 5.3519e-01,
        4.3476e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 52, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.0101, 0.1301, 0.4990, 0.3218, 0.0118, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 52, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0045, 0.0043, 0.0053, 0.9728, 0.0037, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 52, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0038, 0.0085, 0.0135, 0.9523, 0.0078, 0.0086, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 53, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9470, 0.0051, 0.0043, 0.0187, 0.0044, 0.0102, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 53, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.9702e-02, 9.4757e-08, 9.5382e-02, 3.5531e-07, 1.8892e-01, 6.2599e-01,
        4.1857e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.841

[Epoch: 53, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0142, 0.0105, 0.1853, 0.4980, 0.2676, 0.0128, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 53, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0050, 0.0047, 0.0059, 0.9687, 0.0055, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 53, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0044, 0.0122, 0.0137, 0.9416, 0.0115, 0.0097, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 54, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9497, 0.0059, 0.0052, 0.0147, 0.0045, 0.0105, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 54, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4347e-01, 8.6949e-08, 1.2237e-01, 4.9928e-07, 2.0483e-01, 5.2932e-01,
        8.4250e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.858

[Epoch: 54, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0100, 0.1708, 0.4657, 0.3137, 0.0115, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 54, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0051, 0.0048, 0.0055, 0.9712, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 54, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0041, 0.0086, 0.0132, 0.9524, 0.0082, 0.0081, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 55, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9523, 0.0049, 0.0045, 0.0140, 0.0039, 0.0102, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 55, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.7700e-02, 1.0590e-07, 9.6130e-02, 3.5008e-07, 1.7840e-01, 6.3776e-01,
        3.5816e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.837

[Epoch: 55, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.0086, 0.1509, 0.5176, 0.2862, 0.0121, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 55, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0047, 0.0044, 0.0067, 0.9689, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 55, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0044, 0.0110, 0.0167, 0.9409, 0.0108, 0.0106, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 56, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9474, 0.0060, 0.0055, 0.0188, 0.0045, 0.0100, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 56, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1534e-01, 1.1455e-07, 1.1300e-01, 4.5962e-07, 2.1093e-01, 5.6073e-01,
        7.2897e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.852

[Epoch: 56, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0145, 0.0124, 0.2013, 0.4414, 0.3065, 0.0127, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 56, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0042, 0.0050, 0.0052, 0.9711, 0.0046, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 56, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0046, 0.0080, 0.0130, 0.9468, 0.0099, 0.0111, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 57, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9518, 0.0052, 0.0046, 0.0102, 0.0059, 0.0118, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 57, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0622e-01, 1.2118e-07, 1.2049e-01, 5.5530e-07, 1.9258e-01, 5.8072e-01,
        8.0687e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.838

[Epoch: 57, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0161, 0.0117, 0.1467, 0.5518, 0.2505, 0.0106, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 57, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0046, 0.0045, 0.0066, 0.9699, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 57, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0125, 0.0211, 0.9365, 0.0114, 0.0085, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 58, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9411, 0.0061, 0.0058, 0.0238, 0.0042, 0.0092, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 58, batch: 90/228] total loss per batch: 0.478
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0674e-01, 1.5306e-07, 9.4092e-02, 4.6980e-07, 2.1094e-01, 5.8823e-01,
        2.1366e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.868

[Epoch: 58, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0124, 0.0095, 0.1862, 0.3883, 0.3823, 0.0110, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 58, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0056, 0.0059, 0.0070, 0.9657, 0.0047, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 58, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0044, 0.0068, 0.0095, 0.9586, 0.0058, 0.0097, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.021

[Epoch: 59, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9516, 0.0046, 0.0042, 0.0167, 0.0053, 0.0081, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 59, batch: 90/228] total loss per batch: 0.478
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2801e-01, 1.1865e-07, 1.0411e-01, 2.6895e-07, 1.7299e-01, 5.9489e-01,
        1.0895e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.823

[Epoch: 59, batch: 135/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0177, 0.0072, 0.1690, 0.5796, 0.2026, 0.0125, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 59, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.0044, 0.0063, 0.9691, 0.0033, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 59, batch: 225/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0086, 0.0141, 0.0317, 0.9002, 0.0267, 0.0129, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.023

[Epoch: 60, batch: 45/228] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9470, 0.0069, 0.0056, 0.0144, 0.0040, 0.0125, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 60, batch: 90/228] total loss per batch: 0.479
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1367e-01, 2.5484e-07, 1.3083e-01, 3.4447e-07, 2.0630e-01, 5.4920e-01,
        3.1986e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.867

[Epoch: 60, batch: 135/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.0085, 0.1628, 0.4087, 0.3829, 0.0116, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 60, batch: 180/228] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0047, 0.0054, 0.0072, 0.9648, 0.0063, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 60, batch: 225/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0122, 0.0099, 0.9517, 0.0044, 0.0090, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 61, batch: 45/228] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9330, 0.0070, 0.0053, 0.0280, 0.0048, 0.0128, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 61, batch: 90/228] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([4.1616e-02, 3.8349e-08, 7.1104e-02, 1.4162e-07, 1.7007e-01, 7.1721e-01,
        2.0449e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.915

[Epoch: 61, batch: 135/228] total loss per batch: 0.503
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.0068, 0.1571, 0.4742, 0.3167, 0.0170, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 61, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0039, 0.0053, 0.0216, 0.9439, 0.0144, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 61, batch: 225/228] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0031, 0.0063, 0.0074, 0.9665, 0.0051, 0.0074, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 62, batch: 45/228] total loss per batch: 0.505
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9484, 0.0093, 0.0035, 0.0154, 0.0054, 0.0085, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 62, batch: 90/228] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4843e-01, 3.4464e-07, 1.8041e-01, 1.3109e-07, 2.3774e-01, 4.3342e-01,
        9.7412e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.845

[Epoch: 62, batch: 135/228] total loss per batch: 0.507
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0110, 0.0122, 0.2292, 0.3527, 0.3720, 0.0078, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 62, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0048, 0.0019, 0.0037, 0.9734, 0.0063, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 62, batch: 225/228] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0077, 0.0117, 0.0109, 0.9374, 0.0065, 0.0199, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.021

[Epoch: 63, batch: 45/228] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9529, 0.0051, 0.0026, 0.0183, 0.0046, 0.0111, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 63, batch: 90/228] total loss per batch: 0.488
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([6.4054e-02, 1.0118e-07, 3.9612e-02, 1.9291e-07, 1.1675e-01, 7.7958e-01,
        1.7480e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.761

[Epoch: 63, batch: 135/228] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0202, 0.0110, 0.1388, 0.5780, 0.2275, 0.0105, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.035

[Epoch: 63, batch: 180/228] total loss per batch: 0.504
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0042, 0.0028, 0.0041, 0.9724, 0.0058, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 63, batch: 225/228] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0128, 0.0099, 0.9514, 0.0044, 0.0133, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 64, batch: 45/228] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9382, 0.0070, 0.0038, 0.0239, 0.0057, 0.0128, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 64, batch: 90/228] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2937e-01, 2.2844e-07, 1.1050e-01, 4.7404e-07, 3.2331e-01, 4.3682e-01,
        2.8030e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.889

[Epoch: 64, batch: 135/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0119, 0.0078, 0.1827, 0.5117, 0.2544, 0.0138, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 64, batch: 180/228] total loss per batch: 0.493
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0049, 0.0037, 0.0061, 0.9628, 0.0064, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 64, batch: 225/228] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0067, 0.0090, 0.0116, 0.9470, 0.0059, 0.0099, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.030

[Epoch: 65, batch: 45/228] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9501, 0.0084, 0.0039, 0.0164, 0.0048, 0.0107, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 65, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2523e-01, 4.4156e-07, 1.0828e-01, 3.7766e-07, 2.9827e-01, 4.6821e-01,
        1.2537e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.894

[Epoch: 65, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.0087, 0.1886, 0.4808, 0.2760, 0.0125, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 65, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0063, 0.0038, 0.0060, 0.9605, 0.0080, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 65, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0061, 0.0106, 0.0099, 0.9573, 0.0044, 0.0075, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.021

[Epoch: 66, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9550, 0.0060, 0.0043, 0.0124, 0.0038, 0.0109, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 66, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2701e-01, 2.2792e-07, 1.1886e-01, 2.0900e-07, 2.5054e-01, 5.0359e-01,
        9.7366e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.888

[Epoch: 66, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0116, 0.0076, 0.1687, 0.4842, 0.3015, 0.0115, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 66, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0035, 0.0032, 0.0040, 0.9750, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 66, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0108, 0.0106, 0.9568, 0.0046, 0.0074, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.024

[Epoch: 67, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9491, 0.0070, 0.0047, 0.0152, 0.0045, 0.0113, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 67, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1058e-01, 1.6054e-07, 1.0849e-01, 1.6922e-07, 2.3157e-01, 5.4935e-01,
        1.2468e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.873

[Epoch: 67, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0124, 0.0090, 0.1807, 0.4915, 0.2821, 0.0101, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 67, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0037, 0.0050, 0.9699, 0.0059, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 67, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0100, 0.0113, 0.9554, 0.0052, 0.0081, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.024

[Epoch: 68, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9537, 0.0063, 0.0045, 0.0121, 0.0039, 0.0114, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 68, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1967e-01, 1.1706e-07, 1.1864e-01, 1.3153e-07, 1.5772e-01, 6.0397e-01,
        1.2084e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.871

[Epoch: 68, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0129, 0.0088, 0.1659, 0.4796, 0.3096, 0.0107, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 68, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0044, 0.0035, 0.0048, 0.9718, 0.0053, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 68, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0094, 0.0119, 0.9559, 0.0054, 0.0076, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.026

[Epoch: 69, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9479, 0.0070, 0.0051, 0.0164, 0.0044, 0.0108, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 69, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1111e-01, 1.0478e-07, 1.1140e-01, 1.1993e-07, 1.8234e-01, 5.9516e-01,
        1.1670e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.865

[Epoch: 69, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0133, 0.0091, 0.1828, 0.4968, 0.2749, 0.0101, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 69, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0039, 0.0053, 0.9686, 0.0058, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 69, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0110, 0.0127, 0.9516, 0.0059, 0.0087, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.023

[Epoch: 70, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9525, 0.0064, 0.0045, 0.0132, 0.0038, 0.0109, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 70, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1028e-01, 8.4687e-08, 1.1256e-01, 9.6340e-08, 1.8828e-01, 5.8887e-01,
        8.4916e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.868

[Epoch: 70, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0131, 0.0090, 0.1619, 0.4759, 0.3191, 0.0103, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 70, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0049, 0.0040, 0.0050, 0.9708, 0.0050, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 70, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0101, 0.0133, 0.9495, 0.0076, 0.0089, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.023

[Epoch: 71, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9491, 0.0068, 0.0052, 0.0150, 0.0049, 0.0104, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 71, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0588e-01, 8.2448e-08, 1.0975e-01, 9.8335e-08, 1.9211e-01, 5.9226e-01,
        7.6511e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 71, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.0101, 0.1949, 0.4803, 0.2743, 0.0108, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 71, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0048, 0.0039, 0.0047, 0.9724, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 71, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0095, 0.0153, 0.9477, 0.0064, 0.0103, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.025

[Epoch: 72, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9500, 0.0051, 0.0043, 0.0180, 0.0037, 0.0100, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 72, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1641e-01, 5.3757e-08, 1.1973e-01, 7.0704e-08, 1.7656e-01, 5.8729e-01,
        1.0158e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 72, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0123, 0.0094, 0.1535, 0.5029, 0.3014, 0.0102, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 72, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0055, 0.0047, 0.0059, 0.9682, 0.0047, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 72, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0057, 0.0119, 0.0118, 0.9491, 0.0079, 0.0084, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 73, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9560, 0.0068, 0.0046, 0.0111, 0.0048, 0.0092, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 73, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0365e-01, 1.3573e-07, 9.5383e-02, 1.4396e-07, 1.9982e-01, 6.0115e-01,
        7.3535e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 73, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0151, 0.0103, 0.1860, 0.4731, 0.2933, 0.0093, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 73, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0045, 0.0040, 0.0050, 0.9729, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 73, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0120, 0.0163, 0.9401, 0.0080, 0.0130, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 74, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9330, 0.0059, 0.0060, 0.0276, 0.0049, 0.0122, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 74, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3766e-01, 4.4481e-08, 1.3307e-01, 9.9036e-08, 1.8208e-01, 5.4720e-01,
        2.7563e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 74, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0129, 0.0117, 0.1749, 0.4233, 0.3561, 0.0113, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 74, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0043, 0.0047, 0.0039, 0.9720, 0.0047, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 74, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0064, 0.0098, 0.0134, 0.9500, 0.0066, 0.0088, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 75, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9664, 0.0073, 0.0036, 0.0052, 0.0038, 0.0070, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 75, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.1488e-02, 1.4066e-07, 9.9910e-02, 2.5722e-07, 1.8195e-01, 6.2665e-01,
        1.1557e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.847

[Epoch: 75, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0164, 0.0083, 0.1499, 0.5951, 0.2109, 0.0088, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 75, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0028, 0.0043, 0.0039, 0.9767, 0.0044, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 75, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0042, 0.0150, 0.0128, 0.9440, 0.0082, 0.0107, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 76, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9279, 0.0070, 0.0078, 0.0274, 0.0064, 0.0139, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 76, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2367e-01, 5.4858e-08, 1.2553e-01, 8.2729e-08, 1.9044e-01, 5.6036e-01,
        4.8842e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 76, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0174, 0.0167, 0.2662, 0.3638, 0.3106, 0.0108, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 76, batch: 180/228] total loss per batch: 0.527
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0017, 0.0046, 0.0075, 0.9739, 0.0027, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 76, batch: 225/228] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0088, 0.0195, 0.9309, 0.0051, 0.0218, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.021

[Epoch: 77, batch: 45/228] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9353, 0.0027, 0.0031, 0.0398, 0.0049, 0.0094, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.040

[Epoch: 77, batch: 90/228] total loss per batch: 0.513
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4488e-01, 1.9934e-09, 1.3161e-01, 5.5201e-08, 2.0199e-01, 5.2152e-01,
        1.5299e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 77, batch: 135/228] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0239, 0.0065, 0.0514, 0.6539, 0.1612, 0.0149, 0.0883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 77, batch: 180/228] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0084, 0.0048, 0.0038, 0.9693, 0.0052, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 77, batch: 225/228] total loss per batch: 0.515
Policy (actual, predicted): 3 5
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0242, 0.0208, 0.0501, 0.3818, 0.0218, 0.4798, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 78, batch: 45/228] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9080, 0.0048, 0.0037, 0.0509, 0.0151, 0.0108, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.029

[Epoch: 78, batch: 90/228] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1841e-01, 1.1926e-08, 1.0769e-01, 1.9467e-07, 1.8499e-01, 5.8891e-01,
        7.0534e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.861

[Epoch: 78, batch: 135/228] total loss per batch: 0.495
Policy (actual, predicted): 3 4
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0195, 0.0142, 0.1306, 0.3833, 0.4194, 0.0219, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 78, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0019, 0.0034, 0.0040, 0.0044, 0.9802, 0.0041, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 78, batch: 225/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0136, 0.0081, 0.0300, 0.9328, 0.0031, 0.0054, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 79, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9566, 0.0056, 0.0051, 0.0070, 0.0098, 0.0080, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.029

[Epoch: 79, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1278e-01, 1.2048e-08, 1.1339e-01, 1.6474e-07, 1.8378e-01, 5.9005e-01,
        5.6801e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.858

[Epoch: 79, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0251, 0.0162, 0.1545, 0.5528, 0.2272, 0.0146, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 79, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.0033, 0.0043, 0.0047, 0.9773, 0.0050, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 79, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0116, 0.0090, 0.0277, 0.9364, 0.0034, 0.0052, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 80, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9564, 0.0065, 0.0045, 0.0090, 0.0078, 0.0079, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.028

[Epoch: 80, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0096e-01, 1.0452e-08, 1.1073e-01, 1.5384e-07, 1.8667e-01, 6.0164e-01,
        5.4217e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 80, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0199, 0.0134, 0.1879, 0.4350, 0.3177, 0.0154, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 80, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0037, 0.0052, 0.0051, 0.9741, 0.0052, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 80, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0105, 0.0097, 0.0257, 0.9378, 0.0041, 0.0052, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 81, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9550, 0.0062, 0.0047, 0.0089, 0.0081, 0.0084, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.028

[Epoch: 81, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0564e-01, 1.3511e-08, 1.1444e-01, 1.5582e-07, 1.8821e-01, 5.9171e-01,
        3.8318e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 81, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0198, 0.0130, 0.1696, 0.4997, 0.2743, 0.0137, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 81, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0041, 0.0055, 0.0054, 0.9724, 0.0055, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 81, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0100, 0.0101, 0.0219, 0.9414, 0.0042, 0.0055, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 82, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9525, 0.0063, 0.0045, 0.0121, 0.0072, 0.0089, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.026

[Epoch: 82, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0293e-01, 1.0679e-08, 1.1232e-01, 1.2221e-07, 1.8750e-01, 5.9726e-01,
        3.6247e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 82, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0184, 0.0119, 0.1753, 0.4718, 0.3008, 0.0127, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 82, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0042, 0.0055, 0.0053, 0.9718, 0.0054, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 82, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0091, 0.0099, 0.0194, 0.9447, 0.0045, 0.0055, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 83, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9542, 0.0060, 0.0047, 0.0101, 0.0068, 0.0087, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 83, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0979e-01, 1.3137e-08, 1.1781e-01, 1.3198e-07, 1.9004e-01, 5.8235e-01,
        2.9105e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 83, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0184, 0.0121, 0.1728, 0.4802, 0.2942, 0.0129, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 83, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0041, 0.0053, 0.0054, 0.9720, 0.0054, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 83, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0084, 0.0106, 0.0184, 0.9453, 0.0049, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 84, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9489, 0.0066, 0.0046, 0.0144, 0.0068, 0.0096, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 84, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0478e-01, 1.0587e-08, 1.0980e-01, 1.1380e-07, 1.8910e-01, 5.9632e-01,
        3.0245e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 84, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0163, 0.0109, 0.1784, 0.4870, 0.2872, 0.0117, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 84, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.0056, 0.0053, 0.9712, 0.0051, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 84, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0080, 0.0107, 0.0177, 0.9454, 0.0054, 0.0062, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 85, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9531, 0.0061, 0.0044, 0.0115, 0.0063, 0.0088, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 85, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1037e-01, 1.0334e-08, 1.1761e-01, 1.0053e-07, 1.8988e-01, 5.8214e-01,
        2.5620e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.850

[Epoch: 85, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0171, 0.0112, 0.1654, 0.4701, 0.3155, 0.0115, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 85, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0042, 0.0052, 0.0053, 0.9712, 0.0054, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 85, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0075, 0.0110, 0.0176, 0.9459, 0.0054, 0.0063, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 86, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9501, 0.0058, 0.0051, 0.0149, 0.0061, 0.0095, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 86, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0545e-01, 1.2326e-08, 1.0655e-01, 1.1379e-07, 1.9281e-01, 5.9520e-01,
        3.3781e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.866

[Epoch: 86, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0165, 0.0113, 0.1945, 0.5004, 0.2573, 0.0116, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 86, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.0056, 0.0053, 0.9704, 0.0052, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 86, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0070, 0.0104, 0.0162, 0.9468, 0.0059, 0.0070, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 87, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9484, 0.0065, 0.0043, 0.0134, 0.0062, 0.0104, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.029

[Epoch: 87, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0928e-01, 1.1989e-08, 1.1747e-01, 9.6571e-08, 1.7921e-01, 5.9404e-01,
        1.9471e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.833

[Epoch: 87, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0156, 0.0105, 0.1513, 0.4602, 0.3438, 0.0096, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 87, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0042, 0.0049, 0.0049, 0.9714, 0.0056, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 87, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0071, 0.0113, 0.0179, 0.9435, 0.0068, 0.0068, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 88, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9524, 0.0059, 0.0048, 0.0151, 0.0058, 0.0081, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 88, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1201e-01, 2.3678e-08, 1.1408e-01, 1.2642e-07, 1.9158e-01, 5.8233e-01,
        3.5182e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.883

[Epoch: 88, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0107, 0.2066, 0.5117, 0.2318, 0.0135, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 88, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.0044, 0.0046, 0.9737, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 88, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0059, 0.0113, 0.0140, 0.9488, 0.0064, 0.0071, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 89, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9521, 0.0061, 0.0052, 0.0119, 0.0061, 0.0092, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.026

[Epoch: 89, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0900e-01, 2.2937e-08, 1.1598e-01, 2.2898e-07, 1.9637e-01, 5.7866e-01,
        1.6112e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 89, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0134, 0.0115, 0.1526, 0.4044, 0.3952, 0.0125, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 89, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0046, 0.0063, 0.0042, 0.9722, 0.0055, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 89, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0089, 0.0117, 0.0192, 0.9346, 0.0091, 0.0086, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 90, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9363, 0.0077, 0.0052, 0.0218, 0.0090, 0.0093, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.020

[Epoch: 90, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1320e-01, 1.4870e-08, 1.0276e-01, 8.5288e-08, 1.7064e-01, 6.1339e-01,
        5.0394e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.869

[Epoch: 90, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0118, 0.0102, 0.1495, 0.5549, 0.2519, 0.0108, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 90, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0052, 0.0046, 0.0054, 0.9699, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 90, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0064, 0.0107, 0.0162, 0.9458, 0.0070, 0.0072, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 91, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9629, 0.0047, 0.0039, 0.0078, 0.0041, 0.0083, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.031

[Epoch: 91, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3206e-01, 9.6225e-08, 1.2954e-01, 4.6949e-07, 2.0580e-01, 5.3260e-01,
        2.8507e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.867

[Epoch: 91, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0144, 0.0129, 0.1894, 0.4004, 0.3584, 0.0123, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 91, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.0046, 0.0047, 0.0043, 0.9731, 0.0058, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 91, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0063, 0.0110, 0.0133, 0.9443, 0.0086, 0.0084, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 92, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9448, 0.0074, 0.0055, 0.0174, 0.0061, 0.0106, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 92, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.3426e-02, 1.4968e-08, 9.3282e-02, 1.3285e-07, 1.8336e-01, 6.3994e-01,
        4.0523e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.829

[Epoch: 92, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0138, 0.0108, 0.1717, 0.5048, 0.2754, 0.0138, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 92, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0040, 0.0062, 0.0066, 0.9673, 0.0060, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 92, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0067, 0.0105, 0.0167, 0.9466, 0.0067, 0.0062, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 93, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9501, 0.0058, 0.0046, 0.0139, 0.0048, 0.0105, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 93, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0906e-01, 4.1656e-08, 1.2272e-01, 3.2284e-07, 2.0340e-01, 5.6482e-01,
        4.0505e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 93, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.0108, 0.1668, 0.5304, 0.2537, 0.0117, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 93, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0053, 0.0046, 0.0046, 0.9704, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 93, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0059, 0.0099, 0.0149, 0.9468, 0.0085, 0.0071, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 94, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9487, 0.0068, 0.0048, 0.0137, 0.0063, 0.0099, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 94, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1899e-01, 1.4342e-08, 1.0838e-01, 2.8127e-07, 2.0271e-01, 5.6992e-01,
        2.0759e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 94, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0130, 0.0114, 0.1908, 0.4023, 0.3620, 0.0108, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 94, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0061, 0.0064, 0.9656, 0.0064, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 94, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0086, 0.0154, 0.9497, 0.0067, 0.0087, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 95, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9475, 0.0053, 0.0051, 0.0180, 0.0051, 0.0103, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 95, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.4328e-02, 2.1611e-08, 1.1412e-01, 2.5392e-07, 1.8024e-01, 6.1131e-01,
        8.5516e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.845

[Epoch: 95, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0159, 0.0129, 0.1519, 0.6084, 0.1873, 0.0117, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 95, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0051, 0.0041, 0.0043, 0.9736, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 95, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0074, 0.0113, 0.0170, 0.9382, 0.0112, 0.0075, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 96, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9426, 0.0075, 0.0053, 0.0148, 0.0060, 0.0111, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 96, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2654e-01, 7.0452e-08, 1.0767e-01, 3.6982e-07, 1.8701e-01, 5.7878e-01,
        4.5918e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.872

[Epoch: 96, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0124, 0.0109, 0.1591, 0.4158, 0.3796, 0.0135, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 96, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0044, 0.0064, 0.0058, 0.9690, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 96, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0045, 0.0091, 0.0149, 0.9505, 0.0065, 0.0083, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 97, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9571, 0.0063, 0.0055, 0.0086, 0.0057, 0.0092, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 97, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.5503e-02, 3.5240e-08, 1.0325e-01, 4.9960e-07, 2.1296e-01, 5.8828e-01,
        5.9204e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.831

[Epoch: 97, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.0090, 0.1633, 0.5139, 0.2715, 0.0134, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 97, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0047, 0.0050, 0.0047, 0.9710, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 97, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0109, 0.0127, 0.9482, 0.0073, 0.0081, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 98, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9361, 0.0068, 0.0044, 0.0267, 0.0052, 0.0090, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.029

[Epoch: 98, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0930e-01, 2.8689e-08, 1.1722e-01, 2.0495e-07, 1.7381e-01, 5.9966e-01,
        2.1670e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.873

[Epoch: 98, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.0115, 0.1897, 0.4577, 0.3047, 0.0114, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.036

[Epoch: 98, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0056, 0.0045, 0.0053, 0.9706, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 98, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0101, 0.0131, 0.9513, 0.0058, 0.0092, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 99, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9607, 0.0053, 0.0059, 0.0061, 0.0052, 0.0085, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 99, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1579e-01, 2.9352e-08, 9.8960e-02, 2.8323e-07, 1.8567e-01, 5.9958e-01,
        4.0600e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.858

[Epoch: 99, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0145, 0.0059, 0.1277, 0.5672, 0.2592, 0.0144, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 99, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0061, 0.0052, 0.0039, 0.9718, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 99, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0121, 0.0185, 0.9338, 0.0114, 0.0112, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.023

[Epoch: 100, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9418, 0.0071, 0.0060, 0.0172, 0.0060, 0.0120, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 100, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3746e-01, 1.0991e-07, 1.3781e-01, 3.7535e-07, 1.8946e-01, 5.3526e-01,
        7.3368e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.844

[Epoch: 100, batch: 135/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0123, 0.0128, 0.2334, 0.3937, 0.3301, 0.0086, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 100, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0078, 0.0047, 0.0060, 0.9659, 0.0049, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 100, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0105, 0.0168, 0.9471, 0.0084, 0.0064, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 101, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9567, 0.0052, 0.0052, 0.0073, 0.0047, 0.0112, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 101, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.3342e-02, 4.9132e-08, 8.8218e-02, 8.0939e-07, 2.0003e-01, 6.1840e-01,
        5.6711e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.840

[Epoch: 101, batch: 135/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0108, 0.0830, 0.5653, 0.3040, 0.0105, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 101, batch: 180/228] total loss per batch: 0.492
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0058, 0.0036, 0.0062, 0.9699, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 101, batch: 225/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0064, 0.0120, 0.0186, 0.9329, 0.0100, 0.0107, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 102, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9434, 0.0064, 0.0057, 0.0209, 0.0061, 0.0076, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.026

[Epoch: 102, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1413e-01, 9.2935e-08, 1.2098e-01, 3.5367e-07, 1.7680e-01, 5.8809e-01,
        9.5012e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 102, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0101, 0.0100, 0.1985, 0.4750, 0.2840, 0.0121, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 102, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0063, 0.0044, 0.0083, 0.9666, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 102, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0087, 0.0151, 0.9439, 0.0096, 0.0100, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 103, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9393, 0.0078, 0.0048, 0.0188, 0.0065, 0.0108, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.026

[Epoch: 103, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1194e-01, 2.2876e-08, 1.0866e-01, 2.2058e-07, 1.7932e-01, 6.0008e-01,
        3.5157e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 103, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0119, 0.0106, 0.1803, 0.4801, 0.2945, 0.0120, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 103, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0064, 0.0038, 0.0052, 0.9695, 0.0048, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 103, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0057, 0.0102, 0.0154, 0.9415, 0.0098, 0.0102, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 104, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9535, 0.0065, 0.0053, 0.0112, 0.0057, 0.0089, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 104, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0529e-01, 2.5156e-08, 1.0846e-01, 2.1542e-07, 1.9049e-01, 5.9576e-01,
        5.7668e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.851

[Epoch: 104, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0122, 0.0096, 0.1799, 0.4816, 0.2934, 0.0120, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 104, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0060, 0.0044, 0.0066, 0.9674, 0.0056, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 104, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0057, 0.0089, 0.0140, 0.9450, 0.0095, 0.0101, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 105, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9486, 0.0074, 0.0051, 0.0142, 0.0055, 0.0090, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 105, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1499e-01, 2.9984e-08, 1.1096e-01, 2.2320e-07, 1.9581e-01, 5.7824e-01,
        3.9066e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 105, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.0107, 0.1592, 0.5051, 0.2894, 0.0110, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 105, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0060, 0.0039, 0.0053, 0.9705, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 105, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0092, 0.0143, 0.9437, 0.0101, 0.0096, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 106, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9450, 0.0066, 0.0055, 0.0169, 0.0058, 0.0100, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 106, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0459e-01, 2.6202e-08, 1.1076e-01, 2.2573e-07, 1.9151e-01, 5.9314e-01,
        2.7181e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 106, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0139, 0.0099, 0.1951, 0.4591, 0.2994, 0.0112, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 106, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0055, 0.0047, 0.0053, 0.9696, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 106, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0094, 0.0145, 0.9461, 0.0091, 0.0093, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.019

[Epoch: 107, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9537, 0.0060, 0.0046, 0.0124, 0.0046, 0.0086, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 107, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1256e-01, 1.6514e-08, 1.0693e-01, 1.3187e-07, 1.9090e-01, 5.8961e-01,
        2.6529e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.853

[Epoch: 107, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.0098, 0.1508, 0.5144, 0.2901, 0.0100, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 107, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0054, 0.0040, 0.0053, 0.9709, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 107, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0097, 0.0142, 0.9431, 0.0106, 0.0098, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 108, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9420, 0.0071, 0.0054, 0.0195, 0.0057, 0.0106, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 108, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1097e-01, 1.9553e-08, 1.1877e-01, 1.8435e-07, 1.9292e-01, 5.7734e-01,
        2.1549e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 108, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0151, 0.0106, 0.1982, 0.4643, 0.2895, 0.0106, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 108, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0052, 0.0052, 0.9698, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 108, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0061, 0.0092, 0.0155, 0.9446, 0.0093, 0.0096, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.019

[Epoch: 109, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9551, 0.0057, 0.0052, 0.0092, 0.0047, 0.0094, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 109, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0876e-01, 2.4984e-08, 1.0234e-01, 1.6169e-07, 1.7693e-01, 6.1198e-01,
        2.5347e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.840

[Epoch: 109, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0137, 0.0088, 0.1412, 0.5087, 0.3075, 0.0100, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 109, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0041, 0.0052, 0.9713, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 109, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0105, 0.0146, 0.9429, 0.0102, 0.0100, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 110, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9434, 0.0073, 0.0050, 0.0194, 0.0058, 0.0100, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 110, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1431e-01, 4.2242e-08, 1.1416e-01, 2.3592e-07, 2.0914e-01, 5.6239e-01,
        3.7751e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 110, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0175, 0.0114, 0.2051, 0.4502, 0.2903, 0.0125, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 110, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0062, 0.0063, 0.0062, 0.9667, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 110, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0094, 0.0148, 0.9445, 0.0093, 0.0104, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 111, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9544, 0.0056, 0.0042, 0.0104, 0.0041, 0.0097, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 111, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.6024e-02, 3.3125e-08, 1.0267e-01, 4.3469e-07, 1.9759e-01, 6.0372e-01,
        2.9949e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.847

[Epoch: 111, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.0101, 0.1465, 0.4810, 0.3255, 0.0113, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 111, batch: 180/228] total loss per batch: 0.490
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0039, 0.0041, 0.0061, 0.9707, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 111, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0057, 0.0108, 0.0205, 0.9385, 0.0099, 0.0083, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 112, batch: 45/228] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9188, 0.0031, 0.0036, 0.0566, 0.0030, 0.0069, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 112, batch: 90/228] total loss per batch: 0.502
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.8125e-01, 3.6635e-08, 1.3308e-01, 2.2079e-07, 1.7982e-01, 5.0584e-01,
        3.6246e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.873

[Epoch: 112, batch: 135/228] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0072, 0.0043, 0.2492, 0.4562, 0.2639, 0.0112, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 112, batch: 180/228] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0069, 0.0046, 0.0075, 0.9629, 0.0048, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 112, batch: 225/228] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0036, 0.0140, 0.0093, 0.9454, 0.0034, 0.0062, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 113, batch: 45/228] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9595, 0.0041, 0.0061, 0.0034, 0.0064, 0.0139, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 113, batch: 90/228] total loss per batch: 0.489
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([8.4063e-02, 2.6501e-08, 1.0648e-01, 7.5595e-07, 2.2009e-01, 5.8936e-01,
        1.0737e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.837

[Epoch: 113, batch: 135/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0163, 0.0100, 0.1430, 0.5101, 0.2852, 0.0176, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 113, batch: 180/228] total loss per batch: 0.493
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0053, 0.0049, 0.0080, 0.9604, 0.0059, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 113, batch: 225/228] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0066, 0.0123, 0.0165, 0.9371, 0.0077, 0.0098, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 114, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9496, 0.0041, 0.0047, 0.0080, 0.0064, 0.0159, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 114, batch: 90/228] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.4370e-02, 3.2625e-08, 1.0133e-01, 5.4263e-07, 1.9328e-01, 6.1101e-01,
        2.2981e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.830

[Epoch: 114, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0134, 0.0084, 0.1814, 0.5022, 0.2652, 0.0139, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 114, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.0058, 0.0058, 0.9656, 0.0052, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 114, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0067, 0.0113, 0.0171, 0.9385, 0.0089, 0.0097, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 115, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9503, 0.0046, 0.0044, 0.0088, 0.0060, 0.0136, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 115, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0118e-01, 2.6288e-08, 1.0810e-01, 3.8847e-07, 1.9449e-01, 5.9624e-01,
        2.4155e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.840

[Epoch: 115, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0135, 0.0085, 0.1683, 0.4955, 0.2876, 0.0128, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 115, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0061, 0.0057, 0.9658, 0.0053, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 115, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0065, 0.0101, 0.0158, 0.9420, 0.0086, 0.0095, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 116, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9498, 0.0046, 0.0044, 0.0102, 0.0058, 0.0128, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 116, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0319e-01, 2.2400e-08, 1.1089e-01, 3.3040e-07, 1.9170e-01, 5.9421e-01,
        2.5341e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.842

[Epoch: 116, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0138, 0.0084, 0.1747, 0.4909, 0.2873, 0.0124, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 116, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0060, 0.0057, 0.9662, 0.0054, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 116, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0063, 0.0101, 0.0154, 0.9429, 0.0085, 0.0095, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 117, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9507, 0.0045, 0.0045, 0.0106, 0.0056, 0.0121, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 117, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0590e-01, 2.0678e-08, 1.1170e-01, 2.9939e-07, 1.9123e-01, 5.9117e-01,
        2.4067e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 117, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0138, 0.0083, 0.1740, 0.4912, 0.2887, 0.0119, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 117, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0059, 0.0055, 0.9672, 0.0052, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 117, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0062, 0.0096, 0.0151, 0.9441, 0.0084, 0.0095, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 118, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9508, 0.0045, 0.0044, 0.0117, 0.0054, 0.0116, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 118, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0920e-01, 1.7635e-08, 1.1142e-01, 2.6003e-07, 1.9088e-01, 5.8850e-01,
        2.0116e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 118, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0144, 0.0085, 0.1738, 0.4885, 0.2912, 0.0116, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 118, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0052, 0.0057, 0.0054, 0.9674, 0.0053, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 118, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0061, 0.0097, 0.0152, 0.9441, 0.0085, 0.0095, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 119, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9510, 0.0045, 0.0045, 0.0122, 0.0053, 0.0110, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 119, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0788e-01, 1.6358e-08, 1.1091e-01, 2.3489e-07, 1.8723e-01, 5.9398e-01,
        1.9588e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.852

[Epoch: 119, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0142, 0.0084, 0.1754, 0.4876, 0.2919, 0.0111, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 119, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0056, 0.0052, 0.9681, 0.0052, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 119, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0097, 0.0150, 0.9446, 0.0085, 0.0096, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 120, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9505, 0.0045, 0.0045, 0.0134, 0.0052, 0.0109, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 120, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0956e-01, 1.5459e-08, 1.1079e-01, 2.1720e-07, 1.9199e-01, 5.8766e-01,
        1.5624e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 120, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0146, 0.0087, 0.1740, 0.4864, 0.2938, 0.0111, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 120, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0056, 0.0052, 0.9684, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 120, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0095, 0.0151, 0.9445, 0.0087, 0.0096, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 121, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9511, 0.0046, 0.0045, 0.0132, 0.0052, 0.0104, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 121, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0878e-01, 1.2804e-08, 1.1070e-01, 1.9268e-07, 1.9007e-01, 5.9045e-01,
        1.6675e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 121, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0146, 0.0085, 0.1761, 0.4871, 0.2923, 0.0105, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 121, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0054, 0.0051, 0.9685, 0.0051, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 121, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0096, 0.0148, 0.9444, 0.0088, 0.0100, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 122, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9500, 0.0047, 0.0045, 0.0144, 0.0053, 0.0105, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 122, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0956e-01, 1.2428e-08, 1.1044e-01, 1.6749e-07, 1.8729e-01, 5.9271e-01,
        1.2390e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 122, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0148, 0.0091, 0.1716, 0.4888, 0.2940, 0.0108, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 122, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0053, 0.0051, 0.9692, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 122, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0059, 0.0094, 0.0153, 0.9446, 0.0091, 0.0095, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 123, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9509, 0.0047, 0.0048, 0.0139, 0.0051, 0.0101, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 123, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0997e-01, 1.1695e-08, 1.0954e-01, 1.6399e-07, 1.9515e-01, 5.8534e-01,
        1.4063e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.858

[Epoch: 123, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0145, 0.0088, 0.1847, 0.4773, 0.2935, 0.0102, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 123, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0054, 0.0050, 0.9690, 0.0049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 123, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0099, 0.0141, 0.9444, 0.0088, 0.0106, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 124, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9477, 0.0047, 0.0045, 0.0156, 0.0057, 0.0108, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 124, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1012e-01, 1.3554e-08, 1.1172e-01, 1.8325e-07, 1.8929e-01, 5.8886e-01,
        1.5402e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 124, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0156, 0.0102, 0.1529, 0.5115, 0.2886, 0.0107, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 124, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0048, 0.0052, 0.9707, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 124, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0058, 0.0097, 0.0161, 0.9420, 0.0100, 0.0096, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 125, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9525, 0.0048, 0.0049, 0.0140, 0.0046, 0.0099, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 125, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0594e-01, 1.1017e-08, 1.0767e-01, 1.3710e-07, 1.8867e-01, 5.9773e-01,
        1.3984e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 125, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0146, 0.0090, 0.2158, 0.4125, 0.3241, 0.0113, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 125, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0055, 0.0054, 0.0055, 0.9692, 0.0047, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 125, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0060, 0.0090, 0.0140, 0.9467, 0.0082, 0.0103, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 126, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9521, 0.0045, 0.0042, 0.0145, 0.0054, 0.0094, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 126, batch: 90/228] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2119e-01, 2.7360e-08, 1.0356e-01, 3.4851e-07, 1.9917e-01, 5.7608e-01,
        3.8234e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.842

[Epoch: 126, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0141, 0.0089, 0.1152, 0.5885, 0.2532, 0.0092, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 126, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0048, 0.0055, 0.0048, 0.9685, 0.0058, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 126, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0111, 0.0178, 0.9384, 0.0102, 0.0101, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 127, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9484, 0.0053, 0.0050, 0.0142, 0.0053, 0.0106, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 127, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0630e-01, 4.4684e-08, 1.1950e-01, 3.7438e-07, 1.9178e-01, 5.8242e-01,
        2.8389e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 127, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0135, 0.0086, 0.1949, 0.4437, 0.3168, 0.0108, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 127, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0066, 0.0065, 0.0065, 0.9670, 0.0039, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 127, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0083, 0.0078, 0.0156, 0.9433, 0.0103, 0.0080, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 128, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9487, 0.0062, 0.0051, 0.0141, 0.0052, 0.0087, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 128, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.5613e-02, 1.8883e-08, 9.8505e-02, 2.4291e-07, 1.6854e-01, 6.3734e-01,
        2.6702e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.837

[Epoch: 128, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0144, 0.0110, 0.1763, 0.4830, 0.2937, 0.0101, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 128, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0047, 0.0053, 0.0055, 0.9682, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 128, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0063, 0.0110, 0.0159, 0.9411, 0.0098, 0.0098, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 129, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9521, 0.0064, 0.0049, 0.0130, 0.0054, 0.0090, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 129, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.3526e-01, 6.8512e-08, 1.2972e-01, 3.2455e-07, 2.3279e-01, 5.0223e-01,
        2.7815e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.872

[Epoch: 129, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0139, 0.0087, 0.1745, 0.5190, 0.2622, 0.0108, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 129, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0049, 0.0047, 0.0055, 0.9709, 0.0047, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 129, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0092, 0.0147, 0.9457, 0.0094, 0.0093, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 130, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9528, 0.0052, 0.0048, 0.0131, 0.0046, 0.0088, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 130, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([7.7004e-02, 2.4158e-08, 8.6685e-02, 2.3166e-07, 1.5760e-01, 6.7871e-01,
        3.0913e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.823

[Epoch: 130, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0149, 0.0106, 0.1812, 0.4415, 0.3299, 0.0112, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 130, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0051, 0.0051, 0.0045, 0.9704, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 130, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0089, 0.0131, 0.9501, 0.0084, 0.0091, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 131, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9507, 0.0054, 0.0046, 0.0151, 0.0049, 0.0094, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 131, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2921e-01, 4.3442e-08, 1.2323e-01, 2.8638e-07, 2.1273e-01, 5.3483e-01,
        2.8592e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.871

[Epoch: 131, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0174, 0.0109, 0.1597, 0.5089, 0.2807, 0.0098, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 131, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.0053, 0.0058, 0.9676, 0.0054, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 131, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0057, 0.0105, 0.0150, 0.9414, 0.0109, 0.0098, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 132, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9455, 0.0056, 0.0058, 0.0151, 0.0053, 0.0107, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 132, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0521e-01, 4.1245e-08, 1.1416e-01, 2.5281e-07, 1.9462e-01, 5.8600e-01,
        2.4093e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 132, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0135, 0.0095, 0.1879, 0.4817, 0.2876, 0.0099, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 132, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0059, 0.0056, 0.9674, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 132, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0092, 0.0135, 0.9476, 0.0095, 0.0096, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 133, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9489, 0.0058, 0.0055, 0.0150, 0.0051, 0.0099, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 133, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0840e-01, 2.4320e-08, 1.0630e-01, 2.0543e-07, 1.9666e-01, 5.8864e-01,
        1.8754e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.864

[Epoch: 133, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0104, 0.1724, 0.4793, 0.2993, 0.0117, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 133, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0047, 0.0050, 0.9698, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 133, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0063, 0.0105, 0.0157, 0.9404, 0.0096, 0.0099, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 134, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9482, 0.0059, 0.0050, 0.0150, 0.0051, 0.0108, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 134, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1086e-01, 5.6432e-08, 1.1457e-01, 3.4520e-07, 1.9304e-01, 5.8153e-01,
        3.1035e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 134, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0148, 0.0085, 0.1631, 0.5040, 0.2879, 0.0107, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 134, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0040, 0.0049, 0.0051, 0.9721, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 134, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0050, 0.0096, 0.0133, 0.9489, 0.0093, 0.0089, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 135, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9547, 0.0046, 0.0051, 0.0126, 0.0054, 0.0076, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 135, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1720e-01, 5.8597e-08, 9.9405e-02, 2.4299e-07, 1.7274e-01, 6.1065e-01,
        2.9128e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 135, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0144, 0.0094, 0.1942, 0.4629, 0.2982, 0.0099, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 135, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0056, 0.0050, 0.0057, 0.9670, 0.0058, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 135, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0100, 0.0145, 0.9455, 0.0096, 0.0094, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 136, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9515, 0.0052, 0.0049, 0.0157, 0.0051, 0.0091, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 136, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0767e-01, 4.1760e-08, 1.1090e-01, 4.3853e-07, 2.0479e-01, 5.7663e-01,
        3.8986e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 136, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0085, 0.1637, 0.4954, 0.2992, 0.0087, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 136, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.0063, 0.0058, 0.9699, 0.0051, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 136, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0073, 0.0107, 0.0152, 0.9368, 0.0109, 0.0117, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 137, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9462, 0.0046, 0.0054, 0.0168, 0.0054, 0.0094, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 137, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2216e-01, 1.2051e-07, 1.1848e-01, 5.7457e-07, 1.9444e-01, 5.6492e-01,
        1.1886e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 137, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.0123, 0.1823, 0.4740, 0.2938, 0.0100, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 137, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0058, 0.0051, 0.0044, 0.9679, 0.0055, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 137, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0105, 0.0135, 0.9457, 0.0106, 0.0091, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 138, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9458, 0.0058, 0.0056, 0.0157, 0.0052, 0.0097, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 138, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0431e-01, 1.0964e-07, 1.0062e-01, 7.6192e-07, 2.0128e-01, 5.9379e-01,
        2.3604e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 138, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0161, 0.0110, 0.1830, 0.4515, 0.3200, 0.0094, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 138, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0050, 0.0040, 0.0056, 0.9709, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 138, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0047, 0.0089, 0.0138, 0.9471, 0.0097, 0.0104, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 139, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9523, 0.0063, 0.0047, 0.0128, 0.0048, 0.0100, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 139, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0394e-01, 1.4262e-07, 1.1222e-01, 4.6820e-07, 1.8312e-01, 6.0072e-01,
        1.8135e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.866

[Epoch: 139, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0128, 0.0098, 0.1793, 0.5326, 0.2449, 0.0109, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 139, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0061, 0.0049, 0.0046, 0.9697, 0.0052, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 139, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0101, 0.0158, 0.9423, 0.0103, 0.0094, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 140, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9542, 0.0054, 0.0048, 0.0140, 0.0048, 0.0083, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 140, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.2222e-01, 9.0485e-08, 1.0813e-01, 5.1154e-07, 1.8858e-01, 5.8107e-01,
        4.4118e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.832

[Epoch: 140, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0148, 0.0142, 0.1935, 0.4625, 0.2919, 0.0098, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 140, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0047, 0.0045, 0.0064, 0.9710, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 140, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0120, 0.0132, 0.9451, 0.0089, 0.0094, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 141, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9430, 0.0057, 0.0055, 0.0157, 0.0061, 0.0133, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 141, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1607e-01, 7.5138e-08, 1.1293e-01, 5.0899e-07, 1.8552e-01, 5.8549e-01,
        6.7066e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 141, batch: 135/228] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0171, 0.0098, 0.1617, 0.4865, 0.3005, 0.0129, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 141, batch: 180/228] total loss per batch: 0.491
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0065, 0.0028, 0.0061, 0.9680, 0.0060, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 141, batch: 225/228] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0109, 0.0141, 0.9403, 0.0124, 0.0097, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 142, batch: 45/228] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9340, 0.0063, 0.0063, 0.0249, 0.0065, 0.0111, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 142, batch: 90/228] total loss per batch: 0.497
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.7365e-01, 4.5382e-08, 9.2221e-02, 1.7700e-07, 1.6893e-01, 5.6521e-01,
        5.7140e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 142, batch: 135/228] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0133, 0.0125, 0.1071, 0.4631, 0.3817, 0.0116, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 142, batch: 180/228] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0050, 0.0070, 0.0103, 0.9620, 0.0072, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 142, batch: 225/228] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0139, 0.0148, 0.9446, 0.0066, 0.0084, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 143, batch: 45/228] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9566, 0.0030, 0.0045, 0.0173, 0.0025, 0.0088, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 143, batch: 90/228] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.4445e-01, 1.6866e-07, 1.3386e-01, 3.6293e-07, 1.9917e-01, 5.2252e-01,
        5.4998e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.884

[Epoch: 143, batch: 135/228] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0179, 0.0082, 0.1447, 0.5483, 0.2608, 0.0087, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 143, batch: 180/228] total loss per batch: 0.494
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0095, 0.0110, 0.0289, 0.9324, 0.0057, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 143, batch: 225/228] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0043, 0.0098, 0.0116, 0.9516, 0.0081, 0.0079, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 144, batch: 45/228] total loss per batch: 0.499
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9583, 0.0047, 0.0058, 0.0073, 0.0055, 0.0072, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 144, batch: 90/228] total loss per batch: 0.478
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1050e-01, 6.4068e-08, 1.1466e-01, 3.9558e-07, 2.2390e-01, 5.5095e-01,
        2.4995e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 144, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0171, 0.0101, 0.1880, 0.4879, 0.2752, 0.0094, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 144, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0051, 0.0047, 0.0058, 0.9720, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 144, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0101, 0.0127, 0.9484, 0.0092, 0.0082, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 145, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9532, 0.0048, 0.0059, 0.0105, 0.0057, 0.0091, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 145, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0390e-01, 5.9012e-08, 1.0889e-01, 3.8535e-07, 2.0601e-01, 5.8120e-01,
        2.9373e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.867

[Epoch: 145, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0174, 0.0100, 0.1884, 0.4731, 0.2887, 0.0101, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 145, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0052, 0.0045, 0.0059, 0.9716, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 145, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0105, 0.0134, 0.9455, 0.0100, 0.0089, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 146, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9519, 0.0049, 0.0060, 0.0118, 0.0057, 0.0088, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 146, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0395e-01, 5.1687e-08, 1.0753e-01, 3.3549e-07, 1.9445e-01, 5.9407e-01,
        2.3723e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 146, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0173, 0.0097, 0.1762, 0.4934, 0.2821, 0.0097, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 146, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0053, 0.0045, 0.0057, 0.9716, 0.0047, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 146, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0050, 0.0105, 0.0138, 0.9444, 0.0103, 0.0091, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 147, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9506, 0.0049, 0.0060, 0.0129, 0.0055, 0.0091, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 147, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0757e-01, 4.1294e-08, 1.1013e-01, 3.0084e-07, 1.8940e-01, 5.9290e-01,
        2.3808e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 147, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0168, 0.0096, 0.1798, 0.4782, 0.2943, 0.0099, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 147, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0053, 0.0046, 0.0058, 0.9711, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 147, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0105, 0.0138, 0.9442, 0.0104, 0.0093, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 148, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9495, 0.0050, 0.0060, 0.0135, 0.0056, 0.0092, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 148, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0806e-01, 3.5884e-08, 1.0966e-01, 2.6328e-07, 1.8787e-01, 5.9442e-01,
        1.9342e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 148, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0167, 0.0095, 0.1731, 0.4893, 0.2905, 0.0097, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 148, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0054, 0.0047, 0.0057, 0.9706, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 148, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0104, 0.0138, 0.9441, 0.0104, 0.0094, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 149, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9494, 0.0050, 0.0059, 0.0139, 0.0054, 0.0092, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 149, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0967e-01, 3.0138e-08, 1.1014e-01, 2.3039e-07, 1.8894e-01, 5.9125e-01,
        1.7411e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.862

[Epoch: 149, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0163, 0.0094, 0.1783, 0.4810, 0.2946, 0.0097, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 149, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0055, 0.0047, 0.0058, 0.9704, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 149, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0104, 0.0138, 0.9442, 0.0104, 0.0095, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 150, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9491, 0.0051, 0.0059, 0.0142, 0.0054, 0.0093, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 150, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0908e-01, 2.7340e-08, 1.0948e-01, 2.0632e-07, 1.8754e-01, 5.9390e-01,
        1.5042e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.861

[Epoch: 150, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0163, 0.0094, 0.1727, 0.4900, 0.2911, 0.0097, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 150, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0056, 0.0048, 0.0057, 0.9700, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 150, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0102, 0.0139, 0.9443, 0.0103, 0.0096, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 151, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9492, 0.0051, 0.0058, 0.0142, 0.0054, 0.0092, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 151, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0997e-01, 2.2987e-08, 1.0988e-01, 1.8441e-07, 1.9135e-01, 5.8881e-01,
        1.3261e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 151, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.0097, 0.1802, 0.4771, 0.2967, 0.0099, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 151, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0055, 0.0047, 0.0057, 0.9704, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 151, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0103, 0.0139, 0.9440, 0.0104, 0.0097, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 152, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9492, 0.0051, 0.0057, 0.0144, 0.0054, 0.0094, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 152, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0948e-01, 2.2212e-08, 1.0894e-01, 1.7182e-07, 1.8822e-01, 5.9336e-01,
        1.2246e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 152, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.0096, 0.1723, 0.4916, 0.2904, 0.0097, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 152, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0057, 0.0048, 0.0057, 0.9696, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 152, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0102, 0.0140, 0.9442, 0.0103, 0.0096, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 153, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9490, 0.0050, 0.0057, 0.0146, 0.0054, 0.0093, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 153, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1036e-01, 1.9120e-08, 1.0996e-01, 1.5619e-07, 1.9059e-01, 5.8909e-01,
        1.1618e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 153, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.0099, 0.1800, 0.4768, 0.2974, 0.0099, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 153, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0055, 0.0047, 0.0055, 0.9704, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 153, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0100, 0.0141, 0.9445, 0.0102, 0.0097, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 154, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9490, 0.0052, 0.0056, 0.0146, 0.0054, 0.0094, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 154, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0957e-01, 1.8014e-08, 1.0922e-01, 1.4329e-07, 1.8887e-01, 5.9234e-01,
        9.8144e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 154, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.0095, 0.1709, 0.4942, 0.2900, 0.0096, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 154, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0058, 0.0049, 0.0056, 0.9690, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 154, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0100, 0.0143, 0.9443, 0.0102, 0.0097, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 155, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9494, 0.0050, 0.0056, 0.0145, 0.0053, 0.0094, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 155, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1069e-01, 1.5264e-08, 1.1045e-01, 1.3244e-07, 1.9091e-01, 5.8795e-01,
        1.0302e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.857

[Epoch: 155, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0100, 0.1827, 0.4697, 0.3019, 0.0100, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 155, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0056, 0.0047, 0.0054, 0.9704, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 155, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0100, 0.0137, 0.9452, 0.0102, 0.0097, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 156, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9484, 0.0052, 0.0056, 0.0150, 0.0055, 0.0095, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 156, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0961e-01, 1.4781e-08, 1.0909e-01, 1.1599e-07, 1.8668e-01, 5.9462e-01,
        7.2641e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 156, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0095, 0.1661, 0.5055, 0.2840, 0.0095, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 156, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0057, 0.0049, 0.0055, 0.9692, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 156, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0050, 0.0102, 0.0145, 0.9439, 0.0101, 0.0098, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 157, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9508, 0.0049, 0.0053, 0.0140, 0.0050, 0.0096, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 157, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1047e-01, 1.2866e-08, 1.0960e-01, 1.1508e-07, 1.9432e-01, 5.8561e-01,
        1.1261e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.861

[Epoch: 157, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0105, 0.1810, 0.4707, 0.3029, 0.0098, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 157, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0054, 0.0048, 0.0052, 0.9708, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 157, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0047, 0.0097, 0.0130, 0.9467, 0.0101, 0.0096, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 158, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9479, 0.0053, 0.0056, 0.0157, 0.0056, 0.0095, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 158, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0922e-01, 1.7436e-08, 1.0778e-01, 1.1534e-07, 1.9192e-01, 5.9108e-01,
        5.2605e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.848

[Epoch: 158, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0148, 0.0089, 0.1686, 0.5017, 0.2863, 0.0098, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 158, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0058, 0.0052, 0.0057, 0.9678, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 158, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0098, 0.0141, 0.9447, 0.0100, 0.0100, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 159, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9519, 0.0051, 0.0048, 0.0132, 0.0050, 0.0098, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 159, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1169e-01, 9.7969e-09, 1.1328e-01, 9.4369e-08, 1.8325e-01, 5.9179e-01,
        9.2440e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 159, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0110, 0.1843, 0.4643, 0.3054, 0.0101, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 159, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0045, 0.0041, 0.0050, 0.9737, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 159, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0046, 0.0106, 0.0140, 0.9444, 0.0101, 0.0103, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 160, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9474, 0.0054, 0.0062, 0.0159, 0.0053, 0.0093, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 160, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0874e-01, 2.1347e-08, 1.0772e-01, 1.3401e-07, 2.0247e-01, 5.8107e-01,
        7.6006e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.850

[Epoch: 160, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0131, 0.0085, 0.1625, 0.5167, 0.2801, 0.0094, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 160, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0059, 0.0051, 0.0059, 0.9674, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 160, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0045, 0.0091, 0.0141, 0.9477, 0.0103, 0.0090, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 161, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9527, 0.0051, 0.0040, 0.0129, 0.0053, 0.0100, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 161, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1212e-01, 1.3287e-08, 1.0851e-01, 1.4474e-07, 1.8379e-01, 5.9558e-01,
        1.3791e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 161, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0147, 0.0107, 0.1817, 0.4616, 0.3107, 0.0106, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 161, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0052, 0.0046, 0.0047, 0.9710, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 161, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0050, 0.0112, 0.0135, 0.9447, 0.0097, 0.0091, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 162, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9467, 0.0055, 0.0061, 0.0174, 0.0054, 0.0097, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 162, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0568e-01, 1.9967e-08, 1.0663e-01, 1.4520e-07, 1.9425e-01, 5.9344e-01,
        1.2690e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.861

[Epoch: 162, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0139, 0.0102, 0.1712, 0.5043, 0.2805, 0.0095, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 162, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0059, 0.0060, 0.0057, 0.9669, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 162, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0098, 0.0141, 0.9458, 0.0103, 0.0096, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 163, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9512, 0.0051, 0.0049, 0.0132, 0.0052, 0.0100, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 163, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1040e-01, 2.3285e-08, 1.1585e-01, 1.7354e-07, 1.8895e-01, 5.8481e-01,
        8.0031e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.842

[Epoch: 163, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0107, 0.1790, 0.4606, 0.3121, 0.0108, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 163, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.0047, 0.0056, 0.9692, 0.0056, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 163, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0103, 0.0151, 0.9440, 0.0097, 0.0097, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.018

[Epoch: 164, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9461, 0.0052, 0.0054, 0.0178, 0.0054, 0.0099, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 164, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1560e-01, 1.7796e-08, 1.0491e-01, 1.5187e-07, 1.9825e-01, 5.8125e-01,
        8.2330e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 164, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0135, 0.0092, 0.1704, 0.5065, 0.2814, 0.0099, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 164, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0056, 0.0052, 0.0054, 0.9691, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 164, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0105, 0.0152, 0.9446, 0.0100, 0.0092, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 165, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9578, 0.0048, 0.0046, 0.0102, 0.0043, 0.0087, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 165, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1111e-01, 1.8066e-08, 1.1016e-01, 1.8542e-07, 1.8512e-01, 5.9361e-01,
        1.0324e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 165, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0145, 0.0103, 0.1904, 0.4663, 0.2981, 0.0094, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 165, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0056, 0.0057, 0.9684, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 165, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0091, 0.0135, 0.9487, 0.0092, 0.0094, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.019

[Epoch: 166, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9415, 0.0054, 0.0054, 0.0208, 0.0062, 0.0101, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 166, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1209e-01, 2.0387e-08, 1.1186e-01, 1.5631e-07, 1.8969e-01, 5.8636e-01,
        8.1480e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.868

[Epoch: 166, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0095, 0.1556, 0.5002, 0.2987, 0.0106, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 166, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0056, 0.0051, 0.0052, 0.9680, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 166, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0118, 0.0178, 0.9362, 0.0122, 0.0097, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 167, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9524, 0.0054, 0.0052, 0.0104, 0.0050, 0.0107, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 167, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0544e-01, 2.6945e-08, 1.0262e-01, 2.5172e-07, 1.8999e-01, 6.0194e-01,
        1.1674e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 167, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0155, 0.0101, 0.1957, 0.4821, 0.2783, 0.0087, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 167, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0058, 0.0059, 0.9670, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 167, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0063, 0.0097, 0.0133, 0.9476, 0.0081, 0.0093, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 168, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9383, 0.0063, 0.0055, 0.0221, 0.0057, 0.0107, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 168, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1337e-01, 2.8185e-08, 1.1864e-01, 1.9958e-07, 1.8895e-01, 5.7904e-01,
        9.8999e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.848

[Epoch: 168, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0105, 0.1588, 0.4831, 0.3120, 0.0105, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.015

[Epoch: 168, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0053, 0.0044, 0.0051, 0.9708, 0.0048, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 168, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0041, 0.0086, 0.0144, 0.9479, 0.0104, 0.0093, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 169, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9517, 0.0054, 0.0055, 0.0114, 0.0050, 0.0103, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 169, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1030e-01, 1.6873e-08, 9.6755e-02, 1.5627e-07, 1.9244e-01, 6.0051e-01,
        1.5971e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 169, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0141, 0.0094, 0.1812, 0.4977, 0.2794, 0.0090, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 169, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0048, 0.0051, 0.9699, 0.0047, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 169, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0061, 0.0110, 0.0131, 0.9470, 0.0085, 0.0087, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.016

[Epoch: 170, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9586, 0.0048, 0.0043, 0.0113, 0.0045, 0.0074, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 170, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1080e-01, 2.2642e-08, 1.1903e-01, 1.5817e-07, 1.9048e-01, 5.7969e-01,
        1.4111e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.861

[Epoch: 170, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0149, 0.0092, 0.1729, 0.4888, 0.2942, 0.0093, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 170, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0040, 0.0053, 0.0044, 0.9717, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 170, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0100, 0.0163, 0.9403, 0.0122, 0.0105, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 171, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9471, 0.0052, 0.0063, 0.0167, 0.0048, 0.0090, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 171, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1262e-01, 3.7654e-08, 1.1415e-01, 1.2840e-07, 1.8596e-01, 5.8727e-01,
        5.8524e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.847

[Epoch: 171, batch: 135/228] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0129, 0.0124, 0.1832, 0.4563, 0.3139, 0.0083, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 171, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0048, 0.0051, 0.9705, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 171, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0046, 0.0133, 0.0148, 0.9412, 0.0100, 0.0108, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 172, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9478, 0.0057, 0.0053, 0.0125, 0.0062, 0.0129, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 172, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0967e-01, 4.9900e-08, 1.0911e-01, 1.4641e-07, 1.9883e-01, 5.8238e-01,
        8.8381e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.864

[Epoch: 172, batch: 135/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0097, 0.0092, 0.1635, 0.5712, 0.2270, 0.0084, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 172, batch: 180/228] total loss per batch: 0.489
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.0073, 0.0031, 0.9679, 0.0049, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 172, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0042, 0.0117, 0.0150, 0.9433, 0.0097, 0.0100, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.025

[Epoch: 173, batch: 45/228] total loss per batch: 0.497
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9464, 0.0049, 0.0053, 0.0175, 0.0058, 0.0102, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 173, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1070e-01, 1.4286e-08, 9.8687e-02, 1.6661e-07, 1.8147e-01, 6.0914e-01,
        4.0521e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 173, batch: 135/228] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0183, 0.0136, 0.1934, 0.4339, 0.3162, 0.0131, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 173, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0067, 0.0083, 0.0045, 0.9644, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 173, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0047, 0.0098, 0.0152, 0.9446, 0.0113, 0.0094, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.026

[Epoch: 174, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9522, 0.0047, 0.0046, 0.0175, 0.0052, 0.0079, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 174, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0421e-01, 1.1107e-08, 1.0281e-01, 1.2623e-07, 1.8215e-01, 6.1082e-01,
        2.7559e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.844

[Epoch: 174, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0170, 0.0134, 0.1711, 0.4840, 0.2929, 0.0123, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 174, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0058, 0.0066, 0.0056, 0.9658, 0.0050, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 174, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0113, 0.0164, 0.9421, 0.0089, 0.0104, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.020

[Epoch: 175, batch: 45/228] total loss per batch: 0.498
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9571, 0.0048, 0.0046, 0.0108, 0.0043, 0.0094, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 175, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1690e-01, 1.0550e-08, 1.2488e-01, 2.0051e-07, 2.0878e-01, 5.4944e-01,
        1.0635e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.882

[Epoch: 175, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0169, 0.0117, 0.1754, 0.4808, 0.2942, 0.0117, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 175, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0059, 0.0057, 0.0059, 0.9674, 0.0042, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 175, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0116, 0.0160, 0.9421, 0.0101, 0.0096, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.017

[Epoch: 176, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9517, 0.0048, 0.0049, 0.0150, 0.0051, 0.0102, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 176, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0259e-01, 6.8803e-09, 1.0555e-01, 1.4533e-07, 1.8142e-01, 6.1044e-01,
        1.6877e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.859

[Epoch: 176, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0114, 0.1725, 0.4812, 0.2992, 0.0115, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 176, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0056, 0.0056, 0.0056, 0.9673, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 176, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0118, 0.0151, 0.9437, 0.0094, 0.0098, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 177, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9514, 0.0048, 0.0048, 0.0154, 0.0050, 0.0096, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 177, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1106e-01, 8.3135e-09, 1.1236e-01, 1.5633e-07, 1.9138e-01, 5.8520e-01,
        9.9799e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 177, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0112, 0.1755, 0.4853, 0.2930, 0.0109, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 177, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.0054, 0.0052, 0.9688, 0.0043, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 177, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0105, 0.0153, 0.9453, 0.0095, 0.0095, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 178, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9488, 0.0054, 0.0051, 0.0164, 0.0053, 0.0097, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 178, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0955e-01, 7.6376e-09, 1.0949e-01, 1.3124e-07, 1.8856e-01, 5.9240e-01,
        9.8815e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 178, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0109, 0.1744, 0.4818, 0.2977, 0.0108, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 178, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.0053, 0.0053, 0.9689, 0.0044, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 178, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0104, 0.0152, 0.9451, 0.0097, 0.0098, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 179, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9508, 0.0051, 0.0050, 0.0152, 0.0051, 0.0094, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 179, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1013e-01, 6.8283e-09, 1.0929e-01, 1.2236e-07, 1.9085e-01, 5.8973e-01,
        9.6025e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 179, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0151, 0.0106, 0.1750, 0.4888, 0.2908, 0.0105, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 179, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.0050, 0.9702, 0.0042, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 179, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0050, 0.0102, 0.0159, 0.9434, 0.0100, 0.0100, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 180, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9495, 0.0053, 0.0049, 0.0157, 0.0052, 0.0098, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 180, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1164e-01, 6.1773e-09, 1.1194e-01, 1.0516e-07, 1.9064e-01, 5.8578e-01,
        7.8899e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 180, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0150, 0.0106, 0.1754, 0.4805, 0.2985, 0.0106, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 180, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0050, 0.0051, 0.9701, 0.0045, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 180, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0102, 0.0146, 0.9457, 0.0098, 0.0098, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 181, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9513, 0.0052, 0.0050, 0.0146, 0.0049, 0.0094, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 181, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0576e-01, 5.4362e-09, 1.0561e-01, 1.0687e-07, 1.8823e-01, 6.0040e-01,
        7.8661e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.852

[Epoch: 181, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0153, 0.0106, 0.1737, 0.4889, 0.2913, 0.0103, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 181, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0049, 0.0047, 0.9712, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 181, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0048, 0.0100, 0.0147, 0.9468, 0.0093, 0.0096, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 182, batch: 45/228] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9493, 0.0053, 0.0049, 0.0159, 0.0050, 0.0100, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 182, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1472e-01, 5.3727e-09, 1.1873e-01, 8.4346e-08, 1.9248e-01, 5.7407e-01,
        7.7797e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 182, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.0102, 0.1787, 0.4758, 0.2995, 0.0106, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 182, batch: 180/228] total loss per batch: 0.487
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0048, 0.0051, 0.9709, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 182, batch: 225/228] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0046, 0.0101, 0.0149, 0.9455, 0.0102, 0.0097, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 183, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9510, 0.0050, 0.0052, 0.0142, 0.0053, 0.0099, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 183, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0990e-01, 5.0964e-09, 9.9217e-02, 1.2749e-07, 1.8693e-01, 6.0396e-01,
        6.6958e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.849

[Epoch: 183, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0150, 0.0104, 0.1692, 0.4944, 0.2921, 0.0097, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 183, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0053, 0.0048, 0.9709, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 183, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0098, 0.0154, 0.9447, 0.0104, 0.0098, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 184, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9536, 0.0051, 0.0041, 0.0141, 0.0046, 0.0087, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 184, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0944e-01, 1.1439e-08, 1.2333e-01, 1.0446e-07, 1.9479e-01, 5.7244e-01,
        1.7450e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.865

[Epoch: 184, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0142, 0.0099, 0.1810, 0.4675, 0.3073, 0.0101, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 184, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0046, 0.0054, 0.9687, 0.0052, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 184, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0111, 0.0144, 0.9452, 0.0091, 0.0095, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 185, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9511, 0.0044, 0.0047, 0.0172, 0.0047, 0.0093, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 185, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1024e-01, 6.1991e-09, 9.6886e-02, 2.3793e-07, 1.9128e-01, 6.0159e-01,
        1.4375e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.852

[Epoch: 185, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0154, 0.0103, 0.1723, 0.4980, 0.2857, 0.0095, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 185, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.0048, 0.0037, 0.9740, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 185, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0046, 0.0092, 0.0157, 0.9464, 0.0106, 0.0083, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 186, batch: 45/228] total loss per batch: 0.496
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9483, 0.0061, 0.0048, 0.0136, 0.0055, 0.0113, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 186, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0348e-01, 4.8929e-08, 1.1360e-01, 2.8939e-07, 1.8755e-01, 5.9537e-01,
        2.2290e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.839

[Epoch: 186, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.0102, 0.1730, 0.4692, 0.3106, 0.0111, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 186, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0054, 0.0055, 0.0060, 0.9660, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 186, batch: 225/228] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0043, 0.0084, 0.0130, 0.9506, 0.0093, 0.0095, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 187, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9468, 0.0056, 0.0046, 0.0192, 0.0050, 0.0088, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 187, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1222e-01, 1.5622e-08, 1.0673e-01, 3.3277e-07, 1.9977e-01, 5.8127e-01,
        8.9361e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.847

[Epoch: 187, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0151, 0.0097, 0.1823, 0.5029, 0.2683, 0.0112, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 187, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0049, 0.0053, 0.9697, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 187, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0045, 0.0098, 0.0127, 0.9517, 0.0080, 0.0086, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.015

[Epoch: 188, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9515, 0.0052, 0.0051, 0.0120, 0.0053, 0.0110, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 188, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0942e-01, 3.6535e-08, 1.0934e-01, 2.8411e-07, 1.8665e-01, 5.9458e-01,
        5.6350e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.855

[Epoch: 188, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0149, 0.0101, 0.1770, 0.4644, 0.3139, 0.0105, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 188, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0058, 0.0053, 0.9687, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 188, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0044, 0.0099, 0.0159, 0.9464, 0.0094, 0.0096, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 189, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9463, 0.0054, 0.0047, 0.0180, 0.0053, 0.0098, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 189, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0903e-01, 1.0633e-08, 1.0593e-01, 2.2114e-07, 1.9442e-01, 5.9062e-01,
        1.0878e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 189, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0171, 0.0109, 0.1761, 0.4818, 0.2924, 0.0101, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 189, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.0045, 0.0055, 0.9693, 0.0056, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 189, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0049, 0.0105, 0.0165, 0.9428, 0.0105, 0.0098, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 190, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9567, 0.0045, 0.0046, 0.0131, 0.0041, 0.0088, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 190, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1842e-01, 1.8982e-08, 1.2275e-01, 2.5971e-07, 1.9064e-01, 5.6818e-01,
        1.6904e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.863

[Epoch: 190, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0139, 0.0087, 0.1733, 0.4947, 0.2890, 0.0099, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 190, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0052, 0.0050, 0.9704, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 190, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0105, 0.0175, 0.9377, 0.0120, 0.0111, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 191, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9492, 0.0055, 0.0049, 0.0134, 0.0053, 0.0106, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 191, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0305e-01, 3.2116e-08, 9.9575e-02, 3.0444e-07, 1.8473e-01, 6.1265e-01,
        1.2986e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 191, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0141, 0.0099, 0.1728, 0.4737, 0.3106, 0.0093, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 191, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0048, 0.0047, 0.0048, 0.9707, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 191, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0055, 0.0113, 0.0159, 0.9384, 0.0119, 0.0115, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 192, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9492, 0.0050, 0.0049, 0.0172, 0.0049, 0.0099, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 192, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1501e-01, 1.5329e-08, 1.1035e-01, 2.4069e-07, 1.8645e-01, 5.8819e-01,
        8.1918e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.856

[Epoch: 192, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0130, 0.0086, 0.1774, 0.5000, 0.2826, 0.0088, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 192, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0052, 0.0051, 0.0051, 0.9703, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 192, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0053, 0.0099, 0.0162, 0.9408, 0.0118, 0.0104, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 193, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9501, 0.0054, 0.0047, 0.0140, 0.0056, 0.0102, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 193, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0946e-01, 2.3461e-08, 1.1666e-01, 3.0385e-07, 1.9516e-01, 5.7872e-01,
        1.8497e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.848

[Epoch: 193, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0150, 0.0101, 0.1757, 0.4664, 0.3140, 0.0094, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 193, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0054, 0.0059, 0.9675, 0.0057, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 193, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0051, 0.0095, 0.0133, 0.9486, 0.0091, 0.0095, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 194, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9492, 0.0051, 0.0049, 0.0157, 0.0048, 0.0100, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 194, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1240e-01, 2.3245e-08, 1.0333e-01, 2.9769e-07, 1.9003e-01, 5.9424e-01,
        1.2120e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.860

[Epoch: 194, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.0091, 0.1614, 0.5153, 0.2805, 0.0096, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 194, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0053, 0.0055, 0.0051, 0.9675, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 194, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0042, 0.0084, 0.0132, 0.9540, 0.0075, 0.0083, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 195, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9477, 0.0055, 0.0056, 0.0156, 0.0057, 0.0100, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 195, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0969e-01, 2.0112e-08, 1.1669e-01, 1.9746e-07, 1.8643e-01, 5.8719e-01,
        1.2482e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.846

[Epoch: 195, batch: 135/228] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.0108, 0.1985, 0.4445, 0.3083, 0.0109, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 195, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0054, 0.0055, 0.9702, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 195, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0042, 0.0106, 0.0124, 0.9514, 0.0089, 0.0077, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 196, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9437, 0.0058, 0.0055, 0.0171, 0.0053, 0.0108, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 196, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.0554e-01, 3.2864e-08, 1.0767e-01, 2.9352e-07, 1.9472e-01, 5.9207e-01,
        7.2357e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.864

[Epoch: 196, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0165, 0.0109, 0.1577, 0.5104, 0.2828, 0.0108, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 196, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0045, 0.0050, 0.9708, 0.0055, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 196, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0052, 0.0090, 0.0156, 0.9431, 0.0114, 0.0106, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 197, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9479, 0.0054, 0.0053, 0.0155, 0.0052, 0.0091, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 197, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1783e-01, 3.6952e-08, 1.2118e-01, 3.1188e-07, 1.8866e-01, 5.7233e-01,
        1.3198e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.858

[Epoch: 197, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0131, 0.0092, 0.1760, 0.4653, 0.3185, 0.0094, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 197, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0056, 0.0051, 0.0056, 0.9682, 0.0052, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 197, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0061, 0.0113, 0.0138, 0.9396, 0.0129, 0.0102, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 198, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9524, 0.0053, 0.0050, 0.0132, 0.0047, 0.0104, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 198, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([9.5711e-02, 3.9620e-08, 9.0946e-02, 3.4317e-07, 1.9268e-01, 6.2066e-01,
        2.5371e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.854

[Epoch: 198, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0137, 0.0094, 0.1867, 0.5187, 0.2544, 0.0087, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 198, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0049, 0.0052, 0.9690, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 198, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0054, 0.0101, 0.0175, 0.9393, 0.0112, 0.0111, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 199, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9552, 0.0051, 0.0043, 0.0105, 0.0047, 0.0101, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 199, batch: 90/228] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1225e-01, 4.4115e-08, 1.1694e-01, 3.8506e-07, 1.7923e-01, 5.9158e-01,
        9.1237e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.850

[Epoch: 199, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.0104, 0.1559, 0.4622, 0.3397, 0.0082, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 199, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.0060, 0.0059, 0.9660, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 199, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0056, 0.0112, 0.0148, 0.9453, 0.0086, 0.0093, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 200, batch: 45/228] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0050, 0.0150, 0.0050, 0.0100, 0.0100])
Policy pred: tensor([0.9475, 0.0048, 0.0048, 0.0183, 0.0055, 0.0095, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 200, batch: 90/228] total loss per batch: 0.474
Policy (actual, predicted): 5 5
Policy data: tensor([0.1100, 0.0000, 0.1100, 0.0000, 0.1900, 0.5900, 0.0000])
Policy pred: tensor([1.1726e-01, 3.1911e-08, 1.1657e-01, 4.5855e-07, 1.9938e-01, 5.6679e-01,
        8.8125e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.854 0.875

[Epoch: 200, batch: 135/228] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0150, 0.0100, 0.1750, 0.4850, 0.2950, 0.0100, 0.0100])
Policy pred: tensor([0.0156, 0.0103, 0.1847, 0.5046, 0.2640, 0.0102, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 200, batch: 180/228] total loss per batch: 0.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0055, 0.0053, 0.0048, 0.9682, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 200, batch: 225/228] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.9450, 0.0100, 0.0100, 0.0050])
Policy pred: tensor([0.0045, 0.0103, 0.0127, 0.9506, 0.0085, 0.0086, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

