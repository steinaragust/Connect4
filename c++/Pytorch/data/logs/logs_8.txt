Training set samples: 6319
Batch size: 32
[Epoch: 1, batch: 39/198] total loss per batch: 1.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3773e-01, 6.4363e-01, 4.5948e-02, 1.8652e-04, 3.1836e-02, 6.3523e-05,
        4.0604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.169

[Epoch: 1, batch: 78/198] total loss per batch: 1.520
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.2144, 0.4564, 0.0573, 0.0693, 0.0396, 0.0624, 0.1007],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.013

[Epoch: 1, batch: 117/198] total loss per batch: 1.529
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0196, 0.5709, 0.0974, 0.0876, 0.1474, 0.0341, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.027

[Epoch: 1, batch: 156/198] total loss per batch: 1.516
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([2.8459e-01, 4.0409e-02, 4.4682e-02, 3.0881e-04, 3.2063e-02, 4.3200e-02,
        5.5475e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 1, batch: 195/198] total loss per batch: 1.467
Policy (actual, predicted): 1 3
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0807, 0.1199, 0.0092, 0.6444, 0.0295, 0.0777, 0.0386],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.037

[Epoch: 2, batch: 39/198] total loss per batch: 1.200
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3695e-01, 7.1627e-01, 1.4065e-02, 6.3242e-05, 1.7613e-02, 1.5945e-05,
        1.5027e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.236

[Epoch: 2, batch: 78/198] total loss per batch: 1.134
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1404, 0.6863, 0.0333, 0.0380, 0.0155, 0.0298, 0.0568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.002

[Epoch: 2, batch: 117/198] total loss per batch: 1.146
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.8010, 0.0207, 0.0597, 0.0850, 0.0100, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 2, batch: 156/198] total loss per batch: 1.174
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.5644e-01, 1.3484e-02, 1.2599e-02, 4.5973e-05, 5.5862e-03, 8.2287e-03,
        8.0362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.056

[Epoch: 2, batch: 195/198] total loss per batch: 1.143
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1112, 0.3946, 0.0105, 0.2696, 0.0398, 0.1369, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 3, batch: 39/198] total loss per batch: 0.962
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6128e-02, 9.1968e-01, 3.8209e-03, 6.1761e-06, 3.9225e-03, 1.7296e-06,
        6.4422e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.221

[Epoch: 3, batch: 78/198] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0801, 0.7731, 0.0536, 0.0256, 0.0142, 0.0260, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.018

[Epoch: 3, batch: 117/198] total loss per batch: 0.913
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.8325, 0.0171, 0.0663, 0.0690, 0.0046, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.044

[Epoch: 3, batch: 156/198] total loss per batch: 0.940
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.9753e-02, 4.5217e-03, 7.7261e-03, 3.9457e-05, 1.2845e-03, 2.6415e-03,
        9.0403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.051

[Epoch: 3, batch: 195/198] total loss per batch: 0.915
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0399, 0.7914, 0.0043, 0.0530, 0.0325, 0.0693, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 4, batch: 39/198] total loss per batch: 0.853
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3262e-02, 9.7134e-01, 2.2648e-03, 2.3094e-06, 1.6350e-03, 4.1739e-07,
        1.4978e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.250

[Epoch: 4, batch: 78/198] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1028, 0.5784, 0.1108, 0.0514, 0.0423, 0.0479, 0.0665],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.020

[Epoch: 4, batch: 117/198] total loss per batch: 0.851
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.8048, 0.0331, 0.0475, 0.0936, 0.0041, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.030

[Epoch: 4, batch: 156/198] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.7197e-01, 5.0580e-03, 1.3067e-02, 2.2328e-05, 3.3218e-03, 3.2845e-03,
        8.0328e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.045

[Epoch: 4, batch: 195/198] total loss per batch: 0.852
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0385, 0.8429, 0.0039, 0.0413, 0.0308, 0.0275, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 5, batch: 39/198] total loss per batch: 0.826
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.5992e-02, 9.6899e-01, 1.7247e-03, 2.2141e-06, 1.6065e-03, 4.9418e-07,
        1.6805e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.303

[Epoch: 5, batch: 78/198] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1361, 0.5355, 0.0546, 0.0800, 0.0567, 0.0560, 0.0810],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.043

[Epoch: 5, batch: 117/198] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7839, 0.0463, 0.0837, 0.0689, 0.0027, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 5, batch: 156/198] total loss per batch: 0.836
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.0285e-02, 4.1879e-03, 5.6492e-03, 1.8146e-05, 1.6099e-03, 1.4633e-03,
        9.0679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 5, batch: 195/198] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0294, 0.8312, 0.0043, 0.0779, 0.0107, 0.0264, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.003

[Epoch: 6, batch: 39/198] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.7922e-02, 9.7564e-01, 2.3453e-03, 2.5039e-06, 2.8110e-03, 8.7794e-07,
        1.2808e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.332

[Epoch: 6, batch: 78/198] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0922, 0.6003, 0.0773, 0.0665, 0.0644, 0.0402, 0.0590],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.038

[Epoch: 6, batch: 117/198] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.7483, 0.1145, 0.0744, 0.0478, 0.0031, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.035

[Epoch: 6, batch: 156/198] total loss per batch: 0.821
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.6059e-01, 6.3722e-03, 6.7614e-03, 1.9115e-05, 2.6970e-03, 1.8965e-03,
        8.2166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 6, batch: 195/198] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0217, 0.8322, 0.0057, 0.0865, 0.0121, 0.0214, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 7, batch: 39/198] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.1972e-02, 9.8254e-01, 2.0330e-03, 1.7106e-06, 1.7084e-03, 2.3547e-07,
        1.7484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.345

[Epoch: 7, batch: 78/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1122, 0.5538, 0.0478, 0.0908, 0.0722, 0.0482, 0.0749],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.044

[Epoch: 7, batch: 117/198] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7626, 0.1070, 0.0746, 0.0429, 0.0019, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.048

[Epoch: 7, batch: 156/198] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1660e-01, 5.2153e-03, 7.6880e-03, 2.5062e-05, 3.2637e-03, 1.5773e-03,
        8.6564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 7, batch: 195/198] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0418, 0.8504, 0.0038, 0.0694, 0.0060, 0.0127, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.008

[Epoch: 8, batch: 39/198] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.7796e-02, 9.7458e-01, 2.4971e-03, 1.9970e-06, 2.7596e-03, 5.0671e-07,
        2.3683e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.344

[Epoch: 8, batch: 78/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0769, 0.5956, 0.0989, 0.0721, 0.0429, 0.0385, 0.0751],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.061

[Epoch: 8, batch: 117/198] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7565, 0.1018, 0.0651, 0.0599, 0.0021, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 8, batch: 156/198] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.2607e-02, 8.7442e-03, 6.3992e-03, 1.4473e-05, 1.8528e-03, 1.5770e-03,
        8.9881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 8, batch: 195/198] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0721, 0.8419, 0.0037, 0.0494, 0.0110, 0.0119, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 9, batch: 39/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.7860e-03, 9.8276e-01, 1.9720e-03, 1.9486e-06, 3.8196e-03, 4.5269e-07,
        1.6565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.341

[Epoch: 9, batch: 78/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0727, 0.6175, 0.0414, 0.0841, 0.0648, 0.0526, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.057

[Epoch: 9, batch: 117/198] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.7794, 0.0999, 0.0706, 0.0404, 0.0016, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.047

[Epoch: 9, batch: 156/198] total loss per batch: 0.799
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([6.0644e-02, 3.8445e-03, 1.0407e-02, 1.3373e-05, 2.7558e-03, 9.8861e-04,
        9.2135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 9, batch: 195/198] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0270, 0.8639, 0.0048, 0.0791, 0.0073, 0.0101, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 10, batch: 39/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.5519e-02, 9.7704e-01, 2.3724e-03, 2.6246e-06, 2.8499e-03, 3.4700e-07,
        2.2200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.348

[Epoch: 10, batch: 78/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0583, 0.6413, 0.0528, 0.0806, 0.0688, 0.0470, 0.0512],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.069

[Epoch: 10, batch: 117/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0011, 0.7924, 0.1134, 0.0437, 0.0415, 0.0018, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.052

[Epoch: 10, batch: 156/198] total loss per batch: 0.796
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.4838e-02, 5.1377e-03, 6.7028e-03, 1.6197e-05, 1.8253e-03, 2.6027e-03,
        9.2888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 10, batch: 195/198] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0449, 0.8618, 0.0036, 0.0610, 0.0052, 0.0128, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 11, batch: 39/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.5679e-03, 9.8207e-01, 2.2049e-03, 1.7024e-06, 4.1131e-03, 5.7777e-07,
        2.0369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.356

[Epoch: 11, batch: 78/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1012, 0.4877, 0.0620, 0.1115, 0.0780, 0.0759, 0.0838],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.077

[Epoch: 11, batch: 117/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7730, 0.1075, 0.0661, 0.0385, 0.0022, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.048

[Epoch: 11, batch: 156/198] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.9930e-02, 5.3504e-03, 9.2284e-03, 6.2537e-06, 2.6740e-03, 1.7984e-03,
        9.2101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 11, batch: 195/198] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0549, 0.8375, 0.0055, 0.0792, 0.0052, 0.0097, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 12, batch: 39/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.3684e-02, 9.7882e-01, 2.9367e-03, 3.5733e-06, 2.9585e-03, 4.4821e-07,
        1.5955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.386

[Epoch: 12, batch: 78/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0806, 0.5434, 0.1012, 0.0876, 0.0695, 0.0602, 0.0575],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.093

[Epoch: 12, batch: 117/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.7499, 0.1190, 0.0856, 0.0357, 0.0020, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.026

[Epoch: 12, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3275e-01, 9.0378e-03, 9.6985e-03, 1.9393e-05, 3.8701e-03, 3.2680e-03,
        8.4136e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 12, batch: 195/198] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0340, 0.9082, 0.0039, 0.0371, 0.0044, 0.0080, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 13, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.1411e-02, 9.8103e-01, 1.9769e-03, 2.1626e-06, 3.6305e-03, 4.8589e-07,
        1.9514e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 13, batch: 78/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0513, 0.7467, 0.0247, 0.0596, 0.0440, 0.0319, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.070

[Epoch: 13, batch: 117/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7667, 0.1381, 0.0505, 0.0320, 0.0024, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.042

[Epoch: 13, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3795e-01, 6.3738e-03, 1.0894e-02, 1.4146e-05, 3.1526e-03, 2.8102e-03,
        8.3881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 13, batch: 195/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0986, 0.7764, 0.0061, 0.0904, 0.0037, 0.0143, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.029

[Epoch: 14, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([8.7459e-03, 9.8454e-01, 2.4486e-03, 2.7573e-06, 2.6198e-03, 3.5281e-07,
        1.6462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.379

[Epoch: 14, batch: 78/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0817, 0.6672, 0.0295, 0.0598, 0.0661, 0.0286, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.079

[Epoch: 14, batch: 117/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.7791, 0.0850, 0.0904, 0.0337, 0.0022, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 14, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([2.6016e-01, 1.4904e-02, 1.7392e-02, 2.8484e-05, 8.0067e-03, 8.1555e-03,
        6.9135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 14, batch: 195/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0164, 0.9282, 0.0028, 0.0376, 0.0026, 0.0084, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.025

[Epoch: 15, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.2232e-02, 9.7893e-01, 2.1340e-03, 3.0537e-06, 5.0947e-03, 6.5811e-07,
        1.6009e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 15, batch: 78/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1095, 0.5269, 0.0844, 0.0890, 0.0672, 0.0742, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 15, batch: 117/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7664, 0.1418, 0.0389, 0.0430, 0.0020, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 15, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.1330e-02, 8.1167e-03, 9.5711e-03, 7.7670e-06, 9.6086e-04, 2.6181e-03,
        8.9740e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 15, batch: 195/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0814, 0.7895, 0.0078, 0.0884, 0.0080, 0.0163, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 16, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.7104e-03, 9.8168e-01, 2.5509e-03, 4.0693e-06, 4.0071e-03, 2.9190e-07,
        2.0493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.387

[Epoch: 16, batch: 78/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0926, 0.4292, 0.0558, 0.1585, 0.0949, 0.0985, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 16, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7514, 0.1012, 0.1023, 0.0344, 0.0024, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 16, batch: 156/198] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8055e-02, 9.6253e-03, 1.1284e-02, 9.0627e-06, 2.4551e-03, 1.7327e-03,
        8.8684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.002

[Epoch: 16, batch: 195/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0344, 0.8561, 0.0057, 0.0735, 0.0047, 0.0140, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 17, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([7.7847e-03, 9.8374e-01, 2.6921e-03, 2.1726e-06, 3.4564e-03, 5.7760e-07,
        2.3274e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.371

[Epoch: 17, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0894, 0.6272, 0.0431, 0.0579, 0.0654, 0.0650, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 17, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7870, 0.1030, 0.0567, 0.0430, 0.0022, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 17, batch: 156/198] total loss per batch: 0.788
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0404e-01, 1.0149e-02, 1.1967e-02, 9.4405e-06, 2.0941e-03, 2.8541e-03,
        8.6888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 17, batch: 195/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0367, 0.8802, 0.0029, 0.0606, 0.0027, 0.0121, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 18, batch: 39/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6247e-03, 9.7998e-01, 2.2176e-03, 3.6655e-06, 8.9635e-03, 9.2685e-07,
        2.2075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.400

[Epoch: 18, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1138, 0.6400, 0.0461, 0.0631, 0.0473, 0.0227, 0.0669],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 18, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.7358, 0.1298, 0.0862, 0.0365, 0.0020, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 18, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5577e-02, 8.2883e-03, 1.0181e-02, 9.5699e-06, 2.4040e-03, 3.2498e-03,
        8.9029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.002

[Epoch: 18, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0494, 0.8658, 0.0042, 0.0597, 0.0038, 0.0109, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 19, batch: 39/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.4843e-03, 9.8432e-01, 2.5761e-03, 3.0431e-06, 4.6638e-03, 6.1894e-07,
        1.9483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.403

[Epoch: 19, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0761, 0.6318, 0.0685, 0.0693, 0.0662, 0.0342, 0.0540],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.097

[Epoch: 19, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7738, 0.1246, 0.0524, 0.0401, 0.0024, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 19, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.4285e-02, 9.6937e-03, 1.0570e-02, 1.0838e-05, 2.2628e-03, 2.9637e-03,
        9.0021e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 19, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0646, 0.8471, 0.0037, 0.0608, 0.0052, 0.0137, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 20, batch: 39/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([7.2261e-03, 9.8469e-01, 1.4564e-03, 3.6009e-06, 4.1227e-03, 6.6791e-07,
        2.4981e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.386

[Epoch: 20, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1331, 0.4761, 0.0460, 0.1008, 0.0810, 0.0931, 0.0699],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.119

[Epoch: 20, batch: 117/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7535, 0.0801, 0.1173, 0.0381, 0.0024, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 20, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3540e-02, 1.1647e-02, 1.0746e-02, 7.9793e-06, 1.8234e-03, 2.6253e-03,
        8.7961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.004

[Epoch: 20, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0356, 0.8188, 0.0048, 0.1112, 0.0044, 0.0202, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 21, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([8.7374e-03, 9.7846e-01, 3.4327e-03, 4.7168e-06, 6.9961e-03, 3.6884e-07,
        2.3683e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.398

[Epoch: 21, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0557, 0.6482, 0.0414, 0.0960, 0.0582, 0.0567, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.091

[Epoch: 21, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7912, 0.1241, 0.0389, 0.0364, 0.0021, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.024

[Epoch: 21, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.2182e-02, 7.0709e-03, 1.2438e-02, 7.6893e-06, 2.4378e-03, 3.8996e-03,
        8.8196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 21, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0298, 0.9041, 0.0050, 0.0457, 0.0027, 0.0086, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 22, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.8207e-03, 9.7552e-01, 3.1178e-03, 5.8836e-06, 5.5229e-03, 1.7027e-06,
        6.0138e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 22, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0829, 0.6861, 0.0474, 0.0632, 0.0475, 0.0168, 0.0561],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 22, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7606, 0.1002, 0.0911, 0.0401, 0.0022, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.044

[Epoch: 22, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5998e-02, 4.2997e-03, 9.2415e-03, 8.0185e-06, 3.6254e-03, 1.9242e-03,
        8.9490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.001

[Epoch: 22, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0886, 0.7863, 0.0070, 0.0889, 0.0075, 0.0125, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.013

[Epoch: 23, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.1727e-03, 9.8215e-01, 2.5872e-03, 4.1493e-06, 7.6049e-03, 6.0297e-07,
        2.4831e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.444

[Epoch: 23, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0548, 0.5791, 0.0735, 0.0737, 0.0722, 0.0798, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 23, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7266, 0.1544, 0.0689, 0.0393, 0.0023, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 23, batch: 156/198] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1352e-01, 6.8900e-03, 6.8754e-03, 8.1434e-06, 3.6949e-03, 3.5203e-03,
        8.6549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 23, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0178, 0.9321, 0.0033, 0.0339, 0.0018, 0.0094, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 24, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9675e-03, 9.8529e-01, 3.1608e-03, 3.8609e-06, 5.2816e-03, 3.9724e-07,
        2.2968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.398

[Epoch: 24, batch: 78/198] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0899, 0.5212, 0.0592, 0.0886, 0.0934, 0.0768, 0.0709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 24, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.7875, 0.0966, 0.0669, 0.0372, 0.0022, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.046

[Epoch: 24, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3045e-02, 6.4922e-03, 1.4113e-02, 9.6645e-06, 2.8883e-03, 2.0692e-03,
        8.8138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 24, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1053, 0.7657, 0.0056, 0.0981, 0.0060, 0.0132, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 25, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.0367e-02, 9.7540e-01, 3.5397e-03, 5.8658e-06, 7.2250e-03, 1.8569e-06,
        3.4628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 25, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0905, 0.6167, 0.0378, 0.0991, 0.0415, 0.0546, 0.0597],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 25, batch: 117/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7631, 0.1204, 0.0777, 0.0292, 0.0022, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.042

[Epoch: 25, batch: 156/198] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0893e-02, 7.3494e-03, 9.8779e-03, 1.1293e-05, 6.0664e-03, 3.3197e-03,
        8.8248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 25, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0166, 0.9086, 0.0044, 0.0517, 0.0051, 0.0087, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 26, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.7489e-03, 9.8642e-01, 1.8921e-03, 4.9641e-06, 4.2473e-03, 7.6002e-07,
        3.6870e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.383

[Epoch: 26, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0777, 0.6517, 0.0456, 0.0573, 0.0672, 0.0237, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.086

[Epoch: 26, batch: 117/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.7703, 0.1300, 0.0493, 0.0414, 0.0020, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 26, batch: 156/198] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0893e-01, 8.4180e-03, 1.1462e-02, 7.8601e-06, 3.0216e-03, 3.5668e-03,
        8.6459e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 26, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0369, 0.8282, 0.0065, 0.1011, 0.0070, 0.0143, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 27, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.4234e-03, 9.8371e-01, 2.4991e-03, 3.9736e-06, 6.3802e-03, 1.6838e-06,
        2.9859e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.363

[Epoch: 27, batch: 78/198] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0636, 0.6051, 0.0603, 0.0835, 0.0760, 0.0632, 0.0482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.116

[Epoch: 27, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7612, 0.1019, 0.1012, 0.0247, 0.0021, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.021

[Epoch: 27, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2245e-01, 6.7406e-03, 1.4533e-02, 1.0023e-05, 1.7707e-03, 2.2020e-03,
        8.5230e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 27, batch: 195/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0476, 0.8770, 0.0055, 0.0515, 0.0052, 0.0092, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.001

[Epoch: 28, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.6423e-03, 9.8100e-01, 2.4648e-03, 5.4855e-06, 5.7055e-03, 1.7244e-06,
        5.1826e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.408

[Epoch: 28, batch: 78/198] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0956, 0.6143, 0.0534, 0.0613, 0.0661, 0.0615, 0.0479],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

