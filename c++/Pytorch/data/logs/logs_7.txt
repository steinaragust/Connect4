Training set samples: 6539
Batch size: 32
[Epoch: 1, batch: 41/205] total loss per batch: 0.825
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([2.9750e-01, 1.8897e-01, 7.1318e-06, 3.1862e-01, 3.3206e-06, 5.0743e-06,
        1.9490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 0.020

[Epoch: 1, batch: 82/205] total loss per batch: 0.837
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0025, 0.0026, 0.0085, 0.0045, 0.0047, 0.9719, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 1, batch: 123/205] total loss per batch: 0.861
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0561, 0.0207, 0.4526, 0.2830, 0.1207, 0.0466, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.008

[Epoch: 1, batch: 164/205] total loss per batch: 0.853
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5618e-01, 8.4378e-01, 5.6043e-06, 3.3192e-06, 7.2491e-06, 1.2125e-05,
        8.1721e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.258

[Epoch: 1, batch: 205/205] total loss per batch: 0.804
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0081, 0.0674, 0.2669, 0.6028, 0.0115, 0.0359],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 2, batch: 41/205] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([2.8993e-01, 1.0228e-01, 1.1984e-05, 4.1147e-01, 1.9757e-06, 2.7853e-06,
        1.9630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 0.052

[Epoch: 2, batch: 82/205] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0030, 0.0045, 0.0040, 0.0042, 0.9754, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 2, batch: 123/205] total loss per batch: 0.669
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0196, 0.0146, 0.5873, 0.3068, 0.0330, 0.0202, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 2, batch: 164/205] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7292e-02, 9.4270e-01, 2.4745e-06, 3.0905e-07, 1.2466e-06, 6.0132e-06,
        9.9807e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.429

[Epoch: 2, batch: 205/205] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0024, 0.0088, 0.8037, 0.1728, 0.0061, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 3, batch: 41/205] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.1922e-02, 4.1730e-02, 3.8891e-05, 8.1749e-01, 4.0958e-06, 9.7245e-06,
        7.8807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.196

[Epoch: 3, batch: 82/205] total loss per batch: 0.607
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0041, 0.0052, 0.0058, 0.0030, 0.9726, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 3, batch: 123/205] total loss per batch: 0.600
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0215, 0.0235, 0.3079, 0.4047, 0.1919, 0.0158, 0.0347],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 3, batch: 164/205] total loss per batch: 0.622
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8935e-02, 9.6106e-01, 3.4327e-06, 3.4497e-07, 7.6995e-07, 9.6589e-07,
        3.4494e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.555

[Epoch: 3, batch: 205/205] total loss per batch: 0.594
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0056, 0.0153, 0.4599, 0.5000, 0.0087, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 4, batch: 41/205] total loss per batch: 0.587
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.9244e-02, 8.1572e-02, 2.4118e-05, 7.0162e-01, 2.8155e-06, 2.1920e-06,
        1.5754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.245

[Epoch: 4, batch: 82/205] total loss per batch: 0.597
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0072, 0.0040, 0.0061, 0.0036, 0.0048, 0.9697, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 4, batch: 123/205] total loss per batch: 0.583
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0068, 0.0153, 0.4323, 0.0724, 0.4210, 0.0348, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 4, batch: 164/205] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0071e-02, 9.8992e-01, 1.7141e-06, 1.3549e-07, 3.7376e-07, 1.7524e-06,
        3.6348e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.468

[Epoch: 4, batch: 205/205] total loss per batch: 0.583
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0043, 0.0061, 0.7613, 0.2133, 0.0067, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 5, batch: 41/205] total loss per batch: 0.573
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([2.8532e-02, 2.7887e-02, 1.0348e-04, 8.9298e-01, 6.4532e-06, 5.2836e-06,
        5.0481e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.567

[Epoch: 5, batch: 82/205] total loss per batch: 0.591
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0068, 0.0045, 0.0103, 0.0062, 0.0041, 0.9640, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 5, batch: 123/205] total loss per batch: 0.574
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0230, 0.0318, 0.2498, 0.4070, 0.2021, 0.0640, 0.0222],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 5, batch: 164/205] total loss per batch: 0.596
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5256e-02, 9.8474e-01, 1.7120e-06, 6.8560e-07, 1.0892e-06, 1.3231e-06,
        5.5317e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.517

[Epoch: 5, batch: 205/205] total loss per batch: 0.569
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0059, 0.0075, 0.5603, 0.4090, 0.0064, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 6, batch: 41/205] total loss per batch: 0.564
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.4931e-02, 5.9276e-02, 3.2488e-05, 7.4048e-01, 9.3059e-06, 1.3823e-05,
        1.2526e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.553

[Epoch: 6, batch: 82/205] total loss per batch: 0.584
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0031, 0.0045, 0.0042, 0.0048, 0.9755, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 6, batch: 123/205] total loss per batch: 0.567
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0164, 0.0195, 0.3356, 0.3938, 0.1215, 0.0852, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 6, batch: 164/205] total loss per batch: 0.587
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0911e-02, 9.8909e-01, 7.8043e-07, 1.7354e-07, 3.7780e-07, 9.9703e-07,
        5.3323e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.453

[Epoch: 6, batch: 205/205] total loss per batch: 0.557
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0025, 0.0032, 0.6904, 0.2915, 0.0059, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 7, batch: 41/205] total loss per batch: 0.563
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.7468e-02, 7.7338e-02, 6.2230e-05, 7.3584e-01, 8.3239e-06, 1.6762e-05,
        1.0926e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.662

[Epoch: 7, batch: 82/205] total loss per batch: 0.575
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0037, 0.0048, 0.0050, 0.0043, 0.9742, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 7, batch: 123/205] total loss per batch: 0.560
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0081, 0.0105, 0.4407, 0.3772, 0.1067, 0.0429, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 7, batch: 164/205] total loss per batch: 0.582
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0296e-02, 9.7970e-01, 4.3159e-06, 8.4232e-07, 5.7090e-07, 1.0893e-06,
        4.4553e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.398

[Epoch: 7, batch: 205/205] total loss per batch: 0.551
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0036, 0.0109, 0.5628, 0.4106, 0.0037, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 8, batch: 41/205] total loss per batch: 0.556
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.9107e-02, 4.6328e-02, 5.5212e-05, 8.5931e-01, 6.5151e-06, 1.4509e-05,
        4.5180e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.556

[Epoch: 8, batch: 82/205] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0072, 0.0039, 0.0068, 0.0046, 0.0046, 0.9683, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 8, batch: 123/205] total loss per batch: 0.555
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0053, 0.0104, 0.3831, 0.3277, 0.1840, 0.0650, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 8, batch: 164/205] total loss per batch: 0.578
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.3799e-03, 9.9262e-01, 8.6754e-07, 9.1192e-08, 6.9332e-08, 2.2596e-07,
        2.5553e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.484

[Epoch: 8, batch: 205/205] total loss per batch: 0.548
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0019, 0.0051, 0.6861, 0.2923, 0.0061, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 9, batch: 41/205] total loss per batch: 0.551
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.5050e-02, 3.2834e-02, 4.2054e-04, 8.8039e-01, 1.0764e-05, 2.7612e-05,
        5.1264e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 9, batch: 82/205] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0034, 0.0035, 0.0050, 0.0035, 0.9778, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 9, batch: 123/205] total loss per batch: 0.552
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0088, 0.0093, 0.2823, 0.4967, 0.1331, 0.0474, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 9, batch: 164/205] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6985e-02, 9.7301e-01, 3.5257e-06, 1.0607e-06, 6.9673e-07, 2.7345e-06,
        5.8169e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.464

[Epoch: 9, batch: 205/205] total loss per batch: 0.546
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0037, 0.0064, 0.6263, 0.3495, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 10, batch: 41/205] total loss per batch: 0.551
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([1.0420e-01, 1.0831e-01, 5.3926e-05, 6.4521e-01, 2.1605e-05, 1.6944e-05,
        1.4218e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.472

[Epoch: 10, batch: 82/205] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0034, 0.0059, 0.0042, 0.0034, 0.9745, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 10, batch: 123/205] total loss per batch: 0.552
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0058, 0.0112, 0.3448, 0.4584, 0.1253, 0.0347, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 10, batch: 164/205] total loss per batch: 0.572
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.8231e-03, 9.9017e-01, 4.3414e-07, 2.7342e-07, 3.6863e-07, 1.2844e-06,
        4.6748e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.510

[Epoch: 10, batch: 205/205] total loss per batch: 0.544
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.0027, 0.0046, 0.7840, 0.1979, 0.0051, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 11, batch: 41/205] total loss per batch: 0.547
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.6430e-02, 2.7542e-02, 3.3779e-04, 9.0237e-01, 1.2442e-05, 2.0340e-05,
        3.3284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.673

[Epoch: 11, batch: 82/205] total loss per batch: 0.563
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0035, 0.0047, 0.0038, 0.0041, 0.9746, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 11, batch: 123/205] total loss per batch: 0.549
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0081, 0.0085, 0.2601, 0.5252, 0.1221, 0.0547, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 11, batch: 164/205] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.6555e-03, 9.9034e-01, 5.3508e-07, 1.6961e-07, 1.4385e-07, 2.3657e-07,
        1.3576e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.514

[Epoch: 11, batch: 205/205] total loss per batch: 0.542
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0038, 0.0068, 0.4878, 0.4881, 0.0058, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 12, batch: 41/205] total loss per batch: 0.542
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.9980e-02, 4.6001e-02, 1.8031e-04, 8.4584e-01, 1.1911e-05, 1.7541e-05,
        4.7971e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.698

[Epoch: 12, batch: 82/205] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0041, 0.0046, 0.0051, 0.0039, 0.9734, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 12, batch: 123/205] total loss per batch: 0.540
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0077, 0.0086, 0.2912, 0.4457, 0.1764, 0.0484, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 12, batch: 164/205] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0866e-02, 9.8913e-01, 8.9634e-07, 2.4097e-07, 1.0379e-07, 3.6232e-07,
        1.9340e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.497

[Epoch: 12, batch: 205/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.0040, 0.0040, 0.7236, 0.2515, 0.0043, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 13, batch: 41/205] total loss per batch: 0.539
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([9.7024e-02, 6.1418e-02, 1.5298e-04, 7.5387e-01, 1.1800e-05, 1.6258e-05,
        8.7503e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.571

[Epoch: 13, batch: 82/205] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0036, 0.0045, 0.0035, 0.0041, 0.9757, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 13, batch: 123/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0063, 0.0073, 0.3873, 0.3942, 0.1260, 0.0576, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 13, batch: 164/205] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.6033e-03, 9.9340e-01, 8.1752e-07, 1.2822e-07, 1.1646e-07, 2.4246e-07,
        1.3039e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.523

[Epoch: 13, batch: 205/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0048, 0.0059, 0.6537, 0.3189, 0.0045, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 14, batch: 41/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.5094e-02, 2.6166e-02, 7.5046e-05, 9.0270e-01, 6.8313e-06, 1.2539e-05,
        3.5948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.666

[Epoch: 14, batch: 82/205] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0040, 0.0043, 0.0051, 0.0038, 0.9738, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 14, batch: 123/205] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0083, 0.0092, 0.3094, 0.4420, 0.1521, 0.0588, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 14, batch: 164/205] total loss per batch: 0.558
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6283e-03, 9.9137e-01, 7.8549e-07, 3.4486e-07, 1.9898e-07, 2.5137e-07,
        1.6905e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.502

[Epoch: 14, batch: 205/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0058, 0.0068, 0.6331, 0.3395, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 15, batch: 41/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5917e-02, 6.7120e-02, 5.8706e-05, 7.9343e-01, 9.8955e-06, 5.9977e-06,
        7.3457e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.560

[Epoch: 15, batch: 82/205] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0041, 0.0046, 0.0048, 0.0057, 0.9707, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 15, batch: 123/205] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0066, 0.0075, 0.3523, 0.4164, 0.1380, 0.0560, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 15, batch: 164/205] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2145e-03, 9.9178e-01, 1.5403e-06, 2.0700e-07, 1.3883e-07, 3.1373e-07,
        2.3455e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.455

[Epoch: 15, batch: 205/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0044, 0.0037, 0.6872, 0.2880, 0.0050, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 16, batch: 41/205] total loss per batch: 0.539
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.2827e-02, 2.4776e-02, 9.7751e-05, 8.4790e-01, 9.5164e-06, 9.9419e-06,
        6.4383e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.660

[Epoch: 16, batch: 82/205] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0062, 0.0042, 0.0055, 0.0046, 0.0033, 0.9721, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 16, batch: 123/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0078, 0.0082, 0.3226, 0.4623, 0.1273, 0.0485, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 16, batch: 164/205] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2578e-03, 9.9474e-01, 1.0638e-06, 4.1869e-07, 2.7022e-07, 3.3159e-07,
        1.6091e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.514

[Epoch: 16, batch: 205/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0041, 0.0084, 0.6026, 0.3700, 0.0050, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 17, batch: 41/205] total loss per batch: 0.540
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.0858e-02, 6.5505e-02, 4.5542e-05, 7.9696e-01, 1.5996e-05, 6.2031e-06,
        5.6604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.577

[Epoch: 17, batch: 82/205] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0033, 0.0043, 0.0044, 0.0041, 0.9763, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.001

[Epoch: 17, batch: 123/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0074, 0.0073, 0.3446, 0.3753, 0.1602, 0.0773, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 17, batch: 164/205] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9493e-02, 9.8050e-01, 1.4643e-06, 1.9062e-07, 1.1235e-07, 2.1962e-07,
        1.7663e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.486

[Epoch: 17, batch: 205/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0041, 0.0039, 0.7187, 0.2552, 0.0061, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 18, batch: 41/205] total loss per batch: 0.542
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.2421e-02, 2.6956e-02, 5.6475e-05, 8.4409e-01, 1.0287e-05, 1.1046e-05,
        6.6453e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.632

[Epoch: 18, batch: 82/205] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0041, 0.0061, 0.0044, 0.0038, 0.9728, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 18, batch: 123/205] total loss per batch: 0.542
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0078, 0.0091, 0.3154, 0.4575, 0.1200, 0.0666, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 18, batch: 164/205] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8025e-03, 9.9519e-01, 1.4904e-06, 3.7499e-07, 5.4316e-07, 2.6803e-07,
        2.0134e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.501

[Epoch: 18, batch: 205/205] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0043, 0.0057, 0.5967, 0.3793, 0.0049, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 19, batch: 41/205] total loss per batch: 0.545
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.0714e-02, 4.3639e-02, 8.0061e-05, 8.3159e-01, 2.3724e-05, 1.4075e-05,
        5.3939e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.453

[Epoch: 19, batch: 82/205] total loss per batch: 0.560
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0037, 0.0039, 0.0041, 0.0046, 0.9738, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 19, batch: 123/205] total loss per batch: 0.546
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0105, 0.0093, 0.3309, 0.4328, 0.1329, 0.0533, 0.0302],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 19, batch: 164/205] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1639e-03, 9.9383e-01, 9.6878e-07, 2.6170e-07, 1.0076e-07, 2.2040e-07,
        1.4691e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.532

[Epoch: 19, batch: 205/205] total loss per batch: 0.539
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0049, 0.0077, 0.6963, 0.2717, 0.0068, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 20, batch: 41/205] total loss per batch: 0.545
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.8742e-02, 4.3542e-02, 3.5417e-05, 8.1494e-01, 1.6248e-05, 8.9336e-06,
        7.2720e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.726

[Epoch: 20, batch: 82/205] total loss per batch: 0.561
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0083, 0.0049, 0.0075, 0.0053, 0.0054, 0.9630, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 20, batch: 123/205] total loss per batch: 0.549
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0074, 0.0046, 0.4239, 0.3465, 0.1330, 0.0657, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 20, batch: 164/205] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7502e-03, 9.9225e-01, 1.7287e-06, 2.2078e-07, 1.8995e-07, 1.2491e-07,
        1.7156e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.441

[Epoch: 20, batch: 205/205] total loss per batch: 0.542
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0048, 0.0061, 0.5564, 0.4124, 0.0082, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 21, batch: 41/205] total loss per batch: 0.547
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3280e-02, 7.0553e-02, 9.9410e-05, 7.8933e-01, 1.9047e-05, 1.7309e-05,
        7.6696e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.599

[Epoch: 21, batch: 82/205] total loss per batch: 0.563
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0077, 0.0041, 0.0074, 0.0051, 0.0046, 0.9665, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 21, batch: 123/205] total loss per batch: 0.549
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0079, 0.0083, 0.2289, 0.4940, 0.1642, 0.0637, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 21, batch: 164/205] total loss per batch: 0.567
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.5171e-03, 9.9048e-01, 1.3913e-06, 5.3222e-07, 6.2724e-07, 2.1153e-07,
        1.3901e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.457

[Epoch: 21, batch: 205/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.0042, 0.0063, 0.6806, 0.2943, 0.0055, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 22, batch: 41/205] total loss per batch: 0.541
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.9017e-02, 3.6214e-02, 7.1177e-05, 8.2095e-01, 1.9245e-05, 9.7102e-06,
        7.3720e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.541

[Epoch: 22, batch: 82/205] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0035, 0.0048, 0.0042, 0.0041, 0.9750, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 22, batch: 123/205] total loss per batch: 0.540
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0066, 0.0069, 0.3136, 0.4776, 0.1307, 0.0430, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 22, batch: 164/205] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2880e-03, 9.9471e-01, 1.1543e-06, 3.5780e-07, 2.4280e-07, 2.0839e-07,
        9.6532e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.468

[Epoch: 22, batch: 205/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0033, 0.0049, 0.6970, 0.2793, 0.0057, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 23, batch: 41/205] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.8672e-02, 4.1273e-02, 8.5743e-05, 8.6846e-01, 2.0125e-05, 1.0767e-05,
        5.1475e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.640

[Epoch: 23, batch: 82/205] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0037, 0.0046, 0.0052, 0.0044, 0.9727, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 23, batch: 123/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0068, 0.0078, 0.3042, 0.4470, 0.1392, 0.0717, 0.0233],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 23, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.8397e-03, 9.9316e-01, 1.1733e-06, 3.8359e-07, 2.5972e-07, 2.2243e-07,
        1.3570e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.516

[Epoch: 23, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0036, 0.0041, 0.5978, 0.3790, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 24, batch: 41/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([1.0537e-01, 5.2609e-02, 8.1625e-05, 7.5367e-01, 1.7129e-05, 1.0006e-05,
        8.8247e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.646

[Epoch: 24, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0038, 0.0049, 0.0043, 0.0041, 0.9740, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 24, batch: 123/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0060, 0.0059, 0.3419, 0.4378, 0.1222, 0.0642, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 24, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4851e-03, 9.9651e-01, 3.7027e-07, 1.3700e-07, 1.1114e-07, 9.9494e-08,
        7.8727e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.492

[Epoch: 24, batch: 205/205] total loss per batch: 0.527
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0048, 0.7071, 0.2681, 0.0045, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 25, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([2.9171e-02, 2.8729e-02, 3.8482e-05, 9.0596e-01, 1.2927e-05, 8.6162e-06,
        3.6076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.584

[Epoch: 25, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0046, 0.0051, 0.0046, 0.9711, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 25, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0070, 0.0061, 0.3032, 0.4432, 0.1500, 0.0690, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 25, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0661e-03, 9.9393e-01, 5.7823e-07, 1.8779e-07, 1.2058e-07, 1.0096e-07,
        7.3388e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.471

[Epoch: 25, batch: 205/205] total loss per batch: 0.527
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0049, 0.6304, 0.3433, 0.0049, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 26, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.4489e-02, 7.6292e-02, 4.2960e-05, 7.7346e-01, 1.0220e-05, 5.0918e-06,
        6.5705e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.619

[Epoch: 26, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.0055, 0.0052, 0.0046, 0.9705, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 26, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0062, 0.0059, 0.3369, 0.4314, 0.1319, 0.0619, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 26, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1674e-03, 9.9383e-01, 6.6074e-07, 2.1194e-07, 1.9121e-07, 1.1044e-07,
        7.1299e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.528

[Epoch: 26, batch: 205/205] total loss per batch: 0.527
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0037, 0.0052, 0.6302, 0.3456, 0.0042, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 27, batch: 41/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.9235e-02, 2.5003e-02, 3.6589e-05, 8.6978e-01, 1.2374e-05, 6.4561e-06,
        5.5930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.594

[Epoch: 27, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0041, 0.0051, 0.0050, 0.0043, 0.9718, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 27, batch: 123/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0063, 0.0056, 0.3423, 0.4403, 0.1321, 0.0527, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 27, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9823e-03, 9.9502e-01, 5.0544e-07, 1.5816e-07, 1.3128e-07, 6.6971e-08,
        5.1685e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.473

[Epoch: 27, batch: 205/205] total loss per batch: 0.529
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0040, 0.0060, 0.7320, 0.2428, 0.0045, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 28, batch: 41/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.6217e-02, 5.1286e-02, 6.3412e-05, 7.8177e-01, 2.1338e-05, 8.4223e-06,
        8.0634e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.589

[Epoch: 28, batch: 82/205] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0044, 0.0061, 0.0052, 0.0045, 0.9697, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 28, batch: 123/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0048, 0.0058, 0.3347, 0.4301, 0.1251, 0.0732, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 28, batch: 164/205] total loss per batch: 0.557
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4762e-03, 9.9552e-01, 6.8277e-07, 2.4873e-07, 1.8683e-07, 9.8177e-08,
        7.3123e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.504

[Epoch: 28, batch: 205/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0045, 0.0044, 0.6144, 0.3611, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 29, batch: 41/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.6222e-02, 3.8711e-02, 2.8583e-05, 8.8973e-01, 8.1152e-06, 6.7918e-06,
        3.5292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.616

[Epoch: 29, batch: 82/205] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0062, 0.0060, 0.0049, 0.9684, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 29, batch: 123/205] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0059, 0.3699, 0.3986, 0.1412, 0.0574, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 29, batch: 164/205] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7743e-03, 9.9422e-01, 1.1423e-06, 2.4338e-07, 1.6990e-07, 1.4093e-07,
        7.2116e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.486

[Epoch: 29, batch: 205/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0065, 0.0076, 0.5956, 0.3739, 0.0044, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 30, batch: 41/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.6895e-02, 4.4936e-02, 1.1598e-04, 8.0907e-01, 1.6436e-05, 1.3049e-05,
        5.8955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.625

[Epoch: 30, batch: 82/205] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0060, 0.0043, 0.0057, 0.0050, 0.0056, 0.9682, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 30, batch: 123/205] total loss per batch: 0.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0095, 0.0068, 0.2751, 0.4636, 0.1526, 0.0642, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 30, batch: 164/205] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4869e-03, 9.9151e-01, 1.5903e-06, 5.4640e-07, 6.6956e-07, 1.7308e-07,
        3.6164e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.516

[Epoch: 30, batch: 205/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0041, 0.0037, 0.7568, 0.2200, 0.0054, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 31, batch: 41/205] total loss per batch: 0.540
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.3704e-02, 5.2071e-02, 8.9364e-05, 7.9378e-01, 1.9283e-05, 2.3826e-05,
        1.0031e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.606

[Epoch: 31, batch: 82/205] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0041, 0.0058, 0.0045, 0.0040, 0.9726, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 31, batch: 123/205] total loss per batch: 0.538
Policy (actual, predicted): 3 2
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0057, 0.0058, 0.3968, 0.3797, 0.1399, 0.0514, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 31, batch: 164/205] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9056e-03, 9.9609e-01, 1.4482e-06, 1.7871e-07, 1.0142e-07, 1.0582e-07,
        9.2298e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.463

[Epoch: 31, batch: 205/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0065, 0.0045, 0.6268, 0.3462, 0.0047, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 32, batch: 41/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.7850e-02, 3.9791e-02, 9.5305e-05, 8.2944e-01, 1.9215e-05, 1.3265e-05,
        4.2791e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.500

[Epoch: 32, batch: 82/205] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0044, 0.0049, 0.0056, 0.0043, 0.9710, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 32, batch: 123/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0066, 0.0052, 0.3348, 0.4190, 0.1327, 0.0749, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 32, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6985e-03, 9.9530e-01, 8.8648e-07, 3.4301e-07, 1.8704e-07, 2.1518e-07,
        1.0174e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.511

[Epoch: 32, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0047, 0.6275, 0.3479, 0.0045, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 33, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.8926e-02, 4.4486e-02, 5.8185e-05, 8.4365e-01, 1.7607e-05, 8.9789e-06,
        6.2857e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.679

[Epoch: 33, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0036, 0.0046, 0.0041, 0.9740, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 33, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0063, 0.0057, 0.3121, 0.4679, 0.1260, 0.0592, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 33, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4270e-03, 9.9457e-01, 8.5762e-07, 2.2982e-07, 1.1088e-07, 1.1715e-07,
        8.8047e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.484

[Epoch: 33, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0055, 0.6800, 0.2939, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 34, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.2846e-02, 4.1906e-02, 5.0675e-05, 8.3036e-01, 1.3969e-05, 9.0750e-06,
        5.4816e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.631

[Epoch: 34, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0041, 0.0048, 0.0048, 0.0043, 0.9721, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 34, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0070, 0.0056, 0.3459, 0.4219, 0.1292, 0.0678, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 34, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8276e-03, 9.9517e-01, 9.0443e-07, 2.9893e-07, 1.5399e-07, 2.0125e-07,
        1.1178e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.489

[Epoch: 34, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.6673, 0.3072, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 35, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.0053e-02, 4.3363e-02, 3.8641e-05, 8.2641e-01, 1.1125e-05, 4.6538e-06,
        6.0125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 35, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0042, 0.0047, 0.0051, 0.0047, 0.9714, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 35, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0062, 0.0058, 0.3213, 0.4368, 0.1402, 0.0618, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 35, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.5861e-03, 9.9241e-01, 9.5659e-07, 2.8065e-07, 1.4908e-07, 1.1308e-07,
        1.0786e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.498

[Epoch: 35, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0043, 0.0045, 0.6251, 0.3524, 0.0043, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 36, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.6935e-02, 5.2404e-02, 2.5643e-05, 8.3598e-01, 1.2989e-05, 7.4313e-06,
        5.4638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.601

[Epoch: 36, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.0051, 0.0047, 0.0046, 0.9709, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 36, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0046, 0.3394, 0.4290, 0.1330, 0.0648, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 36, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3982e-03, 9.9560e-01, 1.0211e-06, 3.6589e-07, 2.8907e-07, 3.2927e-07,
        1.3738e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.521

[Epoch: 36, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0050, 0.6710, 0.3029, 0.0056, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 37, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.7400e-02, 3.8173e-02, 2.9623e-05, 8.1348e-01, 1.0110e-05, 6.1843e-06,
        7.0897e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.606

[Epoch: 37, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0040, 0.0048, 0.0052, 0.0043, 0.9717, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 37, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0069, 0.0061, 0.3212, 0.4294, 0.1490, 0.0596, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 37, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7209e-03, 9.9628e-01, 2.7736e-07, 1.5856e-07, 7.2151e-08, 5.4348e-08,
        6.2271e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.470

[Epoch: 37, batch: 205/205] total loss per batch: 0.527
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0054, 0.6949, 0.2813, 0.0042, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 38, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.7037e-02, 5.7976e-02, 4.8908e-05, 8.3358e-01, 1.0710e-05, 9.6181e-06,
        5.1335e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.570

[Epoch: 38, batch: 82/205] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0060, 0.0050, 0.0052, 0.0049, 0.0052, 0.9679, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 38, batch: 123/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0065, 0.0073, 0.3491, 0.4077, 0.1283, 0.0699, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 38, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6265e-03, 9.9537e-01, 1.3544e-06, 3.7403e-07, 3.4833e-07, 2.8478e-07,
        1.6423e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.530

[Epoch: 38, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0054, 0.0064, 0.6062, 0.3646, 0.0070, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 39, batch: 41/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.9840e-02, 2.5632e-02, 2.6100e-05, 8.8030e-01, 1.8579e-05, 8.0187e-06,
        5.4174e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.626

[Epoch: 39, batch: 82/205] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0041, 0.0053, 0.0049, 0.0039, 0.9732, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 39, batch: 123/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0060, 0.0051, 0.2922, 0.4778, 0.1488, 0.0497, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 39, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4106e-03, 9.9359e-01, 1.1357e-06, 4.7935e-07, 7.9863e-08, 5.9064e-08,
        3.9147e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.470

[Epoch: 39, batch: 205/205] total loss per batch: 0.529
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0056, 0.0044, 0.6648, 0.3087, 0.0040, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 40, batch: 41/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([1.0980e-01, 6.4401e-02, 5.2157e-05, 7.4086e-01, 9.6379e-06, 7.9765e-06,
        8.4871e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.539

[Epoch: 40, batch: 82/205] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0054, 0.0058, 0.0052, 0.9694, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 40, batch: 123/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0059, 0.3204, 0.4302, 0.1320, 0.0769, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 40, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7431e-03, 9.9325e-01, 1.5779e-06, 3.9024e-07, 5.0749e-07, 5.1102e-07,
        2.3698e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.498

[Epoch: 40, batch: 205/205] total loss per batch: 0.529
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0044, 0.0046, 0.6815, 0.2958, 0.0054, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 41, batch: 41/205] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.4858e-02, 3.8774e-02, 5.6068e-05, 8.6715e-01, 1.9578e-05, 1.8211e-05,
        4.9128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.646

[Epoch: 41, batch: 82/205] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0039, 0.0042, 0.0037, 0.0040, 0.9738, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 41, batch: 123/205] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0065, 0.0046, 0.3549, 0.4279, 0.1193, 0.0639, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 41, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1053e-03, 9.9489e-01, 5.5072e-07, 3.3875e-07, 1.2465e-07, 1.8013e-07,
        9.8569e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.484

[Epoch: 41, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0066, 0.0045, 0.6161, 0.3560, 0.0045, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 42, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.1077e-02, 4.8116e-02, 3.7746e-05, 8.1251e-01, 1.4573e-05, 9.9444e-06,
        6.8232e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.591

[Epoch: 42, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0041, 0.0049, 0.0052, 0.0042, 0.9721, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 42, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0053, 0.3108, 0.4480, 0.1418, 0.0624, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 42, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5061e-03, 9.9449e-01, 8.1579e-07, 3.4785e-07, 1.9141e-07, 2.4721e-07,
        1.3170e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.518

[Epoch: 42, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0047, 0.6551, 0.3203, 0.0048, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 43, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.9159e-02, 4.0915e-02, 4.4183e-05, 8.1201e-01, 1.3319e-05, 1.1933e-05,
        6.7844e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.610

[Epoch: 43, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0046, 0.0047, 0.0045, 0.9714, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 43, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0059, 0.0047, 0.3338, 0.4321, 0.1283, 0.0712, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 43, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6721e-03, 9.9533e-01, 5.8722e-07, 2.8841e-07, 1.0056e-07, 1.0520e-07,
        8.4741e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.493

[Epoch: 43, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0044, 0.6778, 0.2981, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 44, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.8191e-02, 4.1089e-02, 3.3195e-05, 8.5819e-01, 1.1606e-05, 9.2460e-06,
        5.2478e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.602

[Epoch: 44, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0045, 0.0052, 0.0054, 0.0046, 0.9698, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 44, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0059, 0.0054, 0.3151, 0.4396, 0.1448, 0.0638, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 44, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5003e-03, 9.9550e-01, 5.6643e-07, 2.3955e-07, 1.0496e-07, 1.0075e-07,
        8.7398e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.490

[Epoch: 44, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0049, 0.6399, 0.3340, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 45, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.1969e-02, 4.9079e-02, 3.0019e-05, 7.9951e-01, 1.0001e-05, 6.0397e-06,
        6.9400e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.586

[Epoch: 45, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0045, 0.0050, 0.0050, 0.0049, 0.9697, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 45, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0049, 0.3450, 0.4184, 0.1319, 0.0683, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 45, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9447e-03, 9.9205e-01, 5.8330e-07, 2.9962e-07, 1.5302e-07, 1.4168e-07,
        1.0692e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.501

[Epoch: 45, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0049, 0.6434, 0.3323, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 46, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.6755e-02, 3.4891e-02, 2.2506e-05, 8.6560e-01, 6.6699e-06, 4.0921e-06,
        5.2724e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.630

[Epoch: 46, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0042, 0.0050, 0.0048, 0.0040, 0.9723, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 46, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0048, 0.3342, 0.4305, 0.1382, 0.0625, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 46, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8295e-03, 9.9617e-01, 3.5833e-07, 2.1448e-07, 9.8567e-08, 8.5438e-08,
        7.6718e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.491

[Epoch: 46, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0053, 0.6909, 0.2835, 0.0049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 47, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.7083e-02, 5.4096e-02, 2.6154e-05, 7.9089e-01, 1.1283e-05, 8.2230e-06,
        7.7886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.576

[Epoch: 47, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0045, 0.0051, 0.0052, 0.0047, 0.9696, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 47, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0047, 0.3517, 0.4111, 0.1307, 0.0702, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 47, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1510e-03, 9.9385e-01, 6.0988e-07, 3.1376e-07, 1.7559e-07, 1.6822e-07,
        1.0788e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.506

[Epoch: 47, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0045, 0.0041, 0.6374, 0.3379, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 48, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.6503e-02, 4.8449e-02, 1.8456e-05, 8.4246e-01, 4.2121e-06, 4.0666e-06,
        5.2564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.643

[Epoch: 48, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0040, 0.0047, 0.0046, 0.0036, 0.9736, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 48, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0057, 0.0053, 0.3358, 0.4302, 0.1365, 0.0625, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 48, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7946e-03, 9.9620e-01, 3.4626e-07, 1.5975e-07, 5.3902e-08, 3.4978e-08,
        4.0622e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.462

[Epoch: 48, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.0059, 0.6254, 0.3485, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 49, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([8.3232e-02, 3.5865e-02, 2.8639e-05, 8.2098e-01, 1.7798e-05, 7.8876e-06,
        5.9869e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.553

[Epoch: 49, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0041, 0.0058, 0.0045, 0.0051, 0.9707, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 49, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0058, 0.0052, 0.3521, 0.4340, 0.1190, 0.0581, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 49, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1688e-03, 9.9383e-01, 7.8274e-07, 4.0990e-07, 2.0883e-07, 2.9643e-07,
        1.7048e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.520

[Epoch: 49, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0061, 0.7301, 0.2454, 0.0041, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 50, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([3.9526e-02, 4.2785e-02, 3.9383e-05, 8.4915e-01, 4.7013e-06, 6.5790e-06,
        6.8490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.689

[Epoch: 50, batch: 82/205] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0043, 0.0051, 0.0038, 0.9722, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 50, batch: 123/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0047, 0.0057, 0.2997, 0.4355, 0.1647, 0.0653, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 50, batch: 164/205] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0071e-03, 9.9599e-01, 4.4439e-07, 1.7483e-07, 6.9797e-08, 3.2918e-08,
        7.0314e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.446

[Epoch: 50, batch: 205/205] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0043, 0.6072, 0.3646, 0.0072, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 51, batch: 41/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.5142e-02, 4.4821e-02, 1.4024e-05, 8.0260e-01, 1.1341e-05, 6.7370e-06,
        7.7409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.549

[Epoch: 51, batch: 82/205] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0045, 0.0057, 0.0051, 0.0050, 0.9692, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 51, batch: 123/205] total loss per batch: 0.534
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0054, 0.0053, 0.3379, 0.4425, 0.1133, 0.0681, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 51, batch: 164/205] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1010e-03, 9.9490e-01, 1.2819e-06, 4.3847e-07, 2.3571e-07, 1.6204e-07,
        8.5171e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.516

[Epoch: 51, batch: 205/205] total loss per batch: 0.527
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0061, 0.6440, 0.3290, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 52, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4285e-02, 6.1208e-02, 3.1164e-05, 8.2904e-01, 1.0028e-05, 9.7305e-06,
        4.5414e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.595

[Epoch: 52, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0048, 0.0055, 0.0056, 0.0048, 0.9684, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 52, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0053, 0.3265, 0.4375, 0.1359, 0.0633, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 52, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6627e-03, 9.9434e-01, 5.6794e-07, 2.6566e-07, 1.2470e-07, 1.5262e-07,
        1.0300e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.478

[Epoch: 52, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0052, 0.0058, 0.6645, 0.3071, 0.0060, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 53, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.4889e-02, 3.8105e-02, 2.0954e-05, 8.2005e-01, 1.1470e-05, 9.6818e-06,
        6.6910e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.597

[Epoch: 53, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0045, 0.0056, 0.0051, 0.0048, 0.9694, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 53, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0048, 0.3273, 0.4532, 0.1241, 0.0617, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 53, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1998e-03, 9.9480e-01, 5.8364e-07, 2.0195e-07, 1.1030e-07, 8.9715e-08,
        6.8424e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 53, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0046, 0.6455, 0.3291, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 54, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.0173e-02, 4.6124e-02, 1.5556e-05, 8.3562e-01, 5.9328e-06, 5.4358e-06,
        5.8058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.606

[Epoch: 54, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0041, 0.0053, 0.0049, 0.0043, 0.9718, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 54, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0047, 0.3476, 0.4302, 0.1253, 0.0639, 0.0233],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 54, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2492e-03, 9.9475e-01, 4.5175e-07, 1.9191e-07, 1.1184e-07, 9.5370e-08,
        6.2695e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.493

[Epoch: 54, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0050, 0.6580, 0.3168, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 55, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.7055e-02, 4.2430e-02, 1.4162e-05, 8.3000e-01, 7.4009e-06, 5.3379e-06,
        6.0485e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.602

[Epoch: 55, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0044, 0.0055, 0.0051, 0.0044, 0.9701, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 55, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0050, 0.3357, 0.4453, 0.1296, 0.0555, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 55, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1215e-03, 9.9488e-01, 4.1206e-07, 1.7659e-07, 1.0319e-07, 7.5594e-08,
        4.9776e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.481

[Epoch: 55, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0046, 0.6555, 0.3205, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 56, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4544e-02, 4.8667e-02, 9.5950e-06, 8.2416e-01, 5.5313e-06, 4.1784e-06,
        6.2610e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.614

[Epoch: 56, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0054, 0.0051, 0.0046, 0.9702, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 56, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0049, 0.3240, 0.4339, 0.1327, 0.0742, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 56, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0394e-03, 9.9496e-01, 3.6057e-07, 1.2437e-07, 6.3709e-08, 4.6370e-08,
        4.6626e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.501

[Epoch: 56, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0052, 0.6543, 0.3209, 0.0048, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 57, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.1300e-02, 4.1781e-02, 1.3713e-05, 8.3785e-01, 5.3267e-06, 3.9846e-06,
        5.9050e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.587

[Epoch: 57, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0047, 0.0060, 0.0053, 0.0047, 0.9683, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 57, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0049, 0.0049, 0.3636, 0.4211, 0.1289, 0.0530, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 57, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9006e-03, 9.9510e-01, 4.1485e-07, 1.7475e-07, 1.0294e-07, 8.6696e-08,
        4.6538e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.486

[Epoch: 57, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0049, 0.6500, 0.3257, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 58, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.0473e-02, 4.3231e-02, 9.0044e-06, 8.2959e-01, 6.3780e-06, 4.5513e-06,
        5.6684e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.629

[Epoch: 58, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0047, 0.0056, 0.0053, 0.0049, 0.9685, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 58, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0058, 0.0055, 0.2953, 0.4549, 0.1357, 0.0778, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 58, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4714e-03, 9.9453e-01, 3.2774e-07, 1.0985e-07, 6.8885e-08, 4.5179e-08,
        4.2177e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.506

[Epoch: 58, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0054, 0.0054, 0.6811, 0.2946, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 59, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.4204e-02, 4.6914e-02, 1.4063e-05, 8.4525e-01, 3.7888e-06, 2.4473e-06,
        5.3608e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.583

[Epoch: 59, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0049, 0.0068, 0.0059, 0.0051, 0.9667, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 59, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0050, 0.3395, 0.4217, 0.1340, 0.0669, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 59, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8720e-03, 9.9513e-01, 5.9410e-07, 2.5014e-07, 1.1917e-07, 8.7679e-08,
        4.4865e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.480

[Epoch: 59, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0054, 0.0045, 0.6004, 0.3725, 0.0059, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 60, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.6059e-02, 4.5634e-02, 1.7754e-05, 8.0164e-01, 7.3443e-06, 8.2432e-06,
        7.6636e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.623

[Epoch: 60, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0061, 0.0048, 0.0049, 0.0049, 0.0051, 0.9692, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 60, batch: 123/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0054, 0.3342, 0.4465, 0.1271, 0.0571, 0.0244],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 60, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4592e-03, 9.9554e-01, 4.6575e-07, 1.4060e-07, 1.1140e-07, 8.1194e-08,
        8.7846e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.502

[Epoch: 60, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0062, 0.7144, 0.2598, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 61, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3189e-02, 5.1149e-02, 2.0156e-05, 8.3792e-01, 4.4281e-06, 3.4215e-06,
        4.7713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.588

[Epoch: 61, batch: 82/205] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.0062, 0.0055, 0.0039, 0.9702, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 61, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0051, 0.3196, 0.4425, 0.1348, 0.0664, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 61, batch: 164/205] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4253e-03, 9.9257e-01, 7.6378e-07, 2.0321e-07, 9.0412e-08, 9.2918e-08,
        6.3408e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.491

[Epoch: 61, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0066, 0.0048, 0.5985, 0.3728, 0.0051, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 62, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.1540e-02, 3.8429e-02, 1.6421e-05, 8.4069e-01, 7.0077e-06, 6.5142e-06,
        5.9314e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.595

[Epoch: 62, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0053, 0.0044, 0.0043, 0.9715, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 62, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0051, 0.3391, 0.4362, 0.1270, 0.0648, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 62, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7187e-03, 9.9428e-01, 4.4517e-07, 2.0098e-07, 1.4094e-07, 1.0523e-07,
        6.8358e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.495

[Epoch: 62, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0049, 0.6792, 0.2963, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 63, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.5665e-02, 5.8918e-02, 1.6810e-05, 7.9326e-01, 5.0640e-06, 3.8234e-06,
        7.2133e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.604

[Epoch: 63, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0057, 0.0052, 0.0044, 0.9696, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 63, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0054, 0.0051, 0.3329, 0.4368, 0.1324, 0.0621, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 63, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9191e-03, 9.9508e-01, 3.9849e-07, 1.2956e-07, 6.2979e-08, 5.9916e-08,
        4.7926e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.491

[Epoch: 63, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0041, 0.6539, 0.3229, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 64, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.4767e-02, 3.7206e-02, 1.3066e-05, 8.5385e-01, 5.9769e-06, 4.6169e-06,
        5.4149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.599

[Epoch: 64, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0049, 0.0057, 0.0052, 0.0046, 0.9688, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 64, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0054, 0.3301, 0.4268, 0.1402, 0.0672, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 64, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0487e-03, 9.9495e-01, 3.5269e-07, 1.4690e-07, 8.5925e-08, 6.9751e-08,
        4.7020e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.493

[Epoch: 64, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.0046, 0.6548, 0.3216, 0.0048, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 65, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.6200e-02, 4.5389e-02, 1.2168e-05, 8.2747e-01, 4.5095e-06, 3.3586e-06,
        6.0919e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.612

[Epoch: 65, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0049, 0.0060, 0.0055, 0.0049, 0.9678, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 65, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0054, 0.0052, 0.3246, 0.4343, 0.1386, 0.0659, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 65, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9029e-03, 9.9510e-01, 3.5514e-07, 1.1954e-07, 5.9719e-08, 5.2004e-08,
        3.5106e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.493

[Epoch: 65, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0048, 0.6532, 0.3226, 0.0048, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 66, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4228e-02, 4.3665e-02, 1.1586e-05, 8.2754e-01, 4.0011e-06, 2.9155e-06,
        6.4544e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.603

[Epoch: 66, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0047, 0.0053, 0.0050, 0.0046, 0.9698, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 66, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0056, 0.3316, 0.4329, 0.1323, 0.0666, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 66, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6328e-03, 9.9537e-01, 2.5705e-07, 9.6325e-08, 5.7635e-08, 4.2003e-08,
        3.0115e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.488

[Epoch: 66, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0049, 0.6617, 0.3137, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 67, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5846e-02, 4.9914e-02, 1.0965e-05, 8.2841e-01, 4.2623e-06, 2.9327e-06,
        5.5807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.596

[Epoch: 67, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0058, 0.0050, 0.0046, 0.9698, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 67, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0054, 0.0055, 0.3291, 0.4341, 0.1356, 0.0633, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 67, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6803e-03, 9.9432e-01, 2.3316e-07, 9.7604e-08, 5.5170e-08, 4.4141e-08,
        2.7490e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.488

[Epoch: 67, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0054, 0.6470, 0.3268, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 68, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.1011e-02, 3.7696e-02, 8.5636e-06, 8.3602e-01, 2.8921e-06, 2.0412e-06,
        6.5257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.626

[Epoch: 68, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0052, 0.0049, 0.0043, 0.9718, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 68, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0053, 0.0050, 0.3400, 0.4232, 0.1340, 0.0680, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 68, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0056e-03, 9.9599e-01, 2.8734e-07, 1.1195e-07, 6.0335e-08, 3.4720e-08,
        2.7673e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 68, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.0047, 0.6577, 0.3185, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 69, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([7.5599e-02, 4.7118e-02, 1.1705e-05, 8.2362e-01, 3.7771e-06, 3.0181e-06,
        5.3642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.584

[Epoch: 69, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0053, 0.0046, 0.0043, 0.9717, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 69, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0059, 0.3360, 0.4208, 0.1362, 0.0690, 0.0266],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 69, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0496e-03, 9.9395e-01, 2.2005e-07, 1.0859e-07, 6.6417e-08, 6.4912e-08,
        3.0840e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.515

[Epoch: 69, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0060, 0.0055, 0.6556, 0.3167, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 70, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([4.7560e-02, 4.4307e-02, 1.2557e-05, 8.4119e-01, 3.5886e-06, 2.3991e-06,
        6.6924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.615

[Epoch: 70, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0042, 0.0050, 0.0049, 0.0042, 0.9719, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 70, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0043, 0.3159, 0.4681, 0.1302, 0.0544, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 70, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2710e-03, 9.9673e-01, 2.6784e-07, 1.0955e-07, 5.4440e-08, 2.7798e-08,
        2.7814e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.470

[Epoch: 70, batch: 205/205] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0035, 0.0037, 0.6603, 0.3202, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 71, batch: 41/205] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([9.2047e-02, 5.3989e-02, 1.3864e-05, 7.9386e-01, 6.5705e-06, 4.9763e-06,
        6.0083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.588

[Epoch: 71, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0051, 0.0050, 0.0041, 0.9714, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 71, batch: 123/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0069, 0.0063, 0.3284, 0.4122, 0.1438, 0.0726, 0.0298],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 71, batch: 164/205] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.3268e-03, 9.9367e-01, 2.7323e-07, 1.6125e-07, 9.2441e-08, 8.2157e-08,
        5.9189e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.483

[Epoch: 71, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0050, 0.6439, 0.3297, 0.0053, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 72, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([5.3322e-02, 3.9339e-02, 1.4329e-05, 8.4818e-01, 5.4282e-06, 3.4469e-06,
        5.9131e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.622

[Epoch: 72, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.0055, 0.0051, 0.0047, 0.9694, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 72, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0056, 0.3165, 0.4409, 0.1378, 0.0690, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 72, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0196e-03, 9.9498e-01, 3.3148e-07, 1.5365e-07, 6.9570e-08, 5.4159e-08,
        3.9579e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.502

[Epoch: 72, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0047, 0.6701, 0.3057, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 73, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5364e-02, 4.6298e-02, 1.0700e-05, 8.2671e-01, 4.8232e-06, 3.3196e-06,
        6.1612e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.608

[Epoch: 73, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.0051, 0.0048, 0.0044, 0.9713, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 73, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0055, 0.3258, 0.4315, 0.1374, 0.0682, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 73, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5864e-03, 9.9441e-01, 2.4788e-07, 1.1802e-07, 6.4716e-08, 4.7418e-08,
        3.9228e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 73, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.6429, 0.3315, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 74, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3349e-02, 4.2883e-02, 1.1072e-05, 8.3422e-01, 4.6200e-06, 2.6968e-06,
        5.9527e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.601

[Epoch: 74, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0050, 0.0046, 0.0042, 0.9719, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 74, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0051, 0.3318, 0.4317, 0.1352, 0.0654, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 74, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6346e-03, 9.9436e-01, 2.1043e-07, 1.0569e-07, 6.4055e-08, 4.3934e-08,
        3.4048e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.486

[Epoch: 74, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.0051, 0.6582, 0.3166, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 75, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5984e-02, 4.4312e-02, 8.0720e-06, 8.2917e-01, 3.7375e-06, 2.3176e-06,
        6.0524e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.609

[Epoch: 75, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0052, 0.0047, 0.0044, 0.9714, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 75, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0051, 0.3298, 0.4343, 0.1358, 0.0645, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 75, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2438e-03, 9.9476e-01, 2.1394e-07, 1.1233e-07, 6.1203e-08, 4.7679e-08,
        3.4391e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 75, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.0053, 0.6536, 0.3206, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 76, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4121e-02, 4.6093e-02, 6.9478e-06, 8.3041e-01, 3.2456e-06, 2.0082e-06,
        5.9362e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.600

[Epoch: 76, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0053, 0.0049, 0.0043, 0.9710, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 76, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0050, 0.3282, 0.4340, 0.1370, 0.0658, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 76, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6381e-03, 9.9436e-01, 2.1275e-07, 1.0936e-07, 6.1535e-08, 4.2776e-08,
        3.3176e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.488

[Epoch: 76, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0049, 0.6604, 0.3151, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 77, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.7166e-02, 4.3742e-02, 7.0372e-06, 8.2973e-01, 3.2376e-06, 2.0439e-06,
        5.9345e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 77, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0046, 0.0053, 0.0049, 0.0046, 0.9704, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 77, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0053, 0.3321, 0.4351, 0.1330, 0.0643, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 77, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9754e-03, 9.9502e-01, 2.0941e-07, 1.1595e-07, 5.6344e-08, 4.6998e-08,
        3.3492e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.497

[Epoch: 77, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0053, 0.6519, 0.3222, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 78, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3636e-02, 4.5319e-02, 6.3635e-06, 8.3246e-01, 3.0267e-06, 1.8663e-06,
        5.8574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.598

[Epoch: 78, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0047, 0.0054, 0.0051, 0.0045, 0.9698, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 78, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0055, 0.0053, 0.3196, 0.4345, 0.1417, 0.0674, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 78, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.3867e-03, 9.9461e-01, 2.3712e-07, 1.0999e-07, 5.9866e-08, 4.3094e-08,
        3.2355e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.492

[Epoch: 78, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.0048, 0.6499, 0.3253, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 79, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5181e-02, 4.3624e-02, 5.5413e-06, 8.3013e-01, 2.3190e-06, 1.4960e-06,
        6.1052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.623

[Epoch: 79, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0048, 0.0056, 0.0052, 0.0047, 0.9693, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 79, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0055, 0.3449, 0.4208, 0.1329, 0.0668, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 79, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2030e-03, 9.9480e-01, 1.5327e-07, 1.0744e-07, 4.8638e-08, 4.0590e-08,
        3.2655e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.503

[Epoch: 79, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0054, 0.0060, 0.6562, 0.3169, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 80, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.6668e-02, 4.4587e-02, 7.0693e-06, 8.2839e-01, 3.0680e-06, 2.2947e-06,
        6.0340e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.579

[Epoch: 80, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0053, 0.0049, 0.0045, 0.9702, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 80, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0053, 0.0048, 0.3201, 0.4599, 0.1305, 0.0545, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 80, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1728e-03, 9.9383e-01, 2.8429e-07, 1.2348e-07, 1.0146e-07, 6.3026e-08,
        4.3011e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.489

[Epoch: 80, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0044, 0.6565, 0.3180, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 81, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5378e-02, 4.7898e-02, 6.1258e-06, 8.2892e-01, 2.3675e-06, 1.9564e-06,
        5.7793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.620

[Epoch: 81, batch: 82/205] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0053, 0.0049, 0.0045, 0.9702, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 81, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0050, 0.3363, 0.4351, 0.1315, 0.0636, 0.0233],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 81, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.9867e-03, 9.9401e-01, 1.6783e-07, 1.3792e-07, 5.8880e-08, 5.4455e-08,
        4.9839e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.487

[Epoch: 81, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0055, 0.6598, 0.3141, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 82, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.9970e-02, 4.9829e-02, 6.7111e-06, 8.1757e-01, 4.8392e-06, 2.5258e-06,
        6.2619e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.594

[Epoch: 82, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0050, 0.0055, 0.0056, 0.0050, 0.9679, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 82, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0048, 0.0047, 0.3437, 0.4334, 0.1300, 0.0596, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 82, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7229e-03, 9.9428e-01, 2.0482e-07, 1.2092e-07, 7.1911e-08, 5.2256e-08,
        4.2164e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 82, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0043, 0.6530, 0.3237, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 83, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.2007e-02, 4.1410e-02, 6.1415e-06, 8.3832e-01, 3.6357e-06, 2.1350e-06,
        5.8255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.613

[Epoch: 83, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0051, 0.0055, 0.0055, 0.0050, 0.9677, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 83, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0049, 0.0050, 0.3307, 0.4375, 0.1322, 0.0649, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 83, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8957e-03, 9.9510e-01, 1.7333e-07, 1.0175e-07, 5.2284e-08, 4.1813e-08,
        3.5028e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.490

[Epoch: 83, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0050, 0.6543, 0.3211, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 84, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5595e-02, 4.5178e-02, 5.9921e-06, 8.2735e-01, 3.4641e-06, 1.9177e-06,
        6.1864e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.603

[Epoch: 84, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0051, 0.0052, 0.0048, 0.9694, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 84, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0049, 0.0048, 0.3336, 0.4376, 0.1324, 0.0620, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 84, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0074e-03, 9.9499e-01, 1.7135e-07, 9.5347e-08, 4.7284e-08, 3.7593e-08,
        2.8863e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.495

[Epoch: 84, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0049, 0.6586, 0.3164, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 85, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3449e-02, 4.4174e-02, 6.0505e-06, 8.3537e-01, 3.1625e-06, 1.8083e-06,
        5.6992e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 85, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0052, 0.0052, 0.0048, 0.9693, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 85, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0049, 0.3306, 0.4374, 0.1320, 0.0651, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 85, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6949e-03, 9.9530e-01, 1.5104e-07, 8.7087e-08, 4.1869e-08, 3.3788e-08,
        2.4348e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 85, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0052, 0.0051, 0.6488, 0.3257, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 86, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.6699e-02, 4.5159e-02, 5.5023e-06, 8.2449e-01, 2.9251e-06, 1.6500e-06,
        6.3637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.602

[Epoch: 86, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0051, 0.0050, 0.0046, 0.9703, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 86, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0048, 0.3314, 0.4370, 0.1328, 0.0639, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 86, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7508e-03, 9.9525e-01, 1.3007e-07, 7.5187e-08, 3.7549e-08, 3.0320e-08,
        2.1961e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.490

[Epoch: 86, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0050, 0.6557, 0.3188, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 87, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.2250e-02, 4.5454e-02, 5.2333e-06, 8.3614e-01, 2.8087e-06, 1.4995e-06,
        5.6151e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 87, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0053, 0.0051, 0.0047, 0.9698, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 87, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0051, 0.3297, 0.4385, 0.1306, 0.0659, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 87, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7628e-03, 9.9524e-01, 1.2993e-07, 7.8658e-08, 3.4798e-08, 2.5889e-08,
        2.0105e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.497

[Epoch: 87, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0051, 0.6570, 0.3181, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 88, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.8321e-02, 4.3959e-02, 4.8085e-06, 8.2105e-01, 2.4605e-06, 1.4272e-06,
        6.6664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.603

[Epoch: 88, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0048, 0.0052, 0.0050, 0.0047, 0.9698, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 88, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0056, 0.0054, 0.3344, 0.4265, 0.1381, 0.0647, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 88, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9734e-03, 9.9503e-01, 1.2388e-07, 7.1711e-08, 3.6845e-08, 2.9465e-08,
        2.0899e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.488

[Epoch: 88, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0049, 0.6478, 0.3261, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 89, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.1038e-02, 4.6845e-02, 4.5875e-06, 8.4005e-01, 2.6712e-06, 1.4702e-06,
        5.2056e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.606

[Epoch: 89, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0046, 0.0052, 0.0052, 0.0048, 0.9693, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 89, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0053, 0.0051, 0.3172, 0.4444, 0.1329, 0.0698, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 89, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4987e-03, 9.9550e-01, 1.2317e-07, 6.4509e-08, 2.5178e-08, 1.5834e-08,
        1.4893e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.497

[Epoch: 89, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0051, 0.6567, 0.3188, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 90, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.8900e-02, 4.2666e-02, 4.7269e-06, 8.1839e-01, 2.5299e-06, 1.4515e-06,
        7.0037e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.597

[Epoch: 90, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0053, 0.0049, 0.0047, 0.9703, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 90, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0059, 0.0060, 0.3530, 0.4120, 0.1345, 0.0632, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 90, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.3771e-03, 9.9462e-01, 1.2672e-07, 7.8771e-08, 4.4088e-08, 3.4075e-08,
        2.5881e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.484

[Epoch: 90, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0047, 0.6620, 0.3127, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 91, batch: 41/205] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4086e-02, 4.9296e-02, 4.8447e-06, 8.3454e-01, 3.4304e-06, 1.6174e-06,
        5.2068e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.608

[Epoch: 91, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0049, 0.0052, 0.0053, 0.0050, 0.9684, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 91, batch: 123/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0050, 0.3177, 0.4388, 0.1355, 0.0720, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 91, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3702e-03, 9.9563e-01, 1.4623e-07, 7.1354e-08, 3.2135e-08, 1.8464e-08,
        1.7012e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.504

[Epoch: 91, batch: 205/205] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0052, 0.6582, 0.3174, 0.0048, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 92, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.2783e-02, 4.0278e-02, 5.5438e-06, 8.3370e-01, 2.7746e-06, 1.7193e-06,
        6.3232e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.612

[Epoch: 92, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0047, 0.0049, 0.0045, 0.9717, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 92, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0053, 0.0052, 0.3385, 0.4197, 0.1400, 0.0657, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 92, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7085e-03, 9.9529e-01, 1.3053e-07, 7.3198e-08, 3.8072e-08, 2.4939e-08,
        2.1261e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.490

[Epoch: 92, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0052, 0.6518, 0.3232, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 93, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3205e-02, 4.7146e-02, 4.8732e-06, 8.3125e-01, 2.6414e-06, 1.4157e-06,
        5.8388e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.600

[Epoch: 93, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0045, 0.0049, 0.0048, 0.0045, 0.9713, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 93, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0049, 0.3348, 0.4291, 0.1363, 0.0646, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 93, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7768e-03, 9.9522e-01, 1.4599e-07, 8.9368e-08, 4.8448e-08, 3.1640e-08,
        2.5228e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.491

[Epoch: 93, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.0054, 0.6525, 0.3228, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 94, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.7221e-02, 4.4342e-02, 4.5602e-06, 8.2623e-01, 2.4916e-06, 1.3362e-06,
        6.2199e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.605

[Epoch: 94, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0049, 0.0048, 0.0045, 0.9716, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 94, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0050, 0.3253, 0.4351, 0.1372, 0.0663, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 94, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1125e-03, 9.9489e-01, 1.6028e-07, 9.4874e-08, 4.7437e-08, 3.2853e-08,
        2.6570e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 94, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0052, 0.6553, 0.3198, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 95, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.4396e-02, 4.6515e-02, 4.0606e-06, 8.2992e-01, 2.3916e-06, 1.2065e-06,
        5.9158e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.606

[Epoch: 95, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0050, 0.0050, 0.0045, 0.9712, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 95, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0051, 0.3240, 0.4345, 0.1386, 0.0670, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 95, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4720e-03, 9.9453e-01, 1.5114e-07, 9.3515e-08, 4.5789e-08, 3.3685e-08,
        2.7880e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.493

[Epoch: 95, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0050, 0.6576, 0.3178, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 96, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.7996e-02, 4.6292e-02, 3.9345e-06, 8.2453e-01, 2.2176e-06, 1.1580e-06,
        6.1177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.604

[Epoch: 96, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0048, 0.0048, 0.0045, 0.9718, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 96, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0052, 0.0052, 0.3244, 0.4352, 0.1372, 0.0666, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 96, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.3805e-03, 9.9462e-01, 1.4338e-07, 9.4055e-08, 4.4357e-08, 3.2845e-08,
        2.7316e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.495

[Epoch: 96, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0049, 0.6550, 0.3205, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 97, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.5406e-02, 4.5814e-02, 3.7715e-06, 8.2836e-01, 2.1580e-06, 1.0727e-06,
        6.0411e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.608

[Epoch: 97, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0047, 0.0048, 0.0044, 0.9719, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 97, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0051, 0.0051, 0.3275, 0.4321, 0.1392, 0.0656, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 97, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2913e-03, 9.9471e-01, 1.1312e-07, 7.6597e-08, 3.8822e-08, 2.9090e-08,
        2.5216e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 97, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0046, 0.6571, 0.3191, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 98, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.6480e-02, 4.5890e-02, 3.6950e-06, 8.2658e-01, 1.9958e-06, 1.0373e-06,
        6.1049e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.603

[Epoch: 98, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.0047, 0.0047, 0.0044, 0.9723, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 98, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0051, 0.3257, 0.4361, 0.1380, 0.0644, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 98, batch: 164/205] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1909e-03, 9.9481e-01, 1.1151e-07, 7.3934e-08, 3.5527e-08, 2.6645e-08,
        2.3337e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.495

[Epoch: 98, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0047, 0.6559, 0.3202, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 99, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.3193e-02, 4.3725e-02, 3.4531e-06, 8.3450e-01, 2.0018e-06, 1.0251e-06,
        5.8578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.607

[Epoch: 99, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0046, 0.0047, 0.0043, 0.9723, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 99, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0050, 0.3331, 0.4286, 0.1385, 0.0642, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 99, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1313e-03, 9.9487e-01, 9.3699e-08, 6.0725e-08, 3.0403e-08, 2.0227e-08,
        2.0467e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.490

[Epoch: 99, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0047, 0.6552, 0.3210, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 100, batch: 41/205] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0650, 0.0450, 0.0000, 0.8300, 0.0000, 0.0000, 0.0600])
Policy pred: tensor([6.6622e-02, 4.7002e-02, 3.4789e-06, 8.2443e-01, 1.7825e-06, 9.1133e-07,
        6.1935e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.605 -0.605

[Epoch: 100, batch: 82/205] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.0048, 0.0048, 0.0043, 0.9726, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 100, batch: 123/205] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.3300, 0.4350, 0.1350, 0.0650, 0.0250])
Policy pred: tensor([0.0050, 0.0052, 0.3213, 0.4408, 0.1396, 0.0630, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 100, batch: 164/205] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8824e-03, 9.9512e-01, 1.2385e-07, 6.8240e-08, 2.6490e-08, 2.3767e-08,
        1.9795e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.493 -0.494

[Epoch: 100, batch: 205/205] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.6550, 0.3200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0050, 0.6571, 0.3187, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

