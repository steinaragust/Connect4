Training set samples: 6235
Batch size: 32
[Epoch: 1, batch: 39/195] total loss per batch: 1.805
Policy (actual, predicted): 6 4
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1714, 0.1443, 0.1663, 0.1108, 0.2588, 0.0957, 0.0528],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.008

[Epoch: 1, batch: 78/195] total loss per batch: 1.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.7899, 0.0342, 0.0478, 0.0016, 0.0461, 0.0414, 0.0390],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.024

[Epoch: 1, batch: 117/195] total loss per batch: 1.637
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([0.0543, 0.2183, 0.0005, 0.0860, 0.0483, 0.3994, 0.1933],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 1, batch: 156/195] total loss per batch: 1.642
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([0.1473, 0.1271, 0.1875, 0.1792, 0.0003, 0.1738, 0.1848],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.026

[Epoch: 1, batch: 195/195] total loss per batch: 1.644
Policy (actual, predicted): 1 5
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0853, 0.1969, 0.0782, 0.1318, 0.1933, 0.2078, 0.1067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.083

[Epoch: 2, batch: 39/195] total loss per batch: 1.438
Policy (actual, predicted): 6 0
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2529, 0.1141, 0.1197, 0.1855, 0.1837, 0.0713, 0.0728],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.028

[Epoch: 2, batch: 78/195] total loss per batch: 1.384
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([8.6842e-01, 2.0355e-02, 3.0889e-02, 4.5803e-04, 3.5707e-02, 2.7203e-02,
        1.6967e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.157

[Epoch: 2, batch: 117/195] total loss per batch: 1.330
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([6.3578e-03, 8.9683e-02, 5.8455e-05, 4.4146e-02, 1.9068e-02, 7.6714e-01,
        7.3548e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 2, batch: 156/195] total loss per batch: 1.342
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.6070e-01, 7.5660e-02, 1.4311e-01, 9.3754e-02, 8.6240e-05, 2.7413e-01,
        2.5256e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.184

[Epoch: 2, batch: 195/195] total loss per batch: 1.341
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0579, 0.4188, 0.0235, 0.0549, 0.1434, 0.2433, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.119

[Epoch: 3, batch: 39/195] total loss per batch: 1.166
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2102, 0.0992, 0.0603, 0.1117, 0.1916, 0.0642, 0.2628],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.028

[Epoch: 3, batch: 78/195] total loss per batch: 1.110
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.1507e-01, 7.2229e-03, 1.5337e-02, 1.4784e-04, 3.1091e-02, 1.8743e-02,
        1.2391e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.220

[Epoch: 3, batch: 117/195] total loss per batch: 1.067
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.0411e-03, 6.5179e-02, 6.7524e-06, 4.2190e-02, 6.9780e-03, 8.3483e-01,
        4.6779e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.044

[Epoch: 3, batch: 156/195] total loss per batch: 1.064
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.2961e-01, 1.0080e-01, 1.8969e-02, 4.2610e-02, 5.8309e-05, 5.1057e-01,
        1.9738e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.145

[Epoch: 3, batch: 195/195] total loss per batch: 1.070
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0234, 0.8615, 0.0028, 0.0090, 0.0361, 0.0565, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.473

[Epoch: 4, batch: 39/195] total loss per batch: 1.015
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0966, 0.0308, 0.0499, 0.0875, 0.0484, 0.0545, 0.6324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.031

[Epoch: 4, batch: 78/195] total loss per batch: 0.992
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.1562e-01, 5.0125e-03, 1.4747e-02, 6.7681e-05, 3.2286e-02, 1.9935e-02,
        1.2329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.036

[Epoch: 4, batch: 117/195] total loss per batch: 0.969
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4275e-03, 5.1329e-02, 2.3932e-06, 1.8594e-02, 2.9169e-03, 9.0249e-01,
        2.2241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.000

[Epoch: 4, batch: 156/195] total loss per batch: 0.978
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.8829e-02, 9.7770e-02, 9.0971e-03, 1.7644e-02, 1.6550e-05, 7.5744e-01,
        3.9200e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.100

[Epoch: 4, batch: 195/195] total loss per batch: 1.005
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0262, 0.8128, 0.0046, 0.0188, 0.0337, 0.0800, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.418

[Epoch: 5, batch: 39/195] total loss per batch: 0.970
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1569, 0.0235, 0.0458, 0.0595, 0.0360, 0.0189, 0.6594],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.042

[Epoch: 5, batch: 78/195] total loss per batch: 0.968
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4341e-01, 3.4356e-03, 8.2253e-03, 1.3151e-04, 2.0982e-02, 1.2277e-02,
        1.1541e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.151

[Epoch: 5, batch: 117/195] total loss per batch: 0.953
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([9.6000e-03, 3.8199e-02, 1.1454e-05, 3.0227e-02, 1.5596e-02, 8.7420e-01,
        3.2165e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 5, batch: 156/195] total loss per batch: 0.971
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.2346e-02, 3.5562e-02, 2.5051e-03, 2.4763e-02, 1.9613e-05, 8.6702e-01,
        3.7780e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.011

[Epoch: 5, batch: 195/195] total loss per batch: 0.989
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9290, 0.0032, 0.0036, 0.0197, 0.0252, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.698

[Epoch: 6, batch: 39/195] total loss per batch: 0.961
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1683, 0.0284, 0.0486, 0.0493, 0.0553, 0.0196, 0.6305],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 6, batch: 78/195] total loss per batch: 0.952
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6718e-01, 3.8678e-03, 6.1985e-03, 2.4423e-05, 1.2556e-02, 6.0005e-03,
        4.1683e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.267

[Epoch: 6, batch: 117/195] total loss per batch: 0.942
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.1369e-02, 6.4661e-02, 7.9879e-06, 5.5143e-02, 1.1378e-02, 8.1986e-01,
        3.7581e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 6, batch: 156/195] total loss per batch: 0.945
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.0706e-02, 3.8696e-02, 2.3998e-03, 4.2780e-02, 1.6867e-05, 8.8083e-01,
        1.4569e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.225

[Epoch: 6, batch: 195/195] total loss per batch: 0.962
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0161, 0.9220, 0.0049, 0.0042, 0.0209, 0.0190, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.516

[Epoch: 7, batch: 39/195] total loss per batch: 0.934
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1322, 0.0059, 0.0146, 0.0351, 0.0071, 0.0151, 0.7900],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 7, batch: 78/195] total loss per batch: 0.932
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([8.8032e-01, 1.7538e-02, 1.7069e-02, 3.0477e-04, 3.3222e-02, 3.8648e-02,
        1.2899e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.005

[Epoch: 7, batch: 117/195] total loss per batch: 0.917
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([7.1446e-03, 4.3447e-02, 1.5634e-05, 2.3362e-02, 1.3160e-02, 8.5583e-01,
        5.7042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.003

[Epoch: 7, batch: 156/195] total loss per batch: 0.924
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.5364e-02, 2.7558e-02, 5.3455e-03, 1.9690e-02, 1.8403e-05, 9.1620e-01,
        1.5821e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.044

[Epoch: 7, batch: 195/195] total loss per batch: 0.940
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9507, 0.0031, 0.0026, 0.0151, 0.0051, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.488

[Epoch: 8, batch: 39/195] total loss per batch: 0.921
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1511, 0.0138, 0.0283, 0.0241, 0.0304, 0.0190, 0.7333],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.058

[Epoch: 8, batch: 78/195] total loss per batch: 0.916
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4368e-01, 4.9874e-03, 9.0752e-03, 9.0958e-05, 2.4896e-02, 1.0231e-02,
        7.0426e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 -0.011

[Epoch: 8, batch: 117/195] total loss per batch: 0.899
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([7.7842e-03, 3.9173e-02, 1.9597e-05, 3.7742e-02, 2.1276e-02, 8.6252e-01,
        3.1482e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 8, batch: 156/195] total loss per batch: 0.913
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.5230e-02, 1.2263e-01, 4.5809e-03, 7.7495e-02, 4.2927e-05, 7.6285e-01,
        1.7173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.004

[Epoch: 8, batch: 195/195] total loss per batch: 0.931
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9407, 0.0072, 0.0037, 0.0221, 0.0129, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.556

[Epoch: 9, batch: 39/195] total loss per batch: 0.913
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2511, 0.0094, 0.0120, 0.0188, 0.0108, 0.0274, 0.6705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 9, batch: 78/195] total loss per batch: 0.909
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6518e-01, 5.4355e-03, 6.7588e-03, 9.8057e-05, 1.0087e-02, 7.0406e-03,
        5.3954e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.090

[Epoch: 9, batch: 117/195] total loss per batch: 0.892
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([8.4603e-03, 2.4514e-02, 1.1708e-05, 1.4702e-02, 9.2841e-03, 9.2029e-01,
        2.2734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 9, batch: 156/195] total loss per batch: 0.907
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.2532e-02, 7.3819e-03, 4.3473e-03, 1.0802e-02, 1.2802e-05, 9.5860e-01,
        6.3217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.166

[Epoch: 9, batch: 195/195] total loss per batch: 0.925
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.9418, 0.0034, 0.0033, 0.0256, 0.0095, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.380

[Epoch: 10, batch: 39/195] total loss per batch: 0.905
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1509, 0.0072, 0.0126, 0.0120, 0.0086, 0.0079, 0.8008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 10, batch: 78/195] total loss per batch: 0.905
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3308e-01, 1.3409e-02, 1.4441e-02, 1.3277e-04, 2.1033e-02, 1.0253e-02,
        7.6489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.052

[Epoch: 10, batch: 117/195] total loss per batch: 0.892
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9320e-02, 3.1230e-02, 1.8680e-05, 3.6454e-02, 5.3527e-02, 7.9237e-01,
        5.7077e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 10, batch: 156/195] total loss per batch: 0.905
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.4428e-02, 4.0167e-02, 2.7348e-03, 4.5253e-02, 1.9330e-05, 8.8408e-01,
        1.3321e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.001

[Epoch: 10, batch: 195/195] total loss per batch: 0.922
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9518, 0.0036, 0.0028, 0.0129, 0.0135, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.706

[Epoch: 11, batch: 39/195] total loss per batch: 0.905
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2057, 0.0064, 0.0289, 0.0279, 0.0080, 0.0167, 0.7063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 11, batch: 78/195] total loss per batch: 0.902
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4320e-01, 1.2424e-02, 1.2668e-02, 9.8710e-05, 1.5462e-02, 9.7317e-03,
        6.4179e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.094

[Epoch: 11, batch: 117/195] total loss per batch: 0.889
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.0286e-02, 2.8559e-02, 2.6980e-05, 2.1990e-02, 8.8612e-03, 9.0150e-01,
        2.8777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 11, batch: 156/195] total loss per batch: 0.903
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.1146e-02, 1.6599e-02, 7.6768e-03, 2.9522e-02, 1.0629e-05, 9.2386e-01,
        1.1185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.145

[Epoch: 11, batch: 195/195] total loss per batch: 0.921
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.9691, 0.0033, 0.0028, 0.0115, 0.0039, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.477

[Epoch: 12, batch: 39/195] total loss per batch: 0.900
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1376, 0.0081, 0.0153, 0.0137, 0.0077, 0.0162, 0.8014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 12, batch: 78/195] total loss per batch: 0.899
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5698e-01, 5.5040e-03, 6.0454e-03, 8.9270e-05, 1.1533e-02, 1.2122e-02,
        7.7286e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.041

[Epoch: 12, batch: 117/195] total loss per batch: 0.887
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.4793e-02, 9.2424e-03, 9.8036e-06, 2.1430e-02, 3.5633e-02, 8.9437e-01,
        2.4526e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 12, batch: 156/195] total loss per batch: 0.897
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.7570e-03, 4.9926e-02, 3.2205e-03, 5.3540e-02, 2.6289e-05, 8.7326e-01,
        1.0268e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.057

[Epoch: 12, batch: 195/195] total loss per batch: 0.919
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.9619, 0.0042, 0.0034, 0.0089, 0.0086, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.618

[Epoch: 13, batch: 39/195] total loss per batch: 0.899
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1854, 0.0067, 0.0239, 0.0195, 0.0087, 0.0118, 0.7440],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 13, batch: 78/195] total loss per batch: 0.897
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5930e-01, 8.8098e-03, 7.2353e-03, 9.0961e-05, 1.3534e-02, 6.6668e-03,
        4.3597e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.026

[Epoch: 13, batch: 117/195] total loss per batch: 0.884
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7185e-02, 3.9353e-02, 5.5735e-05, 6.2701e-02, 2.5077e-02, 7.8566e-01,
        5.9974e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 13, batch: 156/195] total loss per batch: 0.895
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.8850e-02, 3.0413e-02, 4.6115e-03, 5.0170e-02, 1.5823e-05, 8.8187e-01,
        1.4072e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.117

[Epoch: 13, batch: 195/195] total loss per batch: 0.913
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9545, 0.0045, 0.0060, 0.0105, 0.0071, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 14, batch: 39/195] total loss per batch: 0.897
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2194, 0.0127, 0.0159, 0.0159, 0.0062, 0.0127, 0.7171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 14, batch: 78/195] total loss per batch: 0.895
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5643e-01, 5.4953e-03, 8.4026e-03, 8.8980e-05, 1.1376e-02, 1.0971e-02,
        7.2350e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.200

[Epoch: 14, batch: 117/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.1931e-02, 1.5626e-02, 1.5592e-05, 9.0567e-03, 1.9840e-02, 9.2533e-01,
        1.8200e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 14, batch: 156/195] total loss per batch: 0.894
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.6925e-02, 4.7739e-02, 9.3790e-03, 5.9832e-02, 5.6620e-05, 8.5165e-01,
        1.4415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.019

[Epoch: 14, batch: 195/195] total loss per batch: 0.912
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9643, 0.0028, 0.0012, 0.0117, 0.0113, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.532

[Epoch: 15, batch: 39/195] total loss per batch: 0.900
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1149, 0.0050, 0.0139, 0.0186, 0.0065, 0.0072, 0.8339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 15, batch: 78/195] total loss per batch: 0.895
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.8018e-01, 5.4471e-03, 2.7460e-03, 2.7410e-05, 5.0637e-03, 3.0805e-03,
        3.4565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.074

[Epoch: 15, batch: 117/195] total loss per batch: 0.881
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([7.5746e-03, 2.0376e-02, 1.7209e-05, 2.7685e-02, 1.2377e-02, 9.0566e-01,
        2.6315e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 15, batch: 156/195] total loss per batch: 0.894
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.7086e-03, 1.6724e-02, 1.6376e-03, 3.3759e-02, 6.2070e-05, 9.3620e-01,
        4.9055e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.129

[Epoch: 15, batch: 195/195] total loss per batch: 0.912
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.9616, 0.0039, 0.0040, 0.0088, 0.0084, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.537

[Epoch: 16, batch: 39/195] total loss per batch: 0.898
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2985, 0.0150, 0.0171, 0.0215, 0.0102, 0.0083, 0.6294],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 16, batch: 78/195] total loss per batch: 0.895
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4790e-01, 1.0238e-02, 6.2803e-03, 1.6953e-04, 1.2622e-02, 1.1122e-02,
        1.1664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.138

[Epoch: 16, batch: 117/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.7487e-02, 2.1808e-02, 4.5101e-05, 2.3759e-02, 3.3503e-02, 8.4700e-01,
        3.6399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 16, batch: 156/195] total loss per batch: 0.892
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.3034e-02, 8.3629e-02, 6.9995e-03, 8.0714e-02, 2.2605e-05, 7.8824e-01,
        1.7361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.061

[Epoch: 16, batch: 195/195] total loss per batch: 0.912
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.9451, 0.0064, 0.0033, 0.0137, 0.0129, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.425

[Epoch: 17, batch: 39/195] total loss per batch: 0.896
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1397, 0.0054, 0.0279, 0.0165, 0.0081, 0.0088, 0.7935],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 17, batch: 78/195] total loss per batch: 0.892
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5925e-01, 6.4293e-03, 6.0665e-03, 1.2105e-04, 1.2491e-02, 8.5764e-03,
        7.0688e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.065

[Epoch: 17, batch: 117/195] total loss per batch: 0.877
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.5430e-02, 1.7152e-02, 2.4013e-05, 2.2225e-02, 2.3521e-02, 8.8766e-01,
        3.3991e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 17, batch: 156/195] total loss per batch: 0.895
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.2156e-03, 1.5906e-02, 3.5254e-03, 2.9508e-02, 3.5920e-05, 9.3288e-01,
        9.9280e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.049

[Epoch: 17, batch: 195/195] total loss per batch: 0.915
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9590, 0.0040, 0.0043, 0.0112, 0.0116, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.416

[Epoch: 18, batch: 39/195] total loss per batch: 0.896
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1023, 0.0051, 0.0116, 0.0127, 0.0077, 0.0061, 0.8545],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 18, batch: 78/195] total loss per batch: 0.891
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.2552e-01, 1.9044e-02, 1.3332e-02, 1.9618e-04, 1.3776e-02, 1.6311e-02,
        1.1819e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.092

[Epoch: 18, batch: 117/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4681e-02, 2.0278e-02, 4.0864e-05, 4.6694e-02, 4.2891e-02, 8.3994e-01,
        2.5479e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 18, batch: 156/195] total loss per batch: 0.893
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.2798e-02, 2.6118e-02, 4.7073e-03, 5.0005e-02, 3.1849e-05, 8.9211e-01,
        1.4229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.064

[Epoch: 18, batch: 195/195] total loss per batch: 0.917
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.9609, 0.0038, 0.0037, 0.0054, 0.0125, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.850

[Epoch: 19, batch: 39/195] total loss per batch: 0.897
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.3091, 0.0110, 0.0352, 0.0186, 0.0069, 0.0126, 0.6064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 19, batch: 78/195] total loss per batch: 0.890
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5960e-01, 6.8928e-03, 8.1046e-03, 1.1668e-04, 1.1172e-02, 7.8305e-03,
        6.2810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.077

[Epoch: 19, batch: 117/195] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.1639e-02, 2.4744e-02, 5.8638e-05, 1.8185e-02, 1.8510e-02, 8.7553e-01,
        3.1334e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 19, batch: 156/195] total loss per batch: 0.894
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.5189e-03, 3.3612e-02, 5.4073e-03, 3.5336e-02, 2.5397e-05, 9.1223e-01,
        6.8724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.150

[Epoch: 19, batch: 195/195] total loss per batch: 0.912
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.9542, 0.0050, 0.0065, 0.0150, 0.0114, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.577

[Epoch: 20, batch: 39/195] total loss per batch: 0.894
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1016, 0.0040, 0.0100, 0.0146, 0.0076, 0.0050, 0.8573],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 20, batch: 78/195] total loss per batch: 0.890
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3291e-01, 1.5369e-02, 1.2132e-02, 1.6414e-04, 1.4314e-02, 1.4185e-02,
        1.0929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.138

[Epoch: 20, batch: 117/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.6960e-02, 1.5182e-02, 2.6749e-05, 1.4794e-02, 2.9112e-02, 8.8576e-01,
        3.8161e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 20, batch: 156/195] total loss per batch: 0.892
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.2850e-02, 5.2144e-02, 4.5592e-03, 8.3698e-02, 3.2784e-05, 8.3626e-01,
        1.0451e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.040

[Epoch: 20, batch: 195/195] total loss per batch: 0.908
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9665, 0.0030, 0.0053, 0.0092, 0.0050, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.377

[Epoch: 21, batch: 39/195] total loss per batch: 0.891
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2909, 0.0102, 0.0160, 0.0197, 0.0069, 0.0072, 0.6492],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 21, batch: 78/195] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6054e-01, 5.7136e-03, 7.6918e-03, 1.2987e-04, 1.2534e-02, 6.9017e-03,
        6.4883e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.105

[Epoch: 21, batch: 117/195] total loss per batch: 0.877
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.9036e-02, 3.2116e-02, 6.2046e-05, 4.4452e-02, 2.5811e-02, 8.2283e-01,
        3.5689e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.027

[Epoch: 21, batch: 156/195] total loss per batch: 0.889
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.9874e-03, 2.4241e-02, 3.3138e-03, 2.4850e-02, 3.3322e-05, 9.3333e-01,
        5.2425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.092

[Epoch: 21, batch: 195/195] total loss per batch: 0.907
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9484, 0.0043, 0.0073, 0.0143, 0.0096, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.706

[Epoch: 22, batch: 39/195] total loss per batch: 0.890
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1359, 0.0051, 0.0144, 0.0201, 0.0077, 0.0085, 0.8082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 22, batch: 78/195] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7229e-01, 6.6172e-03, 4.7795e-03, 4.5030e-05, 7.9666e-03, 4.5089e-03,
        3.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.097

[Epoch: 22, batch: 117/195] total loss per batch: 0.877
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.0458e-02, 1.9481e-02, 1.8945e-05, 1.8958e-02, 1.0649e-02, 9.1421e-01,
        2.6223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 22, batch: 156/195] total loss per batch: 0.889
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.2924e-02, 2.9667e-02, 6.9204e-03, 7.4200e-02, 4.6226e-05, 8.6941e-01,
        6.8281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.048

[Epoch: 22, batch: 195/195] total loss per batch: 0.906
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0023, 0.9696, 0.0024, 0.0025, 0.0102, 0.0063, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 23, batch: 39/195] total loss per batch: 0.895
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1749, 0.0123, 0.0158, 0.0085, 0.0077, 0.0098, 0.7710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 23, batch: 78/195] total loss per batch: 0.891
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4295e-01, 7.4227e-03, 1.2867e-02, 9.1958e-05, 1.7766e-02, 1.1137e-02,
        7.7667e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.076

[Epoch: 23, batch: 117/195] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.4141e-02, 3.2056e-02, 3.3520e-05, 1.3343e-02, 2.6478e-02, 8.5963e-01,
        3.4317e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 23, batch: 156/195] total loss per batch: 0.894
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.3939e-03, 2.7683e-02, 2.3321e-03, 3.4109e-02, 3.3502e-05, 9.1857e-01,
        8.8799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 23, batch: 195/195] total loss per batch: 0.910
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9203, 0.0078, 0.0109, 0.0251, 0.0070, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.465

[Epoch: 24, batch: 39/195] total loss per batch: 0.896
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2376, 0.0077, 0.0187, 0.0169, 0.0060, 0.0057, 0.7074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.002

[Epoch: 24, batch: 78/195] total loss per batch: 0.898
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.2729e-01, 1.8213e-02, 7.9275e-03, 2.0181e-04, 1.9881e-02, 1.7224e-02,
        9.2593e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.103

[Epoch: 24, batch: 117/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8910e-02, 1.4405e-02, 1.6088e-05, 2.2065e-02, 2.2147e-02, 8.8113e-01,
        4.1324e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.008

[Epoch: 24, batch: 156/195] total loss per batch: 0.896
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.5227e-02, 6.5736e-02, 2.3042e-03, 7.5565e-02, 4.2334e-05, 8.3327e-01,
        7.8515e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.026

[Epoch: 24, batch: 195/195] total loss per batch: 0.916
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9776, 0.0028, 0.0020, 0.0044, 0.0052, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.555

[Epoch: 25, batch: 39/195] total loss per batch: 0.897
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1866, 0.0147, 0.0104, 0.0119, 0.0050, 0.0068, 0.7646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 25, batch: 78/195] total loss per batch: 0.898
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5170e-01, 7.0988e-03, 7.9735e-03, 6.9743e-05, 1.2024e-02, 1.1483e-02,
        9.6527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.118

[Epoch: 25, batch: 117/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0293e-02, 2.2560e-02, 1.5620e-04, 5.6780e-02, 2.6569e-02, 8.3040e-01,
        4.3246e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 25, batch: 156/195] total loss per batch: 0.893
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.8645e-03, 1.8059e-02, 1.2172e-03, 3.6863e-02, 2.4589e-05, 9.2465e-01,
        9.3234e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.098

[Epoch: 25, batch: 195/195] total loss per batch: 0.913
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.9675, 0.0025, 0.0049, 0.0084, 0.0072, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.400

[Epoch: 26, batch: 39/195] total loss per batch: 0.897
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1514, 0.0097, 0.0082, 0.0108, 0.0068, 0.0059, 0.8072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 26, batch: 78/195] total loss per batch: 0.894
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3483e-01, 1.6174e-02, 1.1580e-02, 2.0394e-04, 1.6561e-02, 7.7987e-03,
        1.2852e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.108

[Epoch: 26, batch: 117/195] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7604e-02, 1.8455e-02, 5.4488e-05, 2.3341e-02, 1.5491e-02, 8.8849e-01,
        3.6562e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.008

[Epoch: 26, batch: 156/195] total loss per batch: 0.889
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.1014e-02, 5.8064e-02, 9.9893e-03, 5.4272e-02, 2.1803e-04, 8.6128e-01,
        5.1649e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 0.008

[Epoch: 26, batch: 195/195] total loss per batch: 0.906
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9514, 0.0052, 0.0063, 0.0144, 0.0101, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.721

[Epoch: 27, batch: 39/195] total loss per batch: 0.890
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2769, 0.0082, 0.0235, 0.0123, 0.0050, 0.0099, 0.6641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 27, batch: 78/195] total loss per batch: 0.887
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6167e-01, 9.3871e-03, 6.9691e-03, 1.6705e-04, 8.7556e-03, 6.5448e-03,
        6.5051e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.012

[Epoch: 27, batch: 117/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.5007e-02, 1.3411e-02, 7.4374e-05, 4.1128e-02, 1.8591e-02, 8.9332e-01,
        1.8464e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.010

[Epoch: 27, batch: 156/195] total loss per batch: 0.885
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0287e-02, 9.1633e-03, 3.5002e-03, 5.4213e-02, 3.4697e-05, 9.1291e-01,
        9.8954e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.032

[Epoch: 27, batch: 195/195] total loss per batch: 0.903
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.9698, 0.0035, 0.0058, 0.0042, 0.0064, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.455

[Epoch: 28, batch: 39/195] total loss per batch: 0.886
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1345, 0.0055, 0.0074, 0.0139, 0.0072, 0.0065, 0.8250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 28, batch: 78/195] total loss per batch: 0.882
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4867e-01, 7.0664e-03, 7.7104e-03, 2.2193e-04, 1.3879e-02, 9.3180e-03,
        1.3131e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.122

[Epoch: 28, batch: 117/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0224e-02, 1.4919e-02, 4.3235e-05, 2.9098e-02, 2.9413e-02, 8.8103e-01,
        2.5274e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.011

[Epoch: 28, batch: 156/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.9380e-03, 1.1224e-01, 3.0453e-03, 5.9269e-02, 4.7040e-05, 8.1517e-01,
        4.2828e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.028

[Epoch: 28, batch: 195/195] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9703, 0.0032, 0.0060, 0.0045, 0.0047, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.557

[Epoch: 29, batch: 39/195] total loss per batch: 0.884
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2045, 0.0108, 0.0157, 0.0164, 0.0090, 0.0080, 0.7357],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 29, batch: 78/195] total loss per batch: 0.879
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3099e-01, 2.1477e-02, 1.3166e-02, 1.8864e-04, 1.3365e-02, 1.0332e-02,
        1.0484e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.050

[Epoch: 29, batch: 117/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.4383e-02, 1.6323e-02, 6.3506e-05, 5.1833e-02, 2.1330e-02, 8.4062e-01,
        3.5451e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 29, batch: 156/195] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.8278e-03, 3.0613e-03, 2.7619e-03, 2.9695e-02, 4.3460e-05, 9.5587e-01,
        3.7403e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.148

[Epoch: 29, batch: 195/195] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9608, 0.0054, 0.0047, 0.0112, 0.0049, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.468

[Epoch: 30, batch: 39/195] total loss per batch: 0.882
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1715, 0.0071, 0.0129, 0.0076, 0.0040, 0.0058, 0.7910],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 30, batch: 78/195] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7361e-01, 4.6042e-03, 5.9780e-03, 6.2477e-05, 5.7970e-03, 4.1499e-03,
        5.7947e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.088

[Epoch: 30, batch: 117/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.6487e-02, 1.1003e-02, 3.7814e-05, 2.6251e-02, 2.0607e-02, 9.0587e-01,
        1.9746e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 30, batch: 156/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0047e-02, 2.6785e-02, 4.5838e-03, 8.6896e-02, 6.1027e-05, 8.6087e-01,
        1.0757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.069

[Epoch: 30, batch: 195/195] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.9694, 0.0042, 0.0064, 0.0042, 0.0067, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.631

[Epoch: 31, batch: 39/195] total loss per batch: 0.881
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2084, 0.0079, 0.0144, 0.0100, 0.0127, 0.0068, 0.7397],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 31, batch: 78/195] total loss per batch: 0.877
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6515e-01, 6.8028e-03, 8.8202e-03, 2.0065e-04, 7.7502e-03, 5.1082e-03,
        6.1713e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.063

[Epoch: 31, batch: 117/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.3108e-02, 2.4284e-02, 7.0355e-05, 3.3917e-02, 2.3342e-02, 8.4121e-01,
        3.4065e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 31, batch: 156/195] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.1273e-02, 1.7602e-02, 2.1754e-03, 3.2225e-02, 3.9083e-05, 9.3112e-01,
        5.5646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.049

[Epoch: 31, batch: 195/195] total loss per batch: 0.897
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.9662, 0.0099, 0.0064, 0.0064, 0.0024, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.444

[Epoch: 32, batch: 39/195] total loss per batch: 0.881
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1847, 0.0088, 0.0167, 0.0079, 0.0056, 0.0063, 0.7701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 32, batch: 78/195] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5375e-01, 8.7101e-03, 9.9408e-03, 1.3145e-04, 1.0670e-02, 6.4210e-03,
        1.0374e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.129

[Epoch: 32, batch: 117/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([7.4757e-03, 1.7672e-02, 5.6610e-05, 1.4858e-02, 1.7795e-02, 9.1399e-01,
        2.8150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 32, batch: 156/195] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.2033e-03, 3.7158e-02, 2.8591e-03, 5.2876e-02, 3.6706e-05, 8.9365e-01,
        7.2137e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.070

[Epoch: 32, batch: 195/195] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9706, 0.0031, 0.0034, 0.0072, 0.0044, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.556

[Epoch: 33, batch: 39/195] total loss per batch: 0.884
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1629, 0.0131, 0.0170, 0.0107, 0.0040, 0.0063, 0.7860],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 33, batch: 78/195] total loss per batch: 0.877
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4989e-01, 1.3317e-02, 9.1363e-03, 1.8383e-04, 9.6752e-03, 7.1651e-03,
        1.0632e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.059

[Epoch: 33, batch: 117/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.5580e-02, 2.9161e-02, 8.7278e-05, 4.7189e-02, 8.0892e-02, 7.7133e-01,
        3.5756e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 33, batch: 156/195] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0670e-02, 3.6798e-02, 3.9153e-03, 3.9416e-02, 8.3001e-05, 9.0401e-01,
        5.1065e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.052

[Epoch: 33, batch: 195/195] total loss per batch: 0.900
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.9665, 0.0050, 0.0042, 0.0044, 0.0033, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.486

[Epoch: 34, batch: 39/195] total loss per batch: 0.885
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2267, 0.0162, 0.0112, 0.0064, 0.0041, 0.0047, 0.7307],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 34, batch: 78/195] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6698e-01, 5.0925e-03, 6.9863e-03, 1.2579e-04, 8.2428e-03, 6.8857e-03,
        5.6841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.178

[Epoch: 34, batch: 117/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.1355e-02, 1.0367e-02, 1.9836e-05, 1.0305e-02, 9.0972e-03, 9.3869e-01,
        2.0163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 34, batch: 156/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.2227e-03, 1.4975e-02, 4.8923e-03, 6.5915e-02, 2.1619e-05, 9.0232e-01,
        3.6537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.056

[Epoch: 34, batch: 195/195] total loss per batch: 0.902
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9690, 0.0049, 0.0033, 0.0090, 0.0041, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.527

[Epoch: 35, batch: 39/195] total loss per batch: 0.889
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1965, 0.0056, 0.0134, 0.0167, 0.0064, 0.0055, 0.7559],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 35, batch: 78/195] total loss per batch: 0.882
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7366e-01, 5.6989e-03, 4.9669e-03, 1.2881e-04, 6.2673e-03, 4.9296e-03,
        4.3502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.063

[Epoch: 35, batch: 117/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.6331e-02, 1.4054e-02, 4.0948e-05, 4.0545e-02, 2.5966e-02, 8.6489e-01,
        3.8175e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 35, batch: 156/195] total loss per batch: 0.884
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.0155e-03, 5.7606e-02, 5.0435e-03, 2.6739e-02, 4.9253e-05, 8.9035e-01,
        1.2196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.074

[Epoch: 35, batch: 195/195] total loss per batch: 0.904
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0136, 0.9357, 0.0095, 0.0093, 0.0069, 0.0149, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.445

[Epoch: 36, batch: 39/195] total loss per batch: 0.895
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1517, 0.0141, 0.0198, 0.0101, 0.0050, 0.0074, 0.7920],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 36, batch: 78/195] total loss per batch: 0.886
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4062e-01, 1.4012e-02, 1.1531e-02, 2.1932e-04, 1.4453e-02, 8.5962e-03,
        1.0565e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.089

[Epoch: 36, batch: 117/195] total loss per batch: 0.873
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3043e-02, 2.6798e-02, 6.5327e-05, 2.2413e-02, 1.1786e-02, 8.9223e-01,
        2.3669e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 36, batch: 156/195] total loss per batch: 0.886
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0918e-02, 3.7931e-02, 2.3489e-03, 1.1116e-01, 2.6133e-05, 8.3013e-01,
        7.4902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.009

[Epoch: 36, batch: 195/195] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.9632, 0.0057, 0.0056, 0.0107, 0.0052, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.608

[Epoch: 37, batch: 39/195] total loss per batch: 0.890
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2969, 0.0169, 0.0091, 0.0148, 0.0050, 0.0067, 0.6506],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 37, batch: 78/195] total loss per batch: 0.887
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5792e-01, 7.3215e-03, 8.2520e-03, 6.2587e-05, 7.8965e-03, 9.2025e-03,
        9.3493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.118

[Epoch: 37, batch: 117/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.8127e-02, 1.4570e-02, 3.8113e-05, 2.3266e-02, 3.1396e-02, 8.4615e-01,
        4.6452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.008

[Epoch: 37, batch: 156/195] total loss per batch: 0.885
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.3259e-03, 1.1579e-02, 7.8819e-03, 3.2251e-02, 4.8611e-05, 9.4186e-01,
        2.0491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.183

[Epoch: 37, batch: 195/195] total loss per batch: 0.903
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9663, 0.0047, 0.0044, 0.0077, 0.0060, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.434

[Epoch: 38, batch: 39/195] total loss per batch: 0.887
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1749, 0.0231, 0.0237, 0.0126, 0.0064, 0.0031, 0.7562],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 38, batch: 78/195] total loss per batch: 0.882
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5592e-01, 7.9221e-03, 1.0831e-02, 2.3991e-04, 1.0192e-02, 6.1886e-03,
        8.7030e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.024

[Epoch: 38, batch: 117/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([9.8563e-03, 2.0083e-02, 4.9400e-05, 2.8002e-02, 1.4663e-02, 9.1635e-01,
        1.1001e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 38, batch: 156/195] total loss per batch: 0.884
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.3008e-03, 1.1441e-01, 7.7472e-03, 3.4082e-02, 5.0739e-05, 8.2662e-01,
        1.0786e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.002

[Epoch: 38, batch: 195/195] total loss per batch: 0.901
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9622, 0.0046, 0.0063, 0.0119, 0.0068, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.554

[Epoch: 39, batch: 39/195] total loss per batch: 0.885
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1489, 0.0051, 0.0088, 0.0053, 0.0036, 0.0046, 0.8237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 39, batch: 78/195] total loss per batch: 0.880
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6328e-01, 8.2021e-03, 1.1990e-02, 7.4272e-05, 6.1082e-03, 4.4411e-03,
        5.9069e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 39, batch: 117/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2026e-02, 1.1621e-02, 5.9238e-05, 1.4598e-02, 9.3672e-03, 9.0794e-01,
        3.4386e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 39, batch: 156/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.8676e-03, 8.3919e-03, 5.1104e-03, 4.1919e-02, 3.8102e-05, 9.2793e-01,
        6.7414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.047

[Epoch: 39, batch: 195/195] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9479, 0.0033, 0.0070, 0.0184, 0.0043, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.546

[Epoch: 40, batch: 39/195] total loss per batch: 0.883
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2212, 0.0095, 0.0232, 0.0112, 0.0042, 0.0041, 0.7266],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 40, batch: 78/195] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4819e-01, 9.3103e-03, 8.4162e-03, 1.3248e-04, 9.9687e-03, 1.3073e-02,
        1.0910e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.108

[Epoch: 40, batch: 117/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9370e-02, 1.9233e-02, 5.2372e-05, 5.4089e-02, 3.8499e-02, 8.3249e-01,
        3.6265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 40, batch: 156/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.6624e-03, 1.5434e-02, 5.7953e-03, 5.3881e-02, 5.8126e-05, 9.1424e-01,
        4.9329e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.039

[Epoch: 40, batch: 195/195] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9655, 0.0036, 0.0040, 0.0056, 0.0070, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.470

[Epoch: 41, batch: 39/195] total loss per batch: 0.881
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1443, 0.0051, 0.0079, 0.0087, 0.0046, 0.0097, 0.8197],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 41, batch: 78/195] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6409e-01, 7.1195e-03, 9.1681e-03, 1.3892e-04, 9.0102e-03, 5.4466e-03,
        5.0220e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.063

[Epoch: 41, batch: 117/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.6105e-02, 2.2614e-02, 5.4556e-05, 1.0888e-02, 1.5580e-02, 8.9282e-01,
        2.1935e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.015

[Epoch: 41, batch: 156/195] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0663e-02, 5.7215e-02, 9.1498e-03, 7.9383e-02, 6.6388e-05, 8.2668e-01,
        1.6847e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.072

[Epoch: 41, batch: 195/195] total loss per batch: 0.896
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.9633, 0.0052, 0.0079, 0.0100, 0.0038, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.529

[Epoch: 42, batch: 39/195] total loss per batch: 0.880
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2242, 0.0070, 0.0150, 0.0078, 0.0060, 0.0057, 0.7342],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 42, batch: 78/195] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.1262e-01, 1.8317e-02, 1.5072e-02, 2.6258e-04, 1.7410e-02, 1.4484e-02,
        2.1837e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.129

[Epoch: 42, batch: 117/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4344e-02, 1.9692e-02, 3.4084e-05, 2.0494e-02, 3.5140e-02, 8.5488e-01,
        4.5417e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 42, batch: 156/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.3559e-03, 1.8833e-02, 2.4518e-03, 3.8135e-02, 6.5117e-05, 9.3343e-01,
        3.7287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.082

[Epoch: 42, batch: 195/195] total loss per batch: 0.896
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9742, 0.0024, 0.0051, 0.0034, 0.0058, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.625

[Epoch: 43, batch: 39/195] total loss per batch: 0.879
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1981, 0.0052, 0.0121, 0.0096, 0.0061, 0.0047, 0.7640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 43, batch: 78/195] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4930e-01, 1.1986e-02, 1.1905e-02, 2.6409e-04, 1.0048e-02, 5.6163e-03,
        1.0880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.066

[Epoch: 43, batch: 117/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6978e-02, 1.2390e-02, 5.4933e-05, 2.4495e-02, 2.5863e-02, 8.9485e-01,
        1.5373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 43, batch: 156/195] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.8818e-03, 2.8953e-02, 6.1032e-03, 8.0042e-02, 3.8332e-05, 8.7016e-01,
        4.8196e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.080

[Epoch: 43, batch: 195/195] total loss per batch: 0.896
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9616, 0.0034, 0.0067, 0.0088, 0.0047, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.402

[Epoch: 44, batch: 39/195] total loss per batch: 0.879
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1433, 0.0086, 0.0117, 0.0071, 0.0039, 0.0064, 0.8190],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 44, batch: 78/195] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6681e-01, 5.0592e-03, 6.6788e-03, 8.8498e-05, 7.1017e-03, 6.1178e-03,
        8.1488e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.031

[Epoch: 44, batch: 117/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1497e-02, 1.8690e-02, 6.7312e-05, 2.6131e-02, 2.1702e-02, 8.5026e-01,
        6.1657e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.015

[Epoch: 44, batch: 156/195] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4430e-03, 3.5779e-02, 3.7343e-03, 2.8903e-02, 7.5074e-05, 9.1724e-01,
        7.8217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.049

[Epoch: 44, batch: 195/195] total loss per batch: 0.895
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9671, 0.0028, 0.0052, 0.0055, 0.0098, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.634

[Epoch: 45, batch: 39/195] total loss per batch: 0.878
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2888, 0.0060, 0.0132, 0.0081, 0.0044, 0.0041, 0.6754],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 45, batch: 78/195] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6235e-01, 6.4798e-03, 9.3201e-03, 1.9329e-04, 8.0480e-03, 5.3899e-03,
        8.2157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 45, batch: 117/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9317e-02, 9.4172e-03, 4.7360e-05, 3.2729e-02, 1.5930e-02, 8.9175e-01,
        2.0808e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 45, batch: 156/195] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.2576e-03, 7.3085e-02, 3.5563e-03, 8.3601e-02, 3.8142e-05, 8.2330e-01,
        7.1601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.083

[Epoch: 45, batch: 195/195] total loss per batch: 0.895
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.9712, 0.0057, 0.0051, 0.0043, 0.0060, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.487

[Epoch: 46, batch: 39/195] total loss per batch: 0.878
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1223, 0.0047, 0.0093, 0.0065, 0.0039, 0.0031, 0.8501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 46, batch: 78/195] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4815e-01, 1.1463e-02, 9.2991e-03, 1.1075e-04, 1.1982e-02, 8.9780e-03,
        1.0013e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.160

[Epoch: 46, batch: 117/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7460e-02, 1.2677e-02, 3.5009e-05, 1.5010e-02, 2.0009e-02, 9.0172e-01,
        2.3090e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 46, batch: 156/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.5149e-03, 8.9127e-03, 3.2384e-03, 2.4272e-02, 6.9793e-05, 9.5343e-01,
        7.5598e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.113

[Epoch: 46, batch: 195/195] total loss per batch: 0.896
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.9713, 0.0024, 0.0026, 0.0082, 0.0054, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.564

[Epoch: 47, batch: 39/195] total loss per batch: 0.879
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2688, 0.0062, 0.0178, 0.0062, 0.0060, 0.0087, 0.6863],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 47, batch: 78/195] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.2085e-01, 1.2937e-02, 1.9176e-02, 2.8136e-04, 1.7619e-02, 8.6690e-03,
        2.0466e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.093

[Epoch: 47, batch: 117/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4204e-02, 1.6909e-02, 7.0326e-05, 2.7575e-02, 4.7025e-02, 8.4356e-01,
        4.0657e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.017

[Epoch: 47, batch: 156/195] total loss per batch: 0.882
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.3281e-03, 3.1224e-02, 3.8315e-03, 4.3044e-02, 7.9366e-05, 9.0974e-01,
        4.7580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.043

[Epoch: 47, batch: 195/195] total loss per batch: 0.897
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9794, 0.0027, 0.0035, 0.0021, 0.0043, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.471

[Epoch: 48, batch: 39/195] total loss per batch: 0.890
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2339, 0.0092, 0.0101, 0.0078, 0.0025, 0.0062, 0.7304],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.002

[Epoch: 48, batch: 78/195] total loss per batch: 0.939
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.9646, 0.0109, 0.0043, 0.0012, 0.0060, 0.0041, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 -0.003

[Epoch: 48, batch: 117/195] total loss per batch: 0.961
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([6.4449e-03, 3.5201e-02, 1.3441e-05, 7.3554e-03, 2.7926e-03, 9.1959e-01,
        2.8607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.030

[Epoch: 48, batch: 156/195] total loss per batch: 0.978
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.0265e-02, 2.8356e-01, 6.2408e-03, 5.7232e-02, 1.4847e-04, 5.5717e-01,
        3.5392e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.038

[Epoch: 48, batch: 195/195] total loss per batch: 1.009
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0600, 0.8839, 0.0075, 0.0082, 0.0217, 0.0066, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.125

[Epoch: 49, batch: 39/195] total loss per batch: 0.971
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1448, 0.0147, 0.0449, 0.0127, 0.0104, 0.0210, 0.7515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 49, batch: 78/195] total loss per batch: 0.949
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7149e-01, 4.9882e-03, 8.8089e-03, 7.5549e-05, 2.5225e-03, 6.3332e-03,
        5.7799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 -0.001

[Epoch: 49, batch: 117/195] total loss per batch: 0.932
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([9.6349e-02, 1.7483e-02, 6.1333e-06, 3.7394e-02, 2.5740e-02, 7.8943e-01,
        3.3594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 49, batch: 156/195] total loss per batch: 0.934
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6262e-03, 4.2525e-03, 1.6344e-03, 2.6041e-02, 6.7779e-05, 9.4450e-01,
        1.8879e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.043

[Epoch: 49, batch: 195/195] total loss per batch: 0.952
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.9278, 0.0035, 0.0069, 0.0424, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.516

[Epoch: 50, batch: 39/195] total loss per batch: 0.917
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2886, 0.0086, 0.0128, 0.0087, 0.0097, 0.0103, 0.6613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 50, batch: 78/195] total loss per batch: 0.905
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3704e-01, 1.5937e-02, 2.2662e-02, 3.1772e-04, 6.8217e-03, 6.6660e-03,
        1.0551e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 -0.002

[Epoch: 50, batch: 117/195] total loss per batch: 0.884
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([5.7283e-02, 8.2662e-03, 1.4833e-05, 4.8261e-02, 2.9455e-02, 8.3496e-01,
        2.1756e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 50, batch: 156/195] total loss per batch: 0.892
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.1582e-02, 2.0451e-02, 2.3039e-03, 4.0564e-02, 3.4322e-05, 9.2111e-01,
        3.9551e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.011

[Epoch: 50, batch: 195/195] total loss per batch: 0.907
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0024, 0.9650, 0.0039, 0.0029, 0.0166, 0.0038, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.748

[Epoch: 51, batch: 39/195] total loss per batch: 0.887
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1297, 0.0107, 0.0188, 0.0097, 0.0042, 0.0067, 0.8203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 51, batch: 78/195] total loss per batch: 0.879
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5661e-01, 1.1623e-02, 1.2116e-02, 1.8609e-04, 5.5265e-03, 6.4739e-03,
        7.4621e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 -0.006

[Epoch: 51, batch: 117/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.8183e-02, 8.1648e-03, 2.9170e-05, 2.3420e-02, 2.6836e-02, 8.7210e-01,
        2.1265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.019

[Epoch: 51, batch: 156/195] total loss per batch: 0.876
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0241e-03, 1.8546e-02, 1.6493e-03, 6.6493e-02, 5.0433e-05, 9.0537e-01,
        3.8631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.065

[Epoch: 51, batch: 195/195] total loss per batch: 0.891
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9709, 0.0049, 0.0035, 0.0096, 0.0030, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 52, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2060, 0.0090, 0.0201, 0.0101, 0.0048, 0.0084, 0.7417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.020

[Epoch: 52, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5285e-01, 1.2371e-02, 1.3465e-02, 2.2219e-04, 6.8526e-03, 6.2724e-03,
        7.9684e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.007

[Epoch: 52, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.7352e-02, 9.4930e-03, 2.5025e-05, 2.5091e-02, 2.8604e-02, 8.8110e-01,
        1.8340e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.019

[Epoch: 52, batch: 156/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.6451e-03, 2.4710e-02, 2.1161e-03, 6.2189e-02, 5.6923e-05, 9.0341e-01,
        3.8682e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.054

[Epoch: 52, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.9687, 0.0057, 0.0039, 0.0083, 0.0039, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.487

[Epoch: 53, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1617, 0.0069, 0.0145, 0.0067, 0.0042, 0.0063, 0.7997],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 53, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5716e-01, 1.0011e-02, 1.3052e-02, 1.7229e-04, 6.6038e-03, 5.5306e-03,
        7.4692e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.032

[Epoch: 53, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3682e-02, 9.7534e-03, 2.1904e-05, 2.4826e-02, 2.8835e-02, 8.8186e-01,
        2.1024e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 53, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9711e-03, 3.0829e-02, 2.5843e-03, 6.0154e-02, 4.4856e-05, 8.9822e-01,
        4.1950e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 53, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.9711, 0.0053, 0.0041, 0.0064, 0.0037, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.521

[Epoch: 54, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2044, 0.0063, 0.0128, 0.0066, 0.0037, 0.0055, 0.7607],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 54, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5718e-01, 9.5828e-03, 1.3414e-02, 1.7459e-04, 6.4600e-03, 5.3422e-03,
        7.8456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.057

[Epoch: 54, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8673e-02, 1.0882e-02, 2.1429e-05, 2.2060e-02, 2.7262e-02, 8.8904e-01,
        2.2062e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 54, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.5352e-03, 3.4396e-02, 2.3152e-03, 5.1872e-02, 4.0603e-05, 9.0380e-01,
        4.0386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.058

[Epoch: 54, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9714, 0.0050, 0.0039, 0.0064, 0.0037, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.523

[Epoch: 55, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1870, 0.0057, 0.0116, 0.0059, 0.0039, 0.0050, 0.7808],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 55, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5630e-01, 9.6341e-03, 1.3550e-02, 1.4738e-04, 6.7405e-03, 5.4744e-03,
        8.1539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.074

[Epoch: 55, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9403e-02, 1.1998e-02, 2.0678e-05, 2.5927e-02, 2.7812e-02, 8.8030e-01,
        2.4540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 55, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.8477e-03, 3.2059e-02, 2.6275e-03, 5.5071e-02, 3.8137e-05, 9.0207e-01,
        4.2809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 55, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9696, 0.0053, 0.0045, 0.0059, 0.0044, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.523

[Epoch: 56, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1964, 0.0053, 0.0112, 0.0061, 0.0034, 0.0050, 0.7726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 56, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5709e-01, 9.6995e-03, 1.3151e-02, 1.3025e-04, 6.3985e-03, 5.2311e-03,
        8.2949e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.102

[Epoch: 56, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5464e-02, 1.2588e-02, 1.9609e-05, 2.3716e-02, 2.7630e-02, 8.8375e-01,
        2.6831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 56, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.6515e-03, 3.5106e-02, 2.6810e-03, 5.2891e-02, 3.5200e-05, 9.0112e-01,
        4.5157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 56, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.9697, 0.0052, 0.0044, 0.0060, 0.0041, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.527

[Epoch: 57, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1999, 0.0055, 0.0111, 0.0057, 0.0045, 0.0054, 0.7678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 57, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5547e-01, 9.9951e-03, 1.2822e-02, 1.4015e-04, 6.9574e-03, 5.5973e-03,
        9.0199e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.098

[Epoch: 57, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7974e-02, 1.3099e-02, 1.8123e-05, 2.4465e-02, 2.6570e-02, 8.8097e-01,
        2.6903e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 57, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9304e-03, 3.1597e-02, 2.8729e-03, 5.3132e-02, 2.9046e-05, 9.0441e-01,
        4.0330e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.083

[Epoch: 57, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9671, 0.0056, 0.0048, 0.0064, 0.0054, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 58, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1833, 0.0047, 0.0104, 0.0060, 0.0035, 0.0049, 0.7871],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 58, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5922e-01, 8.8687e-03, 1.2672e-02, 9.6612e-05, 6.1873e-03, 4.9875e-03,
        7.9671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.101

[Epoch: 58, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2821e-02, 1.4999e-02, 1.9134e-05, 2.5742e-02, 2.6877e-02, 8.8243e-01,
        2.7110e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 58, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.3947e-03, 3.9934e-02, 2.8946e-03, 4.8929e-02, 3.2412e-05, 8.9889e-01,
        4.9249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.060

[Epoch: 58, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9705, 0.0049, 0.0042, 0.0055, 0.0040, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.534

[Epoch: 59, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2106, 0.0059, 0.0121, 0.0056, 0.0053, 0.0065, 0.7540],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 59, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5530e-01, 1.1357e-02, 1.1428e-02, 1.5324e-04, 6.7542e-03, 5.6971e-03,
        9.3099e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.099

[Epoch: 59, batch: 117/195] total loss per batch: 0.856
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4040e-02, 1.4506e-02, 1.6920e-05, 2.0813e-02, 2.2317e-02, 8.9010e-01,
        2.8203e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 59, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.1711e-03, 2.6464e-02, 3.2792e-03, 6.2655e-02, 2.7411e-05, 8.9912e-01,
        4.2792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.081

[Epoch: 59, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9633, 0.0061, 0.0052, 0.0075, 0.0072, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.512

[Epoch: 60, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1867, 0.0042, 0.0097, 0.0050, 0.0029, 0.0037, 0.7877],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 60, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5402e-01, 1.0657e-02, 1.2190e-02, 8.5615e-05, 7.1459e-03, 5.9194e-03,
        9.9780e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.117

[Epoch: 60, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6154e-02, 1.6081e-02, 2.5868e-05, 3.0907e-02, 3.2623e-02, 8.6183e-01,
        3.2381e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 60, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.8243e-03, 3.7721e-02, 2.5569e-03, 3.5809e-02, 4.0710e-05, 9.1373e-01,
        5.3136e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 60, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9716, 0.0035, 0.0036, 0.0056, 0.0045, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.562

[Epoch: 61, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2052, 0.0077, 0.0151, 0.0061, 0.0057, 0.0067, 0.7535],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 61, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5456e-01, 1.2841e-02, 1.2363e-02, 1.6190e-04, 6.6220e-03, 4.9562e-03,
        8.4991e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.094

[Epoch: 61, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2020e-02, 1.6987e-02, 1.8254e-05, 1.8775e-02, 1.7889e-02, 8.9610e-01,
        2.8210e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 61, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.2259e-03, 3.9882e-02, 3.4795e-03, 7.3870e-02, 3.9953e-05, 8.7324e-01,
        4.2597e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.059

[Epoch: 61, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9628, 0.0056, 0.0064, 0.0071, 0.0060, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.469

[Epoch: 62, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1869, 0.0045, 0.0123, 0.0083, 0.0036, 0.0050, 0.7795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 62, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5917e-01, 7.7261e-03, 1.0357e-02, 6.9311e-05, 6.8615e-03, 6.2242e-03,
        9.5935e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.134

[Epoch: 62, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([5.0346e-02, 1.8118e-02, 4.4952e-05, 3.2687e-02, 3.8818e-02, 8.3147e-01,
        2.8514e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 62, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9048e-03, 3.0735e-02, 3.5228e-03, 2.8604e-02, 5.1448e-05, 9.2793e-01,
        5.2501e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.073

[Epoch: 62, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9678, 0.0055, 0.0036, 0.0063, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.571

[Epoch: 63, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1868, 0.0070, 0.0121, 0.0042, 0.0053, 0.0070, 0.7776],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 63, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4762e-01, 1.2461e-02, 1.4648e-02, 2.5518e-04, 8.5069e-03, 5.8739e-03,
        1.0636e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.065

[Epoch: 63, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7000e-02, 1.4585e-02, 1.5574e-05, 2.1992e-02, 1.8278e-02, 8.9375e-01,
        3.4376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 63, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.0343e-03, 3.8476e-02, 6.0604e-03, 7.5719e-02, 4.6560e-05, 8.6667e-01,
        6.9894e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 63, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9641, 0.0048, 0.0046, 0.0086, 0.0057, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.490

[Epoch: 64, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2174, 0.0065, 0.0114, 0.0086, 0.0036, 0.0049, 0.7476],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 64, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7325e-01, 5.1824e-03, 7.9937e-03, 7.7561e-05, 4.2857e-03, 3.6632e-03,
        5.5435e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.099

[Epoch: 64, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.5471e-02, 2.1068e-02, 4.9399e-05, 2.5341e-02, 2.6710e-02, 8.5771e-01,
        3.3653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 64, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4130e-03, 3.8677e-02, 5.0082e-03, 3.9491e-02, 4.1809e-05, 9.0538e-01,
        5.9856e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.051

[Epoch: 64, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9660, 0.0046, 0.0052, 0.0055, 0.0056, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 65, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1783, 0.0064, 0.0160, 0.0063, 0.0056, 0.0066, 0.7809],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 65, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5910e-01, 1.0513e-02, 1.1109e-02, 1.2426e-04, 6.6614e-03, 4.7255e-03,
        7.7689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.112

[Epoch: 65, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.2708e-02, 1.9094e-02, 3.8453e-05, 2.5359e-02, 2.8987e-02, 8.8255e-01,
        3.1264e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 65, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5068e-03, 1.7010e-02, 5.5341e-03, 4.6977e-02, 2.1061e-05, 9.2172e-01,
        4.2323e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.079

[Epoch: 65, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9649, 0.0053, 0.0062, 0.0065, 0.0056, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.520

[Epoch: 66, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1760, 0.0050, 0.0125, 0.0123, 0.0030, 0.0044, 0.7869],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.023

[Epoch: 66, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6667e-01, 6.4959e-03, 7.9912e-03, 5.8239e-05, 5.7380e-03, 5.0888e-03,
        7.9568e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.132

[Epoch: 66, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9401e-02, 1.5201e-02, 1.7247e-05, 1.8127e-02, 1.9160e-02, 8.9209e-01,
        2.6004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 66, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.9537e-03, 3.1476e-02, 6.1058e-03, 5.8825e-02, 6.0180e-05, 8.9293e-01,
        5.6439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.119

[Epoch: 66, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9758, 0.0052, 0.0035, 0.0030, 0.0038, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.562

[Epoch: 67, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2383, 0.0052, 0.0180, 0.0143, 0.0050, 0.0068, 0.7125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 67, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5897e-01, 1.0517e-02, 1.1115e-02, 1.7347e-04, 5.9338e-03, 4.7042e-03,
        8.5842e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.103

[Epoch: 67, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7401e-02, 1.4965e-02, 3.5237e-05, 2.2283e-02, 1.8629e-02, 8.9646e-01,
        3.0220e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 67, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.1109e-02, 5.0880e-02, 3.9044e-03, 4.3982e-02, 3.8156e-05, 8.8266e-01,
        7.4293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.031

[Epoch: 67, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9613, 0.0055, 0.0068, 0.0083, 0.0070, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.490

[Epoch: 68, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1624, 0.0063, 0.0095, 0.0050, 0.0043, 0.0050, 0.8075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 68, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5037e-01, 1.1544e-02, 1.1995e-02, 1.0472e-04, 7.3375e-03, 7.0781e-03,
        1.1571e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.101

[Epoch: 68, batch: 117/195] total loss per batch: 0.861
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9963e-02, 1.7337e-02, 1.9954e-05, 2.7684e-02, 5.2419e-02, 8.4122e-01,
        3.1355e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 68, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.7763e-03, 2.5766e-02, 5.1242e-03, 6.7988e-02, 2.9264e-05, 8.9319e-01,
        4.1274e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.112

[Epoch: 68, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9793, 0.0031, 0.0043, 0.0029, 0.0032, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.524

[Epoch: 69, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2428, 0.0049, 0.0181, 0.0116, 0.0044, 0.0060, 0.7122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.020

[Epoch: 69, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5517e-01, 9.4665e-03, 1.4319e-02, 1.8111e-04, 7.5138e-03, 5.4637e-03,
        7.8848e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.126

[Epoch: 69, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.3048e-02, 1.4024e-02, 3.5371e-05, 2.3244e-02, 8.3257e-03, 9.1718e-01,
        2.4146e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 69, batch: 156/195] total loss per batch: 0.873
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.5957e-03, 2.2739e-02, 3.0086e-03, 2.8121e-02, 2.7267e-05, 9.3346e-01,
        5.0450e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.027

[Epoch: 69, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9635, 0.0069, 0.0065, 0.0053, 0.0051, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.544

[Epoch: 70, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1395, 0.0050, 0.0084, 0.0061, 0.0027, 0.0055, 0.8328],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 70, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5945e-01, 9.6135e-03, 1.0588e-02, 1.4013e-04, 6.9187e-03, 5.5538e-03,
        7.7366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.069

[Epoch: 70, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6738e-02, 1.8245e-02, 3.0138e-05, 2.3642e-02, 2.0140e-02, 8.8396e-01,
        2.7240e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 70, batch: 156/195] total loss per batch: 0.873
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.3325e-03, 3.6567e-02, 4.4479e-03, 1.0628e-01, 4.7347e-05, 8.4116e-01,
        7.1631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.123

[Epoch: 70, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9658, 0.0055, 0.0049, 0.0050, 0.0066, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.510

[Epoch: 71, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2017, 0.0067, 0.0184, 0.0083, 0.0068, 0.0074, 0.7509],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 71, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5573e-01, 8.2333e-03, 1.1695e-02, 2.0896e-04, 7.6411e-03, 5.3850e-03,
        1.1104e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.099

[Epoch: 71, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.2269e-02, 1.6610e-02, 3.6170e-05, 3.6430e-02, 3.0340e-02, 8.4836e-01,
        3.5951e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 71, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.5554e-03, 3.4154e-02, 6.2610e-03, 2.2215e-02, 3.8721e-05, 9.2532e-01,
        4.4577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.045

[Epoch: 71, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.9631, 0.0054, 0.0085, 0.0052, 0.0051, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.497

[Epoch: 72, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2425, 0.0065, 0.0115, 0.0058, 0.0073, 0.0081, 0.7183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 72, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6020e-01, 8.1684e-03, 1.3462e-02, 1.1331e-04, 5.2821e-03, 5.4453e-03,
        7.3328e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.164

[Epoch: 72, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1087e-02, 1.7788e-02, 2.8936e-05, 1.8559e-02, 1.9275e-02, 8.9222e-01,
        3.1046e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 72, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6129e-03, 3.4838e-02, 3.3334e-03, 5.0609e-02, 3.8338e-05, 9.0209e-01,
        4.4828e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.079

[Epoch: 72, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.9651, 0.0046, 0.0034, 0.0068, 0.0071, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.587

[Epoch: 73, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1671, 0.0060, 0.0121, 0.0074, 0.0034, 0.0057, 0.7983],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 73, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6612e-01, 7.1528e-03, 8.5992e-03, 1.1948e-04, 5.8669e-03, 4.6388e-03,
        7.5037e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.073

[Epoch: 73, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1560e-02, 1.3022e-02, 2.0839e-05, 2.1792e-02, 2.1962e-02, 8.9622e-01,
        2.5419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 73, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.1983e-03, 2.6901e-02, 7.6993e-03, 3.9426e-02, 5.3275e-05, 9.1532e-01,
        5.3975e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.057

[Epoch: 73, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.9641, 0.0061, 0.0074, 0.0047, 0.0077, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 74, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1925, 0.0050, 0.0119, 0.0053, 0.0056, 0.0064, 0.7733],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 74, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6496e-01, 7.1948e-03, 9.3901e-03, 1.0123e-04, 5.8458e-03, 5.6533e-03,
        6.8570e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.075

[Epoch: 74, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1842e-02, 2.1351e-02, 3.7762e-05, 1.8295e-02, 2.1772e-02, 8.8479e-01,
        3.1912e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 74, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.9946e-03, 4.0667e-02, 3.2253e-03, 4.8819e-02, 5.7961e-05, 8.9450e-01,
        5.7400e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.118

[Epoch: 74, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9622, 0.0050, 0.0061, 0.0091, 0.0061, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.553

[Epoch: 75, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2148, 0.0066, 0.0121, 0.0061, 0.0045, 0.0058, 0.7501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 75, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6540e-01, 7.1396e-03, 8.0232e-03, 1.1430e-04, 6.9688e-03, 3.9385e-03,
        8.4156e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.098

[Epoch: 75, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4959e-02, 1.5343e-02, 1.7839e-05, 4.8666e-02, 4.0479e-02, 8.3142e-01,
        3.9117e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 75, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.0388e-03, 5.0720e-02, 3.5878e-03, 6.0824e-02, 4.5893e-05, 8.7212e-01,
        3.6583e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.035

[Epoch: 75, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9731, 0.0072, 0.0030, 0.0047, 0.0035, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.498

[Epoch: 76, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1784, 0.0047, 0.0111, 0.0052, 0.0035, 0.0046, 0.7926],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 76, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6665e-01, 5.1818e-03, 1.0091e-02, 5.8482e-05, 5.0645e-03, 6.6435e-03,
        6.3085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.093

[Epoch: 76, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.5960e-02, 1.1011e-02, 1.9635e-05, 1.1355e-02, 1.3971e-02, 9.3166e-01,
        1.6023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 76, batch: 156/195] total loss per batch: 0.872
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.3354e-03, 1.9304e-02, 3.9014e-03, 4.9271e-02, 5.9222e-05, 9.1109e-01,
        1.1039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.115

[Epoch: 76, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9704, 0.0042, 0.0040, 0.0050, 0.0071, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.584

[Epoch: 77, batch: 39/195] total loss per batch: 0.875
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2095, 0.0065, 0.0147, 0.0075, 0.0046, 0.0071, 0.7501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 77, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3810e-01, 1.2797e-02, 1.1810e-02, 1.9679e-04, 1.1709e-02, 1.0500e-02,
        1.4887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.079

[Epoch: 77, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9498e-02, 2.6195e-02, 4.4681e-05, 3.6345e-02, 2.6867e-02, 8.3829e-01,
        4.2756e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 77, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.5136e-03, 8.1909e-02, 4.5642e-03, 7.3127e-02, 7.7539e-05, 8.2979e-01,
        3.0218e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.055

[Epoch: 77, batch: 195/195] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9743, 0.0037, 0.0032, 0.0049, 0.0057, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.386

[Epoch: 78, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2021, 0.0054, 0.0128, 0.0032, 0.0064, 0.0051, 0.7650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 78, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5704e-01, 9.0828e-03, 1.1992e-02, 7.9624e-05, 7.3767e-03, 5.8306e-03,
        8.5958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.098

[Epoch: 78, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3677e-02, 1.7418e-02, 3.5716e-05, 3.1409e-02, 3.7067e-02, 8.4720e-01,
        3.3197e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 78, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.3607e-03, 1.4253e-02, 4.6671e-03, 4.0880e-02, 5.1748e-05, 9.3039e-01,
        4.3947e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.091

[Epoch: 78, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9610, 0.0053, 0.0065, 0.0067, 0.0086, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.641

[Epoch: 79, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1484, 0.0071, 0.0076, 0.0069, 0.0048, 0.0052, 0.8200],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 79, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5388e-01, 1.0167e-02, 1.2050e-02, 2.2987e-04, 7.0694e-03, 7.5997e-03,
        9.0014e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.128

[Epoch: 79, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.6293e-02, 1.5543e-02, 2.5049e-05, 1.7446e-02, 1.4019e-02, 9.1600e-01,
        2.0671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 79, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.5258e-03, 2.0590e-02, 3.6197e-03, 3.7132e-02, 3.4075e-05, 9.3243e-01,
        2.6724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 79, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9633, 0.0057, 0.0049, 0.0077, 0.0059, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.511

[Epoch: 80, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2505, 0.0051, 0.0130, 0.0070, 0.0055, 0.0097, 0.7093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 80, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3586e-01, 1.9176e-02, 1.3923e-02, 2.2343e-04, 9.4732e-03, 7.7064e-03,
        1.3642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.095

[Epoch: 80, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3670e-02, 1.6374e-02, 2.1906e-05, 2.3926e-02, 2.6562e-02, 8.7348e-01,
        2.5964e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 80, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.0712e-03, 4.4825e-02, 6.1474e-03, 9.0859e-02, 1.1949e-04, 8.4133e-01,
        7.6504e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.048

[Epoch: 80, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.9728, 0.0044, 0.0056, 0.0045, 0.0040, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.494

[Epoch: 81, batch: 39/195] total loss per batch: 0.874
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2220, 0.0076, 0.0156, 0.0076, 0.0084, 0.0073, 0.7317],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 81, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6405e-01, 1.0117e-02, 7.6478e-03, 9.5838e-05, 5.4705e-03, 4.7953e-03,
        7.8243e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 81, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1392e-02, 1.6218e-02, 2.5830e-05, 1.8895e-02, 2.5211e-02, 8.8529e-01,
        3.2968e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 81, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4050e-03, 2.8676e-02, 4.9387e-03, 3.1813e-02, 4.7504e-05, 9.2449e-01,
        3.6254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.114

[Epoch: 81, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9489, 0.0086, 0.0068, 0.0107, 0.0112, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.505

[Epoch: 82, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1389, 0.0041, 0.0099, 0.0081, 0.0039, 0.0048, 0.8303],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.027

[Epoch: 82, batch: 78/195] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7108e-01, 4.6826e-03, 8.3531e-03, 8.0289e-05, 5.3653e-03, 4.4017e-03,
        6.0379e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 82, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0888e-02, 1.3695e-02, 2.7320e-05, 2.7181e-02, 2.2079e-02, 8.8587e-01,
        2.0261e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 82, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0012e-02, 4.0384e-02, 4.7734e-03, 4.6871e-02, 5.0109e-05, 8.9381e-01,
        4.1007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.047

[Epoch: 82, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.9753, 0.0044, 0.0033, 0.0048, 0.0030, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.539

[Epoch: 83, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2547, 0.0062, 0.0143, 0.0042, 0.0056, 0.0055, 0.7095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.024

[Epoch: 83, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6064e-01, 1.0010e-02, 8.6421e-03, 7.1689e-05, 5.9387e-03, 6.7357e-03,
        7.9586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.081

[Epoch: 83, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8595e-02, 1.7798e-02, 5.6839e-05, 2.2675e-02, 2.2643e-02, 8.8546e-01,
        3.2768e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 83, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7629e-03, 2.5182e-02, 7.0090e-03, 6.6911e-02, 4.8717e-05, 8.8946e-01,
        5.6264e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.107

[Epoch: 83, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9675, 0.0043, 0.0059, 0.0061, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.480

[Epoch: 84, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1763, 0.0081, 0.0116, 0.0062, 0.0054, 0.0094, 0.7830],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 84, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5090e-01, 9.0631e-03, 1.3115e-02, 1.4342e-04, 7.5102e-03, 7.1276e-03,
        1.2139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.128

[Epoch: 84, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8047e-02, 1.4710e-02, 1.5941e-05, 2.6028e-02, 3.0451e-02, 8.6955e-01,
        4.1200e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 84, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.6178e-03, 7.4218e-02, 4.2491e-03, 4.0599e-02, 3.8291e-05, 8.7090e-01,
        4.3796e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.059

[Epoch: 84, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9748, 0.0050, 0.0039, 0.0032, 0.0036, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.583

[Epoch: 85, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1954, 0.0066, 0.0136, 0.0063, 0.0036, 0.0066, 0.7679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 85, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5196e-01, 1.0415e-02, 1.0748e-02, 1.5662e-04, 9.0802e-03, 7.1601e-03,
        1.0482e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.090

[Epoch: 85, batch: 117/195] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.8081e-02, 1.9318e-02, 8.4140e-05, 3.4133e-02, 2.1306e-02, 8.4642e-01,
        3.0661e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 85, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.4350e-03, 1.4691e-02, 4.2006e-03, 5.3835e-02, 1.4173e-05, 9.1912e-01,
        3.7082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.082

[Epoch: 85, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9720, 0.0031, 0.0049, 0.0037, 0.0064, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.476

[Epoch: 86, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2120, 0.0044, 0.0108, 0.0065, 0.0045, 0.0048, 0.7571],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 86, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6251e-01, 8.0376e-03, 9.9297e-03, 5.0482e-05, 6.1491e-03, 5.7465e-03,
        7.5814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.072

[Epoch: 86, batch: 117/195] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.4601e-02, 1.6139e-02, 6.1711e-05, 2.2416e-02, 2.2133e-02, 8.9267e-01,
        3.1979e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 86, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.0631e-03, 3.2006e-02, 7.3538e-03, 5.1347e-02, 5.7606e-05, 8.9628e-01,
        5.8911e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.073

[Epoch: 86, batch: 195/195] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9670, 0.0065, 0.0066, 0.0048, 0.0038, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.526

[Epoch: 87, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2109, 0.0060, 0.0149, 0.0069, 0.0066, 0.0055, 0.7492],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 87, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5816e-01, 9.8357e-03, 9.4896e-03, 5.9215e-05, 7.2599e-03, 6.3564e-03,
        8.8358e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.147

[Epoch: 87, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.4620e-02, 1.4595e-02, 4.9425e-05, 3.5881e-02, 3.6541e-02, 8.7232e-01,
        2.5996e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 87, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.8738e-03, 3.0355e-02, 3.5897e-03, 4.8751e-02, 3.5299e-05, 9.0784e-01,
        2.5580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.105

[Epoch: 87, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.9624, 0.0057, 0.0043, 0.0059, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.520

[Epoch: 88, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1715, 0.0065, 0.0095, 0.0072, 0.0059, 0.0068, 0.7927],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 88, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7183e-01, 5.5890e-03, 7.1270e-03, 5.3479e-05, 5.6475e-03, 3.7422e-03,
        6.0156e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.079

[Epoch: 88, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3150e-02, 1.6534e-02, 3.2305e-05, 1.6223e-02, 1.2276e-02, 9.1152e-01,
        2.0270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 88, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.5712e-03, 4.5828e-02, 7.8205e-03, 6.6222e-02, 3.0477e-05, 8.6680e-01,
        7.7254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 88, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9659, 0.0055, 0.0076, 0.0039, 0.0065, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.536

[Epoch: 89, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1796, 0.0063, 0.0163, 0.0064, 0.0091, 0.0056, 0.7767],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.021

[Epoch: 89, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4764e-01, 1.0341e-02, 1.1822e-02, 9.9208e-05, 8.1449e-03, 7.6814e-03,
        1.4270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.053

[Epoch: 89, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6489e-02, 1.6757e-02, 5.6888e-05, 2.8631e-02, 2.8915e-02, 8.5683e-01,
        4.2321e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 89, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0397e-02, 2.2556e-02, 4.0397e-03, 3.7954e-02, 2.8412e-05, 9.2023e-01,
        4.7941e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.054

[Epoch: 89, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9702, 0.0040, 0.0044, 0.0051, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.510

[Epoch: 90, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2229, 0.0071, 0.0081, 0.0051, 0.0028, 0.0051, 0.7488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 90, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7176e-01, 7.4484e-03, 6.5453e-03, 8.0801e-05, 5.4113e-03, 3.8295e-03,
        4.9247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.119

[Epoch: 90, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.0416e-02, 1.4722e-02, 4.3695e-05, 3.1267e-02, 2.5551e-02, 8.6362e-01,
        2.4381e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 90, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5945e-03, 4.3026e-02, 6.5221e-03, 6.6759e-02, 8.6960e-05, 8.7365e-01,
        5.3646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.088

[Epoch: 90, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9573, 0.0050, 0.0097, 0.0087, 0.0070, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.487

[Epoch: 91, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1425, 0.0085, 0.0096, 0.0041, 0.0037, 0.0061, 0.8254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 91, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3419e-01, 2.0347e-02, 1.1307e-02, 1.2561e-04, 9.6379e-03, 9.0963e-03,
        1.5298e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.101

[Epoch: 91, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.0662e-02, 1.6227e-02, 3.8393e-05, 1.9331e-02, 2.1420e-02, 9.0050e-01,
        3.1826e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 91, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.7942e-03, 3.0961e-02, 4.0406e-03, 3.7551e-02, 2.1768e-05, 9.1670e-01,
        2.9333e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.107

[Epoch: 91, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0080, 0.9626, 0.0054, 0.0040, 0.0051, 0.0087, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.568

[Epoch: 92, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2471, 0.0058, 0.0176, 0.0073, 0.0076, 0.0082, 0.7064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 92, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6908e-01, 4.8985e-03, 8.1564e-03, 1.1821e-04, 6.5461e-03, 4.5840e-03,
        6.6144e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.049

[Epoch: 92, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0815e-02, 1.3151e-02, 3.1400e-05, 2.3430e-02, 2.0103e-02, 8.9367e-01,
        2.8794e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 92, batch: 156/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.7154e-03, 2.8941e-02, 3.9740e-03, 5.0244e-02, 2.0621e-05, 9.0742e-01,
        5.6812e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.053

[Epoch: 92, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9758, 0.0030, 0.0038, 0.0040, 0.0044, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.492

[Epoch: 93, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1637, 0.0069, 0.0088, 0.0043, 0.0047, 0.0045, 0.8071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 93, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5115e-01, 9.8127e-03, 1.3205e-02, 8.8890e-05, 7.4271e-03, 5.4716e-03,
        1.2843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.110

[Epoch: 93, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5981e-02, 1.8500e-02, 2.7110e-05, 2.6972e-02, 3.1767e-02, 8.6099e-01,
        3.5758e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 93, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.4828e-03, 3.4059e-02, 7.1145e-03, 6.1790e-02, 3.2956e-05, 8.8433e-01,
        5.1891e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.101

[Epoch: 93, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.9763, 0.0044, 0.0023, 0.0047, 0.0045, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.587

[Epoch: 94, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2228, 0.0067, 0.0140, 0.0076, 0.0053, 0.0050, 0.7386],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 94, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6282e-01, 8.9520e-03, 8.7706e-03, 1.3264e-04, 6.2248e-03, 4.4272e-03,
        8.6706e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 94, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3114e-02, 1.4349e-02, 8.3576e-05, 3.6566e-02, 2.5527e-02, 8.5265e-01,
        3.7710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 94, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0770e-03, 3.4772e-02, 6.2210e-03, 4.1726e-02, 6.1307e-05, 9.0518e-01,
        7.9652e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.058

[Epoch: 94, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9666, 0.0053, 0.0065, 0.0071, 0.0034, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.481

[Epoch: 95, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1835, 0.0070, 0.0079, 0.0047, 0.0051, 0.0046, 0.7873],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 95, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6920e-01, 8.3635e-03, 7.4412e-03, 3.6068e-05, 5.4398e-03, 4.4005e-03,
        5.1238e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.121

[Epoch: 95, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1309e-02, 1.9886e-02, 7.5300e-05, 2.3296e-02, 3.0066e-02, 8.7597e-01,
        2.9396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 95, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.1595e-03, 2.9665e-02, 4.1306e-03, 6.3226e-02, 2.5372e-05, 8.8845e-01,
        9.3390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.091

[Epoch: 95, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9604, 0.0068, 0.0077, 0.0081, 0.0050, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.532

[Epoch: 96, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2013, 0.0081, 0.0162, 0.0102, 0.0064, 0.0061, 0.7517],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 96, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4048e-01, 1.2479e-02, 1.2937e-02, 2.5558e-04, 8.6988e-03, 7.5749e-03,
        1.7576e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.110

[Epoch: 96, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0513e-02, 9.6998e-03, 4.7086e-05, 1.6840e-02, 1.8804e-02, 9.1128e-01,
        2.2812e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 96, batch: 156/195] total loss per batch: 0.871
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([1.0832e-02, 2.7548e-02, 3.3068e-03, 5.3203e-02, 2.5547e-05, 9.0252e-01,
        2.5620e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.044

[Epoch: 96, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9724, 0.0042, 0.0047, 0.0033, 0.0051, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.545

[Epoch: 97, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1928, 0.0050, 0.0144, 0.0098, 0.0064, 0.0061, 0.7655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 97, batch: 78/195] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5524e-01, 1.5399e-02, 6.1386e-03, 4.9631e-05, 8.9136e-03, 5.7978e-03,
        8.4569e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.080

[Epoch: 97, batch: 117/195] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0116e-02, 2.1479e-02, 8.4565e-05, 2.9359e-02, 2.2810e-02, 8.5543e-01,
        4.0719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 97, batch: 156/195] total loss per batch: 0.870
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.6726e-03, 8.2971e-02, 6.1874e-03, 3.3323e-02, 4.2751e-05, 8.7006e-01,
        4.7430e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 97, batch: 195/195] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9570, 0.0066, 0.0071, 0.0071, 0.0078, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.492

[Epoch: 98, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2056, 0.0063, 0.0103, 0.0049, 0.0045, 0.0046, 0.7637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 98, batch: 78/195] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6796e-01, 7.7350e-03, 6.6750e-03, 5.1953e-05, 4.7242e-03, 4.4810e-03,
        8.3732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.097

[Epoch: 98, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([4.0269e-02, 1.3402e-02, 6.4471e-05, 2.5641e-02, 2.4791e-02, 8.7650e-01,
        1.9329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 98, batch: 156/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.6994e-03, 5.8292e-03, 7.3033e-03, 5.5260e-02, 2.9360e-05, 9.1790e-01,
        6.9781e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.082

[Epoch: 98, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9632, 0.0055, 0.0047, 0.0058, 0.0089, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 99, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1715, 0.0062, 0.0107, 0.0065, 0.0038, 0.0055, 0.7958],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 99, batch: 78/195] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5892e-01, 1.0755e-02, 8.2281e-03, 3.5011e-05, 7.2946e-03, 6.6879e-03,
        8.0764e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.074

[Epoch: 99, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.6658e-02, 1.3922e-02, 3.2840e-05, 1.9211e-02, 2.1071e-02, 8.9693e-01,
        3.2180e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 99, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.7769e-03, 2.9846e-02, 6.2689e-03, 5.0263e-02, 3.4625e-05, 8.9999e-01,
        6.8220e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.059

[Epoch: 99, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9742, 0.0051, 0.0041, 0.0050, 0.0026, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.562

[Epoch: 100, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2080, 0.0046, 0.0104, 0.0066, 0.0069, 0.0083, 0.7553],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 100, batch: 78/195] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5105e-01, 1.2042e-02, 9.9303e-03, 7.4885e-05, 7.7371e-03, 7.6776e-03,
        1.1490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.097

[Epoch: 100, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5872e-02, 1.4313e-02, 4.3489e-05, 2.4277e-02, 2.0102e-02, 8.9077e-01,
        2.4622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 100, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.6185e-03, 3.2713e-02, 4.3762e-03, 6.0762e-02, 3.2667e-05, 8.8717e-01,
        7.3282e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.083

[Epoch: 100, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9758, 0.0028, 0.0031, 0.0035, 0.0043, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.493

[Epoch: 101, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2009, 0.0064, 0.0090, 0.0049, 0.0038, 0.0049, 0.7702],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 101, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3906e-01, 1.1864e-02, 1.5500e-02, 6.8801e-05, 1.0281e-02, 7.1421e-03,
        1.6088e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.089

[Epoch: 101, batch: 117/195] total loss per batch: 0.857
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3995e-02, 2.0384e-02, 5.4597e-05, 2.7211e-02, 2.6072e-02, 8.4560e-01,
        4.6688e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 101, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7045e-03, 3.3007e-02, 5.5549e-03, 4.0637e-02, 2.3122e-05, 9.1233e-01,
        3.7474e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.084

[Epoch: 101, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9654, 0.0064, 0.0068, 0.0062, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 102, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1998, 0.0064, 0.0104, 0.0059, 0.0038, 0.0058, 0.7679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 102, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6096e-01, 8.7942e-03, 9.6692e-03, 4.6212e-05, 6.3644e-03, 5.8772e-03,
        8.2936e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.120

[Epoch: 102, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1268e-02, 1.3500e-02, 4.3953e-05, 2.2914e-02, 2.2107e-02, 8.9728e-01,
        2.2889e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 102, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.8202e-03, 2.9544e-02, 3.9245e-03, 4.8507e-02, 2.2584e-05, 9.0899e-01,
        5.1913e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.080

[Epoch: 102, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9677, 0.0054, 0.0059, 0.0038, 0.0061, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.528

[Epoch: 103, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1789, 0.0060, 0.0121, 0.0062, 0.0054, 0.0074, 0.7840],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 103, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5998e-01, 9.8043e-03, 9.9297e-03, 4.0750e-05, 5.8094e-03, 5.2384e-03,
        9.2003e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.098

[Epoch: 103, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3529e-02, 1.4863e-02, 2.5628e-05, 2.2919e-02, 2.2479e-02, 8.8585e-01,
        3.0335e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 103, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4069e-03, 3.3087e-02, 5.8537e-03, 5.6503e-02, 2.2367e-05, 8.9300e-01,
        6.1294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.062

[Epoch: 103, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9693, 0.0047, 0.0050, 0.0052, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.537

[Epoch: 104, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2035, 0.0064, 0.0096, 0.0052, 0.0041, 0.0053, 0.7659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 104, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6300e-01, 8.8389e-03, 9.9026e-03, 3.7950e-05, 5.4460e-03, 4.9546e-03,
        7.8172e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.100

[Epoch: 104, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9161e-02, 1.4155e-02, 3.8416e-05, 3.0581e-02, 2.8727e-02, 8.6770e-01,
        2.9642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 104, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7923e-03, 4.4023e-02, 5.1552e-03, 4.4696e-02, 2.4335e-05, 8.9532e-01,
        4.9917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.068

[Epoch: 104, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9708, 0.0047, 0.0053, 0.0046, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.503

[Epoch: 105, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1980, 0.0063, 0.0104, 0.0049, 0.0044, 0.0061, 0.7699],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 105, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6584e-01, 7.5219e-03, 8.5896e-03, 3.3383e-05, 5.8007e-03, 5.3002e-03,
        6.9184e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 105, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5751e-02, 1.5960e-02, 3.5863e-05, 2.8028e-02, 2.7092e-02, 8.6910e-01,
        3.4031e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.044

[Epoch: 105, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.1454e-03, 2.6693e-02, 4.6264e-03, 4.9538e-02, 1.8448e-05, 9.0971e-01,
        5.2721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.074

[Epoch: 105, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9724, 0.0045, 0.0044, 0.0053, 0.0054, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.528

[Epoch: 106, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1743, 0.0052, 0.0097, 0.0051, 0.0049, 0.0053, 0.7955],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 106, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5538e-01, 1.0378e-02, 1.1562e-02, 4.7095e-05, 6.4362e-03, 6.0337e-03,
        1.0163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.100

[Epoch: 106, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4714e-02, 1.6626e-02, 3.5350e-05, 2.0386e-02, 2.3651e-02, 8.8570e-01,
        2.8889e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.045

[Epoch: 106, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5682e-03, 3.2647e-02, 3.3506e-03, 5.9111e-02, 1.6820e-05, 8.9684e-01,
        3.4680e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 106, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9754, 0.0036, 0.0043, 0.0038, 0.0037, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.538

[Epoch: 107, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2053, 0.0069, 0.0085, 0.0043, 0.0045, 0.0046, 0.7658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 107, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4954e-01, 1.1314e-02, 1.1396e-02, 3.9054e-05, 7.6831e-03, 7.0351e-03,
        1.2989e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.110

[Epoch: 107, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2594e-02, 1.5584e-02, 3.0390e-05, 2.6722e-02, 2.3095e-02, 8.7931e-01,
        3.2666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.047

[Epoch: 107, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.7608e-03, 3.2663e-02, 5.0720e-03, 4.0204e-02, 1.5509e-05, 9.1349e-01,
        4.7902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 107, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9631, 0.0078, 0.0071, 0.0056, 0.0065, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.525

[Epoch: 108, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1994, 0.0057, 0.0114, 0.0059, 0.0043, 0.0052, 0.7680],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 108, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6141e-01, 8.8176e-03, 1.0266e-02, 3.6818e-05, 5.8365e-03, 5.2706e-03,
        8.3678e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.100

[Epoch: 108, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9529e-02, 1.2096e-02, 3.6151e-05, 2.3665e-02, 2.2051e-02, 8.9009e-01,
        2.2538e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.046

[Epoch: 108, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6758e-03, 4.3175e-02, 4.3917e-03, 6.4840e-02, 1.3003e-05, 8.7555e-01,
        7.3567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.058

[Epoch: 108, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.9616, 0.0057, 0.0047, 0.0057, 0.0049, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.505

[Epoch: 109, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1915, 0.0061, 0.0085, 0.0066, 0.0041, 0.0061, 0.7771],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 109, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7008e-01, 6.7007e-03, 7.7970e-03, 2.2673e-05, 4.6420e-03, 4.2278e-03,
        6.5264e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.100

[Epoch: 109, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.4715e-02, 1.4597e-02, 2.3297e-05, 2.0052e-02, 2.2587e-02, 8.9907e-01,
        2.8957e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.048

[Epoch: 109, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.1177e-03, 2.8753e-02, 6.0322e-03, 3.8266e-02, 1.7902e-05, 9.1539e-01,
        4.4273e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.090

[Epoch: 109, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.9721, 0.0054, 0.0038, 0.0066, 0.0052, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.530

[Epoch: 110, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1786, 0.0071, 0.0147, 0.0060, 0.0068, 0.0054, 0.7814],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 110, batch: 78/195] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3591e-01, 1.8330e-02, 1.4955e-02, 7.6910e-05, 8.3476e-03, 7.5275e-03,
        1.4856e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 110, batch: 117/195] total loss per batch: 0.856
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.7007e-02, 1.4965e-02, 3.3860e-05, 2.7252e-02, 3.0299e-02, 8.5326e-01,
        3.7184e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.048

[Epoch: 110, batch: 156/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.1157e-03, 3.4181e-02, 5.1419e-03, 5.4927e-02, 2.6621e-05, 8.9106e-01,
        7.5519e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 110, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9641, 0.0045, 0.0046, 0.0068, 0.0082, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 111, batch: 39/195] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2443, 0.0064, 0.0113, 0.0062, 0.0031, 0.0044, 0.7243],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 111, batch: 78/195] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6896e-01, 7.8066e-03, 7.0081e-03, 3.4924e-05, 5.0761e-03, 4.3594e-03,
        6.7572e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.048

[Epoch: 111, batch: 117/195] total loss per batch: 0.856
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1187e-02, 1.7807e-02, 2.9071e-05, 4.7778e-02, 2.2761e-02, 8.6385e-01,
        2.6591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.050

[Epoch: 111, batch: 156/195] total loss per batch: 0.869
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.8199e-03, 4.4242e-02, 4.6437e-03, 4.2021e-02, 4.8627e-05, 8.9511e-01,
        7.1194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.089

[Epoch: 111, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9761, 0.0030, 0.0032, 0.0052, 0.0043, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.533

[Epoch: 112, batch: 39/195] total loss per batch: 0.872
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1142, 0.0051, 0.0076, 0.0031, 0.0046, 0.0033, 0.8621],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 112, batch: 78/195] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4843e-01, 1.0584e-02, 1.3379e-02, 5.6572e-05, 7.6117e-03, 8.0503e-03,
        1.1894e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.119

[Epoch: 112, batch: 117/195] total loss per batch: 0.856
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1676e-02, 1.7365e-02, 3.3094e-05, 1.5530e-02, 3.2726e-02, 8.7961e-01,
        3.3058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.049

[Epoch: 112, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.5376e-03, 2.6371e-02, 3.5146e-03, 6.3513e-02, 1.2705e-05, 8.9995e-01,
        4.1060e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.079

[Epoch: 112, batch: 195/195] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9762, 0.0035, 0.0051, 0.0037, 0.0037, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 113, batch: 39/195] total loss per batch: 0.871
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2407, 0.0062, 0.0121, 0.0054, 0.0059, 0.0054, 0.7243],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 113, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6723e-01, 7.0047e-03, 8.0208e-03, 3.3596e-05, 5.9043e-03, 4.5565e-03,
        7.2502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 113, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4315e-02, 1.3932e-02, 1.4707e-05, 1.9975e-02, 1.7951e-02, 8.9882e-01,
        2.4995e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.052

[Epoch: 113, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.0913e-03, 2.7989e-02, 5.3536e-03, 3.9356e-02, 1.5267e-05, 9.1589e-01,
        6.3087e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 113, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.9736, 0.0040, 0.0031, 0.0046, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.542

[Epoch: 114, batch: 39/195] total loss per batch: 0.871
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2198, 0.0078, 0.0144, 0.0048, 0.0063, 0.0074, 0.7396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 114, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5074e-01, 1.1286e-02, 1.2036e-02, 4.3680e-05, 8.3608e-03, 6.7669e-03,
        1.0770e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.149

[Epoch: 114, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5497e-02, 1.7093e-02, 4.2686e-05, 2.2998e-02, 2.6493e-02, 8.6832e-01,
        3.9555e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.049

[Epoch: 114, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.6920e-03, 4.4139e-02, 6.2272e-03, 5.2774e-02, 2.2609e-05, 8.8547e-01,
        5.6744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 114, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9581, 0.0083, 0.0087, 0.0053, 0.0065, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.503

[Epoch: 115, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1902, 0.0066, 0.0106, 0.0071, 0.0064, 0.0061, 0.7729],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 115, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6429e-01, 8.8060e-03, 9.1464e-03, 3.1783e-05, 5.1578e-03, 4.7448e-03,
        7.8212e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.090

[Epoch: 115, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6542e-02, 1.3608e-02, 2.4956e-05, 2.4216e-02, 2.4628e-02, 8.7924e-01,
        3.1739e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.057

[Epoch: 115, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.9567e-03, 2.9259e-02, 6.7723e-03, 5.0577e-02, 1.6059e-05, 9.0137e-01,
        7.0497e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.068

[Epoch: 115, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.9559, 0.0063, 0.0060, 0.0090, 0.0066, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 116, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2167, 0.0054, 0.0101, 0.0057, 0.0048, 0.0060, 0.7513],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 116, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6229e-01, 1.1637e-02, 8.3184e-03, 2.2578e-05, 6.0275e-03, 4.2546e-03,
        7.4484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.093

[Epoch: 116, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0495e-02, 1.3633e-02, 2.2264e-05, 2.1395e-02, 2.3995e-02, 8.9385e-01,
        2.6611e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.058

[Epoch: 116, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4126e-03, 3.2709e-02, 4.5387e-03, 4.3068e-02, 2.4954e-05, 9.0767e-01,
        5.5717e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.086

[Epoch: 116, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9685, 0.0055, 0.0037, 0.0058, 0.0059, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.518

[Epoch: 117, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1826, 0.0039, 0.0085, 0.0049, 0.0039, 0.0031, 0.7931],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 117, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5940e-01, 1.0673e-02, 1.0758e-02, 3.7944e-05, 5.5428e-03, 5.0560e-03,
        8.5266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.089

[Epoch: 117, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7638e-02, 1.2266e-02, 3.0050e-05, 3.2326e-02, 3.2825e-02, 8.6331e-01,
        3.1603e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 117, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4357e-03, 3.6198e-02, 5.8109e-03, 5.7522e-02, 2.5621e-05, 8.8770e-01,
        6.3103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.078

[Epoch: 117, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9801, 0.0029, 0.0027, 0.0036, 0.0031, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.534

[Epoch: 118, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1832, 0.0043, 0.0111, 0.0039, 0.0038, 0.0040, 0.7897],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 118, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6147e-01, 8.3285e-03, 8.6788e-03, 5.1122e-05, 7.3277e-03, 5.9466e-03,
        8.1977e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.080

[Epoch: 118, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7972e-02, 1.7180e-02, 3.0086e-05, 3.4767e-02, 2.2747e-02, 8.6518e-01,
        3.2126e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.059

[Epoch: 118, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.7996e-03, 3.3754e-02, 5.0645e-03, 4.5202e-02, 2.1096e-05, 9.0744e-01,
        4.7160e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.087

[Epoch: 118, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9755, 0.0034, 0.0043, 0.0044, 0.0043, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.500

[Epoch: 119, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2089, 0.0066, 0.0098, 0.0050, 0.0058, 0.0054, 0.7585],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.000

[Epoch: 119, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4161e-01, 1.2313e-02, 1.4790e-02, 7.2778e-05, 9.0974e-03, 7.2319e-03,
        1.4886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.120

[Epoch: 119, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0948e-02, 1.5377e-02, 3.1295e-05, 1.5784e-02, 2.5274e-02, 8.9717e-01,
        2.5419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.045

[Epoch: 119, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.1008e-03, 2.8769e-02, 4.2960e-03, 6.5269e-02, 2.8343e-05, 8.9365e-01,
        3.8904e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.058

[Epoch: 119, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9703, 0.0058, 0.0047, 0.0046, 0.0040, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.566

[Epoch: 120, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1887, 0.0059, 0.0180, 0.0067, 0.0052, 0.0079, 0.7677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 120, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4816e-01, 1.4114e-02, 1.0112e-02, 3.1938e-05, 8.5404e-03, 6.8720e-03,
        1.2169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.118

[Epoch: 120, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7633e-02, 1.5169e-02, 1.7015e-05, 2.7597e-02, 1.8557e-02, 8.8523e-01,
        2.5798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.050

[Epoch: 120, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9712e-03, 4.4974e-02, 5.5048e-03, 3.9768e-02, 2.2619e-05, 9.0043e-01,
        5.3248e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.058

[Epoch: 120, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9590, 0.0064, 0.0070, 0.0062, 0.0081, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.504

[Epoch: 121, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2126, 0.0071, 0.0097, 0.0058, 0.0062, 0.0057, 0.7529],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 121, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7828e-01, 5.4929e-03, 6.0279e-03, 2.5380e-05, 3.0731e-03, 2.5734e-03,
        4.5235e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.070

[Epoch: 121, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9909e-02, 1.5387e-02, 1.8438e-05, 1.6801e-02, 2.6187e-02, 8.9366e-01,
        2.8040e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 121, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.2167e-03, 2.7373e-02, 8.5605e-03, 5.3876e-02, 4.5347e-05, 8.9804e-01,
        5.8841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.102

[Epoch: 121, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9696, 0.0049, 0.0044, 0.0054, 0.0055, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.486

[Epoch: 122, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1559, 0.0051, 0.0113, 0.0059, 0.0037, 0.0048, 0.8133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 122, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6516e-01, 9.4449e-03, 7.3403e-03, 2.3952e-05, 4.8885e-03, 4.7962e-03,
        8.3440e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.089

[Epoch: 122, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9517e-02, 1.3524e-02, 2.6785e-05, 3.1107e-02, 3.2918e-02, 8.5845e-01,
        3.4459e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.051

[Epoch: 122, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([9.8149e-03, 5.6401e-02, 4.8762e-03, 5.4516e-02, 1.6398e-05, 8.6810e-01,
        6.2755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.050

[Epoch: 122, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9626, 0.0065, 0.0056, 0.0085, 0.0043, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.521

[Epoch: 123, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2213, 0.0041, 0.0138, 0.0052, 0.0041, 0.0054, 0.7461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 123, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5456e-01, 9.0884e-03, 1.2154e-02, 5.2232e-05, 7.3347e-03, 6.4023e-03,
        1.0405e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.115

[Epoch: 123, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0871e-02, 1.8005e-02, 1.9966e-05, 3.0339e-02, 2.4292e-02, 8.7288e-01,
        3.3596e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.047

[Epoch: 123, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([2.8384e-03, 2.1404e-02, 3.4195e-03, 4.8878e-02, 4.3950e-05, 9.2016e-01,
        3.2550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.099

[Epoch: 123, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.9773, 0.0034, 0.0036, 0.0048, 0.0044, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.584

[Epoch: 124, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1901, 0.0049, 0.0079, 0.0040, 0.0052, 0.0064, 0.7815],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 124, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3356e-01, 1.8120e-02, 1.3978e-02, 7.8686e-05, 1.1267e-02, 7.3169e-03,
        1.5679e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.134

[Epoch: 124, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9137e-02, 1.8195e-02, 3.1482e-05, 2.7483e-02, 2.5985e-02, 8.7208e-01,
        2.7092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.044

[Epoch: 124, batch: 156/195] total loss per batch: 0.868
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.0718e-03, 3.2580e-02, 5.6110e-03, 6.2138e-02, 2.8105e-05, 8.9022e-01,
        6.3558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.079

[Epoch: 124, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9738, 0.0036, 0.0047, 0.0043, 0.0042, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 125, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1982, 0.0068, 0.0161, 0.0042, 0.0083, 0.0053, 0.7611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 125, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7160e-01, 5.0584e-03, 8.4828e-03, 3.8615e-05, 4.3430e-03, 3.7200e-03,
        6.7546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.074

[Epoch: 125, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7436e-02, 1.3439e-02, 1.6300e-05, 1.6624e-02, 1.9251e-02, 9.0526e-01,
        2.7973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 125, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.8841e-03, 4.0286e-02, 3.3971e-03, 3.0821e-02, 1.8405e-05, 9.1681e-01,
        4.7866e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.053

[Epoch: 125, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9618, 0.0055, 0.0063, 0.0092, 0.0061, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.494

[Epoch: 126, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2190, 0.0053, 0.0107, 0.0078, 0.0043, 0.0058, 0.7470],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 126, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6393e-01, 8.5767e-03, 7.7430e-03, 2.8687e-05, 6.0909e-03, 4.7746e-03,
        8.8515e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 126, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9758e-02, 1.4684e-02, 2.3743e-05, 3.0276e-02, 2.6122e-02, 8.6880e-01,
        3.0341e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 126, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.4025e-03, 3.7816e-02, 5.4126e-03, 6.9863e-02, 3.7509e-05, 8.7275e-01,
        6.7165e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 126, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9650, 0.0059, 0.0061, 0.0039, 0.0078, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.535

[Epoch: 127, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1707, 0.0050, 0.0095, 0.0045, 0.0044, 0.0078, 0.7981],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 127, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6140e-01, 7.6820e-03, 1.1035e-02, 4.4073e-05, 6.1756e-03, 4.7555e-03,
        8.9073e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.109

[Epoch: 127, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9576e-02, 1.4314e-02, 2.4304e-05, 2.1357e-02, 2.7420e-02, 8.8495e-01,
        3.2363e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 127, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.0525e-03, 2.7757e-02, 4.6179e-03, 2.6209e-02, 2.3143e-05, 9.3151e-01,
        3.8310e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.109

[Epoch: 127, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9719, 0.0040, 0.0041, 0.0049, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.500

[Epoch: 128, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1830, 0.0072, 0.0112, 0.0056, 0.0053, 0.0050, 0.7827],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 128, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6018e-01, 9.4584e-03, 9.2814e-03, 6.0217e-05, 6.2447e-03, 5.2791e-03,
        9.4963e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.067

[Epoch: 128, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.6078e-02, 1.6741e-02, 2.8086e-05, 2.8425e-02, 2.4575e-02, 8.6355e-01,
        3.0600e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 128, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4288e-03, 4.4167e-02, 4.7292e-03, 7.1989e-02, 3.4552e-05, 8.6736e-01,
        5.2904e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.045

[Epoch: 128, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9705, 0.0053, 0.0052, 0.0059, 0.0050, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.530

[Epoch: 129, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1942, 0.0040, 0.0103, 0.0044, 0.0039, 0.0044, 0.7787],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 129, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5718e-01, 9.2285e-03, 1.0189e-02, 3.4438e-05, 6.9067e-03, 6.2299e-03,
        1.0228e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.129

[Epoch: 129, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7270e-02, 1.7285e-02, 3.0522e-05, 2.2144e-02, 2.3619e-02, 8.8984e-01,
        2.9816e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 129, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.6365e-03, 2.5130e-02, 4.0314e-03, 3.9262e-02, 1.6132e-05, 9.2390e-01,
        4.0270e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.093

[Epoch: 129, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9724, 0.0038, 0.0044, 0.0046, 0.0052, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.545

[Epoch: 130, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2076, 0.0054, 0.0101, 0.0054, 0.0061, 0.0047, 0.7608],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 130, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6274e-01, 8.7696e-03, 8.1360e-03, 4.9615e-05, 6.0677e-03, 4.6577e-03,
        9.5783e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.082

[Epoch: 130, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7523e-02, 1.4783e-02, 2.5477e-05, 2.5027e-02, 2.4337e-02, 8.7853e-01,
        2.9773e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 130, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.0764e-03, 3.5247e-02, 5.7758e-03, 4.4688e-02, 1.7633e-05, 9.0393e-01,
        5.2615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 130, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9668, 0.0062, 0.0056, 0.0052, 0.0059, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.520

[Epoch: 131, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1949, 0.0079, 0.0111, 0.0054, 0.0058, 0.0053, 0.7696],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 131, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5937e-01, 8.4806e-03, 1.0954e-02, 2.2422e-05, 6.4083e-03, 4.9995e-03,
        9.7690e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.112

[Epoch: 131, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1272e-02, 1.5378e-02, 2.9473e-05, 2.7262e-02, 2.6814e-02, 8.8020e-01,
        2.9048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 131, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([8.4013e-03, 3.6815e-02, 4.9622e-03, 6.6957e-02, 3.5774e-05, 8.7811e-01,
        4.7154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.053

[Epoch: 131, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9633, 0.0057, 0.0052, 0.0082, 0.0047, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.500

[Epoch: 132, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2267, 0.0053, 0.0131, 0.0063, 0.0053, 0.0054, 0.7380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 132, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5071e-01, 1.5485e-02, 9.6202e-03, 5.0324e-05, 6.5111e-03, 5.7872e-03,
        1.1833e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.077

[Epoch: 132, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8818e-02, 1.4807e-02, 4.1540e-05, 2.1879e-02, 2.1077e-02, 8.7500e-01,
        3.8376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 132, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.9535e-03, 3.4629e-02, 5.5921e-03, 4.1479e-02, 2.1044e-05, 9.0600e-01,
        7.3254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.087

[Epoch: 132, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9609, 0.0072, 0.0065, 0.0069, 0.0070, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.495

[Epoch: 133, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1573, 0.0056, 0.0102, 0.0047, 0.0039, 0.0071, 0.8112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 133, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6813e-01, 6.7121e-03, 8.6340e-03, 1.6438e-05, 5.7604e-03, 3.8060e-03,
        6.9376e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.136

[Epoch: 133, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1157e-02, 1.9364e-02, 3.1707e-05, 2.5500e-02, 3.1992e-02, 8.7741e-01,
        2.4542e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 133, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.6651e-03, 3.6495e-02, 4.3898e-03, 4.4041e-02, 4.0465e-05, 9.0351e-01,
        4.8580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 133, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9727, 0.0039, 0.0043, 0.0047, 0.0052, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.574

[Epoch: 134, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1977, 0.0049, 0.0090, 0.0038, 0.0055, 0.0025, 0.7765],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 134, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5601e-01, 9.9442e-03, 1.1356e-02, 5.2160e-05, 6.0776e-03, 6.1242e-03,
        1.0432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.081

[Epoch: 134, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9843e-02, 1.1888e-02, 2.4643e-05, 2.7700e-02, 2.1303e-02, 8.7245e-01,
        3.6794e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 134, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.6829e-03, 2.9043e-02, 4.7176e-03, 5.8105e-02, 2.6720e-05, 8.9957e-01,
        4.8569e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.069

[Epoch: 134, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9760, 0.0050, 0.0035, 0.0040, 0.0032, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.497

[Epoch: 135, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1994, 0.0032, 0.0099, 0.0032, 0.0042, 0.0033, 0.7769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 135, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5953e-01, 9.4823e-03, 9.9520e-03, 3.6574e-05, 7.1119e-03, 5.2055e-03,
        8.6791e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 135, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3131e-02, 1.6065e-02, 4.0060e-05, 1.9587e-02, 1.6755e-02, 9.0275e-01,
        2.1669e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 135, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6048e-03, 3.7035e-02, 5.2876e-03, 4.3297e-02, 2.4910e-05, 9.0500e-01,
        4.7548e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 135, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9739, 0.0048, 0.0050, 0.0043, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.516

[Epoch: 136, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1930, 0.0081, 0.0093, 0.0070, 0.0055, 0.0050, 0.7721],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.009

[Epoch: 136, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6047e-01, 8.2950e-03, 9.8648e-03, 4.9726e-05, 6.1929e-03, 5.8322e-03,
        9.2970e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.138

[Epoch: 136, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4003e-02, 1.7264e-02, 5.2489e-05, 3.1570e-02, 3.2855e-02, 8.5344e-01,
        4.0817e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 136, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.9652e-03, 2.8093e-02, 4.3885e-03, 5.6513e-02, 1.9499e-05, 8.9922e-01,
        5.8015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.088

[Epoch: 136, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9704, 0.0049, 0.0048, 0.0061, 0.0039, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.527

[Epoch: 137, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2346, 0.0083, 0.0163, 0.0070, 0.0079, 0.0088, 0.7172],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 137, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3628e-01, 1.6575e-02, 1.4011e-02, 4.7844e-05, 9.0691e-03, 7.8612e-03,
        1.6160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.061

[Epoch: 137, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8534e-02, 1.1466e-02, 2.4649e-05, 2.4806e-02, 1.6358e-02, 8.9575e-01,
        2.3066e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 137, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7683e-03, 3.9419e-02, 6.6715e-03, 4.8805e-02, 3.3437e-05, 8.9526e-01,
        5.0429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.060

[Epoch: 137, batch: 195/195] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.9584, 0.0069, 0.0069, 0.0071, 0.0058, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.525

[Epoch: 138, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1584, 0.0071, 0.0091, 0.0063, 0.0049, 0.0041, 0.8100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 138, batch: 78/195] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6639e-01, 8.2438e-03, 7.5807e-03, 3.4013e-05, 4.4609e-03, 4.7947e-03,
        8.5006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.115

[Epoch: 138, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0041e-02, 1.6381e-02, 2.9136e-05, 2.7728e-02, 3.4069e-02, 8.6069e-01,
        4.1058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 138, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.5435e-03, 4.1490e-02, 5.9998e-03, 4.4329e-02, 2.2104e-05, 8.9699e-01,
        5.6205e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.077

[Epoch: 138, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9659, 0.0046, 0.0068, 0.0059, 0.0077, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.493

[Epoch: 139, batch: 39/195] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2136, 0.0048, 0.0090, 0.0057, 0.0057, 0.0036, 0.7576],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 139, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7775e-01, 5.4636e-03, 5.8791e-03, 3.6263e-05, 3.6338e-03, 2.7210e-03,
        4.5178e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.090

[Epoch: 139, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9718e-02, 1.1798e-02, 2.0856e-05, 1.9540e-02, 1.9781e-02, 8.9970e-01,
        1.9444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 139, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7434e-03, 3.2301e-02, 4.1419e-03, 5.2571e-02, 4.9360e-05, 8.9935e-01,
        5.8436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.079

[Epoch: 139, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9741, 0.0042, 0.0027, 0.0052, 0.0033, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.524

[Epoch: 140, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1821, 0.0037, 0.0111, 0.0060, 0.0051, 0.0059, 0.7861],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 140, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6118e-01, 1.0225e-02, 9.6059e-03, 3.1315e-05, 5.6824e-03, 5.1559e-03,
        8.1218e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.095

[Epoch: 140, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4906e-02, 1.6085e-02, 2.6930e-05, 3.1297e-02, 3.1462e-02, 8.6187e-01,
        3.4359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 140, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.3411e-03, 3.1285e-02, 4.2007e-03, 4.2736e-02, 2.0501e-05, 9.1310e-01,
        5.3141e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.085

[Epoch: 140, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9752, 0.0040, 0.0037, 0.0050, 0.0041, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.563

[Epoch: 141, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2035, 0.0067, 0.0102, 0.0044, 0.0055, 0.0047, 0.7650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 141, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4445e-01, 1.1840e-02, 1.4218e-02, 8.0402e-05, 8.9266e-03, 6.5705e-03,
        1.3915e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.116

[Epoch: 141, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.3679e-02, 1.6986e-02, 3.4043e-05, 2.5347e-02, 2.7391e-02, 8.6720e-01,
        2.9361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 141, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.3041e-03, 3.8270e-02, 5.5890e-03, 5.4910e-02, 2.3496e-05, 8.9117e-01,
        4.7296e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.064

[Epoch: 141, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9678, 0.0057, 0.0044, 0.0053, 0.0049, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 142, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2009, 0.0063, 0.0129, 0.0056, 0.0060, 0.0048, 0.7635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 142, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6066e-01, 9.6704e-03, 9.2623e-03, 3.8920e-05, 6.2205e-03, 5.3187e-03,
        8.8278e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 142, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.7860e-02, 1.3420e-02, 3.3737e-05, 2.2284e-02, 1.8526e-02, 9.0421e-01,
        2.3663e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 142, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.2247e-03, 2.9299e-02, 5.3607e-03, 4.7891e-02, 2.2742e-05, 9.0916e-01,
        5.0393e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.061

[Epoch: 142, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9694, 0.0056, 0.0052, 0.0045, 0.0046, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.532

[Epoch: 143, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1938, 0.0058, 0.0124, 0.0052, 0.0060, 0.0058, 0.7710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 143, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6013e-01, 1.0208e-02, 1.0203e-02, 2.3982e-05, 5.6744e-03, 5.1279e-03,
        8.6341e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.108

[Epoch: 143, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8082e-02, 1.2830e-02, 2.0914e-05, 1.9126e-02, 2.5188e-02, 8.9327e-01,
        3.1479e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 143, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.2761e-03, 3.8693e-02, 4.9302e-03, 5.2591e-02, 1.5995e-05, 8.9310e-01,
        5.3910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.081

[Epoch: 143, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9649, 0.0046, 0.0063, 0.0062, 0.0072, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.488

[Epoch: 144, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1885, 0.0047, 0.0093, 0.0053, 0.0057, 0.0041, 0.7823],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 144, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6519e-01, 8.9055e-03, 8.3069e-03, 3.1428e-05, 4.9207e-03, 4.4277e-03,
        8.2183e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.073

[Epoch: 144, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.2193e-02, 1.5309e-02, 2.6513e-05, 3.1242e-02, 2.4048e-02, 8.6670e-01,
        3.0481e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 144, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.4855e-03, 3.2844e-02, 7.0722e-03, 5.0432e-02, 3.7945e-05, 8.9602e-01,
        7.1075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.053

[Epoch: 144, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9709, 0.0047, 0.0046, 0.0048, 0.0055, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.555

[Epoch: 145, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2005, 0.0037, 0.0102, 0.0065, 0.0048, 0.0046, 0.7698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 145, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5929e-01, 1.0562e-02, 9.5096e-03, 3.5351e-05, 6.2738e-03, 6.1257e-03,
        8.2015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.112

[Epoch: 145, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4574e-02, 1.5059e-02, 3.4502e-05, 2.4237e-02, 3.0314e-02, 8.7156e-01,
        3.4221e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 145, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.6172e-03, 3.6619e-02, 4.2431e-03, 5.0877e-02, 1.8339e-05, 8.9794e-01,
        4.6897e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.093

[Epoch: 145, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9735, 0.0045, 0.0035, 0.0057, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.527

[Epoch: 146, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1766, 0.0047, 0.0108, 0.0046, 0.0046, 0.0048, 0.7940],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 146, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4717e-01, 1.1319e-02, 1.3261e-02, 9.5653e-05, 8.5702e-03, 6.9123e-03,
        1.2672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 146, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6280e-02, 1.6768e-02, 2.2815e-05, 2.2718e-02, 2.3439e-02, 8.8030e-01,
        3.0475e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 146, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.5150e-03, 3.5463e-02, 4.5732e-03, 5.1680e-02, 3.1302e-05, 9.0139e-01,
        3.3450e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 146, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9753, 0.0040, 0.0040, 0.0042, 0.0040, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.511

[Epoch: 147, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1944, 0.0054, 0.0123, 0.0053, 0.0063, 0.0047, 0.7716],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 147, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4749e-01, 1.1559e-02, 1.3563e-02, 6.4626e-05, 7.9467e-03, 6.7561e-03,
        1.2625e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.131

[Epoch: 147, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8832e-02, 1.6202e-02, 3.1738e-05, 3.0126e-02, 2.0242e-02, 8.6881e-01,
        3.5761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 147, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.2599e-03, 2.6184e-02, 5.2150e-03, 4.4638e-02, 1.3313e-05, 9.1294e-01,
        5.7468e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.086

[Epoch: 147, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.9592, 0.0071, 0.0064, 0.0064, 0.0061, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.529

[Epoch: 148, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2380, 0.0071, 0.0101, 0.0070, 0.0072, 0.0055, 0.7251],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 148, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6827e-01, 7.6325e-03, 7.1053e-03, 2.8052e-05, 4.7910e-03, 4.5842e-03,
        7.5864e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.063

[Epoch: 148, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9799e-02, 1.5506e-02, 2.2716e-05, 1.8571e-02, 2.6863e-02, 8.9620e-01,
        2.3041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 148, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.4127e-03, 5.0810e-02, 4.2013e-03, 5.8078e-02, 1.4861e-05, 8.7674e-01,
        5.7384e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.048

[Epoch: 148, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.9693, 0.0052, 0.0062, 0.0051, 0.0062, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 149, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1810, 0.0067, 0.0147, 0.0043, 0.0048, 0.0059, 0.7826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.018

[Epoch: 149, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5610e-01, 1.3025e-02, 9.4864e-03, 2.7854e-05, 6.7464e-03, 5.9016e-03,
        8.7089e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.108

[Epoch: 149, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.2698e-02, 1.2482e-02, 1.9561e-05, 2.5969e-02, 2.6711e-02, 8.7379e-01,
        2.8329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 149, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.8286e-03, 2.8112e-02, 5.4842e-03, 4.1466e-02, 2.2365e-05, 9.1131e-01,
        6.7753e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.098

[Epoch: 149, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9644, 0.0059, 0.0053, 0.0054, 0.0066, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 150, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1684, 0.0037, 0.0094, 0.0059, 0.0040, 0.0053, 0.8032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 150, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6859e-01, 6.6861e-03, 8.6565e-03, 6.4876e-05, 4.5659e-03, 4.4149e-03,
        7.0266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.085

[Epoch: 150, batch: 117/195] total loss per batch: 0.855
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.5908e-02, 1.7682e-02, 3.7881e-05, 2.4027e-02, 3.0437e-02, 8.7841e-01,
        3.3501e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 150, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4329e-03, 3.4566e-02, 4.9663e-03, 6.3420e-02, 2.6998e-05, 8.8624e-01,
        5.3487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 150, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9773, 0.0026, 0.0040, 0.0054, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.523

[Epoch: 151, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2118, 0.0047, 0.0091, 0.0063, 0.0059, 0.0052, 0.7570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 151, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4599e-01, 1.1595e-02, 1.2254e-02, 1.1088e-04, 8.5835e-03, 6.9645e-03,
        1.4501e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.100

[Epoch: 151, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0975e-02, 1.3835e-02, 2.3633e-05, 2.8637e-02, 2.5761e-02, 8.7334e-01,
        2.7428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 151, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5863e-03, 3.0745e-02, 4.6380e-03, 3.7084e-02, 1.4421e-05, 9.1808e-01,
        4.8531e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.078

[Epoch: 151, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9723, 0.0056, 0.0045, 0.0038, 0.0042, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.533

[Epoch: 152, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1939, 0.0058, 0.0114, 0.0048, 0.0052, 0.0051, 0.7738],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 152, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5720e-01, 1.0227e-02, 9.8785e-03, 5.6896e-05, 6.6629e-03, 6.1235e-03,
        9.8553e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.120

[Epoch: 152, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7216e-02, 1.6264e-02, 2.6067e-05, 2.2464e-02, 2.2314e-02, 8.8311e-01,
        2.8609e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 152, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6137e-03, 3.8175e-02, 4.8794e-03, 6.5345e-02, 1.8148e-05, 8.8157e-01,
        5.3987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.082

[Epoch: 152, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9673, 0.0055, 0.0063, 0.0047, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.543

[Epoch: 153, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1895, 0.0055, 0.0108, 0.0053, 0.0059, 0.0059, 0.7771],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 153, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6269e-01, 8.9269e-03, 9.5027e-03, 3.5470e-05, 5.4166e-03, 5.0130e-03,
        8.4152e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.106

[Epoch: 153, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2597e-02, 1.3808e-02, 2.4280e-05, 2.4738e-02, 2.3648e-02, 8.8388e-01,
        3.1307e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 153, batch: 156/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.1989e-03, 3.1653e-02, 5.4866e-03, 3.9483e-02, 1.5872e-05, 9.1257e-01,
        5.5925e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.063

[Epoch: 153, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9676, 0.0055, 0.0059, 0.0052, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 154, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1914, 0.0049, 0.0103, 0.0054, 0.0053, 0.0051, 0.7777],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 154, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6215e-01, 9.3661e-03, 8.8803e-03, 3.5550e-05, 5.5926e-03, 5.3011e-03,
        8.6768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.091

[Epoch: 154, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7947e-02, 1.5022e-02, 2.6083e-05, 2.5575e-02, 2.7351e-02, 8.7276e-01,
        3.1320e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 154, batch: 156/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7723e-03, 4.1847e-02, 5.1760e-03, 5.1839e-02, 1.5887e-05, 8.8981e-01,
        5.5409e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 154, batch: 195/195] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9700, 0.0050, 0.0050, 0.0051, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.524

[Epoch: 155, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1976, 0.0044, 0.0099, 0.0050, 0.0050, 0.0049, 0.7732],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 155, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5511e-01, 1.0698e-02, 1.1029e-02, 5.0114e-05, 6.3089e-03, 6.1512e-03,
        1.0654e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.095

[Epoch: 155, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5413e-02, 1.5549e-02, 2.4168e-05, 2.7019e-02, 2.6266e-02, 8.7591e-01,
        2.9820e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 155, batch: 156/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7462e-03, 2.7523e-02, 4.8675e-03, 4.9225e-02, 1.6255e-05, 9.0870e-01,
        4.9249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 155, batch: 195/195] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9733, 0.0045, 0.0041, 0.0044, 0.0045, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 156, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1898, 0.0049, 0.0106, 0.0051, 0.0049, 0.0054, 0.7792],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 156, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5629e-01, 1.0228e-02, 1.0569e-02, 4.7531e-05, 6.4208e-03, 6.1213e-03,
        1.0326e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.104

[Epoch: 156, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3569e-02, 1.5521e-02, 1.6805e-05, 2.4160e-02, 2.1949e-02, 8.8662e-01,
        2.8162e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 156, batch: 156/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.8363e-03, 4.0563e-02, 4.7800e-03, 5.1469e-02, 1.0544e-05, 8.9360e-01,
        4.7468e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 156, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9706, 0.0049, 0.0051, 0.0045, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.538

[Epoch: 157, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2029, 0.0054, 0.0104, 0.0045, 0.0056, 0.0049, 0.7664],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 157, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6004e-01, 9.6364e-03, 9.6964e-03, 3.1118e-05, 5.7084e-03, 5.0485e-03,
        9.8361e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.105

[Epoch: 157, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5677e-02, 1.4429e-02, 1.7089e-05, 2.4831e-02, 2.4257e-02, 8.8117e-01,
        2.9615e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 157, batch: 156/195] total loss per batch: 0.864
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7853e-03, 3.2371e-02, 5.0310e-03, 5.0884e-02, 1.0577e-05, 9.0196e-01,
        4.9601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.066

[Epoch: 157, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9637, 0.0068, 0.0068, 0.0053, 0.0058, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.509

[Epoch: 158, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1976, 0.0062, 0.0106, 0.0063, 0.0054, 0.0065, 0.7674],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 158, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6241e-01, 9.4240e-03, 8.9568e-03, 2.0796e-05, 5.4492e-03, 4.9330e-03,
        8.8023e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.086

[Epoch: 158, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1236e-02, 1.2366e-02, 1.2164e-05, 2.0576e-02, 2.2501e-02, 8.9217e-01,
        3.1138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 158, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4977e-03, 3.7938e-02, 5.5631e-03, 5.0791e-02, 1.1169e-05, 8.9518e-01,
        5.0200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 158, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9681, 0.0044, 0.0049, 0.0058, 0.0056, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.540

[Epoch: 159, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1922, 0.0045, 0.0095, 0.0054, 0.0058, 0.0047, 0.7778],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 159, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6519e-01, 8.3044e-03, 8.3634e-03, 2.3604e-05, 4.6686e-03, 4.4460e-03,
        9.0003e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 159, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.4668e-02, 1.6942e-02, 1.9707e-05, 3.0692e-02, 3.3297e-02, 8.5450e-01,
        2.9883e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 159, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.1132e-03, 3.1360e-02, 4.7742e-03, 4.8157e-02, 9.6972e-06, 9.0600e-01,
        4.5826e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.072

[Epoch: 159, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9674, 0.0056, 0.0047, 0.0066, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.486

[Epoch: 160, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1917, 0.0048, 0.0109, 0.0050, 0.0037, 0.0044, 0.7795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 160, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5544e-01, 9.5289e-03, 1.0490e-02, 3.3770e-05, 7.6906e-03, 6.0434e-03,
        1.0769e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.091

[Epoch: 160, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3731e-02, 1.3712e-02, 1.9587e-05, 1.8707e-02, 2.1892e-02, 8.8628e-01,
        3.5663e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 160, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7585e-03, 3.5204e-02, 5.0567e-03, 4.6126e-02, 1.9533e-05, 9.0324e-01,
        5.5986e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.080

[Epoch: 160, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9729, 0.0050, 0.0042, 0.0048, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.541

[Epoch: 161, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1981, 0.0041, 0.0079, 0.0037, 0.0046, 0.0057, 0.7760],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.002

[Epoch: 161, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6389e-01, 9.9905e-03, 8.2821e-03, 2.4130e-05, 5.4718e-03, 4.7921e-03,
        7.5489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.106

[Epoch: 161, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8237e-02, 1.7586e-02, 1.8091e-05, 3.4059e-02, 2.5938e-02, 8.6643e-01,
        2.7727e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 161, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.7167e-03, 3.7827e-02, 5.4059e-03, 6.6440e-02, 1.4649e-05, 8.8048e-01,
        5.1127e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.080

[Epoch: 161, batch: 195/195] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9754, 0.0031, 0.0039, 0.0032, 0.0041, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.528

[Epoch: 162, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2228, 0.0073, 0.0129, 0.0053, 0.0049, 0.0039, 0.7429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 162, batch: 78/195] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4288e-01, 1.1553e-02, 1.3968e-02, 8.1233e-05, 9.0000e-03, 7.5683e-03,
        1.4950e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.128

[Epoch: 162, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9442e-02, 1.4881e-02, 3.6158e-05, 2.2396e-02, 2.5169e-02, 8.8122e-01,
        3.6852e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 162, batch: 156/195] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9642e-03, 1.9940e-02, 4.0138e-03, 3.3609e-02, 1.5124e-05, 9.3419e-01,
        4.2726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.060

[Epoch: 162, batch: 195/195] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9706, 0.0056, 0.0043, 0.0043, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.522

[Epoch: 163, batch: 39/195] total loss per batch: 0.869
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1843, 0.0062, 0.0120, 0.0085, 0.0079, 0.0085, 0.7726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.017

[Epoch: 163, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5817e-01, 8.3963e-03, 9.8149e-03, 3.3627e-05, 6.1563e-03, 5.5504e-03,
        1.1877e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.066

[Epoch: 163, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4060e-02, 1.5107e-02, 1.5207e-05, 2.4977e-02, 2.0065e-02, 8.9062e-01,
        2.5152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 163, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.6768e-03, 5.5409e-02, 5.4909e-03, 7.0262e-02, 1.9397e-05, 8.5811e-01,
        5.0279e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.093

[Epoch: 163, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.9641, 0.0050, 0.0063, 0.0050, 0.0071, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.540

[Epoch: 164, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1893, 0.0062, 0.0100, 0.0063, 0.0045, 0.0049, 0.7790],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 164, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6436e-01, 9.0244e-03, 8.9228e-03, 1.9430e-05, 5.4688e-03, 4.9096e-03,
        7.2928e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.122

[Epoch: 164, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0807e-02, 1.4139e-02, 2.3971e-05, 2.4170e-02, 3.2230e-02, 8.6520e-01,
        3.3434e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 164, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.0528e-03, 2.9730e-02, 6.7899e-03, 3.5033e-02, 2.2372e-05, 9.1357e-01,
        7.8064e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.057

[Epoch: 164, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9674, 0.0058, 0.0048, 0.0062, 0.0061, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.491

[Epoch: 165, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1836, 0.0054, 0.0084, 0.0053, 0.0046, 0.0039, 0.7888],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 165, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5988e-01, 9.5443e-03, 1.0166e-02, 2.1670e-05, 5.8366e-03, 5.1127e-03,
        9.4351e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.099

[Epoch: 165, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3728e-02, 1.5058e-02, 1.9493e-05, 2.6505e-02, 2.1223e-02, 8.8341e-01,
        3.0052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 165, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0273e-03, 3.0864e-02, 5.0313e-03, 4.6248e-02, 3.5595e-05, 9.0827e-01,
        5.5217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.070

[Epoch: 165, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9728, 0.0049, 0.0043, 0.0038, 0.0054, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.536

[Epoch: 166, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1939, 0.0042, 0.0111, 0.0048, 0.0047, 0.0040, 0.7772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 166, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5951e-01, 9.4289e-03, 1.0556e-02, 3.8421e-05, 5.7688e-03, 5.3851e-03,
        9.3164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.075

[Epoch: 166, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3017e-02, 1.5349e-02, 2.0091e-05, 2.6923e-02, 2.5307e-02, 8.8031e-01,
        2.9069e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 166, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.8659e-03, 3.7626e-02, 5.2875e-03, 5.1633e-02, 2.6612e-05, 8.9562e-01,
        4.9404e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.091

[Epoch: 166, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9758, 0.0038, 0.0038, 0.0042, 0.0036, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.518

[Epoch: 167, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1862, 0.0058, 0.0131, 0.0055, 0.0057, 0.0051, 0.7786],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 167, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5283e-01, 1.1266e-02, 1.0923e-02, 5.4693e-05, 7.4791e-03, 6.0301e-03,
        1.1416e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.109

[Epoch: 167, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7833e-02, 1.6209e-02, 1.9129e-05, 2.4905e-02, 2.5580e-02, 8.8062e-01,
        2.4835e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 167, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.0274e-03, 3.4198e-02, 4.6229e-03, 5.1569e-02, 1.9029e-05, 9.0027e-01,
        4.2935e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.062

[Epoch: 167, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9620, 0.0067, 0.0070, 0.0052, 0.0067, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 168, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2147, 0.0073, 0.0089, 0.0046, 0.0054, 0.0058, 0.7534],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 168, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6243e-01, 9.1115e-03, 9.3291e-03, 3.0501e-05, 5.5111e-03, 4.8553e-03,
        8.7312e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 168, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2103e-02, 1.4289e-02, 1.7791e-05, 2.2728e-02, 1.9994e-02, 8.8820e-01,
        3.2673e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 168, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.5307e-03, 4.0729e-02, 5.5646e-03, 5.2570e-02, 1.9579e-05, 8.8992e-01,
        5.6630e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 168, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9691, 0.0051, 0.0054, 0.0049, 0.0044, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.534

[Epoch: 169, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1855, 0.0049, 0.0104, 0.0049, 0.0050, 0.0047, 0.7846],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 169, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6087e-01, 8.8908e-03, 9.7283e-03, 2.6259e-05, 5.5888e-03, 5.1513e-03,
        9.7484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.094

[Epoch: 169, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3697e-02, 1.4156e-02, 1.2274e-05, 2.2950e-02, 2.5381e-02, 8.8508e-01,
        2.8719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 169, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4985e-03, 2.7905e-02, 5.0520e-03, 4.7038e-02, 2.2076e-05, 9.0921e-01,
        5.2752e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.073

[Epoch: 169, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9642, 0.0056, 0.0059, 0.0066, 0.0064, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.538

[Epoch: 170, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1899, 0.0039, 0.0076, 0.0048, 0.0039, 0.0037, 0.7861],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 170, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5713e-01, 1.0483e-02, 1.0120e-02, 3.5476e-05, 5.9722e-03, 5.5014e-03,
        1.0759e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.069

[Epoch: 170, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0751e-02, 1.5534e-02, 1.5756e-05, 2.7945e-02, 2.7557e-02, 8.6735e-01,
        3.0849e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 170, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4549e-03, 3.8721e-02, 4.2714e-03, 4.7059e-02, 2.2122e-05, 8.9848e-01,
        5.9927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.069

[Epoch: 170, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9749, 0.0042, 0.0039, 0.0047, 0.0036, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 171, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1946, 0.0041, 0.0130, 0.0043, 0.0040, 0.0047, 0.7753],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 171, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5353e-01, 1.0852e-02, 1.0524e-02, 5.5827e-05, 7.3049e-03, 6.1975e-03,
        1.1534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.113

[Epoch: 171, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2808e-02, 1.7908e-02, 1.8616e-05, 2.8749e-02, 2.6922e-02, 8.7563e-01,
        2.7963e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 171, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.2658e-03, 3.2241e-02, 4.6423e-03, 5.6194e-02, 1.9148e-05, 8.9834e-01,
        4.2958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.089

[Epoch: 171, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.9738, 0.0055, 0.0040, 0.0044, 0.0034, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.542

[Epoch: 172, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2176, 0.0063, 0.0106, 0.0064, 0.0076, 0.0069, 0.7446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 172, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5143e-01, 1.1557e-02, 1.1890e-02, 3.0535e-05, 7.2618e-03, 5.8427e-03,
        1.1986e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.106

[Epoch: 172, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6070e-02, 1.5445e-02, 1.6242e-05, 2.1682e-02, 2.1716e-02, 8.7702e-01,
        3.8048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 172, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4104e-03, 3.6262e-02, 4.8599e-03, 4.3395e-02, 1.7325e-05, 9.0667e-01,
        3.3874e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.061

[Epoch: 172, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9666, 0.0048, 0.0072, 0.0058, 0.0048, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 173, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1920, 0.0072, 0.0111, 0.0053, 0.0056, 0.0070, 0.7719],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 173, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6666e-01, 9.4586e-03, 7.1408e-03, 1.4170e-05, 5.2180e-03, 4.4078e-03,
        7.1036e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.113

[Epoch: 173, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9250e-02, 1.3259e-02, 1.7079e-05, 2.1387e-02, 2.3442e-02, 8.9965e-01,
        2.2998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 173, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.8198e-03, 3.5282e-02, 5.1918e-03, 5.7257e-02, 1.5915e-05, 8.9202e-01,
        5.4111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.060

[Epoch: 173, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9613, 0.0064, 0.0056, 0.0082, 0.0065, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.553

[Epoch: 174, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1822, 0.0044, 0.0095, 0.0060, 0.0051, 0.0050, 0.7878],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 174, batch: 78/195] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7296e-01, 5.7652e-03, 7.1266e-03, 1.9678e-05, 4.1538e-03, 3.5242e-03,
        6.4485e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.069

[Epoch: 174, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.9369e-02, 1.5496e-02, 1.4508e-05, 3.0868e-02, 2.6688e-02, 8.5940e-01,
        3.8160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 174, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5244e-03, 3.0355e-02, 4.3990e-03, 4.9599e-02, 3.1378e-05, 9.0605e-01,
        5.0368e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.099

[Epoch: 174, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9744, 0.0034, 0.0044, 0.0043, 0.0050, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.494

[Epoch: 175, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2124, 0.0063, 0.0113, 0.0049, 0.0047, 0.0054, 0.7550],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 175, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5989e-01, 9.8144e-03, 9.8062e-03, 3.1524e-05, 6.0838e-03, 5.3250e-03,
        9.0539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.103

[Epoch: 175, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.6929e-02, 1.7961e-02, 3.7250e-05, 2.7462e-02, 2.5367e-02, 8.7089e-01,
        3.1358e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 175, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.5482e-03, 4.2498e-02, 5.2625e-03, 4.9076e-02, 2.1974e-05, 8.9345e-01,
        5.1401e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.059

[Epoch: 175, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9759, 0.0044, 0.0042, 0.0042, 0.0031, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 176, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1752, 0.0049, 0.0100, 0.0042, 0.0056, 0.0049, 0.7952],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 176, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.3817e-01, 1.6238e-02, 1.4784e-02, 6.6201e-05, 8.8947e-03, 6.4326e-03,
        1.5419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 176, batch: 117/195] total loss per batch: 0.854
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.1027e-02, 1.4351e-02, 1.7807e-05, 2.2716e-02, 2.3975e-02, 8.9333e-01,
        2.4579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 176, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0636e-03, 2.6843e-02, 3.9947e-03, 5.7011e-02, 2.2459e-05, 9.0423e-01,
        3.8317e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.084

[Epoch: 176, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9732, 0.0050, 0.0035, 0.0043, 0.0046, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.567

[Epoch: 177, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1974, 0.0063, 0.0123, 0.0053, 0.0057, 0.0053, 0.7678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 177, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6515e-01, 7.9779e-03, 8.1784e-03, 3.2527e-05, 5.3425e-03, 4.6529e-03,
        8.6663e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.113

[Epoch: 177, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0414e-02, 1.4272e-02, 1.8338e-05, 1.9217e-02, 2.0609e-02, 8.8858e-01,
        2.6884e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 177, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.5817e-03, 5.0299e-02, 6.5116e-03, 4.7275e-02, 1.9844e-05, 8.8534e-01,
        4.9744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.048

[Epoch: 177, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9699, 0.0041, 0.0059, 0.0045, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 178, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1934, 0.0048, 0.0090, 0.0041, 0.0053, 0.0051, 0.7781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 178, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6010e-01, 1.0134e-02, 1.0564e-02, 2.9123e-05, 5.9653e-03, 4.6588e-03,
        8.5512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.075

[Epoch: 178, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4579e-02, 1.2821e-02, 1.4992e-05, 2.5444e-02, 2.7635e-02, 8.8270e-01,
        2.6804e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 178, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7431e-03, 2.4977e-02, 4.8736e-03, 4.6762e-02, 1.9086e-05, 9.1291e-01,
        4.7176e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.092

[Epoch: 178, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9654, 0.0064, 0.0059, 0.0059, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 179, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2091, 0.0052, 0.0109, 0.0057, 0.0043, 0.0045, 0.7602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 179, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5235e-01, 1.1337e-02, 1.1127e-02, 4.3406e-05, 6.9674e-03, 6.1753e-03,
        1.2003e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.088

[Epoch: 179, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8473e-02, 1.6665e-02, 1.6573e-05, 3.3409e-02, 3.3225e-02, 8.5602e-01,
        3.2193e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 179, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7391e-03, 3.1157e-02, 4.5716e-03, 4.9826e-02, 1.6064e-05, 9.0084e-01,
        7.8492e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 179, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9721, 0.0046, 0.0042, 0.0041, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.513

[Epoch: 180, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1688, 0.0045, 0.0091, 0.0043, 0.0040, 0.0047, 0.8045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 180, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5798e-01, 9.9563e-03, 1.0268e-02, 3.0147e-05, 6.3765e-03, 5.3145e-03,
        1.0078e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.113

[Epoch: 180, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3026e-02, 1.6684e-02, 1.6223e-05, 1.7672e-02, 2.0522e-02, 8.9516e-01,
        2.6921e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 180, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6881e-03, 4.2863e-02, 5.2255e-03, 5.0600e-02, 2.0187e-05, 8.9130e-01,
        5.3036e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.087

[Epoch: 180, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9684, 0.0052, 0.0053, 0.0048, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.533

[Epoch: 181, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2204, 0.0061, 0.0106, 0.0048, 0.0056, 0.0048, 0.7478],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 181, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5564e-01, 1.1803e-02, 1.0109e-02, 2.8476e-05, 6.3512e-03, 5.0928e-03,
        1.0976e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.113

[Epoch: 181, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2569e-02, 1.4557e-02, 1.6939e-05, 2.4625e-02, 2.2188e-02, 8.8607e-01,
        2.9978e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 181, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.8493e-03, 2.7903e-02, 4.7421e-03, 4.5724e-02, 1.3310e-05, 9.1295e-01,
        4.8174e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 181, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9650, 0.0060, 0.0064, 0.0054, 0.0050, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.530

[Epoch: 182, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1863, 0.0049, 0.0097, 0.0044, 0.0047, 0.0046, 0.7854],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 182, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6716e-01, 7.4176e-03, 8.5947e-03, 1.3764e-05, 4.8187e-03, 4.4312e-03,
        7.5632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 182, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8350e-02, 1.3965e-02, 1.3743e-05, 2.4506e-02, 2.4294e-02, 8.7951e-01,
        2.9359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 182, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([3.9148e-03, 3.1060e-02, 4.5694e-03, 5.3753e-02, 1.3034e-05, 9.0117e-01,
        5.5217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.067

[Epoch: 182, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9700, 0.0042, 0.0049, 0.0053, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.491

[Epoch: 183, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2032, 0.0057, 0.0117, 0.0066, 0.0060, 0.0058, 0.7611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 183, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6808e-01, 7.3487e-03, 7.7685e-03, 1.3127e-05, 4.5306e-03, 4.1617e-03,
        8.1011e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.075

[Epoch: 183, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.0946e-02, 1.3379e-02, 1.5854e-05, 2.4802e-02, 2.6378e-02, 8.8115e-01,
        3.3329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 183, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.3556e-03, 4.1949e-02, 5.1999e-03, 5.1380e-02, 1.5059e-05, 8.8960e-01,
        5.5021e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.083

[Epoch: 183, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9695, 0.0047, 0.0042, 0.0055, 0.0060, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.550

[Epoch: 184, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1865, 0.0041, 0.0104, 0.0049, 0.0050, 0.0051, 0.7839],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 184, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6070e-01, 1.1022e-02, 9.7260e-03, 3.1338e-05, 5.5167e-03, 4.8127e-03,
        8.1919e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.142

[Epoch: 184, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.1804e-02, 1.6845e-02, 1.8647e-05, 3.0748e-02, 2.7310e-02, 8.5793e-01,
        3.5345e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 184, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0581e-03, 3.5304e-02, 6.4827e-03, 4.8832e-02, 1.5215e-05, 8.9963e-01,
        5.6739e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.076

[Epoch: 184, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9712, 0.0054, 0.0041, 0.0050, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.499

[Epoch: 185, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1904, 0.0048, 0.0090, 0.0049, 0.0055, 0.0054, 0.7800],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 185, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5091e-01, 1.1047e-02, 1.1986e-02, 2.8373e-05, 6.2470e-03, 6.5107e-03,
        1.3272e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.066

[Epoch: 185, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8533e-02, 1.5203e-02, 2.3880e-05, 2.2331e-02, 2.3154e-02, 8.9513e-01,
        2.5626e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 185, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.1015e-03, 3.2489e-02, 4.7762e-03, 5.0043e-02, 1.7231e-05, 9.0158e-01,
        5.9952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 185, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9706, 0.0039, 0.0057, 0.0045, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.538

[Epoch: 186, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2076, 0.0070, 0.0140, 0.0047, 0.0052, 0.0070, 0.7545],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 186, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6624e-01, 7.5711e-03, 9.1366e-03, 2.8639e-05, 5.3199e-03, 4.3572e-03,
        7.3503e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.104

[Epoch: 186, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4223e-02, 1.5020e-02, 2.4212e-05, 2.7239e-02, 2.4104e-02, 8.8118e-01,
        2.8209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 186, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4140e-03, 3.1285e-02, 5.5434e-03, 5.4906e-02, 2.4190e-05, 8.9778e-01,
        5.0501e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.065

[Epoch: 186, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.9753, 0.0038, 0.0041, 0.0047, 0.0035, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.537

[Epoch: 187, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1800, 0.0050, 0.0105, 0.0051, 0.0061, 0.0050, 0.7883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 187, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5982e-01, 9.5717e-03, 1.0191e-02, 2.0978e-05, 5.9722e-03, 4.9656e-03,
        9.4576e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.112

[Epoch: 187, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.3452e-02, 1.4056e-02, 1.6241e-05, 1.9903e-02, 2.0567e-02, 8.9010e-01,
        3.1902e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 187, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.1049e-03, 4.5379e-02, 5.6157e-03, 5.1053e-02, 1.7069e-05, 8.8529e-01,
        6.5443e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.078

[Epoch: 187, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9601, 0.0065, 0.0066, 0.0071, 0.0072, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.513

[Epoch: 188, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2267, 0.0081, 0.0095, 0.0080, 0.0067, 0.0067, 0.7341],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.022

[Epoch: 188, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5790e-01, 1.1174e-02, 1.0697e-02, 1.7044e-05, 6.3863e-03, 4.7077e-03,
        9.1146e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.082

[Epoch: 188, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4422e-02, 1.3806e-02, 1.6037e-05, 2.9083e-02, 2.7301e-02, 8.8241e-01,
        2.2966e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 188, batch: 156/195] total loss per batch: 0.866
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([7.0744e-03, 3.1403e-02, 4.7194e-03, 4.5562e-02, 2.5694e-05, 9.0626e-01,
        4.9602e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.063

[Epoch: 188, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9706, 0.0055, 0.0041, 0.0054, 0.0038, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 189, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1796, 0.0051, 0.0109, 0.0053, 0.0049, 0.0055, 0.7887],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 189, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5845e-01, 8.8923e-03, 1.1827e-02, 3.7622e-05, 6.1068e-03, 5.1813e-03,
        9.5030e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.102

[Epoch: 189, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.4149e-02, 1.5998e-02, 2.2920e-05, 2.5210e-02, 3.0403e-02, 8.5503e-01,
        3.9189e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 189, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7488e-03, 3.6901e-02, 4.9495e-03, 5.1723e-02, 1.8680e-05, 8.9589e-01,
        4.7647e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.078

[Epoch: 189, batch: 195/195] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.9653, 0.0046, 0.0054, 0.0057, 0.0065, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.547

[Epoch: 190, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1725, 0.0044, 0.0091, 0.0049, 0.0042, 0.0049, 0.8001],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 190, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5954e-01, 9.4672e-03, 9.3171e-03, 3.9187e-05, 5.9664e-03, 4.9722e-03,
        1.0694e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.098

[Epoch: 190, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9034e-02, 1.6281e-02, 1.3222e-05, 2.1358e-02, 2.0920e-02, 8.9585e-01,
        2.6539e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 190, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.1712e-03, 2.4637e-02, 4.4678e-03, 4.2656e-02, 2.0315e-05, 9.2031e-01,
        3.7341e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.080

[Epoch: 190, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9698, 0.0059, 0.0051, 0.0058, 0.0041, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.524

[Epoch: 191, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2076, 0.0055, 0.0091, 0.0043, 0.0050, 0.0047, 0.7637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.014

[Epoch: 191, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5021e-01, 1.2534e-02, 1.1310e-02, 3.1240e-05, 7.2555e-03, 5.8759e-03,
        1.2781e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.097

[Epoch: 191, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4889e-02, 1.3822e-02, 1.5914e-05, 2.3273e-02, 2.1703e-02, 8.9204e-01,
        2.4255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 191, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.4393e-03, 4.5653e-02, 4.5855e-03, 6.2348e-02, 9.9848e-06, 8.7732e-01,
        4.6484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.073

[Epoch: 191, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9695, 0.0050, 0.0052, 0.0056, 0.0042, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 192, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1946, 0.0047, 0.0096, 0.0040, 0.0043, 0.0041, 0.7786],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.013

[Epoch: 192, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6580e-01, 7.1612e-03, 8.9613e-03, 1.5980e-05, 4.9776e-03, 4.6065e-03,
        8.4800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.077

[Epoch: 192, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([3.0410e-02, 1.9111e-02, 1.6803e-05, 3.1677e-02, 2.8639e-02, 8.4910e-01,
        4.1042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 192, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.0002e-03, 2.2125e-02, 4.4628e-03, 4.4411e-02, 1.4314e-05, 9.2034e-01,
        4.6441e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.071

[Epoch: 192, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9627, 0.0063, 0.0066, 0.0062, 0.0059, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.526

[Epoch: 193, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2122, 0.0051, 0.0093, 0.0045, 0.0044, 0.0052, 0.7593],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 193, batch: 78/195] total loss per batch: 0.862
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6482e-01, 8.8229e-03, 7.7662e-03, 1.7024e-05, 5.0104e-03, 4.2814e-03,
        9.2804e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.131

[Epoch: 193, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.9134e-02, 1.1656e-02, 1.1817e-05, 1.7983e-02, 2.0726e-02, 9.0562e-01,
        2.4867e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 193, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.3532e-03, 3.7355e-02, 4.2199e-03, 4.8901e-02, 1.2549e-05, 8.9884e-01,
        5.3200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.069

[Epoch: 193, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9715, 0.0044, 0.0046, 0.0043, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.524

[Epoch: 194, batch: 39/195] total loss per batch: 0.867
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1693, 0.0064, 0.0126, 0.0080, 0.0065, 0.0060, 0.7913],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.015

[Epoch: 194, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5764e-01, 1.0128e-02, 1.1206e-02, 1.3145e-05, 5.4652e-03, 5.3760e-03,
        1.0169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.081

[Epoch: 194, batch: 117/195] total loss per batch: 0.852
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.7337e-02, 1.5154e-02, 1.3538e-05, 2.6045e-02, 2.5790e-02, 8.7140e-01,
        3.4261e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 194, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.7500e-03, 4.3115e-02, 5.8010e-03, 5.3346e-02, 1.8852e-05, 8.8587e-01,
        6.0999e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 194, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9681, 0.0059, 0.0042, 0.0068, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 195, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1974, 0.0044, 0.0098, 0.0046, 0.0055, 0.0053, 0.7731],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.010

[Epoch: 195, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5724e-01, 1.1799e-02, 1.0202e-02, 2.0287e-05, 5.6630e-03, 5.3462e-03,
        9.7252e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 195, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.8022e-02, 1.4541e-02, 1.6822e-05, 2.5196e-02, 2.8273e-02, 8.7047e-01,
        3.3484e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 195, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6269e-03, 2.5719e-02, 4.5580e-03, 4.4658e-02, 1.5024e-05, 9.1574e-01,
        4.6870e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.069

[Epoch: 195, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9758, 0.0032, 0.0041, 0.0036, 0.0046, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 196, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2105, 0.0051, 0.0116, 0.0053, 0.0057, 0.0046, 0.7570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 196, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.7076e-01, 6.7252e-03, 7.5101e-03, 1.3020e-05, 4.5681e-03, 4.1545e-03,
        6.2703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.107

[Epoch: 196, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.5054e-02, 1.4916e-02, 2.1455e-05, 2.7917e-02, 2.2714e-02, 8.8354e-01,
        2.5839e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 196, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.4501e-03, 3.5975e-02, 4.5019e-03, 5.3961e-02, 1.8186e-05, 8.9715e-01,
        3.9457e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.092

[Epoch: 196, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9750, 0.0046, 0.0034, 0.0041, 0.0043, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.519

[Epoch: 197, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1919, 0.0072, 0.0093, 0.0047, 0.0056, 0.0080, 0.7733],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.000

[Epoch: 197, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.6441e-01, 7.7942e-03, 9.2420e-03, 2.4191e-05, 5.4936e-03, 4.6983e-03,
        8.3427e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.111

[Epoch: 197, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([1.8881e-02, 1.3696e-02, 2.2250e-05, 2.3097e-02, 2.7507e-02, 8.8803e-01,
        2.8764e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 197, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.9091e-03, 3.9531e-02, 6.2513e-03, 5.3744e-02, 1.2780e-05, 8.8889e-01,
        5.6605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.060

[Epoch: 197, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9716, 0.0047, 0.0037, 0.0046, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 198, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1759, 0.0058, 0.0109, 0.0055, 0.0068, 0.0044, 0.7907],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 198, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5939e-01, 8.7953e-03, 1.1304e-02, 2.0098e-05, 5.4833e-03, 5.3326e-03,
        9.6775e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.089

[Epoch: 198, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4863e-02, 1.7085e-02, 2.9591e-05, 2.3428e-02, 2.2702e-02, 8.7769e-01,
        3.4206e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 198, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([5.3207e-03, 4.0502e-02, 5.7925e-03, 4.5441e-02, 1.4030e-05, 8.9758e-01,
        5.3524e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.075

[Epoch: 198, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9665, 0.0049, 0.0073, 0.0048, 0.0071, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.545

[Epoch: 199, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.2411, 0.0079, 0.0100, 0.0069, 0.0055, 0.0071, 0.7215],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 199, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.5450e-01, 1.1731e-02, 1.1070e-02, 1.5313e-05, 7.1631e-03, 5.7260e-03,
        9.7928e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.096

[Epoch: 199, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.4690e-02, 1.3237e-02, 1.4407e-05, 2.7852e-02, 2.2584e-02, 8.8516e-01,
        2.6465e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 199, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([4.6290e-03, 2.8126e-02, 6.2900e-03, 5.0384e-02, 1.3980e-05, 9.0405e-01,
        6.5052e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.061

[Epoch: 199, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9685, 0.0058, 0.0047, 0.0051, 0.0057, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.533

[Epoch: 200, batch: 39/195] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1950, 0.0050, 0.0100, 0.0050, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.1742, 0.0046, 0.0084, 0.0034, 0.0036, 0.0032, 0.8025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.012

[Epoch: 200, batch: 78/195] total loss per batch: 0.863
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0100, 0.0100, 0.0000, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([9.4277e-01, 1.3475e-02, 1.3696e-02, 2.5280e-05, 7.8210e-03, 7.1317e-03,
        1.5083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.099 0.101

[Epoch: 200, batch: 117/195] total loss per batch: 0.853
Policy (actual, predicted): 5 5
Policy data: tensor([0.0250, 0.0150, 0.0000, 0.0250, 0.0250, 0.8800, 0.0300])
Policy pred: tensor([2.2314e-02, 1.4255e-02, 1.3789e-05, 2.1593e-02, 2.4279e-02, 8.9121e-01,
        2.6340e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 200, batch: 156/195] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0350, 0.0050, 0.0500, 0.0000, 0.9000, 0.0050])
Policy pred: tensor([6.5643e-03, 3.7195e-02, 5.5967e-03, 5.1594e-02, 1.6328e-05, 8.9449e-01,
        4.5478e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.074 -0.083

[Epoch: 200, batch: 195/195] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9686, 0.0052, 0.0056, 0.0051, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.509

