Training set samples: 7130
Batch size: 32
[Epoch: 1, batch: 44/223] total loss per batch: 1.616
Policy (actual, predicted): 6 2
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([0.1311, 0.0005, 0.4563, 0.0009, 0.0015, 0.1371, 0.2724],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.290

[Epoch: 1, batch: 88/223] total loss per batch: 1.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.2493e-02, 1.6688e-01, 3.4344e-02, 4.1304e-01, 1.3539e-01, 1.9842e-05,
        2.1783e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.030

[Epoch: 1, batch: 132/223] total loss per batch: 1.431
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.6704e-02, 9.4099e-01, 5.8927e-03, 2.2854e-04, 1.3869e-02, 1.2285e-02,
        2.7429e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.012

[Epoch: 1, batch: 176/223] total loss per batch: 1.471
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0518, 0.3249, 0.0876, 0.1244, 0.1979, 0.0679, 0.1455],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.024

[Epoch: 1, batch: 220/223] total loss per batch: 1.405
Policy (actual, predicted): 4 3
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0111, 0.0829, 0.1202, 0.3122, 0.2366, 0.1302, 0.1067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 2, batch: 44/223] total loss per batch: 1.205
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.7834e-02, 1.4000e-04, 2.7668e-01, 5.0107e-05, 1.1866e-04, 2.6409e-02,
        6.1877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.824

[Epoch: 2, batch: 88/223] total loss per batch: 1.183
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.9448e-03, 3.7209e-02, 5.9255e-03, 5.3156e-01, 5.7271e-02, 2.2288e-06,
        3.5909e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.027

[Epoch: 2, batch: 132/223] total loss per batch: 1.097
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.6625e-03, 9.9241e-01, 1.6254e-03, 4.8428e-06, 1.3271e-03, 1.9620e-03,
        7.7819e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.056

[Epoch: 2, batch: 176/223] total loss per batch: 1.129
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0152, 0.7010, 0.0206, 0.0320, 0.0265, 0.0146, 0.1902],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.022

[Epoch: 2, batch: 220/223] total loss per batch: 1.079
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0295, 0.0599, 0.1621, 0.2598, 0.2743, 0.1269, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.003

[Epoch: 3, batch: 44/223] total loss per batch: 0.926
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0432e-01, 1.6096e-04, 1.7075e-01, 2.0443e-06, 7.1276e-05, 2.5070e-02,
        6.9963e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 3, batch: 88/223] total loss per batch: 0.914
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.9244e-03, 8.5657e-03, 6.5766e-04, 7.3117e-01, 1.1226e-02, 2.4257e-07,
        2.4646e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 3, batch: 132/223] total loss per batch: 0.866
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2095e-04, 9.9582e-01, 9.8104e-04, 3.7382e-06, 1.2925e-03, 1.2805e-03,
        3.5903e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.004

[Epoch: 3, batch: 176/223] total loss per batch: 0.900
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0175, 0.8259, 0.0204, 0.0210, 0.0064, 0.0048, 0.1040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.014

[Epoch: 3, batch: 220/223] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0114, 0.1772, 0.0945, 0.1520, 0.4305, 0.0400, 0.0943],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 4, batch: 44/223] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.7807e-01, 1.9299e-04, 9.9607e-02, 2.9073e-06, 9.3021e-04, 3.1589e-02,
        6.8961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.919

[Epoch: 4, batch: 88/223] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.3567e-03, 1.6526e-02, 6.3183e-03, 8.3726e-01, 1.5588e-02, 7.7424e-07,
        1.2195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 4, batch: 132/223] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.0488e-04, 9.9356e-01, 1.0743e-03, 5.5402e-06, 2.2374e-03, 2.2166e-03,
        1.5611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.059

[Epoch: 4, batch: 176/223] total loss per batch: 0.829
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([5.3705e-03, 9.6072e-01, 8.7535e-03, 8.9046e-03, 3.8879e-03, 9.0974e-04,
        1.1457e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.006

[Epoch: 4, batch: 220/223] total loss per batch: 0.791
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0058, 0.2900, 0.1665, 0.0753, 0.3811, 0.0329, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 5, batch: 44/223] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.6871e-02, 1.7035e-04, 1.2139e-01, 8.0423e-06, 3.0681e-04, 1.8985e-02,
        8.4227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.885

[Epoch: 5, batch: 88/223] total loss per batch: 0.783
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.7305e-03, 1.5187e-02, 6.4147e-04, 7.4304e-01, 2.9164e-03, 1.9534e-07,
        2.3549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 5, batch: 132/223] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2984e-03, 9.9004e-01, 1.5967e-03, 3.5723e-05, 3.3204e-03, 3.7077e-03,
        5.1643e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.044

[Epoch: 5, batch: 176/223] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9633, 0.0080, 0.0068, 0.0023, 0.0011, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.006

[Epoch: 5, batch: 220/223] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0055, 0.0686, 0.0495, 0.1736, 0.5793, 0.0442, 0.0794],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 6, batch: 44/223] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([2.0470e-01, 7.8982e-04, 1.2275e-01, 1.1001e-05, 1.0798e-03, 8.1748e-02,
        5.8892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.787

[Epoch: 6, batch: 88/223] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.3978e-03, 3.0385e-03, 4.2729e-03, 9.0451e-01, 2.8309e-02, 3.6634e-07,
        5.7474e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 6, batch: 132/223] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.8637e-03, 9.8390e-01, 1.9973e-03, 4.3115e-05, 5.5250e-03, 4.6688e-03,
        1.6329e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.020

[Epoch: 6, batch: 176/223] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9343, 0.0112, 0.0148, 0.0025, 0.0022, 0.0297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.009

[Epoch: 6, batch: 220/223] total loss per batch: 0.716
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0014, 0.3710, 0.0392, 0.0504, 0.4382, 0.0153, 0.0845],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.000

[Epoch: 7, batch: 44/223] total loss per batch: 0.730
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.1107e-02, 6.3024e-04, 3.7366e-01, 6.9523e-06, 1.4717e-03, 1.3008e-01,
        4.1305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.888

[Epoch: 7, batch: 88/223] total loss per batch: 0.734
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.7863e-03, 2.2342e-02, 2.9617e-03, 7.4475e-01, 2.7950e-02, 1.2471e-07,
        1.9821e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 7, batch: 132/223] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1629e-03, 9.9159e-01, 1.5937e-03, 7.0092e-05, 2.5744e-03, 3.0018e-03,
        6.1780e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.028

[Epoch: 7, batch: 176/223] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.9256, 0.0063, 0.0090, 0.0066, 0.0062, 0.0370],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.009

[Epoch: 7, batch: 220/223] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0043, 0.0380, 0.0602, 0.2737, 0.4015, 0.0397, 0.1826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 8, batch: 44/223] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.4992e-02, 8.8079e-04, 1.3564e-01, 1.5033e-05, 4.2019e-04, 1.7042e-01,
        6.0764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.873

[Epoch: 8, batch: 88/223] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.8824e-03, 6.4837e-03, 3.2758e-03, 9.1962e-01, 1.2084e-02, 1.9640e-07,
        5.6654e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 8, batch: 132/223] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([4.5207e-03, 9.7996e-01, 2.4994e-03, 4.6721e-05, 5.9161e-03, 7.0501e-03,
        3.3171e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.044

[Epoch: 8, batch: 176/223] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9198, 0.0106, 0.0090, 0.0045, 0.0044, 0.0482],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.007

[Epoch: 8, batch: 220/223] total loss per batch: 0.694
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0027, 0.4762, 0.0166, 0.0366, 0.3797, 0.0231, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 9, batch: 44/223] total loss per batch: 0.702
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.4702e-02, 7.5148e-04, 1.7914e-01, 6.2073e-06, 2.1842e-03, 1.6438e-01,
        5.5884e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.843

[Epoch: 9, batch: 88/223] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.5643e-03, 1.2478e-02, 7.9615e-03, 8.1645e-01, 3.9085e-02, 1.7589e-07,
        1.2046e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 9, batch: 132/223] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4303e-03, 9.8558e-01, 2.0561e-03, 6.0606e-05, 6.3466e-03, 4.5163e-03,
        6.0046e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.035

[Epoch: 9, batch: 176/223] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.8944, 0.0129, 0.0265, 0.0050, 0.0091, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.007

[Epoch: 9, batch: 220/223] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0039, 0.1202, 0.0287, 0.1126, 0.6648, 0.0113, 0.0585],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.011

[Epoch: 10, batch: 44/223] total loss per batch: 0.700
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2057e-01, 2.7124e-04, 1.3053e-01, 1.1119e-05, 3.9663e-04, 8.1197e-02,
        6.6703e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.974

[Epoch: 10, batch: 88/223] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.2573e-03, 1.3498e-02, 4.0950e-03, 8.8099e-01, 6.3521e-03, 5.8820e-07,
        9.2804e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 10, batch: 132/223] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.9788e-03, 9.7894e-01, 3.0161e-03, 4.9399e-05, 8.3733e-03, 6.6362e-03,
        1.4253e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 10, batch: 176/223] total loss per batch: 0.727
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.9314, 0.0053, 0.0042, 0.0078, 0.0027, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.006

[Epoch: 10, batch: 220/223] total loss per batch: 0.677
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0045, 0.3598, 0.0224, 0.1361, 0.3276, 0.0156, 0.1339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 11, batch: 44/223] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0963e-01, 2.3030e-04, 1.9399e-01, 1.2470e-05, 6.3544e-04, 1.7331e-01,
        5.2220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.914

[Epoch: 11, batch: 88/223] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8253e-03, 9.4234e-03, 7.7157e-03, 8.1460e-01, 4.0252e-02, 1.1020e-07,
        1.2418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 11, batch: 132/223] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.6626e-03, 9.7069e-01, 4.5873e-03, 1.0724e-04, 8.2238e-03, 1.3722e-02,
        6.5399e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.068

[Epoch: 11, batch: 176/223] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9575, 0.0057, 0.0079, 0.0022, 0.0026, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.034

[Epoch: 11, batch: 220/223] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0058, 0.2313, 0.0677, 0.1236, 0.5032, 0.0097, 0.0586],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.003

[Epoch: 12, batch: 44/223] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.5634e-02, 5.7887e-04, 1.0778e-01, 1.2356e-05, 5.5257e-04, 8.0228e-02,
        7.2522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.807

[Epoch: 12, batch: 88/223] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([9.9981e-04, 3.1671e-03, 8.2654e-04, 9.4145e-01, 1.9079e-03, 2.1581e-07,
        5.1653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 12, batch: 132/223] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.8173e-03, 9.7738e-01, 3.3197e-03, 6.4189e-05, 1.0371e-02, 6.0485e-03,
        1.2420e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.072

[Epoch: 12, batch: 176/223] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9210, 0.0069, 0.0110, 0.0079, 0.0051, 0.0413],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.187

[Epoch: 12, batch: 220/223] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0077, 0.2018, 0.0556, 0.1674, 0.4603, 0.0147, 0.0925],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.046

[Epoch: 13, batch: 44/223] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1943e-01, 2.9385e-04, 1.8868e-01, 5.9148e-06, 3.4696e-04, 1.7877e-01,
        5.1247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.923

[Epoch: 13, batch: 88/223] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.1244e-03, 1.1841e-02, 3.3086e-03, 8.9391e-01, 8.5095e-03, 2.0545e-07,
        8.0308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 13, batch: 132/223] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.4272e-03, 9.7794e-01, 3.0014e-03, 1.8811e-04, 8.3649e-03, 8.0766e-03,
        2.9294e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.030

[Epoch: 13, batch: 176/223] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9745, 0.0030, 0.0031, 0.0014, 0.0010, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.358

[Epoch: 13, batch: 220/223] total loss per batch: 0.673
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0125, 0.3193, 0.0170, 0.1278, 0.3767, 0.0154, 0.1313],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 14, batch: 44/223] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.5306e-02, 4.1015e-04, 8.6030e-02, 1.3297e-05, 2.2798e-04, 8.2806e-02,
        7.4521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.868

[Epoch: 14, batch: 88/223] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.9721e-03, 9.0648e-03, 1.5774e-03, 9.2606e-01, 9.2113e-03, 1.2045e-07,
        5.2110e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 14, batch: 132/223] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9408e-03, 9.8504e-01, 1.7937e-03, 1.4212e-05, 4.8606e-03, 6.3530e-03,
        2.1635e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.002

[Epoch: 14, batch: 176/223] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.9584, 0.0054, 0.0086, 0.0069, 0.0038, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.456

[Epoch: 14, batch: 220/223] total loss per batch: 0.670
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0029, 0.2623, 0.0405, 0.1005, 0.5464, 0.0146, 0.0329],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 15, batch: 44/223] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0805e-01, 8.5362e-04, 2.6552e-01, 1.1361e-05, 1.1306e-03, 2.6706e-01,
        3.5737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.892

[Epoch: 15, batch: 88/223] total loss per batch: 0.679
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.0544e-03, 2.1688e-02, 4.7041e-03, 7.6543e-01, 2.9700e-02, 5.8816e-07,
        1.7542e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 15, batch: 132/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.1400e-03, 9.6816e-01, 6.3856e-03, 1.4504e-04, 1.0281e-02, 8.8887e-03,
        2.6950e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.049

[Epoch: 15, batch: 176/223] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9711, 0.0056, 0.0033, 0.0018, 0.0015, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.568

[Epoch: 15, batch: 220/223] total loss per batch: 0.668
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0070, 0.2667, 0.0368, 0.2291, 0.3250, 0.0233, 0.1121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.002

[Epoch: 16, batch: 44/223] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1391e-01, 4.2913e-04, 1.1573e-01, 1.2711e-05, 2.0224e-04, 1.3590e-01,
        6.3381e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.927

[Epoch: 16, batch: 88/223] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.9790e-03, 9.4275e-03, 3.0647e-03, 9.2507e-01, 1.8229e-03, 2.3051e-07,
        5.7632e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 16, batch: 132/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.2051e-03, 9.8405e-01, 2.1701e-03, 4.5539e-05, 5.1284e-03, 5.3953e-03,
        3.4622e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.019

[Epoch: 16, batch: 176/223] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.9145, 0.0129, 0.0202, 0.0108, 0.0043, 0.0310],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.631

[Epoch: 16, batch: 220/223] total loss per batch: 0.662
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0090, 0.2843, 0.0199, 0.0900, 0.5226, 0.0133, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.001

[Epoch: 17, batch: 44/223] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([5.1078e-02, 5.1905e-04, 1.9381e-01, 4.2818e-06, 3.6764e-04, 8.2012e-02,
        6.7221e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.790

[Epoch: 17, batch: 88/223] total loss per batch: 0.674
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.1590e-03, 9.2254e-03, 2.4744e-03, 9.3117e-01, 1.3801e-02, 2.2626e-07,
        4.1172e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 17, batch: 132/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4761e-03, 9.6716e-01, 3.4841e-03, 1.5074e-04, 7.7224e-03, 1.4004e-02,
        5.5388e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.058

[Epoch: 17, batch: 176/223] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9589, 0.0043, 0.0088, 0.0029, 0.0027, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.654

[Epoch: 17, batch: 220/223] total loss per batch: 0.661
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0035, 0.1485, 0.0207, 0.1581, 0.5753, 0.0145, 0.0795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 18, batch: 44/223] total loss per batch: 0.679
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([2.2407e-01, 6.9932e-04, 1.2714e-01, 2.6841e-05, 3.2778e-04, 1.2472e-01,
        5.2301e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.892

[Epoch: 18, batch: 88/223] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1891e-03, 1.7030e-02, 3.1685e-03, 8.9716e-01, 1.2828e-02, 3.3390e-07,
        6.5622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 18, batch: 132/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.2087e-03, 9.7170e-01, 8.2021e-03, 3.7153e-05, 7.3255e-03, 7.5182e-03,
        8.2167e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.022

[Epoch: 18, batch: 176/223] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0030, 0.9542, 0.0055, 0.0076, 0.0101, 0.0032, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.688

[Epoch: 18, batch: 220/223] total loss per batch: 0.663
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0085, 0.3354, 0.0332, 0.1201, 0.3849, 0.0169, 0.1010],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.002

[Epoch: 19, batch: 44/223] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([3.4163e-02, 8.3641e-05, 1.8133e-01, 5.6144e-06, 2.9027e-04, 1.1525e-01,
        6.6888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.921

[Epoch: 19, batch: 88/223] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.5414e-03, 7.5212e-03, 3.6127e-03, 9.2748e-01, 7.6805e-03, 5.0708e-07,
        5.2163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 19, batch: 132/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1002e-02, 9.5115e-01, 9.2358e-03, 5.3977e-04, 1.6439e-02, 1.1623e-02,
        1.2391e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.032

[Epoch: 19, batch: 176/223] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9535, 0.0037, 0.0143, 0.0022, 0.0035, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.729

[Epoch: 19, batch: 220/223] total loss per batch: 0.662
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0089, 0.2057, 0.0294, 0.1028, 0.5256, 0.0256, 0.1021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 20, batch: 44/223] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0297e-02, 3.8729e-04, 8.5268e-02, 7.6499e-06, 4.8835e-04, 8.2823e-02,
        7.4073e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.929

[Epoch: 20, batch: 88/223] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.4181e-03, 2.5802e-02, 4.6232e-03, 8.5178e-01, 2.0874e-02, 3.6689e-07,
        8.8498e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 20, batch: 132/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3491e-02, 9.6293e-01, 8.3674e-03, 4.2171e-05, 7.4458e-03, 7.7117e-03,
        1.4124e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.047

[Epoch: 20, batch: 176/223] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9498, 0.0098, 0.0116, 0.0076, 0.0021, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.640

[Epoch: 20, batch: 220/223] total loss per batch: 0.663
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0034, 0.2968, 0.0274, 0.1714, 0.3648, 0.0156, 0.1205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 21, batch: 44/223] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2380e-01, 5.3263e-04, 2.6158e-01, 3.3169e-05, 8.6815e-04, 2.1149e-01,
        4.0169e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.693

[Epoch: 21, batch: 88/223] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.0663e-03, 1.2283e-02, 3.0895e-03, 8.9210e-01, 1.4087e-02, 6.1146e-07,
        7.6378e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 21, batch: 132/223] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0731e-02, 9.7117e-01, 5.4371e-03, 2.3935e-04, 7.5763e-03, 4.8430e-03,
        2.7531e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.023

[Epoch: 21, batch: 176/223] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.9716, 0.0038, 0.0059, 0.0025, 0.0019, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.691

[Epoch: 21, batch: 220/223] total loss per batch: 0.660
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0045, 0.1925, 0.0154, 0.0883, 0.6587, 0.0143, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 22, batch: 44/223] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.6757e-02, 4.7733e-04, 1.5314e-01, 2.1272e-05, 3.3700e-04, 1.8652e-01,
        5.6275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.936

[Epoch: 22, batch: 88/223] total loss per batch: 0.674
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.2808e-03, 1.0439e-02, 2.8468e-03, 8.3704e-01, 5.0079e-03, 6.1091e-07,
        1.3838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 22, batch: 132/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0817e-02, 9.6433e-01, 5.7222e-03, 6.9062e-05, 8.3108e-03, 1.0743e-02,
        1.0090e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 22, batch: 176/223] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9561, 0.0060, 0.0105, 0.0038, 0.0041, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.726

[Epoch: 22, batch: 220/223] total loss per batch: 0.659
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0079, 0.2852, 0.0202, 0.1301, 0.3705, 0.0159, 0.1703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.000

[Epoch: 23, batch: 44/223] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.3333e-02, 3.3927e-04, 1.4737e-01, 1.1543e-05, 8.7316e-04, 1.5299e-01,
        6.2509e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.922

[Epoch: 23, batch: 88/223] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.5784e-03, 4.4320e-03, 3.1142e-03, 9.2360e-01, 1.5911e-02, 2.0971e-07,
        5.1369e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 23, batch: 132/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0340e-02, 9.6190e-01, 5.5400e-03, 7.3957e-05, 1.0326e-02, 1.1812e-02,
        5.0300e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.041

[Epoch: 23, batch: 176/223] total loss per batch: 0.699
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9424, 0.0058, 0.0208, 0.0061, 0.0038, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.790

[Epoch: 23, batch: 220/223] total loss per batch: 0.659
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0045, 0.3118, 0.0290, 0.1785, 0.4271, 0.0100, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.000

[Epoch: 24, batch: 44/223] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0958e-01, 3.1160e-04, 8.6068e-02, 5.0909e-06, 2.0231e-04, 1.5775e-01,
        6.4608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 24, batch: 88/223] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.2808e-03, 7.7893e-03, 4.3312e-03, 8.9108e-01, 6.7632e-03, 5.4442e-07,
        8.6757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 24, batch: 132/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.5234e-03, 9.6083e-01, 5.6410e-03, 3.2558e-04, 1.4367e-02, 1.0297e-02,
        1.2526e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.063

[Epoch: 24, batch: 176/223] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.9437, 0.0111, 0.0061, 0.0036, 0.0061, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.772

[Epoch: 24, batch: 220/223] total loss per batch: 0.655
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0085, 0.2713, 0.0391, 0.1222, 0.4027, 0.0218, 0.1344],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 25, batch: 44/223] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3111e-01, 5.1372e-04, 1.9268e-01, 1.4538e-05, 6.2169e-04, 1.2073e-01,
        5.5433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.866

[Epoch: 25, batch: 88/223] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.0521e-03, 4.5516e-03, 4.0328e-03, 9.2689e-01, 1.2949e-02, 1.6553e-07,
        4.9529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 25, batch: 132/223] total loss per batch: 0.658
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9370e-02, 9.6139e-01, 6.4392e-03, 3.7754e-05, 6.8106e-03, 5.9483e-03,
        7.2160e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.080

[Epoch: 25, batch: 176/223] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.9670, 0.0065, 0.0082, 0.0016, 0.0055, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.776

[Epoch: 25, batch: 220/223] total loss per batch: 0.651
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0023, 0.2397, 0.0178, 0.2329, 0.4239, 0.0119, 0.0714],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.006

[Epoch: 26, batch: 44/223] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2233e-01, 3.2322e-04, 1.4048e-01, 8.2530e-07, 3.8664e-04, 1.4100e-01,
        5.9548e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.963

[Epoch: 26, batch: 88/223] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.2170e-03, 8.8522e-03, 2.7628e-03, 8.9515e-01, 4.5582e-03, 6.5825e-08,
        8.5458e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 26, batch: 132/223] total loss per batch: 0.655
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5491e-03, 9.6881e-01, 5.0684e-03, 8.9674e-05, 6.7532e-03, 9.7222e-03,
        6.8872e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 26, batch: 176/223] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9503, 0.0095, 0.0091, 0.0061, 0.0040, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.816

[Epoch: 26, batch: 220/223] total loss per batch: 0.651
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0064, 0.2655, 0.0127, 0.0814, 0.5344, 0.0247, 0.0750],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 27, batch: 44/223] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0656e-02, 4.1547e-04, 1.9269e-01, 5.1174e-06, 6.6556e-04, 1.6143e-01,
        5.5415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.772

[Epoch: 27, batch: 88/223] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.8679e-03, 3.8770e-03, 2.7301e-03, 9.2255e-01, 1.4345e-02, 5.3456e-07,
        5.3626e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 27, batch: 132/223] total loss per batch: 0.657
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6810e-02, 9.5985e-01, 8.8627e-03, 2.8033e-05, 7.5709e-03, 6.8739e-03,
        3.5582e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.059

[Epoch: 27, batch: 176/223] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.9678, 0.0067, 0.0073, 0.0039, 0.0042, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.825

[Epoch: 27, batch: 220/223] total loss per batch: 0.651
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0109, 0.3294, 0.0212, 0.2122, 0.3128, 0.0052, 0.1084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 28, batch: 44/223] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.5524e-02, 1.6374e-04, 1.3518e-01, 4.3528e-06, 1.2639e-04, 1.0960e-01,
        6.5940e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.926

[Epoch: 28, batch: 88/223] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.4032e-03, 1.2109e-02, 3.8731e-03, 8.1234e-01, 1.0475e-02, 1.8336e-07,
        1.5880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 28, batch: 132/223] total loss per batch: 0.658
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.6731e-02, 9.5418e-01, 4.3032e-03, 1.9559e-04, 8.4026e-03, 6.1710e-03,
        1.2634e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 28, batch: 176/223] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9550, 0.0058, 0.0079, 0.0042, 0.0036, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.838

[Epoch: 28, batch: 220/223] total loss per batch: 0.656
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0043, 0.1682, 0.0138, 0.1057, 0.6428, 0.0185, 0.0467],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 29, batch: 44/223] total loss per batch: 0.671
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2344e-01, 6.7651e-04, 1.4102e-01, 3.4676e-06, 3.2369e-03, 1.6962e-01,
        5.6200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.781

[Epoch: 29, batch: 88/223] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.4873e-03, 5.8957e-03, 3.0643e-03, 9.1646e-01, 4.4308e-03, 4.9510e-07,
        6.8666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 29, batch: 132/223] total loss per batch: 0.659
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0929e-03, 9.7669e-01, 4.2840e-03, 2.1390e-05, 6.0512e-03, 5.8567e-03,
        4.0964e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.065

[Epoch: 29, batch: 176/223] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9782, 0.0037, 0.0035, 0.0029, 0.0022, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.833

[Epoch: 29, batch: 220/223] total loss per batch: 0.660
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0130, 0.1472, 0.0285, 0.1343, 0.3507, 0.0083, 0.3180],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.001

[Epoch: 30, batch: 44/223] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.7697e-01, 4.5296e-04, 1.1663e-01, 4.9855e-06, 6.8540e-04, 9.2249e-02,
        6.1300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.960

[Epoch: 30, batch: 88/223] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5612e-03, 4.2805e-03, 2.4333e-03, 8.8145e-01, 1.6537e-02, 2.2686e-07,
        9.0736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 30, batch: 132/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.3684e-02, 9.5578e-01, 4.7422e-03, 1.1784e-04, 8.0678e-03, 7.5969e-03,
        1.0799e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.009

[Epoch: 30, batch: 176/223] total loss per batch: 0.700
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9602, 0.0048, 0.0038, 0.0122, 0.0024, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.832

[Epoch: 30, batch: 220/223] total loss per batch: 0.668
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0054, 0.3317, 0.0197, 0.0958, 0.5123, 0.0058, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.102

[Epoch: 31, batch: 44/223] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([6.1497e-02, 1.4276e-03, 1.5745e-01, 5.2566e-06, 7.0927e-04, 1.9484e-01,
        5.8407e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.895

[Epoch: 31, batch: 88/223] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.3286e-03, 7.4456e-03, 7.6326e-03, 9.2591e-01, 1.5344e-02, 5.7952e-08,
        4.0343e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 31, batch: 132/223] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3801e-02, 9.6039e-01, 7.9586e-03, 5.9417e-04, 7.9755e-03, 9.2539e-03,
        2.8446e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 -0.004

[Epoch: 31, batch: 176/223] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9806, 0.0041, 0.0025, 0.0014, 0.0025, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.879

[Epoch: 31, batch: 220/223] total loss per batch: 0.667
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0055, 0.3022, 0.0128, 0.1838, 0.4433, 0.0121, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.003

[Epoch: 32, batch: 44/223] total loss per batch: 0.679
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0396e-01, 6.1030e-05, 1.2134e-01, 7.8243e-06, 7.2154e-05, 7.9198e-02,
        6.9537e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.965

[Epoch: 32, batch: 88/223] total loss per batch: 0.674
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5132e-03, 1.2461e-02, 2.2894e-03, 9.1490e-01, 8.1306e-03, 2.5582e-07,
        5.7710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 32, batch: 132/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.7890e-03, 9.6438e-01, 8.6867e-03, 1.1607e-04, 8.3937e-03, 1.0626e-02,
        5.1482e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.088

[Epoch: 32, batch: 176/223] total loss per batch: 0.700
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.9590, 0.0040, 0.0062, 0.0064, 0.0021, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.759

[Epoch: 32, batch: 220/223] total loss per batch: 0.662
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0075, 0.2158, 0.0209, 0.1123, 0.5065, 0.0064, 0.1304],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.006

[Epoch: 33, batch: 44/223] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.8576e-02, 3.9973e-04, 2.6867e-01, 3.4448e-06, 1.9779e-04, 1.1492e-01,
        5.2723e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.893

[Epoch: 33, batch: 88/223] total loss per batch: 0.665
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.3301e-03, 4.5576e-03, 1.2217e-02, 8.9525e-01, 5.5896e-03, 1.4561e-07,
        8.0052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 33, batch: 132/223] total loss per batch: 0.659
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6426e-02, 9.5218e-01, 1.0162e-02, 1.3008e-04, 7.5828e-03, 1.3494e-02,
        2.1588e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 33, batch: 176/223] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9612, 0.0086, 0.0072, 0.0029, 0.0037, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.820

[Epoch: 33, batch: 220/223] total loss per batch: 0.652
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2907, 0.0283, 0.1983, 0.3864, 0.0102, 0.0755],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 34, batch: 44/223] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4926e-01, 4.2700e-04, 1.0595e-01, 9.7054e-06, 1.3504e-04, 7.5432e-02,
        6.6879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.924

[Epoch: 34, batch: 88/223] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([9.0074e-03, 3.6275e-02, 4.9663e-03, 7.8332e-01, 8.1026e-02, 3.9436e-07,
        8.5403e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 34, batch: 132/223] total loss per batch: 0.651
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5115e-02, 9.6215e-01, 4.9991e-03, 1.2547e-04, 6.8739e-03, 1.0719e-02,
        1.6024e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.051

[Epoch: 34, batch: 176/223] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9479, 0.0102, 0.0054, 0.0056, 0.0049, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.840

[Epoch: 34, batch: 220/223] total loss per batch: 0.644
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0061, 0.4381, 0.0117, 0.1158, 0.3275, 0.0126, 0.0881],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 35, batch: 44/223] total loss per batch: 0.661
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.2675e-02, 1.2568e-04, 1.6493e-01, 5.9057e-06, 2.6131e-04, 1.6573e-01,
        5.8627e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.906

[Epoch: 35, batch: 88/223] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8580e-03, 1.3257e-02, 4.4245e-03, 8.7801e-01, 8.1848e-03, 9.0639e-08,
        9.2269e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 35, batch: 132/223] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8452e-03, 9.6722e-01, 8.8264e-03, 1.0614e-04, 6.8292e-03, 7.1692e-03,
        7.1689e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.049

[Epoch: 35, batch: 176/223] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9767, 0.0031, 0.0043, 0.0025, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.855

[Epoch: 35, batch: 220/223] total loss per batch: 0.639
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0048, 0.1317, 0.0119, 0.1868, 0.5946, 0.0083, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 36, batch: 44/223] total loss per batch: 0.658
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4865e-01, 1.6986e-04, 1.3479e-01, 1.9823e-06, 2.9390e-04, 1.2026e-01,
        5.9585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.905

[Epoch: 36, batch: 88/223] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.7060e-03, 1.0443e-02, 2.2332e-03, 9.2303e-01, 1.1127e-02, 1.8262e-07,
        4.9460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 36, batch: 132/223] total loss per batch: 0.644
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4398e-02, 9.5853e-01, 6.9665e-03, 4.4579e-05, 7.1516e-03, 1.2899e-02,
        9.2689e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 36, batch: 176/223] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9326, 0.0077, 0.0095, 0.0078, 0.0101, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.870

[Epoch: 36, batch: 220/223] total loss per batch: 0.638
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0072, 0.3256, 0.0153, 0.1129, 0.4033, 0.0144, 0.1212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 37, batch: 44/223] total loss per batch: 0.655
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.2209e-02, 1.9047e-04, 1.7241e-01, 2.5319e-06, 3.3787e-04, 9.6674e-02,
        6.5817e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.864

[Epoch: 37, batch: 88/223] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.1923e-03, 5.1806e-03, 4.2652e-03, 8.8238e-01, 6.5538e-03, 9.8672e-08,
        9.8431e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 37, batch: 132/223] total loss per batch: 0.645
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2472e-02, 9.6755e-01, 6.8281e-03, 4.3910e-05, 7.1334e-03, 5.9612e-03,
        1.0535e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.055

[Epoch: 37, batch: 176/223] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9720, 0.0039, 0.0038, 0.0047, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.841

[Epoch: 37, batch: 220/223] total loss per batch: 0.639
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0067, 0.2073, 0.0088, 0.1938, 0.4765, 0.0106, 0.0962],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 38, batch: 44/223] total loss per batch: 0.654
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.6927e-01, 1.3946e-04, 1.1953e-01, 2.2049e-06, 5.9882e-04, 1.9424e-01,
        5.1622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.888

[Epoch: 38, batch: 88/223] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.7159e-03, 1.6712e-02, 2.8968e-03, 8.6408e-01, 8.1294e-03, 3.9645e-07,
        1.0246e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 38, batch: 132/223] total loss per batch: 0.644
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4119e-02, 9.5305e-01, 1.1199e-02, 6.7067e-05, 9.6027e-03, 1.1954e-02,
        8.4229e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 38, batch: 176/223] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.9592, 0.0063, 0.0068, 0.0050, 0.0052, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.830

[Epoch: 38, batch: 220/223] total loss per batch: 0.640
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0149, 0.3158, 0.0162, 0.1534, 0.4281, 0.0072, 0.0643],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 39, batch: 44/223] total loss per batch: 0.655
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.3232e-02, 2.0241e-04, 1.7843e-01, 1.1845e-05, 1.5267e-04, 1.2553e-01,
        6.2245e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.859

[Epoch: 39, batch: 88/223] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.7138e-03, 5.2435e-03, 3.0212e-03, 9.5349e-01, 3.7392e-03, 1.5525e-07,
        3.2793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 39, batch: 132/223] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2388e-02, 9.5357e-01, 1.1502e-02, 2.9994e-05, 6.8211e-03, 1.5673e-02,
        1.2858e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.048

[Epoch: 39, batch: 176/223] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0111, 0.9646, 0.0045, 0.0047, 0.0042, 0.0059, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.883

[Epoch: 39, batch: 220/223] total loss per batch: 0.641
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0064, 0.2509, 0.0109, 0.1655, 0.4745, 0.0106, 0.0813],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.055

[Epoch: 40, batch: 44/223] total loss per batch: 0.655
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.5420e-02, 1.7058e-04, 1.3470e-01, 3.9190e-06, 2.1964e-04, 1.2476e-01,
        6.4472e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.863

[Epoch: 40, batch: 88/223] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.5774e-03, 1.8439e-02, 2.9075e-03, 8.8803e-01, 1.0618e-02, 2.4235e-07,
        7.6432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 40, batch: 132/223] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.2791e-02, 9.4648e-01, 9.1664e-03, 5.5994e-05, 1.1862e-02, 9.6243e-03,
        1.7935e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 40, batch: 176/223] total loss per batch: 0.684
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9556, 0.0076, 0.0068, 0.0044, 0.0120, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.830

[Epoch: 40, batch: 220/223] total loss per batch: 0.641
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0052, 0.3498, 0.0134, 0.1036, 0.4062, 0.0158, 0.1061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.051

[Epoch: 41, batch: 44/223] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5025e-01, 4.1002e-05, 1.7530e-01, 3.9475e-06, 1.2881e-04, 1.6885e-01,
        5.0542e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.868

[Epoch: 41, batch: 88/223] total loss per batch: 0.652
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.0718e-03, 5.5916e-03, 4.3827e-03, 8.9074e-01, 6.7920e-03, 4.7506e-07,
        8.9422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 41, batch: 132/223] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.2372e-03, 9.6754e-01, 1.1248e-02, 3.7445e-05, 5.0262e-03, 7.9025e-03,
        6.4912e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.022

[Epoch: 41, batch: 176/223] total loss per batch: 0.685
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.9557, 0.0032, 0.0046, 0.0101, 0.0081, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.838

[Epoch: 41, batch: 220/223] total loss per batch: 0.643
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0212, 0.1423, 0.0181, 0.2094, 0.4913, 0.0069, 0.1108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.054

[Epoch: 42, batch: 44/223] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.1178e-02, 1.9743e-04, 1.6788e-01, 7.0312e-06, 1.8335e-04, 1.1820e-01,
        6.3236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.911

[Epoch: 42, batch: 88/223] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5752e-03, 7.9222e-03, 1.8970e-03, 9.1037e-01, 1.3223e-02, 1.7025e-07,
        6.2008e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 42, batch: 132/223] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5361e-02, 9.5305e-01, 1.1158e-02, 7.0916e-05, 9.0638e-03, 1.1276e-02,
        2.0366e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.045

[Epoch: 42, batch: 176/223] total loss per batch: 0.684
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.9670, 0.0050, 0.0073, 0.0035, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.832

[Epoch: 42, batch: 220/223] total loss per batch: 0.646
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0102, 0.4170, 0.0169, 0.1470, 0.3181, 0.0089, 0.0820],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.047

[Epoch: 43, batch: 44/223] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.7221e-01, 8.0900e-05, 1.2325e-01, 5.6373e-06, 4.6080e-05, 8.0244e-02,
        6.2416e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.843

[Epoch: 43, batch: 88/223] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.2146e-03, 1.5834e-02, 3.3997e-03, 8.9086e-01, 2.9290e-03, 2.6659e-07,
        8.3761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 43, batch: 132/223] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0175e-02, 9.6931e-01, 6.5668e-03, 1.0343e-05, 4.9820e-03, 8.9396e-03,
        1.6114e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.073

[Epoch: 43, batch: 176/223] total loss per batch: 0.685
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9821, 0.0025, 0.0025, 0.0016, 0.0034, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.883

[Epoch: 43, batch: 220/223] total loss per batch: 0.646
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0078, 0.1878, 0.0185, 0.1354, 0.5569, 0.0098, 0.0838],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 44, batch: 44/223] total loss per batch: 0.663
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.9851e-02, 2.5692e-04, 1.5587e-01, 8.5308e-06, 1.9647e-03, 1.9813e-01,
        5.4393e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.894

[Epoch: 44, batch: 88/223] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.2838e-03, 6.2306e-03, 3.8851e-03, 8.7652e-01, 6.4665e-03, 1.5226e-07,
        1.0462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 44, batch: 132/223] total loss per batch: 0.653
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7313e-03, 9.6266e-01, 8.4176e-03, 7.8242e-06, 6.8805e-03, 1.5297e-02,
        8.4482e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.049

[Epoch: 44, batch: 176/223] total loss per batch: 0.685
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0169, 0.9484, 0.0114, 0.0063, 0.0012, 0.0062, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.807

[Epoch: 44, batch: 220/223] total loss per batch: 0.645
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0064, 0.3918, 0.0066, 0.1447, 0.3364, 0.0129, 0.1012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.050

[Epoch: 45, batch: 44/223] total loss per batch: 0.663
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2896e-01, 7.2090e-05, 1.3167e-01, 1.0035e-05, 9.6476e-05, 1.2140e-01,
        6.1779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.890

[Epoch: 45, batch: 88/223] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.2816e-03, 1.0243e-02, 3.6298e-03, 9.0003e-01, 6.9163e-03, 9.6791e-07,
        7.5895e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 45, batch: 132/223] total loss per batch: 0.654
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.7380e-02, 9.3202e-01, 1.4053e-02, 4.5460e-05, 6.6938e-03, 9.7834e-03,
        2.2577e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.054

[Epoch: 45, batch: 176/223] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.9663, 0.0030, 0.0034, 0.0040, 0.0087, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.854

[Epoch: 45, batch: 220/223] total loss per batch: 0.646
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0172, 0.2518, 0.0101, 0.1412, 0.5100, 0.0070, 0.0628],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 46, batch: 44/223] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([6.8216e-02, 7.9510e-04, 2.3371e-01, 1.1028e-05, 5.0099e-04, 1.3634e-01,
        5.6043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.868

[Epoch: 46, batch: 88/223] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8949e-03, 7.5532e-03, 8.2111e-03, 9.0457e-01, 3.1208e-03, 1.6616e-06,
        7.1652e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 46, batch: 132/223] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5984e-02, 9.4966e-01, 6.4738e-03, 7.8589e-05, 1.2745e-02, 1.5036e-02,
        2.2408e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.315

[Epoch: 46, batch: 176/223] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9513, 0.0066, 0.0025, 0.0020, 0.0070, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.855

[Epoch: 46, batch: 220/223] total loss per batch: 0.643
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0113, 0.2012, 0.0173, 0.1791, 0.3878, 0.0146, 0.1887],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.065

[Epoch: 47, batch: 44/223] total loss per batch: 0.658
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([2.1179e-01, 4.5754e-04, 1.1350e-01, 7.9964e-06, 8.8493e-04, 9.1760e-02,
        5.8159e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.825

[Epoch: 47, batch: 88/223] total loss per batch: 0.652
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2085e-03, 4.2270e-02, 5.2320e-03, 8.5067e-01, 7.4393e-03, 5.5360e-07,
        8.9182e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 47, batch: 132/223] total loss per batch: 0.644
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6302e-02, 9.5723e-01, 7.7614e-03, 3.6398e-05, 9.2769e-03, 9.3840e-03,
        8.7823e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.026

[Epoch: 47, batch: 176/223] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9667, 0.0047, 0.0079, 0.0028, 0.0069, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.883

[Epoch: 47, batch: 220/223] total loss per batch: 0.640
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0034, 0.2469, 0.0045, 0.1139, 0.5946, 0.0032, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 48, batch: 44/223] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([5.2330e-02, 4.1617e-04, 1.9223e-01, 1.5426e-05, 3.5129e-04, 1.3522e-01,
        6.1944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.871

[Epoch: 48, batch: 88/223] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.0874e-02, 4.4144e-03, 6.0123e-03, 8.9418e-01, 4.9755e-03, 1.4240e-06,
        7.9540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 48, batch: 132/223] total loss per batch: 0.643
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.3744e-03, 9.7143e-01, 8.0187e-03, 2.4793e-05, 5.6091e-03, 6.5238e-03,
        1.6417e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 48, batch: 176/223] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9724, 0.0043, 0.0034, 0.0072, 0.0049, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.875

[Epoch: 48, batch: 220/223] total loss per batch: 0.637
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0063, 0.1892, 0.0190, 0.1745, 0.4710, 0.0080, 0.1319],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.059

[Epoch: 49, batch: 44/223] total loss per batch: 0.653
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3645e-01, 5.1185e-04, 1.8216e-01, 3.6231e-06, 7.2038e-04, 1.6929e-01,
        5.1087e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.877

[Epoch: 49, batch: 88/223] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.6199e-03, 9.2736e-03, 4.3962e-03, 8.9282e-01, 9.4787e-03, 5.4036e-07,
        8.0413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 49, batch: 132/223] total loss per batch: 0.643
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.9761e-02, 9.3988e-01, 1.0268e-02, 3.5386e-05, 1.0589e-02, 9.4452e-03,
        2.2054e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.042

[Epoch: 49, batch: 176/223] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0123, 0.9356, 0.0121, 0.0070, 0.0028, 0.0090, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.880

[Epoch: 49, batch: 220/223] total loss per batch: 0.639
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0090, 0.4062, 0.0089, 0.1635, 0.3365, 0.0099, 0.0660],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 50, batch: 44/223] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0509e-01, 2.0267e-04, 1.1927e-01, 2.6038e-05, 4.7954e-04, 9.2858e-02,
        6.8208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.834

[Epoch: 50, batch: 88/223] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9977e-03, 5.6468e-03, 3.7454e-03, 8.9260e-01, 5.3681e-03, 8.5340e-07,
        8.7644e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 50, batch: 132/223] total loss per batch: 0.643
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4551e-02, 9.5538e-01, 1.4274e-02, 5.4815e-05, 5.9377e-03, 9.7807e-03,
        2.3948e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.079

[Epoch: 50, batch: 176/223] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.9677, 0.0042, 0.0042, 0.0050, 0.0096, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.887

[Epoch: 50, batch: 220/223] total loss per batch: 0.639
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0114, 0.1693, 0.0126, 0.1322, 0.5384, 0.0086, 0.1275],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 51, batch: 44/223] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.4663e-02, 8.4455e-05, 1.5616e-01, 5.4101e-06, 2.5378e-04, 1.4643e-01,
        6.0241e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.838

[Epoch: 51, batch: 88/223] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.1087e-03, 8.7817e-03, 6.2530e-03, 8.5751e-01, 1.2182e-02, 9.3320e-07,
        1.1016e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 51, batch: 132/223] total loss per batch: 0.644
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3112e-02, 9.5370e-01, 1.4029e-02, 5.4103e-05, 8.4687e-03, 1.0618e-02,
        2.2106e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.038

[Epoch: 51, batch: 176/223] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9710, 0.0066, 0.0033, 0.0029, 0.0062, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.882

[Epoch: 51, batch: 220/223] total loss per batch: 0.637
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0111, 0.2866, 0.0122, 0.1925, 0.3822, 0.0133, 0.1022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.068

[Epoch: 52, batch: 44/223] total loss per batch: 0.647
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2782e-01, 1.4602e-04, 1.8306e-01, 1.5191e-05, 4.0643e-04, 1.5492e-01,
        5.3364e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.828

[Epoch: 52, batch: 88/223] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9584e-03, 7.0761e-03, 2.3777e-03, 9.1356e-01, 4.0386e-03, 7.8912e-07,
        6.7985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 52, batch: 132/223] total loss per batch: 0.637
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7884e-02, 9.5368e-01, 1.1538e-02, 1.1410e-05, 6.4406e-03, 1.0424e-02,
        2.2179e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.032

[Epoch: 52, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.9629, 0.0077, 0.0082, 0.0051, 0.0061, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.878

[Epoch: 52, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.2783, 0.0055, 0.0871, 0.5452, 0.0079, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 53, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0484e-01, 6.0817e-05, 1.5859e-01, 5.8678e-06, 1.7228e-04, 8.9280e-02,
        6.4705e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.825

[Epoch: 53, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.7195e-03, 4.5426e-03, 4.6450e-03, 9.1130e-01, 4.4144e-03, 6.5994e-07,
        7.0380e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 53, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3370e-02, 9.6405e-01, 9.6274e-03, 2.5477e-05, 5.7408e-03, 7.1795e-03,
        1.1208e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.042

[Epoch: 53, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9557, 0.0078, 0.0078, 0.0073, 0.0092, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.894

[Epoch: 53, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0123, 0.2796, 0.0084, 0.1436, 0.4503, 0.0085, 0.0972],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.049

[Epoch: 54, batch: 44/223] total loss per batch: 0.642
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0592e-01, 9.0993e-05, 1.9588e-01, 8.3957e-06, 1.7103e-04, 1.6762e-01,
        5.3030e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.855

[Epoch: 54, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.6776e-03, 5.9851e-03, 3.4262e-03, 9.0665e-01, 6.1268e-03, 4.3556e-07,
        7.3137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 54, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4274e-02, 9.5898e-01, 1.2436e-02, 8.6934e-06, 5.6654e-03, 8.6259e-03,
        1.3507e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 54, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9755, 0.0041, 0.0029, 0.0026, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.905

[Epoch: 54, batch: 220/223] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0073, 0.2890, 0.0091, 0.1465, 0.4431, 0.0088, 0.0963],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.046

[Epoch: 55, batch: 44/223] total loss per batch: 0.642
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0189e-01, 3.3801e-05, 1.2345e-01, 6.7029e-06, 1.3441e-04, 1.1672e-01,
        6.5776e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.918

[Epoch: 55, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2943e-03, 5.2989e-03, 4.7003e-03, 8.9731e-01, 3.5449e-03, 4.7108e-07,
        8.3847e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 55, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0839e-02, 9.6800e-01, 9.1658e-03, 1.1650e-05, 5.4157e-03, 6.5565e-03,
        9.0753e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.034

[Epoch: 55, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9705, 0.0061, 0.0036, 0.0040, 0.0065, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.899

[Epoch: 55, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0092, 0.2624, 0.0059, 0.1407, 0.5083, 0.0088, 0.0647],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.052

[Epoch: 56, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1187e-01, 4.0591e-05, 1.8104e-01, 3.4278e-06, 1.5940e-04, 1.3217e-01,
        5.7472e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.866

[Epoch: 56, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.9353e-03, 5.6977e-03, 3.2373e-03, 9.2383e-01, 4.6317e-03, 3.4951e-07,
        5.9663e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 56, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.1247e-02, 9.5267e-01, 9.9915e-03, 1.3178e-05, 6.5373e-03, 9.5233e-03,
        1.5957e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 56, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9708, 0.0051, 0.0041, 0.0038, 0.0049, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.892

[Epoch: 56, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.3200, 0.0101, 0.1704, 0.3724, 0.0069, 0.1097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.045

[Epoch: 57, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2219e-01, 9.0229e-05, 1.6930e-01, 1.1744e-05, 1.4607e-04, 1.6761e-01,
        5.4066e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.852

[Epoch: 57, batch: 88/223] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2728e-03, 5.6988e-03, 3.3845e-03, 8.9480e-01, 3.8448e-03, 6.1496e-07,
        8.6997e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 57, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8619e-02, 9.5613e-01, 1.2585e-02, 6.7291e-06, 6.1761e-03, 6.4751e-03,
        6.6790e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.055

[Epoch: 57, batch: 176/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.9660, 0.0055, 0.0030, 0.0064, 0.0063, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 57, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0073, 0.2369, 0.0118, 0.1515, 0.5066, 0.0084, 0.0775],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.048

[Epoch: 58, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0284e-01, 7.0272e-05, 1.2356e-01, 6.5441e-06, 1.4380e-04, 9.8363e-02,
        6.7502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.855

[Epoch: 58, batch: 88/223] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.0023e-03, 8.4308e-03, 7.9034e-03, 8.6777e-01, 8.1267e-03, 4.1791e-07,
        1.0177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 58, batch: 132/223] total loss per batch: 0.637
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2823e-02, 9.6525e-01, 9.3035e-03, 6.4700e-06, 5.8436e-03, 6.7459e-03,
        2.5194e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.065

[Epoch: 58, batch: 176/223] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0081, 0.9635, 0.0048, 0.0057, 0.0052, 0.0066, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.919

[Epoch: 58, batch: 220/223] total loss per batch: 0.632
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.2856, 0.0103, 0.1697, 0.3807, 0.0132, 0.1301],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.045

[Epoch: 59, batch: 44/223] total loss per batch: 0.647
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1497e-01, 7.4260e-05, 2.4746e-01, 2.9520e-06, 1.4391e-04, 1.5347e-01,
        4.8388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.880

[Epoch: 59, batch: 88/223] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9364e-03, 6.9059e-03, 2.2899e-03, 9.1926e-01, 5.5658e-03, 6.5790e-07,
        6.1038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 59, batch: 132/223] total loss per batch: 0.639
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9003e-02, 9.5504e-01, 1.2279e-02, 9.2747e-06, 6.1796e-03, 7.4843e-03,
        2.3659e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.054

[Epoch: 59, batch: 176/223] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9734, 0.0041, 0.0030, 0.0032, 0.0066, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.908

[Epoch: 59, batch: 220/223] total loss per batch: 0.634
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0095, 0.3070, 0.0068, 0.1271, 0.4662, 0.0050, 0.0784],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 60, batch: 44/223] total loss per batch: 0.649
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3897e-01, 8.9556e-05, 1.1316e-01, 4.0888e-06, 2.7078e-04, 1.2303e-01,
        6.2447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.806

[Epoch: 60, batch: 88/223] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.6876e-03, 1.6389e-02, 4.3787e-03, 8.7836e-01, 5.7075e-03, 3.4906e-07,
        8.9476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 60, batch: 132/223] total loss per batch: 0.641
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3794e-02, 9.6658e-01, 7.1425e-03, 4.3655e-05, 5.8234e-03, 6.5961e-03,
        1.6765e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.035

[Epoch: 60, batch: 176/223] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9721, 0.0058, 0.0058, 0.0040, 0.0042, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.894

[Epoch: 60, batch: 220/223] total loss per batch: 0.635
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0064, 0.1921, 0.0061, 0.1643, 0.5243, 0.0086, 0.0982],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 61, batch: 44/223] total loss per batch: 0.651
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.6192e-02, 3.9999e-05, 1.6373e-01, 3.4574e-06, 1.3042e-04, 9.5285e-02,
        6.4462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.867

[Epoch: 61, batch: 88/223] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.0925e-02, 3.5809e-03, 8.2477e-03, 8.2614e-01, 4.2092e-02, 2.0275e-06,
        1.0901e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 61, batch: 132/223] total loss per batch: 0.642
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5087e-02, 9.6011e-01, 1.0318e-02, 4.3347e-05, 6.5274e-03, 7.9010e-03,
        8.4239e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.048

[Epoch: 61, batch: 176/223] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.9167, 0.0146, 0.0083, 0.0122, 0.0217, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.872

[Epoch: 61, batch: 220/223] total loss per batch: 0.636
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0133, 0.3507, 0.0102, 0.1247, 0.3807, 0.0084, 0.1120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.055

[Epoch: 62, batch: 44/223] total loss per batch: 0.651
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.6114e-01, 4.8261e-05, 2.1254e-01, 1.5853e-06, 2.2388e-04, 1.7742e-01,
        4.4862e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.917

[Epoch: 62, batch: 88/223] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.3884e-03, 2.1908e-02, 3.8565e-03, 9.0919e-01, 1.9118e-03, 6.0031e-07,
        5.7746e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 62, batch: 132/223] total loss per batch: 0.641
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4375e-02, 9.6052e-01, 9.2362e-03, 2.7722e-05, 5.5465e-03, 1.0287e-02,
        1.0873e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.106

[Epoch: 62, batch: 176/223] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9731, 0.0039, 0.0031, 0.0055, 0.0028, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.898

[Epoch: 62, batch: 220/223] total loss per batch: 0.635
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0086, 0.3136, 0.0129, 0.1322, 0.4747, 0.0082, 0.0498],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 63, batch: 44/223] total loss per batch: 0.650
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0081e-02, 5.8312e-05, 1.1474e-01, 5.1682e-06, 7.1060e-05, 1.1095e-01,
        6.8410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.887

[Epoch: 63, batch: 88/223] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.0076e-03, 5.9894e-03, 3.0247e-03, 9.1654e-01, 3.0521e-03, 5.4503e-07,
        6.5385e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 63, batch: 132/223] total loss per batch: 0.638
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3044e-02, 9.6445e-01, 1.1074e-02, 1.3374e-05, 5.4356e-03, 5.9783e-03,
        9.3818e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.045

[Epoch: 63, batch: 176/223] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.9688, 0.0063, 0.0035, 0.0028, 0.0090, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.895

[Epoch: 63, batch: 220/223] total loss per batch: 0.632
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0096, 0.2518, 0.0072, 0.1310, 0.4218, 0.0075, 0.1711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.055

[Epoch: 64, batch: 44/223] total loss per batch: 0.647
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.4967e-02, 1.6931e-04, 1.7822e-01, 1.4337e-06, 1.4580e-04, 1.3475e-01,
        6.0175e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 64, batch: 88/223] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.1584e-03, 4.7485e-03, 3.5260e-03, 9.1473e-01, 2.9853e-03, 1.4469e-06,
        7.0854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 64, batch: 132/223] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5747e-02, 9.5865e-01, 9.8165e-03, 1.7051e-05, 8.2062e-03, 7.5559e-03,
        6.5640e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.072

[Epoch: 64, batch: 176/223] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.9626, 0.0078, 0.0050, 0.0062, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.925

[Epoch: 64, batch: 220/223] total loss per batch: 0.631
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0121, 0.2231, 0.0087, 0.1739, 0.5258, 0.0059, 0.0506],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 65, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.6657e-01, 6.7706e-05, 1.9365e-01, 3.1937e-06, 1.1395e-04, 1.0536e-01,
        5.3423e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.855

[Epoch: 65, batch: 88/223] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.8134e-03, 6.6896e-03, 3.9155e-03, 9.2979e-01, 2.8423e-03, 5.7288e-07,
        5.3953e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 65, batch: 132/223] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.0700e-02, 9.4635e-01, 1.8816e-02, 4.2002e-05, 6.3545e-03, 7.7280e-03,
        1.3154e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.066

[Epoch: 65, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.9561, 0.0092, 0.0040, 0.0075, 0.0070, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.890

[Epoch: 65, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0140, 0.3637, 0.0067, 0.1107, 0.4133, 0.0086, 0.0830],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 66, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0896e-02, 2.8480e-05, 1.3095e-01, 5.2748e-06, 1.2936e-04, 1.5002e-01,
        6.2796e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.820

[Epoch: 66, batch: 88/223] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.5409e-03, 6.6078e-03, 4.3843e-03, 8.9377e-01, 3.1456e-03, 2.4788e-06,
        8.5544e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 66, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.8109e-02, 9.4216e-01, 1.0181e-02, 1.4962e-05, 7.8910e-03, 1.1625e-02,
        1.8097e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 66, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9845, 0.0030, 0.0020, 0.0019, 0.0033, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.918

[Epoch: 66, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0110, 0.2421, 0.0128, 0.2017, 0.4160, 0.0070, 0.1093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 67, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0922e-01, 3.6667e-05, 1.6704e-01, 1.1472e-06, 6.6757e-05, 1.4277e-01,
        5.8086e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.892

[Epoch: 67, batch: 88/223] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.0991e-03, 8.5979e-03, 5.0226e-03, 9.0467e-01, 4.4970e-03, 5.9686e-07,
        7.1115e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 67, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2173e-02, 9.6090e-01, 1.4942e-02, 2.0927e-05, 6.2888e-03, 5.6674e-03,
        7.8261e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.082

[Epoch: 67, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9744, 0.0034, 0.0026, 0.0056, 0.0040, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.921

[Epoch: 67, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0111, 0.2220, 0.0079, 0.1410, 0.5321, 0.0084, 0.0775],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 68, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2109e-01, 1.7200e-05, 1.5328e-01, 3.2440e-06, 7.2459e-05, 1.0397e-01,
        6.2157e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.898

[Epoch: 68, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.8221e-03, 4.7991e-03, 2.7643e-03, 9.0369e-01, 2.5818e-03, 6.1078e-07,
        8.3347e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 68, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0368e-02, 9.6577e-01, 1.2230e-02, 7.5510e-06, 4.9822e-03, 6.6352e-03,
        5.7847e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.035

[Epoch: 68, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9734, 0.0046, 0.0021, 0.0040, 0.0053, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.892

[Epoch: 68, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0081, 0.3003, 0.0069, 0.1927, 0.4012, 0.0060, 0.0848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 69, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1137e-01, 3.0755e-05, 1.4970e-01, 1.1910e-06, 1.2549e-04, 1.2647e-01,
        6.1231e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.826

[Epoch: 69, batch: 88/223] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5817e-03, 5.9077e-03, 4.6943e-03, 8.8466e-01, 5.7398e-03, 1.0323e-06,
        9.4415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 69, batch: 132/223] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.0117e-02, 9.5386e-01, 1.3540e-02, 2.8190e-05, 5.7679e-03, 6.6782e-03,
        1.1026e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.055

[Epoch: 69, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9749, 0.0041, 0.0034, 0.0048, 0.0030, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.907

[Epoch: 69, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0062, 0.2511, 0.0065, 0.0915, 0.5498, 0.0086, 0.0863],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 70, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1862e-01, 6.0111e-05, 2.2969e-01, 3.1926e-06, 5.4838e-05, 1.8574e-01,
        4.6583e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.898

[Epoch: 70, batch: 88/223] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.1692e-02, 1.1569e-02, 6.7800e-03, 9.0779e-01, 3.3872e-03, 2.5548e-06,
        5.8778e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 70, batch: 132/223] total loss per batch: 0.638
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1710e-02, 9.6499e-01, 8.9116e-03, 1.3757e-05, 6.3041e-03, 8.0702e-03,
        4.9026e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.073

[Epoch: 70, batch: 176/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9644, 0.0080, 0.0036, 0.0050, 0.0068, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.907

[Epoch: 70, batch: 220/223] total loss per batch: 0.631
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0104, 0.3136, 0.0079, 0.2219, 0.3306, 0.0061, 0.1095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 71, batch: 44/223] total loss per batch: 0.647
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([6.8809e-02, 1.1992e-05, 1.1775e-01, 4.3674e-06, 4.9539e-05, 5.4131e-02,
        7.5925e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.887

[Epoch: 71, batch: 88/223] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.3172e-03, 3.7941e-03, 1.9264e-03, 9.2799e-01, 1.4869e-03, 3.9839e-07,
        6.2487e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 71, batch: 132/223] total loss per batch: 0.640
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2249e-03, 9.7329e-01, 9.2398e-03, 2.4210e-05, 4.3226e-03, 5.8862e-03,
        9.6694e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.054

[Epoch: 71, batch: 176/223] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0022, 0.9859, 0.0029, 0.0018, 0.0019, 0.0021, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.916

[Epoch: 71, batch: 220/223] total loss per batch: 0.631
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0111, 0.2526, 0.0220, 0.1520, 0.4693, 0.0116, 0.0814],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 72, batch: 44/223] total loss per batch: 0.648
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.7199e-01, 4.3715e-05, 1.8956e-01, 4.0825e-06, 6.6233e-05, 2.2293e-01,
        4.1540e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.923

[Epoch: 72, batch: 88/223] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2536e-03, 6.5735e-03, 2.8074e-03, 8.8887e-01, 2.8767e-03, 5.9508e-07,
        9.3618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 72, batch: 132/223] total loss per batch: 0.639
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6741e-02, 9.6013e-01, 9.2269e-03, 6.4356e-06, 7.4692e-03, 6.4200e-03,
        3.7056e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.056

[Epoch: 72, batch: 176/223] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9593, 0.0044, 0.0030, 0.0041, 0.0074, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 72, batch: 220/223] total loss per batch: 0.631
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0072, 0.2157, 0.0075, 0.1455, 0.5104, 0.0055, 0.1082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 73, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([4.3796e-02, 1.2264e-04, 1.8507e-01, 9.0556e-06, 1.4655e-04, 6.9882e-02,
        7.0098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.842

[Epoch: 73, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.9663e-03, 1.4690e-02, 6.0333e-03, 8.8551e-01, 1.9563e-03, 1.2524e-06,
        8.5842e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 73, batch: 132/223] total loss per batch: 0.637
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8019e-02, 9.4825e-01, 1.4024e-02, 4.2345e-05, 9.9731e-03, 9.6833e-03,
        6.1611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 73, batch: 176/223] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9789, 0.0042, 0.0042, 0.0038, 0.0029, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.914

[Epoch: 73, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 1
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0089, 0.4027, 0.0079, 0.1438, 0.3437, 0.0108, 0.0823],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 74, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.8656e-01, 5.3348e-05, 1.7753e-01, 4.1674e-06, 1.2057e-04, 1.4237e-01,
        4.9337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 74, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8897e-03, 5.5379e-03, 8.5293e-03, 8.6592e-01, 5.7751e-03, 2.4745e-06,
        1.0934e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 74, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7930e-02, 9.5563e-01, 1.0709e-02, 7.2274e-05, 7.5178e-03, 8.1304e-03,
        8.6728e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.068

[Epoch: 74, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9624, 0.0061, 0.0042, 0.0053, 0.0065, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.931

[Epoch: 74, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0127, 0.2313, 0.0077, 0.1453, 0.4885, 0.0069, 0.1075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 75, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([5.0615e-02, 4.1975e-05, 1.7592e-01, 7.5074e-06, 9.3370e-05, 1.3031e-01,
        6.4302e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 75, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.0366e-03, 7.6944e-03, 3.8685e-03, 9.1898e-01, 3.0979e-03, 7.9413e-07,
        6.0325e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 75, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.3649e-03, 9.7068e-01, 1.0786e-02, 1.2120e-05, 4.1199e-03, 5.0348e-03,
        5.5997e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.081

[Epoch: 75, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9654, 0.0050, 0.0041, 0.0059, 0.0068, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.908

[Epoch: 75, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0094, 0.2314, 0.0085, 0.1884, 0.4524, 0.0090, 0.1009],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 76, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.6579e-01, 2.6198e-05, 1.4222e-01, 2.7840e-06, 6.7509e-05, 1.0099e-01,
        5.9091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.832

[Epoch: 76, batch: 88/223] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.3616e-03, 4.1528e-03, 4.2385e-03, 9.1501e-01, 2.6102e-03, 1.9161e-06,
        6.8621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 76, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0716e-02, 9.6958e-01, 9.3879e-03, 1.8918e-05, 5.1152e-03, 5.1795e-03,
        5.3324e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.056

[Epoch: 76, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.9520, 0.0099, 0.0046, 0.0086, 0.0069, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.929

[Epoch: 76, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0070, 0.2942, 0.0060, 0.1205, 0.4946, 0.0058, 0.0719],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 77, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.3681e-02, 4.4240e-05, 1.9278e-01, 6.0097e-06, 1.5051e-04, 1.7791e-01,
        5.4543e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.911

[Epoch: 77, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.6105e-03, 9.0835e-03, 5.2305e-03, 9.0859e-01, 6.7026e-03, 7.3288e-07,
        6.3785e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 77, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8221e-02, 9.4848e-01, 1.8586e-02, 3.4120e-05, 5.8490e-03, 8.8165e-03,
        9.9900e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.048

[Epoch: 77, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.9515, 0.0060, 0.0066, 0.0079, 0.0071, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 77, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0088, 0.2579, 0.0082, 0.1599, 0.4818, 0.0072, 0.0762],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 78, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1809e-01, 4.1627e-05, 1.3769e-01, 2.9013e-06, 5.8488e-05, 1.1840e-01,
        6.2571e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.839

[Epoch: 78, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.7967e-03, 5.7548e-03, 3.6386e-03, 8.8479e-01, 2.3562e-03, 6.9697e-07,
        9.8663e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 78, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4117e-02, 9.6329e-01, 1.0466e-02, 1.7178e-05, 6.1891e-03, 5.9149e-03,
        5.8307e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.050

[Epoch: 78, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9660, 0.0060, 0.0032, 0.0035, 0.0071, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.926

[Epoch: 78, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0089, 0.2712, 0.0078, 0.1657, 0.4002, 0.0080, 0.1381],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 79, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3076e-01, 2.4013e-05, 1.8791e-01, 1.6453e-05, 6.9085e-05, 1.3044e-01,
        5.5078e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.915

[Epoch: 79, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.5056e-03, 6.6087e-03, 7.3213e-03, 9.2651e-01, 7.6138e-03, 5.6149e-07,
        4.6444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 79, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7062e-02, 9.5343e-01, 1.3825e-02, 2.5521e-05, 5.5743e-03, 1.0072e-02,
        1.4004e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.066

[Epoch: 79, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9692, 0.0053, 0.0032, 0.0066, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.889

[Epoch: 79, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0134, 0.2927, 0.0065, 0.1098, 0.5022, 0.0060, 0.0695],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 80, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.3082e-02, 4.0690e-05, 1.6871e-01, 6.1591e-06, 7.3215e-05, 1.1132e-01,
        6.3677e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.870

[Epoch: 80, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([9.9765e-03, 1.4096e-02, 8.8318e-03, 8.5239e-01, 6.4809e-03, 2.0863e-06,
        1.0822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 80, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4630e-02, 9.6534e-01, 9.9011e-03, 1.0284e-05, 4.7914e-03, 5.3238e-03,
        4.0059e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.054

[Epoch: 80, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.9566, 0.0052, 0.0036, 0.0058, 0.0117, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.938

[Epoch: 80, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0086, 0.2065, 0.0088, 0.2354, 0.4307, 0.0117, 0.0982],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 81, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3155e-01, 6.4006e-05, 1.5695e-01, 7.2191e-06, 3.2376e-05, 1.4562e-01,
        5.6578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.862

[Epoch: 81, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.7294e-03, 8.2538e-03, 5.5413e-03, 9.1615e-01, 3.7810e-03, 1.3533e-06,
        6.0545e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 81, batch: 132/223] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4727e-02, 9.6013e-01, 1.0978e-02, 3.5381e-05, 6.9610e-03, 7.1646e-03,
        7.7426e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.070

[Epoch: 81, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.9667, 0.0043, 0.0036, 0.0059, 0.0053, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 81, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0108, 0.3910, 0.0071, 0.0925, 0.4204, 0.0038, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 82, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3459e-01, 8.0878e-05, 1.9522e-01, 7.8054e-06, 8.1707e-05, 1.2485e-01,
        5.4516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.853

[Epoch: 82, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1295e-03, 4.0566e-03, 4.4311e-03, 9.4667e-01, 4.7043e-03, 5.1336e-07,
        3.6003e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 82, batch: 132/223] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2358e-02, 9.6294e-01, 1.3948e-02, 3.1003e-05, 4.2403e-03, 6.4776e-03,
        5.5835e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.048

[Epoch: 82, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.9582, 0.0063, 0.0052, 0.0063, 0.0059, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.919

[Epoch: 82, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0082, 0.1835, 0.0061, 0.1694, 0.5072, 0.0069, 0.1187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 83, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0517e-01, 2.6403e-05, 1.4990e-01, 4.7952e-06, 4.0885e-05, 1.0020e-01,
        6.4465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.899

[Epoch: 83, batch: 88/223] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.2367e-03, 3.5664e-03, 5.0165e-03, 8.5226e-01, 2.1130e-03, 5.9785e-07,
        1.3281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 83, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5373e-02, 9.6090e-01, 1.1016e-02, 2.0621e-05, 6.8370e-03, 5.8411e-03,
        6.7929e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.086

[Epoch: 83, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9752, 0.0041, 0.0038, 0.0035, 0.0052, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 83, batch: 220/223] total loss per batch: 0.630
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0144, 0.3175, 0.0072, 0.1217, 0.4680, 0.0073, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 84, batch: 44/223] total loss per batch: 0.646
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1231e-01, 4.3376e-05, 1.5247e-01, 4.7539e-06, 3.5627e-05, 1.4827e-01,
        5.8686e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.837

[Epoch: 84, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.2140e-03, 9.3349e-03, 8.4495e-03, 9.3045e-01, 4.9385e-03, 1.8439e-06,
        4.0608e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 84, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6392e-02, 9.5412e-01, 1.3323e-02, 2.4680e-05, 8.9257e-03, 7.1962e-03,
        1.9604e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.072

[Epoch: 84, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9662, 0.0046, 0.0044, 0.0038, 0.0069, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.918

[Epoch: 84, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0173, 0.2292, 0.0095, 0.1871, 0.3823, 0.0111, 0.1637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 85, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3726e-01, 2.3240e-05, 1.7477e-01, 8.3738e-06, 1.1724e-04, 1.2405e-01,
        5.6377e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.927

[Epoch: 85, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4025e-03, 7.3983e-03, 1.1645e-02, 8.8193e-01, 7.2646e-03, 1.8221e-06,
        8.6361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 85, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4238e-02, 9.5810e-01, 1.4383e-02, 5.7954e-05, 6.8904e-03, 6.3143e-03,
        1.2882e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.065

[Epoch: 85, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0110, 0.9466, 0.0081, 0.0122, 0.0057, 0.0068, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.922

[Epoch: 85, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0064, 0.3036, 0.0071, 0.1164, 0.5171, 0.0046, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.017

[Epoch: 86, batch: 44/223] total loss per batch: 0.645
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0883e-01, 1.0509e-04, 1.9365e-01, 5.2318e-06, 4.0089e-05, 1.4655e-01,
        5.5081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.856

[Epoch: 86, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.9380e-03, 4.8699e-03, 3.5800e-03, 9.0145e-01, 3.7143e-03, 9.0947e-07,
        8.3444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 86, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3450e-02, 9.5309e-01, 1.6344e-02, 1.6544e-05, 8.8829e-03, 8.2016e-03,
        1.2522e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.070

[Epoch: 86, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.9803, 0.0039, 0.0017, 0.0040, 0.0035, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.907

[Epoch: 86, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0095, 0.2695, 0.0055, 0.2163, 0.4048, 0.0072, 0.0871],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 87, batch: 44/223] total loss per batch: 0.644
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.8326e-02, 3.5067e-05, 1.3336e-01, 2.0670e-06, 9.0701e-05, 1.2250e-01,
        6.4568e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.871

[Epoch: 87, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.3900e-03, 3.0184e-03, 7.3240e-03, 9.1540e-01, 9.3909e-03, 7.9295e-07,
        6.0472e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 87, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8866e-02, 9.4945e-01, 1.6735e-02, 2.8357e-05, 7.3295e-03, 7.5779e-03,
        8.6335e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.053

[Epoch: 87, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9711, 0.0042, 0.0037, 0.0044, 0.0059, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.926

[Epoch: 87, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0106, 0.2833, 0.0124, 0.1022, 0.4777, 0.0060, 0.1078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 88, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2166e-01, 1.0458e-05, 1.4095e-01, 3.4519e-06, 1.3302e-05, 9.4196e-02,
        6.4317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.912

[Epoch: 88, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([7.2318e-03, 8.3932e-03, 8.4608e-03, 8.8239e-01, 5.1975e-03, 2.3271e-06,
        8.8329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 88, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4218e-02, 9.6650e-01, 8.9608e-03, 1.9393e-05, 5.4341e-03, 4.8666e-03,
        4.6957e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.087

[Epoch: 88, batch: 176/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9768, 0.0045, 0.0025, 0.0031, 0.0056, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.920

[Epoch: 88, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0068, 0.2737, 0.0058, 0.1565, 0.4693, 0.0052, 0.0826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 89, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.8692e-02, 4.4743e-05, 1.7250e-01, 1.9909e-06, 4.5343e-05, 2.0754e-01,
        5.4118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.872

[Epoch: 89, batch: 88/223] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.3348e-03, 6.3075e-03, 7.0536e-03, 8.6659e-01, 6.5280e-03, 3.5853e-06,
        1.0518e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 89, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5690e-03, 9.6627e-01, 1.2765e-02, 3.7361e-05, 4.3071e-03, 7.0445e-03,
        6.1740e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 89, batch: 176/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9647, 0.0057, 0.0064, 0.0073, 0.0063, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.901

[Epoch: 89, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0110, 0.2420, 0.0139, 0.1502, 0.4633, 0.0079, 0.1117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 90, batch: 44/223] total loss per batch: 0.642
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5909e-01, 5.7522e-05, 1.5293e-01, 1.7961e-06, 6.6509e-05, 9.5906e-02,
        5.9195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.884

[Epoch: 90, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.7899e-03, 4.8310e-03, 6.2016e-03, 9.3023e-01, 5.7688e-03, 8.5411e-07,
        5.0177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 90, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3477e-02, 9.6721e-01, 9.3057e-03, 1.4375e-05, 4.9158e-03, 5.0725e-03,
        5.4601e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.055

[Epoch: 90, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9623, 0.0087, 0.0035, 0.0029, 0.0090, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.931

[Epoch: 90, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0120, 0.2905, 0.0066, 0.1621, 0.4439, 0.0054, 0.0796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 91, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.4516e-02, 5.0684e-05, 1.6889e-01, 4.7007e-06, 3.7730e-05, 1.2598e-01,
        6.2053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.844

[Epoch: 91, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.0700e-03, 5.2419e-03, 4.5807e-03, 9.3186e-01, 3.3479e-03, 3.1766e-07,
        5.1897e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 91, batch: 132/223] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6967e-02, 9.5770e-01, 1.3461e-02, 5.4044e-05, 5.0471e-03, 6.7611e-03,
        1.0185e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.079

[Epoch: 91, batch: 176/223] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.9841, 0.0026, 0.0032, 0.0031, 0.0027, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.903

[Epoch: 91, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0118, 0.2826, 0.0114, 0.1233, 0.4533, 0.0064, 0.1112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 92, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4030e-01, 1.7155e-05, 1.7324e-01, 1.6006e-06, 9.5455e-05, 1.8075e-01,
        5.0560e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.889

[Epoch: 92, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.1337e-03, 2.4688e-03, 3.2314e-03, 8.7507e-01, 3.5753e-03, 6.4508e-07,
        1.1252e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 92, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7170e-02, 9.5691e-01, 1.3694e-02, 3.1924e-05, 5.5429e-03, 6.6464e-03,
        7.4857e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.070

[Epoch: 92, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.9769, 0.0038, 0.0022, 0.0037, 0.0045, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.934

[Epoch: 92, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0111, 0.2968, 0.0054, 0.1743, 0.4489, 0.0064, 0.0569],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 93, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.8890e-02, 1.1632e-05, 1.3905e-01, 3.9240e-06, 2.0621e-05, 8.1472e-02,
        6.9055e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.845

[Epoch: 93, batch: 88/223] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.5065e-03, 1.3118e-02, 9.2955e-03, 8.9366e-01, 9.3860e-03, 2.2770e-06,
        6.6031e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 93, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9962e-02, 9.5782e-01, 9.6238e-03, 3.2738e-05, 6.7925e-03, 5.7555e-03,
        1.0559e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.067

[Epoch: 93, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9666, 0.0069, 0.0044, 0.0051, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.914

[Epoch: 93, batch: 220/223] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0075, 0.2377, 0.0104, 0.1318, 0.4621, 0.0091, 0.1415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 94, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4643e-01, 3.3201e-05, 2.0003e-01, 2.4246e-06, 6.2281e-05, 1.7725e-01,
        4.7619e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.894

[Epoch: 94, batch: 88/223] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.0842e-03, 6.0231e-03, 3.3138e-03, 9.0040e-01, 3.0357e-03, 2.0383e-06,
        8.3142e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 94, batch: 132/223] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.8856e-03, 9.7245e-01, 1.1032e-02, 2.0010e-05, 2.4773e-03, 5.1286e-03,
        4.4516e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.077

[Epoch: 94, batch: 176/223] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9771, 0.0040, 0.0022, 0.0038, 0.0036, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.895

[Epoch: 94, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0065, 0.3157, 0.0050, 0.1466, 0.4706, 0.0034, 0.0523],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 95, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.8224e-02, 2.6439e-05, 1.7345e-01, 7.4772e-06, 8.4703e-05, 1.3892e-01,
        6.0928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.862

[Epoch: 95, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.3723e-03, 3.7991e-03, 3.1621e-03, 9.2414e-01, 2.0563e-03, 6.4760e-07,
        6.3471e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 95, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6403e-02, 9.6114e-01, 1.2211e-02, 2.5911e-05, 5.8934e-03, 4.3198e-03,
        6.2243e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.066

[Epoch: 95, batch: 176/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.9668, 0.0053, 0.0045, 0.0049, 0.0058, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.940

[Epoch: 95, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0121, 0.2592, 0.0056, 0.1506, 0.4641, 0.0069, 0.1015],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 96, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5941e-01, 9.1652e-05, 1.4902e-01, 2.3630e-06, 3.2537e-05, 8.4941e-02,
        6.0650e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.833

[Epoch: 96, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.7728e-03, 4.3458e-03, 3.5084e-03, 8.8856e-01, 4.0109e-03, 7.9590e-07,
        9.4801e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 96, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2322e-02, 9.6504e-01, 1.2794e-02, 2.9272e-05, 4.7855e-03, 5.0206e-03,
        6.9594e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.087

[Epoch: 96, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9681, 0.0051, 0.0042, 0.0052, 0.0059, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 96, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0103, 0.2819, 0.0070, 0.1404, 0.4336, 0.0091, 0.1177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 97, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([6.3178e-02, 8.3129e-05, 1.7712e-01, 6.9556e-06, 8.7926e-05, 1.4295e-01,
        6.1657e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.890

[Epoch: 97, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9127e-03, 4.9211e-03, 5.1162e-03, 9.3110e-01, 3.4466e-03, 6.6650e-07,
        5.0505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 97, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7796e-02, 9.5578e-01, 1.3974e-02, 1.9565e-05, 5.8926e-03, 6.5337e-03,
        5.8616e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.066

[Epoch: 97, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0119, 0.9531, 0.0088, 0.0059, 0.0081, 0.0061, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.932

[Epoch: 97, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0082, 0.2331, 0.0061, 0.1940, 0.4994, 0.0050, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 98, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4309e-01, 8.5739e-05, 1.6140e-01, 4.1274e-06, 6.0653e-05, 1.0710e-01,
        5.8826e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.889

[Epoch: 98, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8089e-03, 4.7798e-03, 4.3615e-03, 8.9586e-01, 2.7078e-03, 3.2391e-07,
        8.7486e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 98, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2197e-02, 9.6537e-01, 1.1780e-02, 2.1048e-05, 4.8549e-03, 5.7685e-03,
        7.4872e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.055

[Epoch: 98, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0078, 0.9640, 0.0060, 0.0024, 0.0045, 0.0079, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.924

[Epoch: 98, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0081, 0.3112, 0.0060, 0.1022, 0.4395, 0.0077, 0.1253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 99, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.7013e-02, 7.1374e-05, 1.7952e-01, 7.0829e-06, 6.5334e-05, 1.3486e-01,
        5.8846e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.807

[Epoch: 99, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.5629e-03, 6.2172e-03, 6.5314e-03, 8.8451e-01, 4.4410e-03, 1.1159e-06,
        9.1736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 99, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6537e-02, 9.5417e-01, 1.7284e-02, 7.2048e-06, 5.2236e-03, 6.7679e-03,
        9.6865e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.069

[Epoch: 99, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9664, 0.0074, 0.0053, 0.0041, 0.0055, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.938

[Epoch: 99, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0160, 0.2657, 0.0099, 0.1986, 0.4224, 0.0093, 0.0782],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 100, batch: 44/223] total loss per batch: 0.642
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4443e-01, 4.5187e-05, 1.4030e-01, 1.9021e-06, 3.1486e-05, 1.4634e-01,
        5.6886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.923

[Epoch: 100, batch: 88/223] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([1.2249e-02, 8.6094e-03, 6.6800e-03, 8.9454e-01, 6.2026e-03, 1.8432e-06,
        7.1713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 100, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0257e-02, 9.6200e-01, 1.3312e-02, 3.0437e-05, 8.5190e-03, 5.8784e-03,
        7.8640e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.074

[Epoch: 100, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9667, 0.0053, 0.0034, 0.0068, 0.0059, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.913

[Epoch: 100, batch: 220/223] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0093, 0.2699, 0.0059, 0.1115, 0.4848, 0.0051, 0.1134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 101, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1246e-01, 4.1089e-05, 1.6619e-01, 2.2821e-06, 4.3511e-05, 1.7321e-01,
        5.4805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.903

[Epoch: 101, batch: 88/223] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8041e-03, 2.8555e-03, 4.0451e-03, 9.2630e-01, 3.0468e-03, 4.6772e-07,
        5.9953e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 101, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3842e-02, 9.6438e-01, 1.1541e-02, 3.2175e-05, 4.0536e-03, 6.1453e-03,
        8.4378e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.041

[Epoch: 101, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9619, 0.0092, 0.0037, 0.0058, 0.0075, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 101, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0124, 0.2879, 0.0082, 0.1604, 0.4464, 0.0063, 0.0783],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 102, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.9626e-02, 5.1188e-05, 1.4093e-01, 5.3675e-06, 2.9822e-05, 8.4264e-02,
        6.7510e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.825

[Epoch: 102, batch: 88/223] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1968e-03, 2.9223e-03, 3.5811e-03, 9.1169e-01, 4.6753e-03, 6.8626e-07,
        7.2930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 102, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5250e-02, 9.5865e-01, 1.5296e-02, 1.0408e-05, 5.1554e-03, 5.6347e-03,
        7.6395e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.072

[Epoch: 102, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9713, 0.0049, 0.0038, 0.0045, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.917

[Epoch: 102, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0083, 0.2696, 0.0053, 0.1357, 0.4645, 0.0065, 0.1101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 103, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2036e-01, 3.8149e-05, 1.8475e-01, 2.9803e-06, 4.9016e-05, 1.5001e-01,
        5.4479e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.880

[Epoch: 103, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.4368e-03, 6.3973e-03, 5.5685e-03, 8.8701e-01, 7.0375e-03, 9.3224e-07,
        8.7544e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 103, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6586e-02, 9.5664e-01, 1.5098e-02, 1.4249e-05, 5.3704e-03, 6.2855e-03,
        8.4520e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.069

[Epoch: 103, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9745, 0.0052, 0.0038, 0.0047, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.933

[Epoch: 103, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0120, 0.2742, 0.0072, 0.1661, 0.4402, 0.0074, 0.0930],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 104, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1560e-01, 2.6992e-05, 1.6047e-01, 2.9148e-06, 1.9035e-05, 1.2724e-01,
        5.9664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.858

[Epoch: 104, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.1657e-03, 5.3619e-03, 6.6650e-03, 9.0459e-01, 5.8245e-03, 1.2168e-06,
        7.1387e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 104, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3844e-02, 9.6449e-01, 1.2832e-02, 8.1406e-06, 3.9913e-03, 4.8336e-03,
        5.4860e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.077

[Epoch: 104, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9687, 0.0056, 0.0035, 0.0044, 0.0067, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.929

[Epoch: 104, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0085, 0.2848, 0.0049, 0.1338, 0.4750, 0.0056, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 105, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1160e-01, 2.5521e-05, 1.6032e-01, 1.9519e-06, 2.9674e-05, 1.3451e-01,
        5.9351e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.892

[Epoch: 105, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8413e-03, 5.1145e-03, 3.5801e-03, 9.0375e-01, 3.7396e-03, 8.3580e-07,
        7.9972e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 105, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3598e-02, 9.6007e-01, 1.5457e-02, 7.3330e-06, 5.4805e-03, 5.3855e-03,
        6.6512e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 105, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9755, 0.0051, 0.0035, 0.0044, 0.0032, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.934

[Epoch: 105, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0120, 0.2679, 0.0061, 0.1642, 0.4413, 0.0059, 0.1025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 106, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1888e-01, 1.7110e-05, 1.6247e-01, 2.1136e-06, 1.7169e-05, 1.1853e-01,
        6.0008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.880

[Epoch: 106, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.3513e-03, 4.1979e-03, 4.7603e-03, 9.1330e-01, 4.4527e-03, 7.7435e-07,
        6.7938e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 106, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5158e-02, 9.6082e-01, 1.4102e-02, 8.5371e-06, 4.0609e-03, 5.8428e-03,
        7.0123e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.068

[Epoch: 106, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9755, 0.0041, 0.0024, 0.0037, 0.0050, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.933

[Epoch: 106, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0079, 0.2709, 0.0053, 0.1315, 0.5030, 0.0052, 0.0762],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 107, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1086e-01, 2.2129e-05, 1.6158e-01, 3.3441e-06, 2.5822e-05, 1.4246e-01,
        5.8504e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 107, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.0834e-03, 6.0263e-03, 6.6822e-03, 8.6719e-01, 5.1584e-03, 1.0106e-06,
        1.0686e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 107, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.0579e-02, 9.5165e-01, 1.6404e-02, 1.6150e-05, 6.6652e-03, 4.6849e-03,
        5.9835e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.077

[Epoch: 107, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9756, 0.0046, 0.0039, 0.0034, 0.0031, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.937

[Epoch: 107, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0150, 0.2950, 0.0069, 0.1848, 0.3758, 0.0074, 0.1151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 108, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2893e-01, 2.4524e-05, 1.6132e-01, 1.4730e-06, 2.0641e-05, 1.1789e-01,
        5.9181e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 108, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8656e-03, 4.9632e-03, 3.3356e-03, 9.3983e-01, 3.5444e-03, 1.0077e-06,
        4.4461e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 108, batch: 132/223] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.3129e-03, 9.7033e-01, 1.0933e-02, 1.0209e-05, 4.4043e-03, 6.0088e-03,
        5.6997e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 108, batch: 176/223] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.9566, 0.0081, 0.0052, 0.0058, 0.0064, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.915

[Epoch: 108, batch: 220/223] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0086, 0.2513, 0.0078, 0.1181, 0.5149, 0.0053, 0.0941],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 109, batch: 44/223] total loss per batch: 0.641
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.9255e-02, 2.8629e-05, 1.5381e-01, 3.9938e-06, 3.0952e-05, 1.4473e-01,
        6.0214e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.887

[Epoch: 109, batch: 88/223] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8755e-03, 4.4513e-03, 3.3995e-03, 8.8643e-01, 4.2758e-03, 2.6903e-07,
        9.6572e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 109, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4485e-02, 9.5840e-01, 1.6715e-02, 7.4725e-06, 5.5062e-03, 4.8755e-03,
        6.9619e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 109, batch: 176/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.9597, 0.0062, 0.0046, 0.0064, 0.0077, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.914

[Epoch: 109, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0119, 0.2672, 0.0052, 0.1728, 0.4584, 0.0044, 0.0802],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 110, batch: 44/223] total loss per batch: 0.643
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1222e-01, 1.5403e-05, 1.6266e-01, 9.7376e-07, 3.5265e-05, 1.2605e-01,
        5.9903e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.827

[Epoch: 110, batch: 88/223] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9761e-03, 5.0801e-03, 4.5994e-03, 9.1907e-01, 8.2564e-03, 1.3076e-06,
        5.8015e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 110, batch: 132/223] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9052e-02, 9.5337e-01, 1.7354e-02, 1.0823e-05, 3.7033e-03, 6.4989e-03,
        1.0064e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.080

[Epoch: 110, batch: 176/223] total loss per batch: 0.668
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9737, 0.0045, 0.0038, 0.0034, 0.0041, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.938

[Epoch: 110, batch: 220/223] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0122, 0.3154, 0.0068, 0.1313, 0.4334, 0.0060, 0.0948],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 111, batch: 44/223] total loss per batch: 0.642
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3628e-01, 2.1346e-05, 1.6672e-01, 2.6210e-06, 3.1004e-05, 1.3385e-01,
        5.6309e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.946

[Epoch: 111, batch: 88/223] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([9.5088e-03, 5.6518e-03, 7.4797e-03, 8.8796e-01, 4.5677e-03, 2.0097e-06,
        8.4826e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 111, batch: 132/223] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1704e-02, 9.6697e-01, 1.1412e-02, 1.3600e-05, 5.4725e-03, 4.4245e-03,
        5.1746e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.063

[Epoch: 111, batch: 176/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9700, 0.0058, 0.0048, 0.0045, 0.0045, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.927

[Epoch: 111, batch: 220/223] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0099, 0.2312, 0.0073, 0.1690, 0.4709, 0.0069, 0.1048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 112, batch: 44/223] total loss per batch: 0.641
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0664e-02, 1.6319e-05, 1.8135e-01, 1.9167e-06, 1.7855e-05, 1.4086e-01,
        5.8709e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.833

[Epoch: 112, batch: 88/223] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.5029e-03, 6.3846e-03, 4.6077e-03, 8.9745e-01, 7.5041e-03, 6.0651e-07,
        8.0549e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 112, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2551e-02, 9.6303e-01, 1.1982e-02, 1.2165e-05, 6.8530e-03, 5.5667e-03,
        4.2796e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.073

[Epoch: 112, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.9642, 0.0070, 0.0050, 0.0045, 0.0069, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.933

[Epoch: 112, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0083, 0.2743, 0.0048, 0.1454, 0.4821, 0.0060, 0.0791],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 113, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3707e-01, 2.3033e-05, 1.5686e-01, 3.6071e-06, 3.5503e-05, 1.2839e-01,
        5.7762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.872

[Epoch: 113, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9739e-03, 4.7319e-03, 5.2874e-03, 9.1563e-01, 3.8571e-03, 7.5795e-07,
        6.5522e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 113, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4227e-02, 9.6353e-01, 1.2520e-02, 4.8096e-06, 3.8868e-03, 5.8264e-03,
        3.9601e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.084

[Epoch: 113, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9710, 0.0051, 0.0038, 0.0040, 0.0043, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.936

[Epoch: 113, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0080, 0.3048, 0.0070, 0.1175, 0.4476, 0.0067, 0.1083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 114, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0703e-02, 1.4845e-05, 1.5320e-01, 2.5885e-06, 2.0074e-05, 1.1984e-01,
        6.3622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.870

[Epoch: 114, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.3931e-03, 5.3155e-03, 7.3573e-03, 8.8442e-01, 3.6246e-03, 1.8683e-06,
        9.2892e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 114, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3381e-02, 9.5959e-01, 1.5737e-02, 8.3265e-06, 5.9163e-03, 5.3652e-03,
        5.3821e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.081

[Epoch: 114, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.9631, 0.0074, 0.0056, 0.0048, 0.0065, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.932

[Epoch: 114, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0116, 0.2412, 0.0054, 0.2030, 0.4452, 0.0068, 0.0868],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 115, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3696e-01, 1.7782e-05, 1.6132e-01, 2.0842e-06, 2.5159e-05, 1.4116e-01,
        5.6052e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.840

[Epoch: 115, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8268e-03, 6.4264e-03, 5.5357e-03, 9.1780e-01, 7.6260e-03, 8.5887e-07,
        5.7785e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 115, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6582e-02, 9.5891e-01, 1.2647e-02, 8.4701e-06, 5.4328e-03, 6.4110e-03,
        4.1473e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.082

[Epoch: 115, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9604, 0.0059, 0.0039, 0.0061, 0.0078, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.942

[Epoch: 115, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0096, 0.2853, 0.0048, 0.0992, 0.5040, 0.0058, 0.0913],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 116, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.8170e-02, 1.3938e-05, 1.7188e-01, 2.4148e-06, 2.6124e-05, 1.4315e-01,
        5.9676e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 116, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.8643e-03, 3.7679e-03, 4.4669e-03, 8.8820e-01, 4.0061e-03, 4.0913e-07,
        9.6698e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 116, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5355e-02, 9.5761e-01, 1.5808e-02, 1.0852e-05, 5.1462e-03, 6.0599e-03,
        7.5227e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.081

[Epoch: 116, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.9749, 0.0059, 0.0041, 0.0034, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.930

[Epoch: 116, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0089, 0.2590, 0.0060, 0.1869, 0.4298, 0.0069, 0.1024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 117, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5532e-01, 1.2485e-05, 1.3922e-01, 8.5128e-07, 8.3539e-06, 7.9084e-02,
        6.2635e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 117, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4327e-03, 6.5695e-03, 5.9208e-03, 9.1487e-01, 5.6326e-03, 6.7658e-07,
        6.1569e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 117, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2758e-02, 9.6170e-01, 1.3646e-02, 1.2638e-05, 6.5622e-03, 5.3147e-03,
        4.3063e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.069

[Epoch: 117, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.9644, 0.0053, 0.0047, 0.0059, 0.0054, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.939

[Epoch: 117, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0121, 0.3055, 0.0065, 0.1129, 0.4579, 0.0066, 0.0986],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 118, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([6.9993e-02, 1.6324e-05, 1.6632e-01, 2.2636e-06, 4.7055e-05, 2.0773e-01,
        5.5589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.887

[Epoch: 118, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([9.8043e-03, 7.2325e-03, 7.3452e-03, 8.8250e-01, 6.0861e-03, 1.5203e-06,
        8.7027e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 118, batch: 132/223] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4456e-02, 9.6233e-01, 1.2577e-02, 7.2800e-06, 5.4676e-03, 5.1619e-03,
        4.1499e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.110

[Epoch: 118, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9699, 0.0058, 0.0042, 0.0037, 0.0054, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.932

[Epoch: 118, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.2302, 0.0051, 0.1787, 0.4803, 0.0075, 0.0877],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 119, batch: 44/223] total loss per batch: 0.641
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4823e-01, 3.0709e-05, 1.6320e-01, 2.5638e-06, 3.8302e-05, 1.0790e-01,
        5.8059e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.876

[Epoch: 119, batch: 88/223] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.3589e-03, 4.0092e-03, 2.2953e-03, 9.2523e-01, 3.2255e-03, 8.2919e-07,
        6.2881e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 119, batch: 132/223] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3481e-02, 9.6547e-01, 1.1538e-02, 7.3735e-06, 4.5391e-03, 4.9654e-03,
        4.0713e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.060

[Epoch: 119, batch: 176/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9696, 0.0056, 0.0032, 0.0036, 0.0070, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.939

[Epoch: 119, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0065, 0.2979, 0.0066, 0.1372, 0.4589, 0.0038, 0.0891],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 120, batch: 44/223] total loss per batch: 0.641
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.6443e-02, 3.7709e-05, 1.7401e-01, 4.7057e-06, 6.6342e-05, 1.2349e-01,
        6.2595e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.868

[Epoch: 120, batch: 88/223] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.5023e-03, 6.1992e-03, 4.9894e-03, 8.9827e-01, 5.4690e-03, 1.0664e-06,
        7.9565e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 120, batch: 132/223] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8810e-02, 9.5113e-01, 1.9391e-02, 1.1899e-05, 4.7131e-03, 5.9310e-03,
        1.0554e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.073

[Epoch: 120, batch: 176/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.9780, 0.0043, 0.0047, 0.0033, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.938

[Epoch: 120, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0124, 0.2964, 0.0074, 0.1653, 0.4028, 0.0067, 0.1089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.050

[Epoch: 121, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4246e-01, 2.7490e-05, 1.4223e-01, 2.8570e-06, 2.7035e-05, 1.3814e-01,
        5.7711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 121, batch: 88/223] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.5327e-03, 6.2526e-03, 9.1384e-03, 8.9144e-01, 7.4533e-03, 1.2841e-06,
        7.9181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 121, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1749e-02, 9.6585e-01, 1.1088e-02, 5.0837e-06, 5.3066e-03, 5.9970e-03,
        6.1159e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.052

[Epoch: 121, batch: 176/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9696, 0.0061, 0.0041, 0.0038, 0.0065, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.931

[Epoch: 121, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0085, 0.2249, 0.0056, 0.1380, 0.5187, 0.0070, 0.0972],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 122, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0123e-01, 3.8466e-05, 1.6489e-01, 6.8064e-06, 2.9878e-05, 1.0907e-01,
        6.2473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 122, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8575e-03, 6.2636e-03, 4.2686e-03, 9.0442e-01, 3.1586e-03, 9.9530e-07,
        7.8032e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 122, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5905e-02, 9.5788e-01, 1.4407e-02, 1.0734e-05, 5.1035e-03, 6.6860e-03,
        6.5348e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.074

[Epoch: 122, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9608, 0.0070, 0.0061, 0.0071, 0.0049, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.946

[Epoch: 122, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0096, 0.3524, 0.0054, 0.1209, 0.4179, 0.0049, 0.0889],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 123, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0551e-01, 2.5752e-05, 1.6884e-01, 2.1478e-06, 2.7602e-05, 1.6730e-01,
        5.5830e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.861

[Epoch: 123, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.7238e-03, 3.1702e-03, 4.7334e-03, 9.1967e-01, 4.4995e-03, 1.0138e-06,
        6.4207e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 123, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5352e-02, 9.6018e-01, 1.5001e-02, 6.7504e-06, 3.8513e-03, 5.6044e-03,
        7.4013e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.064

[Epoch: 123, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9710, 0.0062, 0.0052, 0.0039, 0.0055, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.938

[Epoch: 123, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2186, 0.0068, 0.1616, 0.4779, 0.0073, 0.1186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 124, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2829e-01, 2.4647e-05, 1.4695e-01, 2.0668e-06, 4.1729e-05, 1.0155e-01,
        6.2315e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.836

[Epoch: 124, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8565e-03, 4.2986e-03, 7.5721e-03, 8.8023e-01, 3.8095e-03, 1.6222e-06,
        9.9229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 124, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6476e-02, 9.5859e-01, 1.4716e-02, 5.8848e-06, 4.8028e-03, 5.4055e-03,
        5.8305e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.078

[Epoch: 124, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9715, 0.0039, 0.0055, 0.0058, 0.0033, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.931

[Epoch: 124, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0100, 0.2917, 0.0056, 0.1562, 0.4782, 0.0058, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 125, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0519e-01, 2.1891e-05, 1.8006e-01, 2.9932e-06, 4.1364e-05, 1.6312e-01,
        5.5156e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 125, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.9320e-03, 5.5898e-03, 5.2846e-03, 9.2204e-01, 4.6472e-03, 1.7885e-06,
        5.6505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 125, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4562e-02, 9.6371e-01, 1.3021e-02, 4.7313e-06, 4.0110e-03, 4.6874e-03,
        3.6852e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.051

[Epoch: 125, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9670, 0.0055, 0.0049, 0.0055, 0.0067, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.951

[Epoch: 125, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0071, 0.2702, 0.0055, 0.1563, 0.4185, 0.0059, 0.1366],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 126, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2698e-01, 2.6515e-05, 1.5250e-01, 3.9532e-06, 4.2814e-05, 1.0957e-01,
        6.1088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 126, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.7632e-03, 4.4997e-03, 4.4214e-03, 8.8034e-01, 2.5160e-03, 8.7533e-07,
        1.0446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.022

[Epoch: 126, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3271e-02, 9.6383e-01, 1.3790e-02, 3.7462e-06, 4.1892e-03, 4.9098e-03,
        5.3828e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 126, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.9706, 0.0045, 0.0037, 0.0049, 0.0063, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.934

[Epoch: 126, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0093, 0.2962, 0.0043, 0.1406, 0.4817, 0.0056, 0.0623],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 127, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.9913e-02, 1.3792e-05, 1.7861e-01, 9.8622e-07, 2.6877e-05, 1.1420e-01,
        6.0723e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.850

[Epoch: 127, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4716e-03, 5.1041e-03, 6.4608e-03, 9.2178e-01, 7.9414e-03, 1.7652e-06,
        5.3242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.022

[Epoch: 127, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6482e-02, 9.5627e-01, 1.7810e-02, 4.6738e-06, 4.5799e-03, 4.8467e-03,
        4.8936e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.063

[Epoch: 127, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.9774, 0.0042, 0.0044, 0.0033, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.941

[Epoch: 127, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0124, 0.2350, 0.0085, 0.1541, 0.4504, 0.0063, 0.1333],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 128, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4121e-01, 1.3123e-05, 1.5604e-01, 2.5943e-06, 3.9718e-05, 1.4572e-01,
        5.5698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.884

[Epoch: 128, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4521e-03, 6.5085e-03, 1.0957e-02, 8.8066e-01, 5.0095e-03, 1.6549e-06,
        9.1407e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.023

[Epoch: 128, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3244e-02, 9.6010e-01, 1.5698e-02, 5.5270e-06, 5.4815e-03, 5.4613e-03,
        4.5175e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.072

[Epoch: 128, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9687, 0.0061, 0.0051, 0.0047, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.947

[Epoch: 128, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0079, 0.3277, 0.0077, 0.1580, 0.4256, 0.0063, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 129, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.6872e-02, 3.7142e-05, 1.6403e-01, 3.7245e-06, 4.6631e-05, 1.3045e-01,
        6.0856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.866

[Epoch: 129, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.2477e-03, 3.6174e-03, 2.3112e-03, 9.2547e-01, 3.6076e-03, 1.1262e-06,
        6.0747e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.023

[Epoch: 129, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3680e-02, 9.6303e-01, 1.3246e-02, 1.0570e-05, 5.5463e-03, 4.4770e-03,
        5.8423e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.054

[Epoch: 129, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9690, 0.0068, 0.0043, 0.0045, 0.0056, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.944

[Epoch: 129, batch: 220/223] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0117, 0.1996, 0.0050, 0.1415, 0.5300, 0.0041, 0.1081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 130, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2467e-01, 2.2505e-05, 1.4019e-01, 1.2097e-06, 3.9843e-05, 1.3498e-01,
        6.0010e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.832

[Epoch: 130, batch: 88/223] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.4524e-03, 5.8007e-03, 7.1464e-03, 9.1045e-01, 5.9623e-03, 7.0902e-07,
        6.6189e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.024

[Epoch: 130, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6193e-02, 9.5551e-01, 1.6181e-02, 7.8557e-06, 6.3861e-03, 5.7163e-03,
        7.5584e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.082

[Epoch: 130, batch: 176/223] total loss per batch: 0.666
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9594, 0.0066, 0.0081, 0.0057, 0.0080, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.933

[Epoch: 130, batch: 220/223] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0088, 0.3480, 0.0059, 0.1684, 0.3926, 0.0055, 0.0707],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 131, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0659e-02, 1.8969e-05, 1.7563e-01, 2.6629e-06, 3.6140e-05, 1.1178e-01,
        6.2187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.920

[Epoch: 131, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([8.4011e-03, 4.9290e-03, 4.1634e-03, 8.8892e-01, 2.7359e-03, 1.2128e-06,
        9.0854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 131, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7068e-02, 9.5455e-01, 1.7445e-02, 1.2694e-05, 5.9634e-03, 4.9504e-03,
        1.1511e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.070

[Epoch: 131, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.9600, 0.0052, 0.0069, 0.0061, 0.0061, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.927

[Epoch: 131, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0129, 0.2433, 0.0065, 0.1306, 0.4785, 0.0079, 0.1203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 132, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5539e-01, 2.2945e-05, 1.5906e-01, 4.6805e-06, 8.8257e-05, 1.6551e-01,
        5.1992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.857

[Epoch: 132, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.6173e-03, 6.9471e-03, 4.8247e-03, 9.1860e-01, 1.0266e-02, 2.3415e-06,
        5.3738e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.025

[Epoch: 132, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3397e-02, 9.6332e-01, 1.2352e-02, 7.2017e-06, 5.4930e-03, 5.4190e-03,
        8.6191e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.062

[Epoch: 132, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9642, 0.0078, 0.0071, 0.0054, 0.0054, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.933

[Epoch: 132, batch: 220/223] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0108, 0.2536, 0.0091, 0.1697, 0.4526, 0.0069, 0.0973],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 133, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.3116e-02, 1.7089e-05, 1.7071e-01, 1.6093e-06, 2.2495e-05, 1.2681e-01,
        6.1932e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 133, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.1126e-03, 4.1901e-03, 3.8128e-03, 8.8911e-01, 4.2896e-03, 7.2223e-07,
        9.2488e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.024

[Epoch: 133, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2347e-02, 9.6087e-01, 1.5703e-02, 7.4333e-06, 5.8958e-03, 5.1663e-03,
        8.3027e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.068

[Epoch: 133, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9771, 0.0043, 0.0037, 0.0034, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.943

[Epoch: 133, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0086, 0.3138, 0.0047, 0.1361, 0.4476, 0.0061, 0.0831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 134, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3366e-01, 1.7547e-05, 1.3682e-01, 2.3881e-06, 6.6159e-05, 1.1797e-01,
        6.1146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.840

[Epoch: 134, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2338e-03, 6.4690e-03, 6.1273e-03, 8.9978e-01, 8.3145e-03, 2.6901e-06,
        7.4076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.026

[Epoch: 134, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4854e-02, 9.6523e-01, 1.2057e-02, 5.7935e-06, 3.1898e-03, 4.6535e-03,
        5.5877e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.097

[Epoch: 134, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9658, 0.0060, 0.0054, 0.0065, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.928

[Epoch: 134, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2573, 0.0063, 0.1551, 0.4524, 0.0067, 0.1115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 135, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0059e-01, 2.4586e-05, 1.8335e-01, 1.6539e-06, 1.8528e-05, 1.4568e-01,
        5.7034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.912

[Epoch: 135, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([7.8172e-03, 5.9940e-03, 6.6242e-03, 8.9924e-01, 4.5274e-03, 1.7466e-06,
        7.5800e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 135, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3768e-02, 9.5929e-01, 1.3506e-02, 8.1696e-06, 6.9964e-03, 6.4182e-03,
        9.3713e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.089

[Epoch: 135, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9752, 0.0040, 0.0035, 0.0042, 0.0034, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.940

[Epoch: 135, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0097, 0.2928, 0.0063, 0.1548, 0.4638, 0.0047, 0.0679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 136, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2164e-01, 3.2777e-05, 1.6550e-01, 2.0352e-06, 3.1365e-05, 1.2211e-01,
        5.9068e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.860

[Epoch: 136, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.2842e-03, 5.2744e-03, 4.6987e-03, 9.2539e-01, 4.7873e-03, 1.7721e-06,
        5.5564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 136, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8325e-02, 9.5768e-01, 1.4894e-02, 1.3119e-05, 4.5362e-03, 4.5512e-03,
        4.0489e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.093

[Epoch: 136, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9712, 0.0063, 0.0048, 0.0043, 0.0036, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.944

[Epoch: 136, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0089, 0.2596, 0.0056, 0.1303, 0.4578, 0.0073, 0.1305],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 137, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0656e-01, 2.4570e-05, 1.5989e-01, 1.5718e-06, 2.5925e-05, 1.3233e-01,
        6.0117e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.859

[Epoch: 137, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([2.8723e-03, 3.3150e-03, 2.8116e-03, 8.9035e-01, 3.1463e-03, 3.1128e-07,
        9.7507e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 137, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3391e-02, 9.6058e-01, 1.4677e-02, 6.1232e-06, 5.6441e-03, 5.6913e-03,
        6.8955e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.065

[Epoch: 137, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9704, 0.0048, 0.0040, 0.0054, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.932

[Epoch: 137, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0108, 0.2735, 0.0062, 0.1691, 0.4671, 0.0048, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 138, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0268e-01, 2.0883e-05, 1.6814e-01, 1.7451e-06, 1.8835e-05, 1.4245e-01,
        5.8670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.897

[Epoch: 138, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9292e-03, 4.1694e-03, 3.4880e-03, 9.1281e-01, 4.3351e-03, 1.1558e-06,
        7.0263e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 138, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3004e-02, 9.6100e-01, 1.4514e-02, 1.2696e-05, 4.2845e-03, 7.1775e-03,
        6.3770e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 138, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9740, 0.0047, 0.0044, 0.0034, 0.0040, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.948

[Epoch: 138, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2802, 0.0044, 0.1391, 0.4522, 0.0065, 0.1069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 139, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1101e-01, 1.5020e-05, 1.3728e-01, 1.7857e-06, 1.4387e-05, 1.0520e-01,
        6.4647e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 139, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([7.2022e-03, 6.6150e-03, 6.3535e-03, 8.9988e-01, 7.0819e-03, 2.4523e-06,
        7.2863e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 139, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6672e-02, 9.5717e-01, 1.7242e-02, 1.2513e-05, 4.9641e-03, 3.9369e-03,
        5.2886e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.070

[Epoch: 139, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9702, 0.0064, 0.0044, 0.0046, 0.0027, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.947

[Epoch: 139, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0099, 0.2425, 0.0065, 0.1748, 0.4825, 0.0066, 0.0772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 140, batch: 44/223] total loss per batch: 0.640
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0584e-01, 2.0677e-05, 1.8535e-01, 1.8555e-06, 3.2061e-05, 1.5812e-01,
        5.5064e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.850

[Epoch: 140, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.4166e-03, 3.2741e-03, 3.6555e-03, 9.1175e-01, 2.1592e-03, 6.3070e-07,
        7.5741e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 140, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0959e-02, 9.7145e-01, 1.1084e-02, 7.1582e-06, 2.7537e-03, 3.7372e-03,
        4.4611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.102

[Epoch: 140, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9648, 0.0047, 0.0069, 0.0072, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.953

[Epoch: 140, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.3138, 0.0064, 0.1319, 0.4202, 0.0056, 0.1116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 141, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1905e-01, 1.6473e-05, 1.6559e-01, 1.4244e-06, 1.6177e-05, 1.1088e-01,
        6.0444e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.888

[Epoch: 141, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.0511e-03, 5.4239e-03, 4.6127e-03, 8.9280e-01, 8.8180e-03, 4.3418e-07,
        8.3291e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 141, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0857e-02, 9.6440e-01, 1.3462e-02, 2.0105e-05, 6.6212e-03, 4.6260e-03,
        1.6959e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 141, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9750, 0.0040, 0.0033, 0.0042, 0.0048, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.943

[Epoch: 141, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0126, 0.2490, 0.0055, 0.1244, 0.5101, 0.0094, 0.0890],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 142, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3494e-01, 1.9577e-05, 1.7646e-01, 3.2506e-06, 2.3854e-05, 1.3498e-01,
        5.5357e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.891

[Epoch: 142, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.7439e-03, 4.9415e-03, 6.7022e-03, 8.9928e-01, 4.9577e-03, 1.7922e-06,
        7.7373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 142, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.1329e-02, 9.5137e-01, 1.5644e-02, 7.2280e-06, 6.0161e-03, 5.6199e-03,
        9.5892e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.073

[Epoch: 142, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9743, 0.0043, 0.0042, 0.0030, 0.0034, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.949

[Epoch: 142, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2817, 0.0050, 0.1920, 0.4114, 0.0047, 0.0960],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 143, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.0103e-02, 1.7214e-05, 1.4856e-01, 1.3535e-06, 3.3070e-05, 1.6231e-01,
        5.9897e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.851

[Epoch: 143, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.2279e-03, 7.1070e-03, 6.0835e-03, 8.8414e-01, 4.9590e-03, 1.3655e-06,
        9.1477e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 143, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6089e-02, 9.5609e-01, 1.5773e-02, 1.0357e-05, 5.5128e-03, 6.5127e-03,
        1.3236e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 143, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9623, 0.0052, 0.0040, 0.0072, 0.0069, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.951

[Epoch: 143, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0100, 0.2763, 0.0051, 0.1167, 0.4878, 0.0062, 0.0979],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 144, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3407e-01, 3.5871e-05, 1.6361e-01, 1.7906e-06, 4.4185e-05, 1.0359e-01,
        5.9865e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.879

[Epoch: 144, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.4895e-03, 3.1466e-03, 3.8529e-03, 9.1480e-01, 3.6929e-03, 1.2488e-06,
        7.1014e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 144, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4168e-02, 9.5831e-01, 1.4907e-02, 1.2000e-05, 6.7626e-03, 5.8332e-03,
        1.0484e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.107

[Epoch: 144, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9688, 0.0057, 0.0049, 0.0038, 0.0060, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.942

[Epoch: 144, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0121, 0.2750, 0.0054, 0.1705, 0.4370, 0.0073, 0.0928],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 145, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1975e-01, 7.8820e-06, 1.6427e-01, 1.7595e-06, 1.3762e-05, 1.1717e-01,
        5.9879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.887

[Epoch: 145, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5941e-03, 6.6669e-03, 4.5491e-03, 9.1320e-01, 6.7020e-03, 1.3264e-06,
        6.4283e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.022

[Epoch: 145, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3581e-02, 9.5765e-01, 1.7172e-02, 9.6705e-06, 5.1662e-03, 6.4128e-03,
        9.8042e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.081

[Epoch: 145, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9643, 0.0065, 0.0057, 0.0068, 0.0051, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.952

[Epoch: 145, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0103, 0.2783, 0.0077, 0.1432, 0.4533, 0.0070, 0.1002],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 146, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0264e-01, 2.5468e-05, 1.5107e-01, 3.0073e-06, 2.8987e-05, 1.7397e-01,
        5.7226e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.858

[Epoch: 146, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.0543e-03, 4.1125e-03, 4.6615e-03, 9.0690e-01, 3.7505e-03, 1.2923e-06,
        7.7521e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 146, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4477e-02, 9.6277e-01, 1.2204e-02, 1.1258e-05, 5.7255e-03, 4.8008e-03,
        9.0946e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 146, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9727, 0.0047, 0.0052, 0.0052, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.943

[Epoch: 146, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0078, 0.2802, 0.0041, 0.1438, 0.4871, 0.0053, 0.0717],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 147, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0877e-01, 1.5694e-05, 1.5973e-01, 2.0301e-06, 8.6872e-06, 7.7602e-02,
        6.5388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.840

[Epoch: 147, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.2781e-03, 4.4741e-03, 4.7192e-03, 9.1566e-01, 3.5607e-03, 9.2758e-07,
        6.7303e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 147, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3823e-02, 9.6122e-01, 1.6015e-02, 8.4319e-06, 5.1455e-03, 3.7769e-03,
        7.4373e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.069

[Epoch: 147, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.9788, 0.0045, 0.0029, 0.0033, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.945

[Epoch: 147, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0084, 0.2676, 0.0052, 0.1622, 0.4338, 0.0060, 0.1168],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 148, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4523e-01, 1.3577e-05, 1.6467e-01, 1.8947e-06, 1.7456e-05, 1.5233e-01,
        5.3773e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.896

[Epoch: 148, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.9374e-03, 6.2148e-03, 4.0388e-03, 9.0048e-01, 3.3179e-03, 6.4944e-07,
        8.1006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 148, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3359e-02, 9.6426e-01, 1.3757e-02, 6.1096e-06, 4.4643e-03, 4.1508e-03,
        6.8429e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.058

[Epoch: 148, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9611, 0.0077, 0.0068, 0.0062, 0.0071, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.945

[Epoch: 148, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0106, 0.2897, 0.0053, 0.1538, 0.4726, 0.0051, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 149, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.8971e-02, 1.6069e-05, 1.6166e-01, 3.0398e-06, 2.2607e-05, 1.4232e-01,
        6.0701e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.865

[Epoch: 149, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([7.5544e-03, 6.8452e-03, 6.0024e-03, 8.8574e-01, 5.0309e-03, 1.0698e-06,
        8.8829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 149, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5315e-02, 9.5913e-01, 1.5736e-02, 9.3308e-06, 4.7589e-03, 5.0485e-03,
        5.8623e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.085

[Epoch: 149, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.9744, 0.0041, 0.0031, 0.0042, 0.0035, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.946

[Epoch: 149, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0105, 0.2547, 0.0048, 0.1219, 0.4508, 0.0069, 0.1505],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 150, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3568e-01, 2.1370e-05, 1.5211e-01, 1.4360e-06, 2.0265e-05, 1.0465e-01,
        6.0752e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.870

[Epoch: 150, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.1796e-03, 4.2924e-03, 6.5575e-03, 8.9500e-01, 4.5042e-03, 2.2867e-06,
        8.3463e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 150, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2619e-02, 9.6430e-01, 1.3330e-02, 4.8785e-06, 4.7366e-03, 5.0042e-03,
        8.0586e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.085

[Epoch: 150, batch: 176/223] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9684, 0.0048, 0.0039, 0.0057, 0.0057, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.952

[Epoch: 150, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0116, 0.2652, 0.0048, 0.1997, 0.4567, 0.0057, 0.0562],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 151, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0437e-01, 1.5746e-05, 1.7584e-01, 2.6042e-06, 3.2875e-05, 1.5509e-01,
        5.6464e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 151, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8240e-03, 5.8767e-03, 5.5128e-03, 9.2224e-01, 6.9637e-03, 1.7153e-06,
        5.5582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 151, batch: 132/223] total loss per batch: 0.630
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8280e-02, 9.5485e-01, 1.7162e-02, 2.0112e-05, 4.3990e-03, 5.2807e-03,
        1.2034e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.074

[Epoch: 151, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.9579, 0.0068, 0.0066, 0.0065, 0.0080, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.929

[Epoch: 151, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0095, 0.2926, 0.0047, 0.1085, 0.4654, 0.0057, 0.1136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 152, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1942e-01, 1.9749e-05, 1.4669e-01, 3.2480e-06, 2.0516e-05, 1.1411e-01,
        6.1973e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.851

[Epoch: 152, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.6159e-03, 5.2901e-03, 4.1019e-03, 8.8983e-01, 3.9540e-03, 1.3471e-06,
        9.1210e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 152, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5043e-02, 9.5758e-01, 1.5239e-02, 1.1433e-05, 5.7066e-03, 6.4102e-03,
        1.1843e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.082

[Epoch: 152, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9716, 0.0045, 0.0052, 0.0051, 0.0041, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.949

[Epoch: 152, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0118, 0.2694, 0.0060, 0.1719, 0.4462, 0.0070, 0.0877],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 153, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1482e-01, 1.1692e-05, 1.6762e-01, 1.5596e-06, 1.4072e-05, 1.3825e-01,
        5.7928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.862

[Epoch: 153, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1991e-03, 3.9009e-03, 4.4469e-03, 9.2820e-01, 4.6176e-03, 1.3069e-06,
        5.4638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 153, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7061e-02, 9.5813e-01, 1.5418e-02, 7.9934e-06, 4.5422e-03, 4.8319e-03,
        7.5764e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 153, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9708, 0.0050, 0.0054, 0.0043, 0.0044, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.943

[Epoch: 153, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0100, 0.2664, 0.0056, 0.1528, 0.4616, 0.0058, 0.0978],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 154, batch: 44/223] total loss per batch: 0.636
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2019e-01, 1.0243e-05, 1.5740e-01, 2.1155e-06, 1.2429e-05, 1.3024e-01,
        5.9215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.882

[Epoch: 154, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.4696e-03, 6.0071e-03, 5.7548e-03, 8.8174e-01, 5.0691e-03, 1.1929e-06,
        9.4954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 154, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3874e-02, 9.6433e-01, 1.2795e-02, 5.4196e-06, 4.2439e-03, 4.7477e-03,
        7.3564e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.092

[Epoch: 154, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9716, 0.0046, 0.0049, 0.0042, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.958

[Epoch: 154, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0097, 0.2976, 0.0047, 0.1394, 0.4541, 0.0056, 0.0889],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 155, batch: 44/223] total loss per batch: 0.636
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1157e-01, 8.1396e-06, 1.5758e-01, 1.3043e-06, 9.1171e-06, 1.2981e-01,
        6.0102e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 155, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4608e-03, 5.4613e-03, 5.2140e-03, 9.1258e-01, 5.1021e-03, 1.4664e-06,
        6.6177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 155, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5132e-02, 9.6273e-01, 1.3422e-02, 4.2834e-06, 3.9105e-03, 4.7990e-03,
        6.3509e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.087

[Epoch: 155, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9724, 0.0046, 0.0045, 0.0046, 0.0042, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.950

[Epoch: 155, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0103, 0.2551, 0.0050, 0.1454, 0.4834, 0.0053, 0.0956],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 156, batch: 44/223] total loss per batch: 0.636
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2998e-01, 7.9597e-06, 1.6117e-01, 1.5247e-06, 9.6480e-06, 1.2995e-01,
        5.7888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.869

[Epoch: 156, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.0050e-03, 4.8732e-03, 4.4722e-03, 9.1574e-01, 4.0293e-03, 8.3961e-07,
        6.6882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 156, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6563e-02, 9.5747e-01, 1.5199e-02, 4.6306e-06, 5.0065e-03, 5.7491e-03,
        6.6675e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.068

[Epoch: 156, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9754, 0.0043, 0.0041, 0.0041, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.962

[Epoch: 156, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2963, 0.0049, 0.1516, 0.4424, 0.0060, 0.0882],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 157, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.8047e-02, 1.2393e-05, 1.5729e-01, 1.4768e-06, 1.1475e-05, 1.4374e-01,
        6.0090e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.884

[Epoch: 157, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2111e-03, 5.8426e-03, 5.1632e-03, 8.8717e-01, 5.1573e-03, 9.0410e-07,
        9.1455e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 157, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4894e-02, 9.5904e-01, 1.4905e-02, 4.4984e-06, 5.5472e-03, 5.6084e-03,
        5.8525e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 157, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.9642, 0.0063, 0.0054, 0.0062, 0.0053, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.952

[Epoch: 157, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0104, 0.2652, 0.0053, 0.1569, 0.4587, 0.0057, 0.0979],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 158, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2728e-01, 5.7648e-06, 1.5009e-01, 7.5442e-07, 6.1769e-06, 1.0679e-01,
        6.1582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.853

[Epoch: 158, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8058e-03, 3.0517e-03, 3.8829e-03, 9.3071e-01, 3.2934e-03, 5.7873e-07,
        5.5252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 158, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4347e-02, 9.6453e-01, 1.1911e-02, 5.2826e-06, 4.5213e-03, 4.6796e-03,
        4.2565e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.102

[Epoch: 158, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.9674, 0.0051, 0.0043, 0.0051, 0.0066, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.957

[Epoch: 158, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0103, 0.2471, 0.0058, 0.1507, 0.4784, 0.0054, 0.1021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 159, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.0485e-02, 7.8440e-06, 1.6835e-01, 9.6863e-07, 1.0805e-05, 1.6147e-01,
        5.8968e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 159, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.8899e-03, 4.4313e-03, 7.9727e-03, 8.7994e-01, 5.4440e-03, 1.4989e-06,
        9.6317e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 159, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1934e-02, 9.6748e-01, 1.3046e-02, 5.1815e-06, 3.8198e-03, 3.7053e-03,
        5.4768e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.084

[Epoch: 159, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.9670, 0.0067, 0.0072, 0.0046, 0.0040, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.961

[Epoch: 159, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0118, 0.3236, 0.0051, 0.1529, 0.4092, 0.0074, 0.0900],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 160, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.7809e-01, 1.3469e-05, 1.5392e-01, 1.4791e-06, 4.6838e-06, 1.0374e-01,
        5.6423e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.871

[Epoch: 160, batch: 88/223] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.7399e-03, 4.6445e-03, 3.3826e-03, 9.1572e-01, 5.6442e-03, 8.4166e-07,
        6.4869e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 160, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7030e-02, 9.5801e-01, 1.3176e-02, 9.3486e-06, 5.6871e-03, 6.0810e-03,
        7.3398e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.056

[Epoch: 160, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9708, 0.0052, 0.0045, 0.0036, 0.0071, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.957

[Epoch: 160, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0096, 0.2368, 0.0078, 0.1258, 0.5136, 0.0042, 0.1022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 161, batch: 44/223] total loss per batch: 0.639
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([7.8805e-02, 1.6976e-05, 1.6499e-01, 2.1804e-06, 2.4911e-05, 1.4185e-01,
        6.1432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 161, batch: 88/223] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.1280e-03, 5.3365e-03, 8.4076e-03, 8.8379e-01, 7.9004e-03, 1.6593e-06,
        8.9440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 161, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7940e-02, 9.5606e-01, 1.4401e-02, 9.5328e-06, 5.9211e-03, 5.6579e-03,
        1.2326e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.072

[Epoch: 161, batch: 176/223] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.9654, 0.0067, 0.0055, 0.0060, 0.0037, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.956

[Epoch: 161, batch: 220/223] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0112, 0.2882, 0.0054, 0.1751, 0.4354, 0.0067, 0.0780],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 162, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2392e-01, 1.6890e-05, 1.5653e-01, 2.5502e-06, 1.8144e-05, 1.1557e-01,
        6.0394e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.861

[Epoch: 162, batch: 88/223] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.8533e-03, 4.6525e-03, 5.1373e-03, 9.1378e-01, 7.5682e-03, 1.3832e-06,
        6.3004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 162, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8026e-02, 9.5655e-01, 1.5804e-02, 8.8351e-06, 4.4727e-03, 5.1266e-03,
        9.0776e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.078

[Epoch: 162, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9728, 0.0050, 0.0043, 0.0048, 0.0044, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.963

[Epoch: 162, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0099, 0.2754, 0.0065, 0.1332, 0.4655, 0.0080, 0.1014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 163, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0265e-01, 1.0540e-05, 1.6588e-01, 1.7963e-06, 1.8726e-05, 1.3369e-01,
        5.9775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.886

[Epoch: 163, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5420e-03, 4.8992e-03, 3.3526e-03, 9.0867e-01, 4.7746e-03, 1.0540e-06,
        7.3757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 163, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4326e-02, 9.6046e-01, 1.4791e-02, 6.1995e-06, 5.2470e-03, 5.1640e-03,
        4.0717e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.116

[Epoch: 163, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.9741, 0.0046, 0.0048, 0.0041, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.957

[Epoch: 163, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0094, 0.2748, 0.0054, 0.1668, 0.4410, 0.0064, 0.0962],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 164, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1493e-01, 1.0586e-05, 1.5299e-01, 1.1332e-06, 1.6005e-05, 1.2416e-01,
        6.0789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.840

[Epoch: 164, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.1326e-03, 4.4377e-03, 3.8634e-03, 9.0318e-01, 4.5087e-03, 1.2755e-06,
        7.8877e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 164, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3804e-02, 9.6339e-01, 1.3411e-02, 6.0162e-06, 4.9795e-03, 4.4066e-03,
        4.4266e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.089

[Epoch: 164, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.9730, 0.0049, 0.0042, 0.0048, 0.0046, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.958

[Epoch: 164, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2740, 0.0058, 0.1533, 0.4520, 0.0065, 0.0993],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 165, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1662e-01, 8.8031e-06, 1.7588e-01, 1.4768e-06, 9.6453e-06, 1.3977e-01,
        5.6771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.895

[Epoch: 165, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.7156e-03, 5.6972e-03, 4.7147e-03, 9.0703e-01, 4.6381e-03, 1.5404e-06,
        7.1206e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 165, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5495e-02, 9.5975e-01, 1.4355e-02, 6.0480e-06, 5.4874e-03, 4.8977e-03,
        5.4407e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.059

[Epoch: 165, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9706, 0.0055, 0.0048, 0.0045, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.951

[Epoch: 165, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0099, 0.2802, 0.0046, 0.1232, 0.4928, 0.0049, 0.0844],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 166, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2125e-01, 7.3131e-06, 1.5910e-01, 1.5961e-06, 1.0791e-05, 1.2819e-01,
        5.9145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 166, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.6206e-03, 5.6749e-03, 5.2909e-03, 8.8989e-01, 5.3034e-03, 1.2927e-06,
        8.8219e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 166, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5737e-02, 9.5996e-01, 1.4325e-02, 4.4253e-06, 4.9214e-03, 5.0493e-03,
        6.6176e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.083

[Epoch: 166, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9672, 0.0055, 0.0051, 0.0056, 0.0061, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.952

[Epoch: 166, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0095, 0.2630, 0.0052, 0.1741, 0.4322, 0.0065, 0.1095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 167, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.6885e-02, 8.2246e-06, 1.5800e-01, 1.0115e-06, 1.5594e-05, 1.3001e-01,
        6.1508e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.865

[Epoch: 167, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.0870e-03, 5.9605e-03, 7.2516e-03, 9.1050e-01, 5.7873e-03, 1.2638e-06,
        6.5408e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 167, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8787e-02, 9.5313e-01, 1.6982e-02, 5.7827e-06, 5.2923e-03, 5.7946e-03,
        6.0137e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.093

[Epoch: 167, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9678, 0.0048, 0.0050, 0.0056, 0.0044, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.952

[Epoch: 167, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0119, 0.2786, 0.0049, 0.1437, 0.4721, 0.0066, 0.0821],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 168, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.4263e-01, 6.4978e-06, 1.5934e-01, 1.9413e-06, 1.1227e-05, 1.3857e-01,
        5.5945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.875

[Epoch: 168, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.4523e-03, 3.5588e-03, 3.0347e-03, 9.0934e-01, 3.6637e-03, 1.1794e-06,
        7.5945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 168, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0668e-02, 9.6544e-01, 1.3973e-02, 5.6774e-06, 4.5875e-03, 5.3224e-03,
        4.9430e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.118

[Epoch: 168, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9705, 0.0056, 0.0044, 0.0053, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.962

[Epoch: 168, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0093, 0.2860, 0.0070, 0.1441, 0.4458, 0.0044, 0.1034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 169, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.1400e-02, 5.8706e-06, 1.5049e-01, 1.0276e-06, 1.0990e-05, 1.1304e-01,
        6.5505e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.843

[Epoch: 169, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.8120e-03, 3.4051e-03, 4.4328e-03, 9.1994e-01, 3.1283e-03, 1.3244e-06,
        6.5278e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 169, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8324e-02, 9.5627e-01, 1.4397e-02, 7.1176e-06, 5.8634e-03, 5.1355e-03,
        4.2058e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.062

[Epoch: 169, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9724, 0.0044, 0.0043, 0.0041, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.960

[Epoch: 169, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2617, 0.0047, 0.1500, 0.4642, 0.0072, 0.1032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 170, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.5622e-01, 7.5034e-06, 1.6760e-01, 1.8443e-06, 8.9002e-06, 1.2701e-01,
        5.4914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.885

[Epoch: 170, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.3278e-03, 5.5923e-03, 3.9199e-03, 9.0359e-01, 4.7384e-03, 9.8321e-07,
        7.6828e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.024

[Epoch: 170, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4570e-02, 9.6371e-01, 1.3026e-02, 4.1913e-06, 4.3807e-03, 4.3023e-03,
        4.3632e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.069

[Epoch: 170, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.9622, 0.0070, 0.0051, 0.0059, 0.0065, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.975

[Epoch: 170, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0083, 0.2766, 0.0055, 0.1667, 0.4614, 0.0044, 0.0771],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 171, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.5547e-02, 1.4141e-05, 1.7058e-01, 1.2708e-06, 1.9197e-05, 1.7179e-01,
        5.6206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.860

[Epoch: 171, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8430e-03, 6.6946e-03, 5.3754e-03, 8.8221e-01, 5.1121e-03, 1.4309e-06,
        9.5764e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 171, batch: 132/223] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5578e-02, 9.6172e-01, 1.3424e-02, 4.7857e-06, 4.3023e-03, 4.9641e-03,
        3.8593e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.089

[Epoch: 171, batch: 176/223] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9720, 0.0054, 0.0046, 0.0041, 0.0041, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.969

[Epoch: 171, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0090, 0.2789, 0.0040, 0.1367, 0.4671, 0.0055, 0.0988],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 172, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1866e-01, 7.1503e-06, 1.4640e-01, 2.6173e-06, 1.0896e-05, 1.0423e-01,
        6.3068e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.897

[Epoch: 172, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.9789e-03, 3.4727e-03, 6.2707e-03, 9.1687e-01, 5.7767e-03, 2.7028e-06,
        6.1632e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 172, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8195e-02, 9.5326e-01, 1.7903e-02, 8.8567e-06, 4.7814e-03, 5.8462e-03,
        8.5944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.074

[Epoch: 172, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9716, 0.0048, 0.0039, 0.0046, 0.0055, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.963

[Epoch: 172, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0117, 0.2818, 0.0053, 0.1429, 0.4629, 0.0061, 0.0893],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 173, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1133e-01, 9.4629e-06, 1.5777e-01, 1.1740e-06, 1.0777e-05, 1.2850e-01,
        6.0238e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.805

[Epoch: 173, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2527e-03, 6.3577e-03, 5.1402e-03, 9.0087e-01, 5.6208e-03, 2.0250e-06,
        7.6752e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 173, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6556e-02, 9.5890e-01, 1.3266e-02, 5.9958e-06, 6.0398e-03, 5.2260e-03,
        6.6586e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.094

[Epoch: 173, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.9715, 0.0042, 0.0047, 0.0045, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.962

[Epoch: 173, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0079, 0.2620, 0.0054, 0.1480, 0.4617, 0.0054, 0.1096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 174, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1947e-01, 1.0568e-05, 1.7110e-01, 1.7438e-06, 1.5334e-05, 1.5101e-01,
        5.5840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.905

[Epoch: 174, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2273e-03, 4.4207e-03, 3.7038e-03, 8.9872e-01, 4.3272e-03, 1.8976e-06,
        8.3604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 174, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4115e-02, 9.5824e-01, 1.6343e-02, 1.1498e-05, 5.5239e-03, 5.7633e-03,
        7.0838e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 174, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9728, 0.0046, 0.0042, 0.0042, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.956

[Epoch: 174, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0109, 0.2889, 0.0061, 0.1622, 0.4436, 0.0056, 0.0826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 175, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0822e-01, 6.3976e-06, 1.4916e-01, 1.0461e-06, 1.0294e-05, 1.1696e-01,
        6.2564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.903

[Epoch: 175, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.2386e-03, 3.7433e-03, 3.8793e-03, 9.2462e-01, 3.9830e-03, 1.1481e-06,
        5.9530e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.019

[Epoch: 175, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7734e-02, 9.5492e-01, 1.5900e-02, 5.7318e-06, 5.9872e-03, 5.4451e-03,
        5.6966e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.087

[Epoch: 175, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9719, 0.0048, 0.0050, 0.0040, 0.0049, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.956

[Epoch: 175, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0101, 0.2642, 0.0062, 0.1344, 0.4745, 0.0053, 0.1054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 176, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1823e-01, 7.6878e-06, 1.6773e-01, 8.5754e-07, 6.5735e-06, 1.4212e-01,
        5.7191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.863

[Epoch: 176, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.3908e-03, 4.9646e-03, 5.1206e-03, 8.8599e-01, 5.3318e-03, 2.2149e-06,
        9.2196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 176, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3190e-02, 9.6309e-01, 1.4539e-02, 6.5913e-06, 4.4382e-03, 4.7311e-03,
        5.7224e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 176, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9695, 0.0047, 0.0055, 0.0050, 0.0047, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.967

[Epoch: 176, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0092, 0.2666, 0.0051, 0.1814, 0.4444, 0.0058, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 177, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2206e-01, 7.7335e-06, 1.6700e-01, 1.7995e-06, 1.6861e-05, 1.2652e-01,
        5.8439e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.892

[Epoch: 177, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.9388e-03, 5.9585e-03, 7.8538e-03, 9.0117e-01, 5.0908e-03, 2.8750e-06,
        7.2988e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.023

[Epoch: 177, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4066e-02, 9.6040e-01, 1.4142e-02, 5.8041e-06, 5.6448e-03, 5.7305e-03,
        6.2814e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.106

[Epoch: 177, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.9757, 0.0046, 0.0039, 0.0040, 0.0038, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.962

[Epoch: 177, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0106, 0.2953, 0.0051, 0.1111, 0.4836, 0.0043, 0.0901],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 178, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0618e-01, 5.7902e-06, 1.4871e-01, 1.2461e-06, 8.2764e-06, 1.2756e-01,
        6.1753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.854

[Epoch: 178, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.7943e-03, 6.6461e-03, 4.4805e-03, 9.1396e-01, 3.4472e-03, 1.9363e-06,
        6.7667e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 178, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5898e-02, 9.5718e-01, 1.5858e-02, 4.7413e-06, 5.3874e-03, 5.6715e-03,
        4.7145e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.113

[Epoch: 178, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.9695, 0.0057, 0.0051, 0.0042, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.953

[Epoch: 178, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2611, 0.0052, 0.1693, 0.4542, 0.0063, 0.0948],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 179, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3506e-01, 8.3793e-06, 1.6542e-01, 1.8869e-06, 1.3936e-05, 1.3509e-01,
        5.6441e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.894

[Epoch: 179, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.9419e-03, 4.4018e-03, 3.9551e-03, 9.0718e-01, 5.3101e-03, 9.5999e-07,
        7.5206e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 179, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3131e-02, 9.5556e-01, 1.8434e-02, 5.7410e-06, 6.3686e-03, 6.4914e-03,
        6.9182e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 179, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.9653, 0.0048, 0.0050, 0.0069, 0.0047, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.966

[Epoch: 179, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0102, 0.2829, 0.0053, 0.1431, 0.4648, 0.0043, 0.0894],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 180, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.5527e-02, 7.0846e-06, 1.6396e-01, 1.6492e-06, 1.2625e-05, 1.3741e-01,
        6.0308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.872

[Epoch: 180, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8441e-03, 3.7351e-03, 3.3550e-03, 9.0329e-01, 4.2734e-03, 2.2198e-06,
        8.0495e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 180, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7053e-02, 9.5981e-01, 1.3092e-02, 7.4481e-06, 4.9356e-03, 5.1022e-03,
        3.7990e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.053

[Epoch: 180, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9694, 0.0050, 0.0055, 0.0054, 0.0044, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.942

[Epoch: 180, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2668, 0.0057, 0.1637, 0.4376, 0.0069, 0.1087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 181, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2073e-01, 9.4599e-06, 1.5636e-01, 1.0442e-06, 9.0577e-06, 1.0310e-01,
        6.1980e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.866

[Epoch: 181, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.0635e-03, 4.4245e-03, 4.5456e-03, 9.1365e-01, 4.0209e-03, 1.5305e-06,
        6.9299e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 181, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0904e-02, 9.6798e-01, 1.1250e-02, 4.9598e-06, 5.1094e-03, 4.7451e-03,
        3.8825e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.063

[Epoch: 181, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9693, 0.0054, 0.0052, 0.0045, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.970

[Epoch: 181, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0117, 0.2730, 0.0068, 0.1564, 0.4562, 0.0050, 0.0909],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 182, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1083e-01, 4.7312e-06, 1.5801e-01, 1.1175e-06, 9.4352e-06, 1.3592e-01,
        5.9522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.864

[Epoch: 182, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.8557e-03, 5.6275e-03, 5.9064e-03, 8.8658e-01, 8.4647e-03, 2.2050e-06,
        8.6560e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 182, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5256e-02, 9.5289e-01, 2.0182e-02, 7.0191e-06, 6.3655e-03, 5.2931e-03,
        6.3484e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.105

[Epoch: 182, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9680, 0.0047, 0.0056, 0.0062, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.971

[Epoch: 182, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0097, 0.2864, 0.0049, 0.1400, 0.4612, 0.0058, 0.0919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 183, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3013e-01, 1.0246e-05, 1.6631e-01, 2.1211e-06, 1.5670e-05, 1.4300e-01,
        5.6053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.880

[Epoch: 183, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.7415e-03, 5.5947e-03, 5.2125e-03, 9.0393e-01, 3.5122e-03, 1.6880e-06,
        7.5004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 183, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7020e-02, 9.5660e-01, 1.4619e-02, 5.9835e-06, 5.5704e-03, 6.1750e-03,
        8.1593e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.059

[Epoch: 183, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9725, 0.0049, 0.0041, 0.0039, 0.0052, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.939

[Epoch: 183, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0096, 0.2771, 0.0050, 0.1307, 0.4762, 0.0050, 0.0964],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 184, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([9.8908e-02, 7.1477e-06, 1.5190e-01, 1.3481e-06, 1.2205e-05, 1.1744e-01,
        6.3173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.874

[Epoch: 184, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.8297e-03, 5.6232e-03, 5.1305e-03, 9.0434e-01, 6.1658e-03, 2.1646e-06,
        7.2911e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 184, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6487e-02, 9.5774e-01, 1.4996e-02, 5.3502e-06, 5.2302e-03, 5.5324e-03,
        6.9891e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.078

[Epoch: 184, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.9683, 0.0054, 0.0056, 0.0050, 0.0046, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.964

[Epoch: 184, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0097, 0.2798, 0.0042, 0.1722, 0.4366, 0.0056, 0.0919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 185, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3443e-01, 1.0622e-05, 1.5641e-01, 1.5154e-06, 1.7456e-05, 1.4345e-01,
        5.6568e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.876

[Epoch: 185, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.4774e-03, 5.5667e-03, 5.0354e-03, 9.1039e-01, 8.5957e-03, 3.0702e-06,
        6.4936e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 185, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4953e-02, 9.6033e-01, 1.4545e-02, 6.3997e-06, 4.4781e-03, 5.6791e-03,
        6.2180e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.093

[Epoch: 185, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9706, 0.0053, 0.0042, 0.0056, 0.0040, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.957

[Epoch: 185, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0115, 0.2518, 0.0063, 0.1547, 0.4767, 0.0067, 0.0922],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 186, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([8.9161e-02, 6.7114e-06, 1.6401e-01, 1.5638e-06, 7.5800e-06, 1.2346e-01,
        6.2336e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.878

[Epoch: 186, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.4941e-03, 4.0483e-03, 5.3765e-03, 8.9232e-01, 3.1254e-03, 1.4967e-06,
        9.0631e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 186, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4372e-02, 9.6011e-01, 1.3673e-02, 5.6867e-06, 5.9995e-03, 5.8311e-03,
        4.0520e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.077

[Epoch: 186, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.9779, 0.0037, 0.0040, 0.0035, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.966

[Epoch: 186, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0110, 0.2889, 0.0056, 0.1447, 0.4432, 0.0050, 0.1017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 187, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.3660e-01, 4.4609e-06, 1.4167e-01, 8.9402e-07, 6.3696e-06, 1.1928e-01,
        6.0244e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.881

[Epoch: 187, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.4632e-03, 3.9111e-03, 3.8342e-03, 9.2259e-01, 4.6816e-03, 2.1100e-06,
        6.0520e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 187, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1335e-02, 9.6507e-01, 1.3749e-02, 3.8286e-06, 5.1273e-03, 4.7145e-03,
        3.1256e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 187, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.9687, 0.0057, 0.0057, 0.0044, 0.0059, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.953

[Epoch: 187, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0087, 0.2635, 0.0056, 0.1560, 0.4725, 0.0052, 0.0885],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 188, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0630e-01, 1.2302e-05, 1.8488e-01, 2.3319e-06, 1.0912e-05, 1.4503e-01,
        5.6376e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.858

[Epoch: 188, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([7.1995e-03, 5.2444e-03, 6.4703e-03, 8.8488e-01, 4.7919e-03, 2.1707e-06,
        9.1412e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 188, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6430e-02, 9.6049e-01, 1.3325e-02, 3.0278e-06, 4.9271e-03, 4.8206e-03,
        4.5621e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.090

[Epoch: 188, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9705, 0.0044, 0.0053, 0.0048, 0.0042, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.965

[Epoch: 188, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0102, 0.2734, 0.0043, 0.1389, 0.4677, 0.0056, 0.0998],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 189, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1788e-01, 6.8466e-06, 1.6968e-01, 1.2866e-06, 1.1520e-05, 1.4046e-01,
        5.7196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.849

[Epoch: 189, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.2601e-03, 6.5867e-03, 5.3348e-03, 9.0783e-01, 5.7817e-03, 1.6731e-06,
        6.9205e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 189, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6833e-02, 9.5508e-01, 1.6074e-02, 3.5280e-06, 5.4642e-03, 6.5428e-03,
        6.5401e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.074

[Epoch: 189, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.9658, 0.0059, 0.0045, 0.0047, 0.0077, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.964

[Epoch: 189, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.3067, 0.0049, 0.1465, 0.4380, 0.0043, 0.0905],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 190, batch: 44/223] total loss per batch: 0.638
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1140e-01, 7.9025e-06, 1.5154e-01, 1.8124e-06, 7.9470e-06, 1.1816e-01,
        6.1888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.877

[Epoch: 190, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.5115e-03, 4.9878e-03, 5.2448e-03, 9.0442e-01, 5.4344e-03, 2.3129e-06,
        7.5404e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 190, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2613e-02, 9.6436e-01, 1.3397e-02, 3.6113e-06, 4.4717e-03, 5.1504e-03,
        3.5420e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.094

[Epoch: 190, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.9645, 0.0067, 0.0057, 0.0057, 0.0045, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.972

[Epoch: 190, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2298, 0.0053, 0.1714, 0.4799, 0.0069, 0.0960],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 191, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1920e-01, 1.1074e-05, 1.5923e-01, 1.0773e-06, 1.1071e-05, 1.3793e-01,
        5.8361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.870

[Epoch: 191, batch: 88/223] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.9254e-03, 3.6290e-03, 3.4990e-03, 9.0467e-01, 3.6491e-03, 1.0415e-06,
        8.0630e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 191, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4636e-02, 9.5795e-01, 1.6862e-02, 3.7818e-06, 4.8231e-03, 5.7153e-03,
        5.1459e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.080

[Epoch: 191, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.9678, 0.0060, 0.0056, 0.0050, 0.0040, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.971

[Epoch: 191, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0120, 0.2813, 0.0071, 0.1460, 0.4420, 0.0049, 0.1067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 192, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0470e-01, 5.8751e-06, 1.7624e-01, 7.8833e-07, 5.1066e-06, 1.2094e-01,
        5.9811e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.870

[Epoch: 192, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([3.9891e-03, 5.2153e-03, 3.8826e-03, 9.1575e-01, 5.1480e-03, 1.7720e-06,
        6.6009e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 192, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2364e-02, 9.6434e-01, 1.3280e-02, 4.1443e-06, 5.2421e-03, 4.7623e-03,
        4.4443e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.076

[Epoch: 192, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.9719, 0.0057, 0.0062, 0.0044, 0.0040, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.959

[Epoch: 192, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0079, 0.2806, 0.0054, 0.1363, 0.4719, 0.0047, 0.0933],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 193, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2684e-01, 7.1189e-06, 1.4401e-01, 1.0168e-06, 4.9174e-06, 1.2080e-01,
        6.0833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.867

[Epoch: 193, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.1936e-03, 4.8240e-03, 6.6317e-03, 8.9111e-01, 4.8110e-03, 1.5653e-06,
        8.6427e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 193, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5628e-02, 9.6155e-01, 1.4444e-02, 3.4325e-06, 4.0191e-03, 4.3560e-03,
        4.9746e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.094

[Epoch: 193, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.9651, 0.0049, 0.0046, 0.0054, 0.0059, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.959

[Epoch: 193, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0092, 0.2720, 0.0045, 0.1547, 0.4592, 0.0046, 0.0958],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 194, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1240e-01, 1.1346e-05, 1.7928e-01, 1.6920e-06, 7.1017e-06, 1.4661e-01,
        5.6168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.888

[Epoch: 194, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.4468e-03, 7.8596e-03, 7.5137e-03, 8.9936e-01, 7.5718e-03, 2.3562e-06,
        7.1242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 194, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4277e-02, 9.6154e-01, 1.2940e-02, 7.0226e-06, 5.5681e-03, 5.6579e-03,
        9.3497e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.078

[Epoch: 194, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9669, 0.0050, 0.0060, 0.0063, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.963

[Epoch: 194, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0093, 0.2861, 0.0042, 0.1298, 0.4821, 0.0052, 0.0833],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 195, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1736e-01, 8.5674e-06, 1.5123e-01, 7.9063e-07, 1.0623e-05, 1.2677e-01,
        6.0463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.834

[Epoch: 195, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([5.0448e-03, 3.9202e-03, 4.7193e-03, 9.0656e-01, 4.0549e-03, 1.7495e-06,
        7.5698e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 195, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5153e-02, 9.5574e-01, 1.8800e-02, 5.1658e-06, 3.9715e-03, 6.3227e-03,
        6.9803e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.082

[Epoch: 195, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.9759, 0.0050, 0.0035, 0.0035, 0.0043, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.964

[Epoch: 195, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0102, 0.2623, 0.0054, 0.1841, 0.4285, 0.0056, 0.1039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 196, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1212e-01, 7.5784e-06, 1.5374e-01, 1.4592e-06, 8.3032e-06, 1.2719e-01,
        6.0693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.885

[Epoch: 196, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.4845e-03, 4.6696e-03, 4.6583e-03, 9.2039e-01, 5.1423e-03, 2.5095e-06,
        6.0656e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 196, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6078e-02, 9.5781e-01, 1.5022e-02, 6.4958e-06, 5.5321e-03, 5.5442e-03,
        3.5186e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.065

[Epoch: 196, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.9712, 0.0043, 0.0055, 0.0049, 0.0043, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.957

[Epoch: 196, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0097, 0.2808, 0.0050, 0.1407, 0.4665, 0.0056, 0.0917],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 197, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1237e-01, 8.3461e-06, 1.6322e-01, 9.5723e-07, 6.3790e-06, 1.4445e-01,
        5.7995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.890

[Epoch: 197, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1417e-03, 4.2588e-03, 4.7866e-03, 8.9597e-01, 4.7550e-03, 2.5709e-06,
        8.6084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 197, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3428e-02, 9.6321e-01, 1.4191e-02, 4.7287e-06, 4.2266e-03, 4.9314e-03,
        5.8507e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.089

[Epoch: 197, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.9695, 0.0057, 0.0056, 0.0045, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.964

[Epoch: 197, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0091, 0.2705, 0.0054, 0.1333, 0.4780, 0.0041, 0.0997],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 198, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.2592e-01, 7.4753e-06, 1.6048e-01, 9.5504e-07, 7.3793e-06, 1.2969e-01,
        5.8389e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.874

[Epoch: 198, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([6.1383e-03, 6.9203e-03, 6.3455e-03, 8.9901e-01, 3.7472e-03, 2.4214e-06,
        7.7837e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 198, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7273e-02, 9.5589e-01, 1.5227e-02, 3.8576e-06, 6.1335e-03, 5.4691e-03,
        3.5800e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.071

[Epoch: 198, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9692, 0.0055, 0.0057, 0.0037, 0.0042, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.963

[Epoch: 198, batch: 220/223] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0110, 0.2900, 0.0046, 0.1660, 0.4296, 0.0068, 0.0922],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 199, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.0690e-01, 8.4347e-06, 1.7244e-01, 1.2264e-06, 8.7937e-06, 1.2400e-01,
        5.9663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.852

[Epoch: 199, batch: 88/223] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.8783e-03, 6.0308e-03, 5.7797e-03, 9.0601e-01, 7.2162e-03, 2.1072e-06,
        7.0087e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 199, batch: 132/223] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5983e-02, 9.5693e-01, 1.5008e-02, 3.4261e-06, 5.7876e-03, 6.2810e-03,
        6.1421e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.075

[Epoch: 199, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.9701, 0.0053, 0.0047, 0.0052, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.970

[Epoch: 199, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0098, 0.2585, 0.0047, 0.1315, 0.5006, 0.0037, 0.0912],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 200, batch: 44/223] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.1150, 0.0000, 0.1600, 0.0000, 0.0000, 0.1300, 0.5950])
Policy pred: tensor([1.1981e-01, 9.0466e-06, 1.5123e-01, 8.3704e-07, 9.0722e-06, 1.4013e-01,
        5.8880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.869 0.877

[Epoch: 200, batch: 88/223] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9050, 0.0050, 0.0000, 0.0750])
Policy pred: tensor([4.1043e-03, 4.6537e-03, 4.7009e-03, 9.0395e-01, 3.9578e-03, 1.9785e-06,
        7.8629e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 200, batch: 132/223] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9600, 0.0150, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6442e-02, 9.5698e-01, 1.6475e-02, 5.0921e-06, 4.0341e-03, 6.0552e-03,
        4.2040e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.087 0.131

[Epoch: 200, batch: 176/223] total loss per batch: 0.662
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.9651, 0.0061, 0.0057, 0.0056, 0.0058, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.969 -0.970

[Epoch: 200, batch: 220/223] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.2750, 0.0050, 0.1500, 0.4600, 0.0050, 0.0950])
Policy pred: tensor([0.0107, 0.2837, 0.0060, 0.1835, 0.4032, 0.0082, 0.1047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

