Training set samples: 6306
Batch size: 32
[Epoch: 1, batch: 39/198] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.1796e-02, 4.8183e-01, 7.4846e-06, 6.1202e-06, 2.6700e-05, 4.5618e-06,
        4.5633e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 0.070

[Epoch: 1, batch: 78/198] total loss per batch: 0.830
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9631, 0.0097, 0.0068, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 1, batch: 117/198] total loss per batch: 0.816
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0034, 0.1673, 0.0137, 0.7975, 0.0025, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 1, batch: 156/198] total loss per batch: 0.826
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0153, 0.0256, 0.2614, 0.4699, 0.2090, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.009

[Epoch: 1, batch: 195/198] total loss per batch: 0.779
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.9721, 0.0057, 0.0028, 0.0072, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 2, batch: 39/198] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([3.2392e-02, 7.1994e-01, 2.3931e-05, 4.1380e-05, 4.5826e-05, 3.5307e-06,
        2.4755e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.288

[Epoch: 2, batch: 78/198] total loss per batch: 0.655
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0054, 0.9748, 0.0065, 0.0036, 0.0022, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 2, batch: 117/198] total loss per batch: 0.649
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0030, 0.0455, 0.0164, 0.9207, 0.0040, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 2, batch: 156/198] total loss per batch: 0.669
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0013, 0.0015, 0.0018, 0.0043, 0.9787, 0.0104, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 2, batch: 195/198] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0042, 0.9675, 0.0112, 0.0027, 0.0068, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.002

[Epoch: 3, batch: 39/198] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([2.4640e-02, 8.0368e-01, 3.9904e-06, 4.7534e-06, 1.4624e-05, 6.3534e-07,
        1.7166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.376

[Epoch: 3, batch: 78/198] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0063, 0.9725, 0.0074, 0.0028, 0.0045, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 3, batch: 117/198] total loss per batch: 0.581
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0037, 0.0166, 0.0077, 0.9610, 0.0044, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 3, batch: 156/198] total loss per batch: 0.602
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0013, 0.0019, 0.0043, 0.0051, 0.9793, 0.0060, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 3, batch: 195/198] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0034, 0.9741, 0.0072, 0.0022, 0.0060, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 4, batch: 39/198] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([2.1911e-02, 9.1991e-01, 2.6013e-06, 1.6738e-06, 3.9541e-06, 3.9459e-07,
        5.8173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.116

[Epoch: 4, batch: 78/198] total loss per batch: 0.587
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0065, 0.9729, 0.0065, 0.0026, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 4, batch: 117/198] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0036, 0.0709, 0.0091, 0.9055, 0.0040, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 4, batch: 156/198] total loss per batch: 0.580
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.0012, 0.0067, 0.0077, 0.9720, 0.0072, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 4, batch: 195/198] total loss per batch: 0.567
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0030, 0.9773, 0.0061, 0.0030, 0.0044, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 5, batch: 39/198] total loss per batch: 0.600
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.0104e-02, 9.1134e-01, 2.0563e-06, 3.1204e-06, 1.2508e-05, 1.2201e-06,
        7.8536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.468

[Epoch: 5, batch: 78/198] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0050, 0.9741, 0.0070, 0.0027, 0.0037, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 5, batch: 117/198] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0029, 0.0172, 0.0072, 0.9600, 0.0058, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 5, batch: 156/198] total loss per batch: 0.568
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0015, 0.0012, 0.0019, 0.0116, 0.9784, 0.0035, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 5, batch: 195/198] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0034, 0.9680, 0.0087, 0.0035, 0.0063, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 6, batch: 39/198] total loss per batch: 0.592
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1527e-03, 9.6224e-01, 2.3860e-06, 4.5381e-06, 5.0397e-06, 8.8731e-07,
        3.2589e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.486

[Epoch: 6, batch: 78/198] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0035, 0.9782, 0.0027, 0.0040, 0.0044, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 6, batch: 117/198] total loss per batch: 0.548
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0025, 0.0101, 0.0059, 0.9697, 0.0037, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 6, batch: 156/198] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.0020, 0.0048, 0.0059, 0.9771, 0.0048, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 6, batch: 195/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0029, 0.9718, 0.0054, 0.0032, 0.0069, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 7, batch: 39/198] total loss per batch: 0.585
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.8324e-02, 9.3435e-01, 5.8337e-06, 4.5452e-06, 6.1386e-06, 3.9846e-07,
        4.7310e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.492

[Epoch: 7, batch: 78/198] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0046, 0.9759, 0.0035, 0.0030, 0.0039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 7, batch: 117/198] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0043, 0.0590, 0.0096, 0.9162, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 7, batch: 156/198] total loss per batch: 0.559
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0019, 0.0011, 0.0023, 0.0095, 0.9766, 0.0048, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 7, batch: 195/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0048, 0.9649, 0.0064, 0.0068, 0.0084, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 8, batch: 39/198] total loss per batch: 0.582
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.1827e-03, 9.7275e-01, 7.5291e-07, 1.4226e-06, 2.4784e-06, 5.0060e-07,
        2.3067e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.573

[Epoch: 8, batch: 78/198] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.9782, 0.0035, 0.0031, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 8, batch: 117/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0025, 0.0120, 0.0052, 0.9690, 0.0044, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 8, batch: 156/198] total loss per batch: 0.558
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0019, 0.0062, 0.0082, 0.9718, 0.0053, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 8, batch: 195/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0045, 0.9708, 0.0058, 0.0045, 0.0052, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 9, batch: 39/198] total loss per batch: 0.579
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.0193e-02, 9.6670e-01, 3.3953e-06, 1.5356e-06, 5.8176e-06, 7.2953e-07,
        2.3092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.485

[Epoch: 9, batch: 78/198] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0050, 0.9756, 0.0035, 0.0039, 0.0030, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.006

[Epoch: 9, batch: 117/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0045, 0.0100, 0.0059, 0.9655, 0.0054, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 9, batch: 156/198] total loss per batch: 0.553
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0022, 0.0014, 0.0017, 0.0057, 0.9806, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.003

[Epoch: 9, batch: 195/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0041, 0.9688, 0.0068, 0.0047, 0.0057, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 10, batch: 39/198] total loss per batch: 0.577
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.9288e-03, 9.6715e-01, 3.1830e-06, 1.6749e-06, 3.3331e-06, 4.3035e-07,
        2.6909e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.462

[Epoch: 10, batch: 78/198] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.9763, 0.0040, 0.0024, 0.0030, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.006

[Epoch: 10, batch: 117/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.0026, 0.0279, 0.0051, 0.9554, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 10, batch: 156/198] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0029, 0.0041, 0.0068, 0.9733, 0.0039, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.008

[Epoch: 10, batch: 195/198] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0039, 0.9723, 0.0044, 0.0053, 0.0043, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 11, batch: 39/198] total loss per batch: 0.576
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.3101e-02, 9.6996e-01, 4.2734e-06, 1.2214e-06, 2.6271e-06, 7.4193e-07,
        1.6933e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.447

[Epoch: 11, batch: 78/198] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0043, 0.9688, 0.0058, 0.0053, 0.0041, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 11, batch: 117/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0033, 0.0235, 0.0062, 0.9559, 0.0039, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 11, batch: 156/198] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0030, 0.0031, 0.0093, 0.9680, 0.0063, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 11, batch: 195/198] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.9628, 0.0061, 0.0052, 0.0063, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 12, batch: 39/198] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.7405e-03, 9.7551e-01, 2.3874e-06, 9.7548e-07, 2.9160e-06, 5.7681e-07,
        1.8740e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.514

[Epoch: 12, batch: 78/198] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0051, 0.9716, 0.0039, 0.0040, 0.0033, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 12, batch: 117/198] total loss per batch: 0.525
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0034, 0.0223, 0.0075, 0.9556, 0.0036, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 12, batch: 156/198] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0032, 0.0048, 0.0111, 0.9622, 0.0059, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 12, batch: 195/198] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0039, 0.9666, 0.0051, 0.0065, 0.0048, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 13, batch: 39/198] total loss per batch: 0.567
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.5374e-03, 9.7308e-01, 2.3553e-06, 8.0733e-07, 2.7243e-06, 5.9247e-07,
        1.9372e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.472

[Epoch: 13, batch: 78/198] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0060, 0.9729, 0.0035, 0.0037, 0.0032, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 13, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0037, 0.0215, 0.0071, 0.9565, 0.0039, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 13, batch: 156/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0030, 0.0041, 0.0096, 0.9649, 0.0058, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.004

[Epoch: 13, batch: 195/198] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0033, 0.9697, 0.0047, 0.0054, 0.0065, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 14, batch: 39/198] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4008e-03, 9.8233e-01, 1.6346e-06, 5.1701e-07, 2.5637e-06, 5.1068e-07,
        1.2264e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.523

[Epoch: 14, batch: 78/198] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0054, 0.9733, 0.0041, 0.0036, 0.0031, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 14, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0038, 0.0257, 0.0075, 0.9509, 0.0047, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 14, batch: 156/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0031, 0.0059, 0.0136, 0.9584, 0.0060, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 14, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.9676, 0.0053, 0.0062, 0.0062, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 15, batch: 39/198] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.7491e-03, 9.7629e-01, 3.2404e-06, 6.2145e-07, 2.8064e-06, 4.4728e-07,
        1.6957e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.424

[Epoch: 15, batch: 78/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0057, 0.9707, 0.0037, 0.0051, 0.0035, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 15, batch: 117/198] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0032, 0.0244, 0.0051, 0.9565, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 15, batch: 156/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0032, 0.0038, 0.0127, 0.9632, 0.0058, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 15, batch: 195/198] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0036, 0.9691, 0.0044, 0.0057, 0.0062, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 16, batch: 39/198] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.0623e-03, 9.8420e-01, 1.7843e-06, 8.6836e-07, 5.8362e-06, 9.7356e-07,
        8.7315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.465

[Epoch: 16, batch: 78/198] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.9739, 0.0046, 0.0037, 0.0033, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 16, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0041, 0.0210, 0.0083, 0.9521, 0.0054, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 16, batch: 156/198] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0034, 0.0051, 0.0128, 0.9625, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 16, batch: 195/198] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.9673, 0.0059, 0.0058, 0.0055, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 17, batch: 39/198] total loss per batch: 0.567
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.0689e-02, 9.7458e-01, 3.0649e-06, 6.6757e-07, 3.1166e-06, 4.6173e-07,
        1.4726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.466

[Epoch: 17, batch: 78/198] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.9728, 0.0050, 0.0040, 0.0028, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 17, batch: 117/198] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0035, 0.0198, 0.0059, 0.9601, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 17, batch: 156/198] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0032, 0.0047, 0.0116, 0.9620, 0.0067, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 17, batch: 195/198] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.9676, 0.0051, 0.0057, 0.0051, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 18, batch: 39/198] total loss per batch: 0.567
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.6651e-03, 9.7842e-01, 2.2618e-06, 6.3165e-07, 1.8757e-06, 3.3406e-07,
        1.3905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.529

[Epoch: 18, batch: 78/198] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0041, 0.9784, 0.0025, 0.0032, 0.0029, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 18, batch: 117/198] total loss per batch: 0.526
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0038, 0.0449, 0.0080, 0.9308, 0.0048, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 18, batch: 156/198] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0049, 0.0090, 0.0197, 0.9416, 0.0078, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 18, batch: 195/198] total loss per batch: 0.536
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0057, 0.9655, 0.0064, 0.0041, 0.0051, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.002

[Epoch: 19, batch: 39/198] total loss per batch: 0.569
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([1.0208e-02, 9.7105e-01, 1.7891e-06, 6.8138e-07, 4.9156e-06, 5.4002e-07,
        1.8734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.479

[Epoch: 19, batch: 78/198] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0057, 0.9719, 0.0049, 0.0037, 0.0041, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 19, batch: 117/198] total loss per batch: 0.527
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0038, 0.0190, 0.0047, 0.9596, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 19, batch: 156/198] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0054, 0.0043, 0.0120, 0.9638, 0.0060, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.008

[Epoch: 19, batch: 195/198] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0035, 0.9723, 0.0039, 0.0051, 0.0059, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 20, batch: 39/198] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.6999e-03, 9.7705e-01, 5.7661e-07, 9.5256e-07, 2.4686e-06, 5.1538e-07,
        1.5251e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.460

[Epoch: 20, batch: 78/198] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0040, 0.9765, 0.0038, 0.0034, 0.0045, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 20, batch: 117/198] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0157, 0.0085, 0.9585, 0.0040, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 20, batch: 156/198] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0054, 0.0065, 0.0138, 0.9529, 0.0083, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.009

[Epoch: 20, batch: 195/198] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0058, 0.9679, 0.0051, 0.0086, 0.0036, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 21, batch: 39/198] total loss per batch: 0.573
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.4808e-03, 9.8357e-01, 5.7424e-06, 7.4633e-07, 4.7437e-06, 1.1755e-06,
        9.9377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.410

[Epoch: 21, batch: 78/198] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0059, 0.9724, 0.0046, 0.0049, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 21, batch: 117/198] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0221, 0.0078, 0.9521, 0.0042, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.005

[Epoch: 21, batch: 156/198] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0027, 0.0060, 0.0128, 0.9655, 0.0059, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 21, batch: 195/198] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.0051, 0.9591, 0.0049, 0.0070, 0.0092, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 22, batch: 39/198] total loss per batch: 0.569
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.1629e-03, 9.7556e-01, 5.7697e-06, 1.5436e-06, 3.3431e-06, 1.8770e-06,
        1.7264e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.445

[Epoch: 22, batch: 78/198] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.9722, 0.0040, 0.0038, 0.0038, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 22, batch: 117/198] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0039, 0.0156, 0.0059, 0.9602, 0.0048, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 22, batch: 156/198] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0023, 0.0060, 0.0189, 0.9568, 0.0065, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 22, batch: 195/198] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0037, 0.9732, 0.0047, 0.0034, 0.0048, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 23, batch: 39/198] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.5840e-03, 9.8222e-01, 5.2940e-06, 8.7782e-07, 3.7786e-06, 8.4408e-07,
        1.0187e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.514

[Epoch: 23, batch: 78/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0057, 0.9714, 0.0043, 0.0039, 0.0033, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 23, batch: 117/198] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0045, 0.0342, 0.0070, 0.9372, 0.0072, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 23, batch: 156/198] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0034, 0.0059, 0.0153, 0.9606, 0.0051, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 23, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0049, 0.9695, 0.0053, 0.0042, 0.0058, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 24, batch: 39/198] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.0032e-03, 9.8264e-01, 4.2631e-06, 8.3912e-07, 3.3287e-06, 7.7756e-07,
        1.1350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.497

[Epoch: 24, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0050, 0.9713, 0.0049, 0.0038, 0.0041, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 24, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0212, 0.0071, 0.9535, 0.0050, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 24, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0034, 0.0042, 0.0143, 0.9645, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 24, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.9684, 0.0054, 0.0043, 0.0046, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 25, batch: 39/198] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.3681e-03, 9.8437e-01, 4.7974e-06, 7.7979e-07, 3.0818e-06, 6.8808e-07,
        1.0253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.438

[Epoch: 25, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9712, 0.0046, 0.0042, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 25, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0201, 0.0063, 0.9545, 0.0051, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 25, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0047, 0.0068, 0.0150, 0.9551, 0.0054, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 25, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0043, 0.9675, 0.0058, 0.0047, 0.0055, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 26, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.0810e-03, 9.8316e-01, 3.3075e-06, 5.4463e-07, 2.6194e-06, 6.2591e-07,
        1.0753e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.527

[Epoch: 26, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9735, 0.0046, 0.0037, 0.0031, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 26, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.0287, 0.0076, 0.9435, 0.0049, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 26, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0038, 0.0054, 0.0134, 0.9602, 0.0059, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 26, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0053, 0.9702, 0.0050, 0.0047, 0.0040, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 27, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4580e-03, 9.8518e-01, 3.5327e-06, 6.3400e-07, 3.6151e-06, 6.1807e-07,
        9.3515e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.442

[Epoch: 27, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0049, 0.9711, 0.0044, 0.0048, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 27, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0047, 0.0250, 0.0060, 0.9525, 0.0042, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 27, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0034, 0.0043, 0.0148, 0.9633, 0.0045, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 27, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.9667, 0.0057, 0.0052, 0.0065, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 28, batch: 39/198] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.8240e-03, 9.8348e-01, 2.4177e-06, 3.5477e-07, 1.7675e-06, 6.3084e-07,
        9.6861e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.510

[Epoch: 28, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0049, 0.9746, 0.0038, 0.0031, 0.0030, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 28, batch: 117/198] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0056, 0.0228, 0.0063, 0.9477, 0.0065, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 28, batch: 156/198] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0046, 0.0080, 0.0161, 0.9489, 0.0076, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 28, batch: 195/198] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0056, 0.9649, 0.0056, 0.0059, 0.0047, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 29, batch: 39/198] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.2797e-03, 9.8276e-01, 4.0190e-06, 5.3889e-07, 2.5100e-06, 4.2687e-07,
        1.0954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.434

[Epoch: 29, batch: 78/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0052, 0.9685, 0.0054, 0.0050, 0.0047, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 29, batch: 117/198] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0061, 0.0337, 0.0063, 0.9434, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 29, batch: 156/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0027, 0.0039, 0.0154, 0.9666, 0.0044, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 29, batch: 195/198] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0047, 0.9648, 0.0070, 0.0055, 0.0056, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 30, batch: 39/198] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.9498e-03, 9.8360e-01, 2.4127e-06, 6.5300e-07, 2.2555e-06, 5.0702e-07,
        9.4419e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.491

[Epoch: 30, batch: 78/198] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0050, 0.9739, 0.0045, 0.0028, 0.0036, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 30, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0038, 0.0226, 0.0047, 0.9542, 0.0050, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 30, batch: 156/198] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0047, 0.0062, 0.0100, 0.9628, 0.0058, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 30, batch: 195/198] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0060, 0.9653, 0.0058, 0.0060, 0.0049, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 31, batch: 39/198] total loss per batch: 0.567
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.8523e-03, 9.7742e-01, 6.0066e-06, 6.4949e-07, 3.2626e-06, 6.7869e-07,
        1.4719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.471

[Epoch: 31, batch: 78/198] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0064, 0.9664, 0.0057, 0.0055, 0.0047, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 31, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0052, 0.0221, 0.0061, 0.9518, 0.0062, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 31, batch: 156/198] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0033, 0.0060, 0.0159, 0.9582, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 31, batch: 195/198] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0051, 0.9661, 0.0052, 0.0044, 0.0062, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 32, batch: 39/198] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.9886e-03, 9.8419e-01, 4.1860e-06, 8.2622e-07, 2.6608e-06, 8.3348e-07,
        9.8105e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.492

[Epoch: 32, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.9730, 0.0041, 0.0036, 0.0040, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 32, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0038, 0.0227, 0.0056, 0.9538, 0.0049, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 32, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0048, 0.0059, 0.0152, 0.9558, 0.0058, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 32, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.9685, 0.0055, 0.0057, 0.0046, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 33, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([7.2082e-03, 9.8321e-01, 4.5328e-06, 6.8936e-07, 3.2644e-06, 8.3392e-07,
        9.5726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.467

[Epoch: 33, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.9738, 0.0040, 0.0039, 0.0036, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 33, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.0289, 0.0080, 0.9442, 0.0043, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 33, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0048, 0.0058, 0.0133, 0.9600, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 33, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.9667, 0.0054, 0.0048, 0.0059, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 34, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.4563e-03, 9.8811e-01, 2.6249e-06, 3.5298e-07, 1.5575e-06, 5.0951e-07,
        7.4294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.501

[Epoch: 34, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.9734, 0.0045, 0.0033, 0.0036, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 34, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0156, 0.0054, 0.9609, 0.0048, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 34, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.0055, 0.0147, 0.9596, 0.0052, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 34, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0046, 0.9702, 0.0051, 0.0048, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 35, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.4643e-03, 9.8713e-01, 2.9457e-06, 3.5643e-07, 1.7889e-06, 3.6024e-07,
        8.3961e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.477

[Epoch: 35, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.9754, 0.0039, 0.0033, 0.0030, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 35, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.0226, 0.0062, 0.9518, 0.0047, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 35, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0042, 0.0048, 0.0143, 0.9613, 0.0045, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 35, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.9699, 0.0055, 0.0044, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 36, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.3217e-03, 9.8772e-01, 1.9983e-06, 2.6849e-07, 1.0596e-06, 2.8720e-07,
        7.9505e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.495

[Epoch: 36, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0052, 0.9710, 0.0048, 0.0039, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 36, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0052, 0.0363, 0.0056, 0.9390, 0.0049, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 36, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.0054, 0.0135, 0.9608, 0.0046, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 36, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.9689, 0.0053, 0.0052, 0.0050, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 37, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.3162e-03, 9.8883e-01, 1.9519e-06, 2.9552e-07, 1.5080e-06, 3.7013e-07,
        6.8469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.450

[Epoch: 37, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.9729, 0.0040, 0.0040, 0.0032, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 37, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0042, 0.0185, 0.0057, 0.9575, 0.0047, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 37, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0041, 0.0050, 0.0169, 0.9561, 0.0060, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 37, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0050, 0.9680, 0.0058, 0.0052, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 38, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.9467e-03, 9.8800e-01, 2.1553e-06, 3.0237e-07, 9.1822e-07, 2.4729e-07,
        7.0546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.498

[Epoch: 38, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0047, 0.9715, 0.0046, 0.0041, 0.0050, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 38, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0060, 0.0326, 0.0060, 0.9387, 0.0063, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 38, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0051, 0.0127, 0.9635, 0.0043, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 38, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0041, 0.9731, 0.0042, 0.0042, 0.0040, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 39, batch: 39/198] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.2199e-03, 9.8822e-01, 1.4146e-06, 3.8153e-07, 3.0500e-06, 7.4646e-07,
        7.5526e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.465

[Epoch: 39, batch: 78/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.9749, 0.0043, 0.0037, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 39, batch: 117/198] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0048, 0.0159, 0.0047, 0.9636, 0.0030, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 39, batch: 156/198] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.0046, 0.0150, 0.9588, 0.0047, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 39, batch: 195/198] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.0059, 0.9623, 0.0060, 0.0066, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 40, batch: 39/198] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.9285e-03, 9.8592e-01, 2.0626e-06, 5.1911e-07, 1.2065e-06, 4.6712e-07,
        8.1423e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.492

[Epoch: 40, batch: 78/198] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0044, 0.9750, 0.0046, 0.0030, 0.0031, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 40, batch: 117/198] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0292, 0.0074, 0.9418, 0.0080, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 40, batch: 156/198] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0065, 0.0067, 0.0158, 0.9539, 0.0060, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 40, batch: 195/198] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0049, 0.9686, 0.0057, 0.0051, 0.0055, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 41, batch: 39/198] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.9378e-03, 9.8509e-01, 4.1732e-06, 5.3272e-07, 2.2926e-06, 6.7132e-07,
        8.9651e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.449

[Epoch: 41, batch: 78/198] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.9747, 0.0044, 0.0032, 0.0032, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 41, batch: 117/198] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0057, 0.0127, 0.0048, 0.9652, 0.0031, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 41, batch: 156/198] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0054, 0.0046, 0.0155, 0.9587, 0.0052, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 41, batch: 195/198] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0049, 0.9679, 0.0046, 0.0048, 0.0060, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 42, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.8215e-03, 9.8636e-01, 2.1836e-06, 2.8476e-07, 1.1389e-06, 4.2814e-07,
        7.8133e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.497

[Epoch: 42, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9746, 0.0043, 0.0037, 0.0032, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 42, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.0275, 0.0050, 0.9489, 0.0054, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 42, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0054, 0.0156, 0.9604, 0.0052, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 42, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0045, 0.9714, 0.0050, 0.0041, 0.0042, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 43, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.2097e-03, 9.8637e-01, 2.8537e-06, 4.9011e-07, 2.5327e-06, 6.6475e-07,
        7.4090e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.474

[Epoch: 43, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0041, 0.9739, 0.0049, 0.0038, 0.0040, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 43, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0254, 0.0067, 0.9495, 0.0040, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 43, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0058, 0.0161, 0.9572, 0.0055, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 43, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.9724, 0.0043, 0.0044, 0.0043, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 44, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1245e-03, 9.8860e-01, 2.3922e-06, 3.5671e-07, 1.7464e-06, 4.2246e-07,
        6.2741e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.495

[Epoch: 44, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9731, 0.0046, 0.0040, 0.0037, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 44, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0246, 0.0059, 0.9506, 0.0046, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 44, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0061, 0.0154, 0.9591, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 44, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.9725, 0.0050, 0.0041, 0.0042, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 45, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.7427e-03, 9.8746e-01, 2.0248e-06, 3.0138e-07, 2.0626e-06, 4.2580e-07,
        6.7890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.474

[Epoch: 45, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.9726, 0.0049, 0.0038, 0.0040, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 45, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.0229, 0.0059, 0.9526, 0.0040, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 45, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0048, 0.0156, 0.9613, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 45, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.9698, 0.0051, 0.0050, 0.0046, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 46, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.8287e-03, 9.8803e-01, 2.0244e-06, 3.3985e-07, 2.1133e-06, 4.4743e-07,
        7.1336e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.475

[Epoch: 46, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0053, 0.9707, 0.0050, 0.0043, 0.0040, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 46, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0058, 0.0223, 0.0060, 0.9520, 0.0045, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 46, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0061, 0.0140, 0.9601, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 46, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.9721, 0.0049, 0.0042, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 47, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.6601e-03, 9.8749e-01, 2.3883e-06, 3.4096e-07, 1.7343e-06, 3.6917e-07,
        6.8402e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.478

[Epoch: 47, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.9732, 0.0051, 0.0038, 0.0040, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 47, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0332, 0.0055, 0.9428, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 47, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0050, 0.0156, 0.9582, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 47, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.9717, 0.0045, 0.0053, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 48, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1661e-03, 9.8825e-01, 1.9253e-06, 3.5104e-07, 2.0130e-06, 4.5653e-07,
        6.5800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.467

[Epoch: 48, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0053, 0.9694, 0.0054, 0.0045, 0.0044, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 48, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0169, 0.0055, 0.9598, 0.0042, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 48, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0042, 0.0059, 0.0139, 0.9606, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 48, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0047, 0.9698, 0.0052, 0.0045, 0.0046, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 49, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.6350e-03, 9.8748e-01, 2.1265e-06, 3.1987e-07, 1.7304e-06, 3.8770e-07,
        6.8822e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.486

[Epoch: 49, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0046, 0.9739, 0.0047, 0.0039, 0.0038, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 49, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0278, 0.0053, 0.9486, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 49, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0047, 0.0041, 0.0157, 0.9600, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 49, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0055, 0.9708, 0.0050, 0.0045, 0.0038, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 50, batch: 39/198] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.0284e-03, 9.8914e-01, 2.9627e-06, 6.5084e-07, 1.9603e-06, 4.2432e-07,
        5.8282e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.482

[Epoch: 50, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.9729, 0.0047, 0.0036, 0.0030, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 50, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0054, 0.0282, 0.0048, 0.9496, 0.0040, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 50, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0032, 0.0059, 0.0165, 0.9599, 0.0048, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 50, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0035, 0.9707, 0.0052, 0.0052, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 51, batch: 39/198] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.7869e-03, 9.8534e-01, 2.6681e-06, 4.2174e-07, 2.1068e-06, 9.6607e-07,
        7.8697e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.469

[Epoch: 51, batch: 78/198] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.9714, 0.0044, 0.0039, 0.0062, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 51, batch: 117/198] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0044, 0.0420, 0.0054, 0.9363, 0.0039, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 51, batch: 156/198] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0044, 0.0045, 0.0149, 0.9610, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 51, batch: 195/198] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0066, 0.9631, 0.0055, 0.0059, 0.0077, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 52, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.5681e-03, 9.8816e-01, 4.6400e-06, 1.0126e-06, 2.9858e-06, 6.9517e-07,
        6.2604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.479

[Epoch: 52, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0048, 0.9737, 0.0043, 0.0038, 0.0035, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 52, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0054, 0.0234, 0.0053, 0.9517, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 52, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0046, 0.0057, 0.0184, 0.9546, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 52, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0052, 0.9659, 0.0058, 0.0058, 0.0061, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 53, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.2466e-03, 9.8662e-01, 3.0745e-06, 4.5674e-07, 1.8351e-06, 4.8005e-07,
        7.1241e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.463

[Epoch: 53, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.9734, 0.0046, 0.0039, 0.0037, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 53, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.0193, 0.0054, 0.9573, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 53, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0057, 0.0066, 0.0156, 0.9541, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 53, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0063, 0.9645, 0.0057, 0.0056, 0.0055, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 54, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.5250e-03, 9.8810e-01, 3.0024e-06, 6.1952e-07, 2.2373e-06, 5.0097e-07,
        6.3655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.480

[Epoch: 54, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.9740, 0.0045, 0.0036, 0.0035, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 54, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0054, 0.0261, 0.0060, 0.9474, 0.0051, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 54, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0057, 0.0155, 0.9569, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 54, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0057, 0.9649, 0.0058, 0.0058, 0.0058, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 55, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1476e-03, 9.8837e-01, 2.6613e-06, 4.1554e-07, 1.6119e-06, 4.1472e-07,
        6.4788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.488

[Epoch: 55, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.9733, 0.0047, 0.0037, 0.0038, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 55, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.0233, 0.0056, 0.9510, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 55, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0050, 0.0051, 0.0144, 0.9587, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 55, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0056, 0.9669, 0.0056, 0.0052, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 56, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4492e-03, 9.8796e-01, 2.2782e-06, 4.8077e-07, 2.2123e-06, 4.1714e-07,
        6.5838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.463

[Epoch: 56, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.9749, 0.0044, 0.0036, 0.0035, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 56, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0271, 0.0058, 0.9473, 0.0051, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 56, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0060, 0.0161, 0.9557, 0.0057, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 56, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0054, 0.9680, 0.0054, 0.0056, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 57, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.2391e-03, 9.9050e-01, 2.2464e-06, 3.6353e-07, 1.3604e-06, 3.5090e-07,
        5.2543e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.489

[Epoch: 57, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.9727, 0.0047, 0.0038, 0.0037, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 57, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0058, 0.0245, 0.0059, 0.9486, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 57, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0037, 0.0040, 0.0130, 0.9661, 0.0045, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 57, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9711, 0.0048, 0.0047, 0.0046, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 58, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4520e-03, 9.8844e-01, 2.2054e-06, 4.4494e-07, 2.5944e-06, 4.0803e-07,
        6.0974e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.471

[Epoch: 58, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0046, 0.9720, 0.0049, 0.0042, 0.0044, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 58, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0053, 0.0225, 0.0051, 0.9532, 0.0050, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 58, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0058, 0.0044, 0.0146, 0.9595, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 58, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.9686, 0.0052, 0.0060, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 59, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([3.5131e-03, 9.9177e-01, 1.8970e-06, 3.6451e-07, 1.2703e-06, 3.1904e-07,
        4.7147e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.490

[Epoch: 59, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0043, 0.9736, 0.0045, 0.0036, 0.0036, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 59, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0049, 0.0285, 0.0066, 0.9435, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 59, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0046, 0.0129, 0.9645, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.006

[Epoch: 59, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.9715, 0.0050, 0.0045, 0.0043, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 60, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([6.6916e-03, 9.8589e-01, 2.6432e-06, 5.7195e-07, 2.4362e-06, 5.3260e-07,
        7.4161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.462

[Epoch: 60, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0051, 0.9687, 0.0057, 0.0053, 0.0047, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 60, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0062, 0.0235, 0.0047, 0.9514, 0.0050, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 60, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0042, 0.0042, 0.0138, 0.9640, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 60, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0055, 0.9711, 0.0046, 0.0047, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 61, batch: 39/198] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.0322e-03, 9.9058e-01, 1.5382e-06, 3.1275e-07, 1.4289e-06, 3.4629e-07,
        5.3818e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.475

[Epoch: 61, batch: 78/198] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.9727, 0.0043, 0.0036, 0.0038, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 61, batch: 117/198] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0044, 0.0179, 0.0057, 0.9567, 0.0044, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 61, batch: 156/198] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0056, 0.0048, 0.0153, 0.9580, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 61, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.9682, 0.0058, 0.0050, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 62, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.5869e-03, 9.8731e-01, 2.1487e-06, 4.5410e-07, 2.3191e-06, 4.7279e-07,
        7.0976e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.473

[Epoch: 62, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0057, 0.9709, 0.0049, 0.0045, 0.0039, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 62, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0281, 0.0057, 0.9473, 0.0045, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 62, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0043, 0.0153, 0.9603, 0.0050, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 62, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.9683, 0.0051, 0.0054, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 63, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.4381e-03, 9.8980e-01, 1.8183e-06, 3.0145e-07, 1.2595e-06, 3.1482e-07,
        5.7546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.480

[Epoch: 63, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0063, 0.9660, 0.0058, 0.0047, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 63, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.0207, 0.0048, 0.9565, 0.0045, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 63, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.0049, 0.0155, 0.9578, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 63, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.9685, 0.0051, 0.0052, 0.0050, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 64, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1558e-03, 9.8849e-01, 1.6661e-06, 3.2894e-07, 1.7708e-06, 3.0928e-07,
        6.3462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.477

[Epoch: 64, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0062, 0.9678, 0.0056, 0.0046, 0.0043, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 64, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0259, 0.0052, 0.9501, 0.0044, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 64, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0049, 0.0150, 0.9594, 0.0049, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 64, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.9691, 0.0050, 0.0051, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 65, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.3833e-03, 9.8816e-01, 2.2961e-06, 3.8695e-07, 1.7977e-06, 3.6154e-07,
        6.4499e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.473

[Epoch: 65, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0063, 0.9656, 0.0058, 0.0051, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 65, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0043, 0.0233, 0.0046, 0.9550, 0.0043, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 65, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0052, 0.0151, 0.9585, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 65, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0049, 0.9706, 0.0051, 0.0051, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 66, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.2659e-03, 9.8847e-01, 1.6137e-06, 3.0029e-07, 1.7337e-06, 2.8413e-07,
        6.2562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.481

[Epoch: 66, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0056, 0.9685, 0.0054, 0.0045, 0.0045, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 66, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0042, 0.0258, 0.0047, 0.9527, 0.0041, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 66, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0059, 0.0051, 0.0157, 0.9566, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 66, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0049, 0.9714, 0.0047, 0.0048, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 67, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.6413e-03, 9.8820e-01, 2.2845e-06, 4.0126e-07, 2.1305e-06, 3.9438e-07,
        6.1529e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.467

[Epoch: 67, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0054, 0.9701, 0.0048, 0.0046, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 67, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0046, 0.0288, 0.0049, 0.9490, 0.0040, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 67, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.0053, 0.0140, 0.9603, 0.0053, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 67, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0047, 0.9713, 0.0049, 0.0049, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 68, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1791e-03, 9.8836e-01, 1.4708e-06, 2.8070e-07, 1.7243e-06, 2.8588e-07,
        6.4560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.493

[Epoch: 68, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.9708, 0.0053, 0.0041, 0.0042, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 68, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0038, 0.0222, 0.0043, 0.9576, 0.0042, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 68, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0058, 0.0050, 0.0167, 0.9550, 0.0058, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 68, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.9713, 0.0047, 0.0053, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 69, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.3811e-03, 9.8918e-01, 2.0058e-06, 4.0013e-07, 1.9372e-06, 3.8273e-07,
        5.4350e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.458

[Epoch: 69, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.9740, 0.0042, 0.0042, 0.0038, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 69, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0267, 0.0053, 0.9494, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 69, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0043, 0.0060, 0.0139, 0.9604, 0.0055, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 69, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.9699, 0.0049, 0.0048, 0.0056, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 70, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4079e-03, 9.8885e-01, 1.4535e-06, 2.5844e-07, 1.1521e-06, 2.6869e-07,
        5.7393e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.491

[Epoch: 70, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9711, 0.0052, 0.0041, 0.0039, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 70, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0303, 0.0053, 0.9444, 0.0052, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 70, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0047, 0.0149, 0.9599, 0.0059, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 70, batch: 195/198] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0044, 0.9704, 0.0049, 0.0057, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 71, batch: 39/198] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.8264e-03, 9.8907e-01, 1.3440e-06, 3.0567e-07, 1.7989e-06, 3.7513e-07,
        5.1030e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.468

[Epoch: 71, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.9731, 0.0050, 0.0038, 0.0035, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 71, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0061, 0.0294, 0.0065, 0.9395, 0.0060, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 71, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0060, 0.0066, 0.0137, 0.9560, 0.0058, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 71, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.9694, 0.0051, 0.0049, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 72, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.9015e-03, 9.8857e-01, 1.5985e-06, 3.5146e-07, 1.4505e-06, 4.3521e-07,
        5.5279e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.495

[Epoch: 72, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.9713, 0.0048, 0.0039, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 72, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0063, 0.0240, 0.0071, 0.9444, 0.0060, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 72, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0055, 0.0135, 0.9615, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 72, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0047, 0.9696, 0.0049, 0.0050, 0.0049, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 73, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.0730e-03, 9.8988e-01, 1.6348e-06, 2.9619e-07, 1.5258e-06, 3.4802e-07,
        5.0402e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.470

[Epoch: 73, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0055, 0.9693, 0.0053, 0.0044, 0.0047, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 73, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0061, 0.0219, 0.0065, 0.9475, 0.0059, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 73, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0043, 0.0043, 0.0129, 0.9655, 0.0043, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 73, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.9681, 0.0053, 0.0051, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 74, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.8340e-03, 9.9026e-01, 1.4048e-06, 2.9784e-07, 1.3829e-06, 3.2812e-07,
        4.8996e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.478

[Epoch: 74, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0056, 0.9681, 0.0054, 0.0045, 0.0049, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 74, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0058, 0.0207, 0.0062, 0.9503, 0.0057, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 74, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0041, 0.0042, 0.0138, 0.9653, 0.0043, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 74, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0053, 0.9660, 0.0056, 0.0056, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 75, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.4937e-03, 9.9034e-01, 1.4982e-06, 2.7963e-07, 1.1584e-06, 2.7095e-07,
        5.1677e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.474

[Epoch: 75, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0057, 0.9677, 0.0055, 0.0049, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 75, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.0239, 0.0059, 0.9489, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 75, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0046, 0.0040, 0.0141, 0.9644, 0.0041, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 75, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0056, 0.9648, 0.0058, 0.0058, 0.0057, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 76, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.5187e-03, 9.9027e-01, 1.2965e-06, 2.5213e-07, 1.0452e-06, 2.6383e-07,
        5.2069e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.479

[Epoch: 76, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0055, 0.9687, 0.0054, 0.0046, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 76, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0216, 0.0054, 0.9532, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 76, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0049, 0.0157, 0.9597, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 76, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0057, 0.9652, 0.0056, 0.0058, 0.0057, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 77, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.5933e-03, 9.9017e-01, 1.4584e-06, 2.5934e-07, 1.0604e-06, 2.2185e-07,
        5.2294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.476

[Epoch: 77, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.9700, 0.0050, 0.0045, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 77, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0253, 0.0052, 0.9502, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 77, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.0050, 0.0159, 0.9580, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 77, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0056, 0.9666, 0.0056, 0.0056, 0.0053, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 78, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.7134e-03, 9.9023e-01, 1.1243e-06, 2.2876e-07, 9.6316e-07, 2.1602e-07,
        5.0552e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.474

[Epoch: 78, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.9720, 0.0049, 0.0040, 0.0042, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 78, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.0248, 0.0050, 0.9519, 0.0046, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 78, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.0057, 0.0158, 0.9568, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 78, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0051, 0.9699, 0.0051, 0.0051, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 79, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.5901e-03, 9.9019e-01, 1.2035e-06, 2.3835e-07, 1.1018e-06, 2.1052e-07,
        5.2214e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.485

[Epoch: 79, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.9729, 0.0043, 0.0041, 0.0042, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 79, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0042, 0.0265, 0.0047, 0.9508, 0.0045, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 79, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.0054, 0.0163, 0.9562, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 79, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.9720, 0.0048, 0.0042, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 80, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.5208e-03, 9.9002e-01, 9.3255e-07, 2.2442e-07, 9.7174e-07, 1.9989e-07,
        5.4596e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.466

[Epoch: 80, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.9739, 0.0048, 0.0039, 0.0037, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 80, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0248, 0.0050, 0.9512, 0.0046, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 80, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.0050, 0.0148, 0.9585, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 80, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0044, 0.9737, 0.0048, 0.0045, 0.0042, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 81, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.4468e-03, 9.9045e-01, 1.1270e-06, 2.2226e-07, 1.1958e-06, 2.3079e-07,
        5.1041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.488

[Epoch: 81, batch: 78/198] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.9718, 0.0041, 0.0046, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 81, batch: 117/198] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0040, 0.0242, 0.0044, 0.9532, 0.0045, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 81, batch: 156/198] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0058, 0.0163, 0.9544, 0.0062, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 81, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0038, 0.9774, 0.0034, 0.0035, 0.0042, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 82, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.6873e-03, 9.9031e-01, 9.8601e-07, 2.4704e-07, 1.2765e-06, 3.1252e-07,
        5.0046e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.483

[Epoch: 82, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0051, 0.9706, 0.0050, 0.0045, 0.0043, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 82, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0256, 0.0049, 0.9503, 0.0044, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 82, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0052, 0.0141, 0.9608, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 82, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0042, 0.9742, 0.0046, 0.0041, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 83, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.8090e-03, 9.9038e-01, 1.2153e-06, 2.3356e-07, 1.2527e-06, 2.7944e-07,
        4.8110e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.471

[Epoch: 83, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0053, 0.9695, 0.0047, 0.0047, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 83, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0243, 0.0046, 0.9519, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 83, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0052, 0.0145, 0.9602, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 83, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.9706, 0.0050, 0.0048, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 84, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.7468e-03, 9.9001e-01, 1.0001e-06, 2.1239e-07, 1.1646e-06, 2.8498e-07,
        5.2396e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.480

[Epoch: 84, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.9697, 0.0050, 0.0045, 0.0046, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 84, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0275, 0.0050, 0.9476, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 84, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0045, 0.0141, 0.9623, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 84, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.9673, 0.0057, 0.0053, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 85, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.4258e-03, 9.8936e-01, 1.0777e-06, 1.8891e-07, 9.8145e-07, 2.4116e-07,
        5.2085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.477

[Epoch: 85, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.9698, 0.0049, 0.0045, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 85, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0250, 0.0050, 0.9497, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 85, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.0049, 0.0155, 0.9597, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 85, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.9659, 0.0058, 0.0057, 0.0058, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 86, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.9624e-03, 9.8985e-01, 9.1966e-07, 1.8407e-07, 8.8530e-07, 2.2440e-07,
        5.1841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.480

[Epoch: 86, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.9707, 0.0048, 0.0044, 0.0045, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 86, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0248, 0.0052, 0.9494, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 86, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.0047, 0.0148, 0.9611, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 86, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0058, 0.9652, 0.0058, 0.0058, 0.0057, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 87, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.3894e-03, 9.8948e-01, 8.9010e-07, 1.6909e-07, 7.9876e-07, 2.0688e-07,
        5.1278e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.474

[Epoch: 87, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.9716, 0.0046, 0.0042, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 87, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.0251, 0.0052, 0.9489, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 87, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0048, 0.0151, 0.9608, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 87, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0055, 0.9667, 0.0056, 0.0054, 0.0058, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 88, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.6095e-03, 9.9034e-01, 7.5393e-07, 1.6745e-07, 7.5985e-07, 1.8559e-07,
        5.0457e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.485

[Epoch: 88, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.9722, 0.0046, 0.0042, 0.0043, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 88, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0256, 0.0052, 0.9482, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 88, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0049, 0.0149, 0.9608, 0.0048, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 88, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0055, 0.9667, 0.0054, 0.0057, 0.0056, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 89, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.0822e-03, 9.8966e-01, 6.8665e-07, 1.3979e-07, 6.8004e-07, 1.7510e-07,
        5.2531e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.465

[Epoch: 89, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.9725, 0.0045, 0.0041, 0.0044, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 89, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.0228, 0.0052, 0.9511, 0.0048, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 89, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0049, 0.0143, 0.9620, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 89, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.9699, 0.0051, 0.0047, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 90, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.5729e-03, 9.9063e-01, 7.1991e-07, 1.8373e-07, 8.3564e-07, 1.8371e-07,
        4.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.488

[Epoch: 90, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9732, 0.0045, 0.0041, 0.0041, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 90, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.0231, 0.0050, 0.9530, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 90, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0047, 0.0149, 0.9607, 0.0048, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 90, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.9693, 0.0052, 0.0055, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 91, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1986e-03, 9.8938e-01, 6.8586e-07, 1.4002e-07, 7.2940e-07, 1.8560e-07,
        5.4156e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.476

[Epoch: 91, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0049, 0.9707, 0.0049, 0.0045, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 91, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0054, 0.0268, 0.0050, 0.9476, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 91, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0051, 0.0153, 0.9596, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 91, batch: 195/198] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0046, 0.9738, 0.0045, 0.0041, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 92, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.9142e-03, 9.8992e-01, 8.4614e-07, 1.8737e-07, 9.1147e-07, 2.0962e-07,
        5.1657e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.479

[Epoch: 92, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.9717, 0.0046, 0.0044, 0.0045, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 92, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.0248, 0.0048, 0.9515, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 92, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0051, 0.0151, 0.9595, 0.0050, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 92, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.9730, 0.0047, 0.0046, 0.0044, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 93, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.2402e-03, 9.8954e-01, 8.9645e-07, 1.8380e-07, 9.2168e-07, 2.3596e-07,
        5.2187e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.473

[Epoch: 93, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9713, 0.0047, 0.0044, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 93, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0248, 0.0048, 0.9511, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 93, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.0155, 0.9584, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 93, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0045, 0.9734, 0.0045, 0.0045, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 94, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.2013e-03, 9.8947e-01, 1.0038e-06, 1.8579e-07, 9.1714e-07, 2.2970e-07,
        5.3242e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.481

[Epoch: 94, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9711, 0.0047, 0.0045, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 94, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0245, 0.0048, 0.9519, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 94, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0052, 0.0148, 0.9592, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 94, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0046, 0.9722, 0.0048, 0.0047, 0.0046, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 95, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.6731e-03, 9.8898e-01, 1.0262e-06, 1.8569e-07, 9.9870e-07, 2.3019e-07,
        5.3482e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.472

[Epoch: 95, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.9710, 0.0048, 0.0045, 0.0044, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 95, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0264, 0.0050, 0.9489, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 95, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.0152, 0.9588, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 95, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.9717, 0.0047, 0.0047, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 96, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.1824e-03, 9.8967e-01, 9.4262e-07, 1.7173e-07, 8.7737e-07, 2.0364e-07,
        5.1456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.484

[Epoch: 96, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.9703, 0.0048, 0.0046, 0.0046, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 96, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0258, 0.0049, 0.9493, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 96, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0053, 0.0147, 0.9591, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 96, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.9706, 0.0050, 0.0049, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 97, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.5472e-03, 9.8932e-01, 9.8009e-07, 1.7210e-07, 9.4344e-07, 2.1179e-07,
        5.1261e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.472

[Epoch: 97, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9698, 0.0050, 0.0046, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 97, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0256, 0.0052, 0.9487, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 97, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0051, 0.0152, 0.9595, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 97, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.9696, 0.0049, 0.0051, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 98, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.9243e-03, 9.9013e-01, 8.1223e-07, 1.5221e-07, 7.7389e-07, 1.7072e-07,
        4.9413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.484

[Epoch: 98, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.9690, 0.0052, 0.0047, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 98, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0053, 0.0253, 0.0052, 0.9490, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 98, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0051, 0.0148, 0.9595, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 98, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.9687, 0.0052, 0.0052, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 99, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([5.3758e-03, 9.8963e-01, 9.2162e-07, 1.6915e-07, 8.6461e-07, 2.0346e-07,
        4.9955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.472

[Epoch: 99, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.9690, 0.0051, 0.0047, 0.0049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 99, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0055, 0.0246, 0.0056, 0.9483, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 99, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.0051, 0.0152, 0.9598, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 99, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.9677, 0.0053, 0.0053, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 100, batch: 39/198] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9900, 0.0000, 0.0000, 0.0000, 0.0000, 0.0050])
Policy pred: tensor([4.7655e-03, 9.8960e-01, 6.0897e-07, 1.2807e-07, 6.6377e-07, 1.4328e-07,
        5.6303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.478 -0.483

[Epoch: 100, batch: 78/198] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0054, 0.9680, 0.0054, 0.0046, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 100, batch: 117/198] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0250, 0.0050, 0.9500, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.0245, 0.0052, 0.9486, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.005

[Epoch: 100, batch: 156/198] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0150, 0.9600, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0049, 0.0146, 0.9607, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.005

[Epoch: 100, batch: 195/198] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9680, 0.0053, 0.0055, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

