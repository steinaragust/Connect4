Training set samples: 6465
Batch size: 32
[Epoch: 1, batch: 40/203] total loss per batch: 0.998
Policy (actual, predicted): 0 6
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([4.9575e-01, 1.4130e-06, 7.4245e-06, 2.2861e-09, 1.6989e-08, 3.4665e-09,
        5.0424e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.531

[Epoch: 1, batch: 80/203] total loss per batch: 1.036
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0090, 0.0090, 0.0191, 0.9423, 0.0054, 0.0087, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.004

[Epoch: 1, batch: 120/203] total loss per batch: 1.065
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0188, 0.0677, 0.0679, 0.0596, 0.7174, 0.0254, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.025

[Epoch: 1, batch: 160/203] total loss per batch: 1.085
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0103, 0.0267, 0.0251, 0.8957, 0.0217, 0.0145, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.023

[Epoch: 1, batch: 200/203] total loss per batch: 1.055
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.4412e-02, 3.5384e-01, 3.1895e-02, 1.6367e-06, 4.5646e-01, 4.5962e-02,
        8.7432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.184

[Epoch: 2, batch: 40/203] total loss per batch: 0.755
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.6832e-01, 1.0870e-05, 2.2270e-05, 8.9015e-08, 2.1709e-07, 7.0357e-09,
        1.3164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.317

[Epoch: 2, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0050, 0.0072, 0.9734, 0.0026, 0.0046, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.001

[Epoch: 2, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0049, 0.0093, 0.0094, 0.9650, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.011

[Epoch: 2, batch: 160/203] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0068, 0.0221, 0.0200, 0.9212, 0.0153, 0.0119, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.010

[Epoch: 2, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([7.7357e-03, 6.5555e-02, 6.0125e-03, 1.1850e-06, 8.9789e-01, 5.4053e-03,
        1.7397e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 3, batch: 40/203] total loss per batch: 0.626
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([5.7646e-01, 6.0933e-06, 1.7081e-05, 3.4046e-08, 9.1737e-08, 2.4494e-08,
        4.2352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.384

[Epoch: 3, batch: 80/203] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0055, 0.0070, 0.9690, 0.0030, 0.0086, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.008

[Epoch: 3, batch: 120/203] total loss per batch: 0.641
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0025, 0.0041, 0.0063, 0.9806, 0.0037, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.072

[Epoch: 3, batch: 160/203] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0059, 0.0199, 0.0123, 0.9384, 0.0122, 0.0089, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.007

[Epoch: 3, batch: 200/203] total loss per batch: 0.640
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([6.4385e-03, 1.7370e-02, 3.4650e-03, 2.6245e-06, 9.5514e-01, 6.6389e-03,
        1.0946e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.145

[Epoch: 4, batch: 40/203] total loss per batch: 0.571
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.6565e-01, 6.0883e-06, 1.7555e-05, 1.4561e-08, 1.1829e-07, 1.9391e-08,
        1.3432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.415

[Epoch: 4, batch: 80/203] total loss per batch: 0.555
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0082, 0.0058, 0.9684, 0.0027, 0.0081, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 4, batch: 120/203] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0046, 0.0080, 0.0127, 0.9657, 0.0051, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.011

[Epoch: 4, batch: 160/203] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0157, 0.0147, 0.9468, 0.0093, 0.0064, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 4, batch: 200/203] total loss per batch: 0.599
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.9156e-03, 1.2707e-02, 1.9657e-03, 1.5117e-06, 9.6254e-01, 5.9785e-03,
        1.2890e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.148

[Epoch: 5, batch: 40/203] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3117e-01, 9.9166e-06, 2.5697e-05, 3.9675e-08, 1.1574e-07, 5.2505e-08,
        2.6880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.438

[Epoch: 5, batch: 80/203] total loss per batch: 0.539
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0073, 0.0054, 0.9674, 0.0030, 0.0086, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 5, batch: 120/203] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0045, 0.0113, 0.0075, 0.9684, 0.0042, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.025

[Epoch: 5, batch: 160/203] total loss per batch: 0.561
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0187, 0.0095, 0.9534, 0.0065, 0.0063, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 5, batch: 200/203] total loss per batch: 0.581
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5434e-03, 2.0753e-02, 2.8173e-03, 3.2268e-06, 9.5752e-01, 4.5355e-03,
        1.0830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.129

[Epoch: 6, batch: 40/203] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.1911e-01, 3.5043e-06, 1.7255e-05, 3.1032e-08, 8.6507e-08, 5.7663e-08,
        1.8087e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.365

[Epoch: 6, batch: 80/203] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0067, 0.0043, 0.9715, 0.0027, 0.0067, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.011

[Epoch: 6, batch: 120/203] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.0028, 0.0064, 0.0083, 0.9760, 0.0032, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.017

[Epoch: 6, batch: 160/203] total loss per batch: 0.545
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0162, 0.0103, 0.9578, 0.0056, 0.0053, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 6, batch: 200/203] total loss per batch: 0.567
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.5820e-03, 1.5942e-02, 2.3641e-03, 1.6674e-06, 9.6487e-01, 4.3079e-03,
        7.9275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.147

[Epoch: 7, batch: 40/203] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.8084e-01, 4.6444e-06, 1.4979e-05, 1.0057e-08, 4.8327e-08, 1.7110e-08,
        3.1914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.360

[Epoch: 7, batch: 80/203] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0063, 0.0048, 0.9731, 0.0025, 0.0058, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 7, batch: 120/203] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0031, 0.0112, 0.0075, 0.9686, 0.0043, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.002

[Epoch: 7, batch: 160/203] total loss per batch: 0.532
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0128, 0.0088, 0.9642, 0.0047, 0.0042, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.000

[Epoch: 7, batch: 200/203] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.0186e-03, 9.4497e-03, 2.4945e-03, 2.2129e-06, 9.6941e-01, 4.1696e-03,
        9.4550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.148

[Epoch: 8, batch: 40/203] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([9.0999e-01, 2.0863e-06, 9.0959e-06, 1.1130e-08, 2.8993e-08, 7.2716e-09,
        8.9998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.271

[Epoch: 8, batch: 80/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0045, 0.0031, 0.9773, 0.0025, 0.0039, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 8, batch: 120/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0042, 0.0061, 0.0055, 0.9771, 0.0038, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.013

[Epoch: 8, batch: 160/203] total loss per batch: 0.523
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0128, 0.0073, 0.9674, 0.0038, 0.0037, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.003

[Epoch: 8, batch: 200/203] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4381e-03, 9.0168e-03, 2.1880e-03, 9.5551e-07, 9.7296e-01, 4.4213e-03,
        6.9699e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 9, batch: 40/203] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.4234e-01, 2.6501e-06, 8.3342e-06, 8.3756e-09, 1.7494e-08, 6.5789e-09,
        3.5765e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.305

[Epoch: 9, batch: 80/203] total loss per batch: 0.504
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0045, 0.0041, 0.9775, 0.0023, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 9, batch: 120/203] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.0020, 0.0046, 0.0050, 0.9820, 0.0022, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.008

[Epoch: 9, batch: 160/203] total loss per batch: 0.519
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0139, 0.0061, 0.9668, 0.0050, 0.0033, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 9, batch: 200/203] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.4962e-03, 8.4208e-03, 2.5585e-03, 1.8345e-06, 9.7259e-01, 4.1219e-03,
        6.8139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.149

[Epoch: 10, batch: 40/203] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0696e-01, 2.1312e-06, 6.4055e-06, 4.2549e-09, 1.3998e-08, 1.2126e-08,
        1.9303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.295

[Epoch: 10, batch: 80/203] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.0040, 0.0030, 0.9762, 0.0023, 0.0058, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 10, batch: 120/203] total loss per batch: 0.527
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.0030, 0.0049, 0.0038, 0.9802, 0.0042, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.000

[Epoch: 10, batch: 160/203] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0108, 0.0066, 0.9714, 0.0035, 0.0033, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.000

[Epoch: 10, batch: 200/203] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([6.2730e-03, 9.6571e-03, 2.1929e-03, 2.8322e-06, 9.6780e-01, 6.5860e-03,
        7.4871e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.163

[Epoch: 11, batch: 40/203] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6820e-01, 2.9061e-06, 7.3328e-06, 1.1876e-08, 3.5840e-08, 1.2328e-08,
        2.3179e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.277

[Epoch: 11, batch: 80/203] total loss per batch: 0.500
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0052, 0.0042, 0.9750, 0.0022, 0.0050, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 11, batch: 120/203] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0022, 0.0046, 0.0039, 0.9843, 0.0019, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.003

[Epoch: 11, batch: 160/203] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0100, 0.0071, 0.9688, 0.0052, 0.0039, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.001

[Epoch: 11, batch: 200/203] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.6274e-03, 5.9956e-03, 1.1837e-03, 4.1085e-07, 9.8040e-01, 2.7198e-03,
        5.0694e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.166

[Epoch: 12, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6651e-01, 1.7965e-06, 5.3866e-06, 6.2923e-09, 1.4378e-08, 8.9487e-09,
        2.3348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.246

[Epoch: 12, batch: 80/203] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0041, 0.0029, 0.9762, 0.0024, 0.0061, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 12, batch: 120/203] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0022, 0.0066, 0.0032, 0.9813, 0.0033, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.014

[Epoch: 12, batch: 160/203] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0088, 0.0046, 0.9743, 0.0032, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 12, batch: 200/203] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.6847e-03, 7.2052e-03, 1.9712e-03, 2.3849e-06, 9.7195e-01, 6.4811e-03,
        6.7098e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.164

[Epoch: 13, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9991e-01, 1.5469e-06, 6.6421e-06, 4.7380e-09, 1.5411e-08, 6.7757e-09,
        2.0008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.263

[Epoch: 13, batch: 80/203] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.0026, 0.9796, 0.0020, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 13, batch: 120/203] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0027, 0.0045, 0.0040, 0.9813, 0.0033, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.017

[Epoch: 13, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0075, 0.0050, 0.9734, 0.0053, 0.0037, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.003

[Epoch: 13, batch: 200/203] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.7520e-03, 9.7148e-03, 1.4264e-03, 1.0592e-06, 9.7263e-01, 4.5055e-03,
        5.9675e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.161

[Epoch: 14, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3513e-01, 2.7710e-06, 9.1679e-06, 6.2931e-09, 2.5596e-08, 1.3220e-08,
        2.6486e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.328

[Epoch: 14, batch: 80/203] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0031, 0.0029, 0.9796, 0.0030, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 14, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0034, 0.0068, 0.0066, 0.9746, 0.0037, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.024

[Epoch: 14, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0081, 0.0044, 0.9746, 0.0045, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.004

[Epoch: 14, batch: 200/203] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.8748e-03, 9.6752e-03, 1.9386e-03, 1.3534e-06, 9.7218e-01, 5.0234e-03,
        5.3048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 15, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.5536e-01, 1.0660e-06, 7.3332e-06, 2.6588e-09, 1.4192e-08, 6.1244e-09,
        1.4463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.267

[Epoch: 15, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0037, 0.0023, 0.9797, 0.0021, 0.0044, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 15, batch: 120/203] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0045, 0.0038, 0.0033, 0.9821, 0.0024, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.009

[Epoch: 15, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0129, 0.0040, 0.9692, 0.0040, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 15, batch: 200/203] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([7.5760e-03, 4.8318e-03, 2.0095e-03, 1.6322e-06, 9.7056e-01, 7.0397e-03,
        7.9857e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.135

[Epoch: 16, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.5662e-01, 1.5328e-06, 1.2563e-05, 6.1872e-09, 2.6442e-08, 1.3443e-08,
        3.4337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.324

[Epoch: 16, batch: 80/203] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0041, 0.0033, 0.9772, 0.0024, 0.0041, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 16, batch: 120/203] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0043, 0.0058, 0.0061, 0.9769, 0.0026, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.044

[Epoch: 16, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0060, 0.0044, 0.9771, 0.0040, 0.0041, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.003

[Epoch: 16, batch: 200/203] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.8885e-03, 1.0583e-02, 1.8665e-03, 1.6122e-06, 9.7256e-01, 5.6213e-03,
        4.4792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.216

[Epoch: 17, batch: 40/203] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.5160e-01, 1.6761e-06, 6.3097e-06, 4.3902e-09, 1.2422e-08, 7.5222e-09,
        1.4839e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.236

[Epoch: 17, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0027, 0.0023, 0.9788, 0.0030, 0.0051, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 17, batch: 120/203] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.0020, 0.0052, 0.0030, 0.9831, 0.0024, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.024

[Epoch: 17, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0096, 0.0039, 0.9721, 0.0038, 0.0047, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 17, batch: 200/203] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.9756e-03, 5.1785e-03, 2.4469e-03, 1.0259e-06, 9.7753e-01, 5.0017e-03,
        3.8712e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.205

[Epoch: 18, batch: 40/203] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.1865e-01, 1.2187e-06, 5.5293e-06, 3.7530e-09, 1.4177e-08, 6.7683e-09,
        1.8134e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.356

[Epoch: 18, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0024, 0.0029, 0.9808, 0.0029, 0.0043, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 18, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0039, 0.0077, 0.0055, 0.9754, 0.0037, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 18, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0074, 0.0042, 0.9753, 0.0035, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 18, batch: 200/203] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.5744e-03, 4.7462e-03, 1.4158e-03, 1.0012e-06, 9.7791e-01, 4.7923e-03,
        5.5617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.179

[Epoch: 19, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7553e-01, 8.4111e-07, 7.9284e-06, 7.1105e-09, 2.8709e-08, 8.5190e-09,
        2.2446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.171

[Epoch: 19, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0042, 0.0034, 0.9774, 0.0034, 0.0044, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 19, batch: 120/203] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.0024, 0.0040, 0.0030, 0.9845, 0.0022, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 19, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0045, 0.0040, 0.9797, 0.0039, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.003

[Epoch: 19, batch: 200/203] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.3187e-03, 9.2731e-03, 4.2863e-03, 1.6319e-06, 9.6622e-01, 7.6420e-03,
        7.2617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.163

[Epoch: 20, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0061e-01, 9.4005e-07, 6.1453e-06, 3.6884e-09, 1.7970e-08, 1.3980e-08,
        1.9938e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.357

[Epoch: 20, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0021, 0.0021, 0.9825, 0.0025, 0.0051, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 20, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0031, 0.0026, 0.0038, 0.9827, 0.0042, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 20, batch: 160/203] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0088, 0.0034, 0.9735, 0.0034, 0.0054, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 20, batch: 200/203] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.5031e-03, 5.2993e-03, 1.9496e-03, 7.2609e-07, 9.7780e-01, 4.2117e-03,
        6.2354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.185

[Epoch: 21, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.6650e-01, 1.1516e-06, 4.7348e-06, 5.1038e-09, 2.4372e-08, 2.3257e-08,
        3.3350e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 21, batch: 80/203] total loss per batch: 0.497
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0034, 0.9796, 0.0026, 0.0040, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 21, batch: 120/203] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0021, 0.0038, 0.0037, 0.9850, 0.0022, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.065

[Epoch: 21, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0055, 0.0036, 0.9792, 0.0028, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.006

[Epoch: 21, batch: 200/203] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.0809e-03, 4.0702e-03, 2.0704e-03, 8.6694e-07, 9.7921e-01, 7.7834e-03,
        2.7814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.206

[Epoch: 22, batch: 40/203] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.8835e-01, 1.2072e-06, 4.5076e-06, 2.3702e-09, 2.2543e-08, 2.0302e-08,
        1.1165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.217

[Epoch: 22, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0020, 0.0025, 0.9827, 0.0031, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 22, batch: 120/203] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0022, 0.0037, 0.0032, 0.9843, 0.0033, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.027

[Epoch: 22, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0056, 0.0045, 0.9749, 0.0048, 0.0039, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.000

[Epoch: 22, batch: 200/203] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([7.9194e-03, 7.5370e-03, 2.6910e-03, 1.3713e-06, 9.6911e-01, 4.2381e-03,
        8.5037e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.156

[Epoch: 23, batch: 40/203] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.6009e-01, 9.1419e-07, 3.1097e-06, 5.0318e-09, 1.8742e-08, 1.6834e-08,
        3.3990e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.322

[Epoch: 23, batch: 80/203] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0022, 0.0025, 0.9810, 0.0029, 0.0049, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 23, batch: 120/203] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0034, 0.0044, 0.0077, 0.9760, 0.0051, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.035

[Epoch: 23, batch: 160/203] total loss per batch: 0.519
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0048, 0.0029, 0.9775, 0.0031, 0.0043, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.006

[Epoch: 23, batch: 200/203] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5869e-03, 7.5552e-03, 2.1231e-03, 4.6767e-07, 9.7779e-01, 5.7111e-03,
        4.2287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 24, batch: 40/203] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.3152e-01, 1.4495e-06, 6.8046e-06, 2.2330e-08, 1.3079e-08, 3.7811e-08,
        1.6847e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.243

[Epoch: 24, batch: 80/203] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0023, 0.0033, 0.9770, 0.0033, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 24, batch: 120/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0062, 0.0057, 0.0038, 0.9778, 0.0020, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 24, batch: 160/203] total loss per batch: 0.572
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0037, 0.0033, 0.9836, 0.0027, 0.0026, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.038

[Epoch: 24, batch: 200/203] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([9.7258e-03, 1.0815e-01, 3.0287e-03, 6.5824e-06, 8.5797e-01, 1.4287e-02,
        6.8320e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.195

[Epoch: 25, batch: 40/203] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.4473e-01, 1.6066e-05, 7.9054e-06, 4.3330e-09, 9.3558e-08, 9.2169e-08,
        1.5525e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.486

[Epoch: 25, batch: 80/203] total loss per batch: 0.575
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0022, 0.0026, 0.9824, 0.0015, 0.0045, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.019

[Epoch: 25, batch: 120/203] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([9.3336e-04, 1.1068e-03, 3.2466e-03, 1.6761e-03, 9.8977e-01, 2.7802e-03,
        4.8483e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.040

[Epoch: 25, batch: 160/203] total loss per batch: 0.588
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0030, 0.0051, 0.9759, 0.0038, 0.0065, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.011

[Epoch: 25, batch: 200/203] total loss per batch: 0.615
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4909e-03, 2.7614e-03, 1.3145e-02, 1.6643e-05, 9.2231e-01, 3.0611e-02,
        2.6666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.189

[Epoch: 26, batch: 40/203] total loss per batch: 0.576
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.8230e-01, 3.9737e-06, 9.2756e-06, 1.0110e-08, 1.9110e-08, 1.6776e-07,
        3.1769e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.309

[Epoch: 26, batch: 80/203] total loss per batch: 0.559
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.0077, 0.9732, 0.0019, 0.0075, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.007

[Epoch: 26, batch: 120/203] total loss per batch: 0.576
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0132, 0.0059, 0.0032, 0.9694, 0.0039, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 26, batch: 160/203] total loss per batch: 0.550
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0086, 0.0089, 0.9676, 0.0040, 0.0063, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.001

[Epoch: 26, batch: 200/203] total loss per batch: 0.570
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7585e-03, 4.7503e-03, 4.6013e-03, 9.6062e-07, 9.5573e-01, 1.0074e-02,
        2.2084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.132

[Epoch: 27, batch: 40/203] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7532e-01, 1.0301e-06, 5.2960e-06, 1.7683e-09, 4.2476e-09, 3.7594e-09,
        2.2467e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.223

[Epoch: 27, batch: 80/203] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0023, 0.0028, 0.9818, 0.0015, 0.0063, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 27, batch: 120/203] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.0045, 0.0058, 0.0038, 0.9757, 0.0034, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.070

[Epoch: 27, batch: 160/203] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0072, 0.0048, 0.9733, 0.0040, 0.0049, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.010

[Epoch: 27, batch: 200/203] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7986e-03, 3.4651e-03, 2.8951e-03, 1.9111e-06, 9.7009e-01, 5.4977e-03,
        1.4252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.151

[Epoch: 28, batch: 40/203] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9422e-01, 1.4525e-06, 6.5037e-06, 1.7176e-09, 5.2101e-09, 7.6095e-09,
        2.0578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.242

[Epoch: 28, batch: 80/203] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0017, 0.0031, 0.9812, 0.0023, 0.0052, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.013

[Epoch: 28, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0040, 0.0046, 0.0033, 0.9811, 0.0029, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 28, batch: 160/203] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0099, 0.0066, 0.9683, 0.0041, 0.0052, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 28, batch: 200/203] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9371e-03, 3.1790e-03, 3.0618e-03, 8.0922e-07, 9.6950e-01, 9.7028e-03,
        1.1621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.165

[Epoch: 29, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3901e-01, 1.5025e-06, 4.9900e-06, 1.5252e-09, 3.7758e-09, 4.6207e-09,
        2.6099e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.285

[Epoch: 29, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0016, 0.0027, 0.9838, 0.0017, 0.0048, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 29, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0042, 0.0045, 0.0037, 0.9800, 0.0030, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.065

[Epoch: 29, batch: 160/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0081, 0.0056, 0.9715, 0.0043, 0.0047, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.002

[Epoch: 29, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9017e-03, 2.9075e-03, 2.4971e-03, 6.5073e-07, 9.7849e-01, 5.5310e-03,
        7.6745e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.173

[Epoch: 30, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9346e-01, 1.2943e-06, 5.4820e-06, 2.0929e-09, 3.9323e-09, 6.0734e-09,
        2.0653e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.261

[Epoch: 30, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0018, 0.0032, 0.9817, 0.0019, 0.0056, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 30, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0047, 0.0045, 0.0029, 0.9816, 0.0024, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.059

[Epoch: 30, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0068, 0.0049, 0.9746, 0.0039, 0.0041, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 30, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3808e-03, 2.9037e-03, 2.9374e-03, 5.1228e-07, 9.7806e-01, 4.6263e-03,
        8.0870e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 31, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6336e-01, 1.1003e-06, 4.4019e-06, 1.1093e-09, 3.1199e-09, 4.0742e-09,
        2.3664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.294

[Epoch: 31, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0017, 0.0029, 0.9825, 0.0021, 0.0049, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 31, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0039, 0.0040, 0.0029, 0.9823, 0.0027, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 31, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0064, 0.0049, 0.9757, 0.0035, 0.0038, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 31, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1237e-03, 2.9876e-03, 3.0438e-03, 4.6813e-07, 9.7999e-01, 4.8841e-03,
        5.9748e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.177

[Epoch: 32, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8158e-01, 1.0827e-06, 4.0992e-06, 1.5642e-09, 3.4359e-09, 4.4526e-09,
        2.1841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.273

[Epoch: 32, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0018, 0.0030, 0.9827, 0.0019, 0.0050, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.019

[Epoch: 32, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0046, 0.0062, 0.0032, 0.9791, 0.0024, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.066

[Epoch: 32, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0059, 0.0042, 0.9769, 0.0036, 0.0037, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 32, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2865e-03, 3.0983e-03, 3.1910e-03, 3.7836e-07, 9.8055e-01, 3.5489e-03,
        6.3204e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.185

[Epoch: 33, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6683e-01, 8.8991e-07, 3.9985e-06, 9.8600e-10, 2.3849e-09, 3.0036e-09,
        2.3317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.309

[Epoch: 33, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0017, 0.0031, 0.9821, 0.0021, 0.0048, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 33, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0037, 0.0038, 0.0028, 0.9831, 0.0027, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 33, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0056, 0.0037, 0.9792, 0.0029, 0.0034, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 33, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7404e-03, 3.0793e-03, 2.4320e-03, 2.5702e-07, 9.8289e-01, 4.3134e-03,
        4.5400e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.183

[Epoch: 34, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7310e-01, 7.2072e-07, 2.6798e-06, 7.7287e-10, 1.9874e-09, 2.0828e-09,
        2.2690e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.256

[Epoch: 34, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0018, 0.0028, 0.9826, 0.0020, 0.0047, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 34, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0040, 0.0046, 0.0032, 0.9802, 0.0027, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 34, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0056, 0.0039, 0.9766, 0.0038, 0.0038, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 34, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2343e-03, 3.5057e-03, 3.0089e-03, 3.8735e-07, 9.7789e-01, 4.3269e-03,
        8.0313e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.200

[Epoch: 35, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6930e-01, 7.3375e-07, 3.9315e-06, 6.6915e-10, 1.7498e-09, 1.8803e-09,
        2.3069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.323

[Epoch: 35, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0017, 0.0033, 0.9827, 0.0021, 0.0047, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 35, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0030, 0.0034, 0.0029, 0.9838, 0.0030, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.070

[Epoch: 35, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0048, 0.0038, 0.9795, 0.0027, 0.0035, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 35, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0239e-03, 3.2278e-03, 3.1141e-03, 1.6565e-07, 9.8275e-01, 4.0281e-03,
        3.8525e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.185

[Epoch: 36, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8100e-01, 9.3726e-07, 3.7872e-06, 1.5925e-09, 4.1768e-09, 6.0926e-09,
        2.1899e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.257

[Epoch: 36, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0015, 0.0033, 0.9819, 0.0024, 0.0046, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 36, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0073, 0.0061, 0.0052, 0.9692, 0.0043, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.025

[Epoch: 36, batch: 160/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0052, 0.0034, 0.9786, 0.0034, 0.0036, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 36, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4743e-03, 3.6528e-03, 5.3201e-03, 3.5631e-07, 9.7725e-01, 3.8199e-03,
        6.4855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.177

[Epoch: 37, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6304e-01, 5.0192e-07, 2.5915e-06, 3.9549e-10, 1.3817e-09, 1.3084e-09,
        2.3696e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.300

[Epoch: 37, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0020, 0.0033, 0.9816, 0.0023, 0.0051, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 37, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.0024, 0.0033, 0.0033, 0.9846, 0.0025, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.064

[Epoch: 37, batch: 160/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0042, 0.0038, 0.9792, 0.0024, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 37, batch: 200/203] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2044e-03, 3.8635e-03, 3.5911e-03, 1.4593e-07, 9.7991e-01, 4.4244e-03,
        5.0104e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.206

[Epoch: 38, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8684e-01, 8.3506e-07, 4.3988e-06, 1.1380e-09, 4.1338e-09, 4.6206e-09,
        2.1315e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.314

[Epoch: 38, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0017, 0.0026, 0.9831, 0.0023, 0.0043, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 38, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0043, 0.0032, 0.0040, 0.9801, 0.0028, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 38, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.0074, 0.0048, 0.9717, 0.0050, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 38, batch: 200/203] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.8322e-03, 2.8318e-03, 3.8899e-03, 5.3399e-07, 9.8017e-01, 4.7427e-03,
        4.5303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.164

[Epoch: 39, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6567e-01, 7.0006e-07, 3.3525e-06, 9.1213e-10, 3.7379e-09, 3.5367e-09,
        2.3433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.259

[Epoch: 39, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0019, 0.0046, 0.9822, 0.0023, 0.0037, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 39, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0035, 0.0051, 0.0033, 0.9799, 0.0035, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.087

[Epoch: 39, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0057, 0.0029, 0.9770, 0.0034, 0.0045, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 39, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3175e-03, 5.2295e-03, 4.1983e-03, 4.4546e-07, 9.7997e-01, 3.2237e-03,
        4.0615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.201

[Epoch: 40, batch: 40/203] total loss per batch: 0.505
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9273e-01, 9.8497e-07, 3.0506e-06, 8.9445e-10, 2.9578e-09, 8.4578e-09,
        2.0727e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.290

[Epoch: 40, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0026, 0.0037, 0.9783, 0.0038, 0.0039, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 40, batch: 120/203] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0056, 0.0039, 0.0047, 0.9782, 0.0026, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.064

[Epoch: 40, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0087, 0.0067, 0.9695, 0.0030, 0.0046, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 40, batch: 200/203] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1105e-03, 4.3518e-03, 3.2666e-03, 7.3195e-07, 9.7983e-01, 4.3208e-03,
        5.1209e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.144

[Epoch: 41, batch: 40/203] total loss per batch: 0.505
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5799e-01, 1.1809e-06, 3.5366e-06, 6.7878e-10, 1.7461e-09, 1.6231e-09,
        2.4201e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.299

[Epoch: 41, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0020, 0.0030, 0.9805, 0.0032, 0.0046, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 41, batch: 120/203] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0059, 0.0086, 0.0051, 0.9658, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.000

[Epoch: 41, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0046, 0.0043, 0.9792, 0.0038, 0.0024, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 41, batch: 200/203] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4251e-03, 3.4269e-03, 4.2673e-03, 4.0616e-07, 9.7898e-01, 5.2783e-03,
        4.6253e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.202

[Epoch: 42, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6613e-01, 4.9649e-07, 2.7360e-06, 1.3304e-09, 5.9303e-09, 5.7370e-09,
        2.3387e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 42, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0019, 0.0026, 0.9847, 0.0023, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.018

[Epoch: 42, batch: 120/203] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.0022, 0.0017, 0.0028, 0.9866, 0.0029, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.117

[Epoch: 42, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0047, 0.0091, 0.0039, 0.9692, 0.0035, 0.0057, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 42, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([6.8967e-03, 7.8603e-03, 4.6639e-03, 9.5820e-07, 9.6607e-01, 3.5197e-03,
        1.0992e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 43, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0554e-01, 1.6109e-06, 3.7597e-06, 2.0244e-09, 9.7856e-09, 2.9245e-08,
        1.9446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.231

[Epoch: 43, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0025, 0.0035, 0.9804, 0.0024, 0.0049, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 43, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0042, 0.0055, 0.0038, 0.9764, 0.0043, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.026

[Epoch: 43, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0063, 0.0036, 0.9794, 0.0027, 0.0026, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 43, batch: 200/203] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.8797e-03, 5.1057e-03, 3.5371e-03, 5.3491e-07, 9.8197e-01, 5.5425e-03,
        1.9608e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.216

[Epoch: 44, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.1399e-01, 7.2482e-07, 3.2768e-06, 3.5484e-10, 2.5126e-09, 6.5610e-09,
        2.8601e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.389

[Epoch: 44, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0020, 0.0031, 0.9822, 0.0027, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 44, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0048, 0.0075, 0.0063, 0.9713, 0.0032, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.066

[Epoch: 44, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0049, 0.0032, 0.9798, 0.0033, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 44, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4287e-03, 3.9398e-03, 3.5350e-03, 9.0739e-07, 9.7881e-01, 5.1980e-03,
        5.0835e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.204

[Epoch: 45, batch: 40/203] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0806e-01, 7.4225e-07, 2.4445e-06, 7.4107e-10, 5.5397e-09, 3.8575e-09,
        1.9194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 45, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0026, 0.0038, 0.9778, 0.0035, 0.0048, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 45, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0022, 0.0042, 0.0038, 0.9816, 0.0024, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.043

[Epoch: 45, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0099, 0.0045, 0.9715, 0.0030, 0.0044, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 45, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.2068e-03, 6.3936e-03, 3.9945e-03, 1.1975e-06, 9.7793e-01, 4.7706e-03,
        4.7061e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 46, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5814e-01, 1.8106e-06, 4.1197e-06, 8.6198e-10, 1.1581e-08, 1.9361e-08,
        2.4185e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.289

[Epoch: 46, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0016, 0.0024, 0.9831, 0.0025, 0.0042, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 46, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0031, 0.0025, 0.0032, 0.9831, 0.0028, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.033

[Epoch: 46, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0049, 0.0026, 0.9784, 0.0019, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 46, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7359e-03, 3.5354e-03, 4.5311e-03, 4.7400e-07, 9.8150e-01, 4.5133e-03,
        3.1875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.213

[Epoch: 47, batch: 40/203] total loss per batch: 0.505
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8234e-01, 1.0958e-06, 3.0454e-06, 8.1141e-10, 1.0070e-08, 7.3959e-09,
        2.1765e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.240

[Epoch: 47, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0031, 0.0028, 0.9814, 0.0027, 0.0045, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 47, batch: 120/203] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0061, 0.0039, 0.0083, 0.9712, 0.0043, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.094

[Epoch: 47, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0041, 0.0041, 0.9791, 0.0037, 0.0036, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.001

[Epoch: 47, batch: 200/203] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4078e-03, 6.2232e-03, 8.6023e-03, 7.3000e-07, 9.6855e-01, 8.5901e-03,
        3.6299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.152

[Epoch: 48, batch: 40/203] total loss per batch: 0.507
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.4918e-01, 1.0727e-06, 3.1532e-06, 3.5538e-10, 7.3673e-09, 8.3576e-09,
        2.5082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.254

[Epoch: 48, batch: 80/203] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0021, 0.0024, 0.9845, 0.0021, 0.0025, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 48, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0040, 0.0046, 0.0045, 0.9809, 0.0018, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.070

[Epoch: 48, batch: 160/203] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0039, 0.0020, 0.9832, 0.0030, 0.0023, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 48, batch: 200/203] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.9210e-03, 1.9105e-03, 1.9201e-03, 5.1539e-07, 9.8717e-01, 2.3158e-03,
        4.7575e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.215

[Epoch: 49, batch: 40/203] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.0918e-01, 1.4838e-06, 2.1220e-06, 5.3011e-10, 4.6576e-09, 2.3413e-09,
        3.9082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.382

[Epoch: 49, batch: 80/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0055, 0.0044, 0.0046, 0.9745, 0.0029, 0.0048, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 49, batch: 120/203] total loss per batch: 0.554
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0010, 0.0037, 0.0085, 0.0067, 0.9775, 0.0014, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.071

[Epoch: 49, batch: 160/203] total loss per batch: 0.544
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0056, 0.0062, 0.9781, 0.0030, 0.0012, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.001

[Epoch: 49, batch: 200/203] total loss per batch: 0.578
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1055e-03, 2.9976e-03, 3.8522e-03, 3.9622e-07, 8.7124e-01, 2.7511e-02,
        9.1292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.129

[Epoch: 50, batch: 40/203] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.2227e-01, 1.7243e-06, 1.4805e-06, 2.7997e-09, 3.2007e-08, 8.5076e-08,
        2.7773e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.318

[Epoch: 50, batch: 80/203] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0025, 0.0075, 0.9741, 0.0040, 0.0040, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.062

[Epoch: 50, batch: 120/203] total loss per batch: 0.569
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.0021, 0.0044, 0.0048, 0.9843, 0.0016, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.103

[Epoch: 50, batch: 160/203] total loss per batch: 0.544
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0223, 0.0051, 0.9599, 0.0041, 0.0042, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 50, batch: 200/203] total loss per batch: 0.565
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([6.3068e-03, 1.0517e-02, 1.7364e-03, 9.2414e-07, 9.7259e-01, 7.7397e-03,
        1.1047e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.111

[Epoch: 51, batch: 40/203] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.5011e-01, 6.1581e-07, 4.0797e-06, 2.6688e-09, 5.6654e-08, 1.4201e-07,
        1.4988e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.312

[Epoch: 51, batch: 80/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0040, 0.0026, 0.9814, 0.0028, 0.0028, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 51, batch: 120/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0110, 0.0077, 0.0039, 0.9710, 0.0035, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 51, batch: 160/203] total loss per batch: 0.523
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0099, 0.0070, 0.9703, 0.0036, 0.0031, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 51, batch: 200/203] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4369e-03, 7.0486e-03, 1.6228e-03, 5.9572e-07, 9.8374e-01, 2.2150e-03,
        9.3185e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.165

[Epoch: 52, batch: 40/203] total loss per batch: 0.508
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9696e-01, 2.8244e-07, 2.4094e-06, 1.8447e-09, 2.6504e-08, 1.6121e-08,
        2.0303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.234

[Epoch: 52, batch: 80/203] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0025, 0.9815, 0.0033, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 52, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.0087, 0.0055, 0.0050, 0.9737, 0.0035, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 52, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0129, 0.0066, 0.9679, 0.0040, 0.0028, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 52, batch: 200/203] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.0537e-03, 8.1160e-03, 1.5349e-03, 1.0822e-06, 9.8130e-01, 3.8600e-03,
        1.1365e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.160

[Epoch: 53, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9123e-01, 2.9517e-07, 2.2098e-06, 1.5404e-09, 2.9391e-08, 1.3259e-08,
        2.0876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.272

[Epoch: 53, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0025, 0.9815, 0.0033, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 53, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.0065, 0.0047, 0.0039, 0.9782, 0.0034, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 53, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0103, 0.0052, 0.9728, 0.0037, 0.0024, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 53, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.1598e-03, 6.9900e-03, 1.3725e-03, 5.1873e-07, 9.8295e-01, 3.3771e-03,
        1.1466e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 54, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7376e-01, 1.8431e-07, 1.7542e-06, 6.1777e-10, 1.4536e-08, 4.2775e-09,
        2.2623e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.251

[Epoch: 54, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0025, 0.9814, 0.0033, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 54, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0056, 0.0048, 0.0040, 0.9788, 0.0031, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 54, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0097, 0.0048, 0.9741, 0.0037, 0.0024, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 54, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.8436e-03, 6.0541e-03, 1.4241e-03, 5.3803e-07, 9.8391e-01, 3.5424e-03,
        1.2213e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 55, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6925e-01, 2.2849e-07, 2.1654e-06, 9.8973e-10, 2.1793e-08, 7.0011e-09,
        2.3075e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 55, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0024, 0.9815, 0.0033, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 55, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0051, 0.0042, 0.0039, 0.9802, 0.0031, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.059

[Epoch: 55, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0085, 0.0043, 0.9757, 0.0036, 0.0025, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 55, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7783e-03, 5.7870e-03, 1.3710e-03, 5.2159e-07, 9.8413e-01, 3.5987e-03,
        1.3364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 56, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8053e-01, 1.6823e-07, 1.5601e-06, 6.3538e-10, 1.4283e-08, 5.1976e-09,
        2.1947e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.268

[Epoch: 56, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0031, 0.0026, 0.9808, 0.0033, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 56, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0049, 0.0046, 0.0039, 0.9795, 0.0031, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.059

[Epoch: 56, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0080, 0.0044, 0.9764, 0.0035, 0.0025, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 56, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5464e-03, 5.2082e-03, 1.4906e-03, 5.4766e-07, 9.8492e-01, 3.5180e-03,
        1.3117e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.196

[Epoch: 57, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6891e-01, 1.8280e-07, 1.7382e-06, 6.5377e-10, 1.5632e-08, 4.4572e-09,
        2.3109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.292

[Epoch: 57, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0027, 0.9807, 0.0034, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 57, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0051, 0.0042, 0.0039, 0.9796, 0.0032, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 57, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0072, 0.0038, 0.9780, 0.0032, 0.0025, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 57, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7619e-03, 5.2889e-03, 1.5077e-03, 4.7114e-07, 9.8394e-01, 3.9969e-03,
        1.5043e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.199

[Epoch: 58, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8037e-01, 1.4250e-07, 1.1850e-06, 4.4200e-10, 1.1493e-08, 4.1090e-09,
        2.1963e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.274

[Epoch: 58, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0026, 0.9810, 0.0033, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 58, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0043, 0.0045, 0.0038, 0.9801, 0.0030, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 58, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0069, 0.0041, 0.9770, 0.0036, 0.0027, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 58, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.6685e-03, 4.6692e-03, 1.5317e-03, 5.0754e-07, 9.8463e-01, 3.9001e-03,
        1.5951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.195

[Epoch: 59, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7351e-01, 1.5110e-07, 1.5182e-06, 5.5203e-10, 1.3325e-08, 3.7053e-09,
        2.2648e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.289

[Epoch: 59, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0028, 0.9802, 0.0034, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 59, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0041, 0.0039, 0.0037, 0.9806, 0.0033, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 59, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0067, 0.0034, 0.9789, 0.0031, 0.0025, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 59, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5727e-03, 3.8857e-03, 1.5462e-03, 3.6408e-07, 9.8570e-01, 3.6524e-03,
        1.6385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.195

[Epoch: 60, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6355e-01, 1.2160e-07, 9.8188e-07, 3.5927e-10, 1.0666e-08, 3.9445e-09,
        2.3645e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.279

[Epoch: 60, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0026, 0.9811, 0.0032, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 60, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0043, 0.0042, 0.0036, 0.9800, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 60, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0060, 0.0040, 0.9774, 0.0038, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 60, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5125e-03, 4.2720e-03, 1.3814e-03, 3.9086e-07, 9.8501e-01, 3.9819e-03,
        1.8452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.185

[Epoch: 61, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8869e-01, 1.4032e-07, 1.3386e-06, 5.0910e-10, 1.0052e-08, 3.1402e-09,
        2.1131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.291

[Epoch: 61, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0031, 0.9802, 0.0036, 0.0032, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 61, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0037, 0.0041, 0.0037, 0.9802, 0.0038, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 61, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0068, 0.0039, 0.9771, 0.0034, 0.0026, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 61, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.6740e-03, 3.2660e-03, 1.9868e-03, 2.8939e-07, 9.8562e-01, 3.5493e-03,
        1.9002e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.191

[Epoch: 62, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6607e-01, 8.9032e-08, 6.0416e-07, 2.0317e-10, 7.8947e-09, 1.7738e-09,
        2.3393e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.286

[Epoch: 62, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0026, 0.9800, 0.0033, 0.0036, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 62, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0045, 0.0041, 0.0035, 0.9792, 0.0034, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 62, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0062, 0.0042, 0.9763, 0.0039, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 62, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.2625e-03, 4.7431e-03, 2.4776e-03, 4.0294e-07, 9.8194e-01, 4.5795e-03,
        1.9958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 63, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7172e-01, 1.0536e-07, 1.1640e-06, 3.5594e-10, 1.0518e-08, 3.4853e-09,
        2.2828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.294

[Epoch: 63, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0024, 0.0032, 0.9814, 0.0033, 0.0029, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 63, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0030, 0.0034, 0.0050, 0.9795, 0.0038, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.059

[Epoch: 63, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0049, 0.0035, 0.9802, 0.0033, 0.0025, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 63, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.1654e-03, 2.5733e-03, 1.8475e-03, 5.8852e-07, 9.8638e-01, 3.2232e-03,
        1.8103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.181

[Epoch: 64, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9379e-01, 2.1218e-07, 1.4141e-06, 4.9071e-10, 1.2524e-08, 5.5771e-09,
        2.0621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 64, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0024, 0.9808, 0.0036, 0.0044, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 64, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0051, 0.0049, 0.0039, 0.9772, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.037

[Epoch: 64, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0063, 0.0033, 0.9771, 0.0038, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 64, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.9391e-03, 5.3221e-03, 2.0449e-03, 4.2614e-07, 9.8178e-01, 3.9680e-03,
        2.9455e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 65, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5950e-01, 1.1408e-07, 7.1362e-07, 2.5587e-10, 8.7008e-09, 2.8127e-09,
        2.4050e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.268

[Epoch: 65, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0022, 0.0039, 0.9779, 0.0039, 0.0031, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 65, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0029, 0.0034, 0.0041, 0.9794, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.065

[Epoch: 65, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0054, 0.0034, 0.9790, 0.0037, 0.0029, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 65, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.9489e-03, 4.4041e-03, 2.2820e-03, 6.3758e-07, 9.8114e-01, 4.3041e-03,
        2.9154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.166

[Epoch: 66, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9304e-01, 2.1272e-07, 1.0488e-06, 3.6870e-10, 9.1683e-09, 7.1464e-09,
        2.0696e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.303

[Epoch: 66, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0025, 0.0028, 0.9830, 0.0028, 0.0035, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 66, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0045, 0.0039, 0.0048, 0.9768, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 66, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0050, 0.0037, 0.9779, 0.0036, 0.0025, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 66, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.8423e-03, 4.8342e-03, 2.1508e-03, 3.7228e-07, 9.8252e-01, 3.5824e-03,
        3.0721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.223

[Epoch: 67, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3094e-01, 1.5379e-07, 8.9701e-07, 3.2033e-10, 9.0704e-09, 2.0181e-09,
        2.6905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 67, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0029, 0.0026, 0.9810, 0.0031, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 67, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0041, 0.0035, 0.0044, 0.9773, 0.0044, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 67, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0043, 0.0029, 0.9809, 0.0033, 0.0024, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.031

[Epoch: 67, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1092e-03, 2.8376e-03, 2.9484e-03, 1.5073e-06, 9.8279e-01, 5.4326e-03,
        2.8776e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.167

[Epoch: 68, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.2197e-01, 1.7643e-07, 9.6388e-07, 5.1786e-10, 1.5821e-08, 7.2103e-09,
        1.7802e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.301

[Epoch: 68, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0027, 0.0037, 0.9809, 0.0029, 0.0029, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 68, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0041, 0.0036, 0.0043, 0.9795, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.065

[Epoch: 68, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0053, 0.0044, 0.9758, 0.0050, 0.0027, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.003

[Epoch: 68, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.4950e-03, 3.7342e-03, 1.7585e-03, 2.9368e-07, 9.8255e-01, 3.0486e-03,
        3.4106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.219

[Epoch: 69, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3337e-01, 2.3248e-07, 9.3922e-07, 3.2989e-10, 1.1453e-08, 3.9516e-09,
        2.6663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 69, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0030, 0.0022, 0.9827, 0.0024, 0.0039, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 69, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0041, 0.0057, 0.9734, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 69, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0059, 0.0042, 0.9768, 0.0035, 0.0031, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 69, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3733e-03, 3.7252e-03, 2.3078e-03, 4.7602e-07, 9.8404e-01, 3.0996e-03,
        3.4546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.185

[Epoch: 70, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0397e-01, 2.3471e-07, 1.2036e-06, 3.6859e-10, 8.7557e-09, 5.6681e-09,
        1.9603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.302

[Epoch: 70, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0020, 0.0032, 0.9839, 0.0028, 0.0027, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 70, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0030, 0.0028, 0.0033, 0.9818, 0.0045, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.061

[Epoch: 70, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0039, 0.0047, 0.9800, 0.0025, 0.0026, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 70, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.9241e-03, 3.4559e-03, 2.7499e-03, 1.1818e-06, 9.7958e-01, 5.7753e-03,
        4.5151e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.179

[Epoch: 71, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5567e-01, 2.0968e-07, 8.5077e-07, 3.9439e-10, 1.1866e-08, 2.0195e-09,
        2.4433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.300

[Epoch: 71, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0026, 0.0032, 0.9807, 0.0032, 0.0041, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 71, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0029, 0.0064, 0.0040, 0.9759, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 71, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0050, 0.0052, 0.9743, 0.0052, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 71, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4942e-03, 4.6189e-03, 3.6861e-03, 5.0987e-07, 9.8113e-01, 3.8612e-03,
        3.2118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.170

[Epoch: 72, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8271e-01, 3.1024e-07, 1.3038e-06, 1.2094e-09, 2.9350e-08, 5.2394e-08,
        2.1729e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 72, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0036, 0.0028, 0.9777, 0.0040, 0.0047, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 72, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0037, 0.0045, 0.9779, 0.0041, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 72, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0053, 0.0036, 0.9785, 0.0037, 0.0024, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 72, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.6254e-03, 2.8364e-03, 3.9723e-03, 4.4796e-07, 9.8118e-01, 4.3629e-03,
        3.0267e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.225

[Epoch: 73, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9143e-01, 1.6409e-07, 7.3751e-07, 7.3520e-10, 1.3969e-08, 1.5852e-09,
        2.0857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 73, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0025, 0.0030, 0.9820, 0.0036, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 73, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0043, 0.0036, 0.0038, 0.9768, 0.0056, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.031

[Epoch: 73, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0049, 0.0063, 0.9753, 0.0037, 0.0023, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 73, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9909e-03, 4.7968e-03, 3.3070e-03, 5.6663e-07, 9.8056e-01, 3.9007e-03,
        4.4438e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.204

[Epoch: 74, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5564e-01, 1.6228e-07, 9.1933e-07, 6.8923e-10, 1.8993e-08, 1.0374e-08,
        2.4436e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 74, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0030, 0.9824, 0.0027, 0.0039, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 74, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0035, 0.0029, 0.0035, 0.9814, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.090

[Epoch: 74, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0054, 0.0055, 0.9751, 0.0045, 0.0027, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 74, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.9328e-03, 3.7996e-03, 4.0444e-03, 5.9612e-07, 9.8293e-01, 2.4202e-03,
        2.8724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.142

[Epoch: 75, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8766e-01, 2.7847e-07, 9.3109e-07, 6.5427e-10, 2.3739e-08, 8.7765e-09,
        2.1234e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.295

[Epoch: 75, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0038, 0.0033, 0.9777, 0.0042, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 75, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0020, 0.0030, 0.0029, 0.9843, 0.0024, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.034

[Epoch: 75, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.0041, 0.9810, 0.0033, 0.0022, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 75, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7574e-03, 2.8396e-03, 2.9588e-03, 1.0707e-06, 9.8359e-01, 3.9361e-03,
        2.9157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.218

[Epoch: 76, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7115e-01, 2.6119e-07, 8.3072e-07, 6.4995e-10, 1.3063e-08, 8.6453e-09,
        2.2885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.296

[Epoch: 76, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0027, 0.0039, 0.9788, 0.0040, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 76, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0033, 0.0046, 0.9788, 0.0034, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 76, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0053, 0.0038, 0.9793, 0.0031, 0.0019, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 76, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5613e-03, 1.3622e-02, 2.5566e-01, 5.9386e-06, 7.0733e-01, 1.7481e-02,
        3.3356e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 77, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5549e-01, 3.2599e-07, 8.6383e-07, 1.4439e-09, 3.7495e-08, 3.7394e-08,
        2.4451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.241

[Epoch: 77, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0016, 0.0021, 0.9855, 0.0027, 0.0034, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 77, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0019, 0.0023, 0.0037, 0.9836, 0.0035, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 77, batch: 160/203] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0088, 0.0056, 0.9703, 0.0059, 0.0034, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.003

[Epoch: 77, batch: 200/203] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.6581e-03, 2.5528e-02, 1.1074e-02, 3.8457e-06, 8.9230e-01, 8.0621e-03,
        5.7376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.124

[Epoch: 78, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0187e-01, 1.7311e-07, 1.2528e-06, 1.2655e-10, 4.1679e-09, 1.9183e-09,
        1.9813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.328

[Epoch: 78, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0031, 0.9795, 0.0039, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 78, batch: 120/203] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0020, 0.0023, 0.0029, 0.9811, 0.0019, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 78, batch: 160/203] total loss per batch: 0.519
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0045, 0.0083, 0.9735, 0.0035, 0.0039, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 78, batch: 200/203] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.8732e-03, 2.0391e-02, 1.3969e-02, 5.5246e-06, 9.3858e-01, 7.5189e-03,
        1.7662e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 79, batch: 40/203] total loss per batch: 0.506
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5672e-01, 2.9458e-07, 8.6509e-07, 5.3736e-10, 1.8237e-08, 4.8631e-09,
        2.4328e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.294

[Epoch: 79, batch: 80/203] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0056, 0.0035, 0.9754, 0.0051, 0.0035, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 79, batch: 120/203] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0028, 0.0023, 0.0042, 0.9799, 0.0031, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.028

[Epoch: 79, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0058, 0.0041, 0.9777, 0.0027, 0.0038, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 79, batch: 200/203] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7665e-03, 1.1289e-02, 6.0372e-03, 1.3610e-06, 9.5376e-01, 1.2115e-02,
        1.3033e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.178

[Epoch: 80, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0046e-01, 2.9453e-07, 1.3427e-06, 5.4162e-10, 7.3287e-09, 3.2416e-09,
        1.9954e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.268

[Epoch: 80, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0030, 0.0031, 0.9804, 0.0041, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 80, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0039, 0.0023, 0.0030, 0.9820, 0.0030, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.060

[Epoch: 80, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0050, 0.0047, 0.9782, 0.0030, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 80, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.6814e-03, 5.8963e-03, 4.1022e-03, 9.0887e-07, 9.7892e-01, 4.4576e-03,
        4.9451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.199

[Epoch: 81, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6106e-01, 4.4402e-07, 1.1875e-06, 7.1812e-10, 2.0642e-08, 6.4281e-09,
        2.3893e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.291

[Epoch: 81, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0029, 0.0033, 0.9821, 0.0029, 0.0027, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 81, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0032, 0.0030, 0.0034, 0.9795, 0.0031, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.062

[Epoch: 81, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0046, 0.0045, 0.9799, 0.0025, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 81, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.1635e-03, 5.9559e-03, 4.2716e-03, 6.6974e-07, 9.7357e-01, 6.4263e-03,
        7.6154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.184

[Epoch: 82, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5441e-01, 3.6706e-07, 8.5749e-07, 4.8360e-10, 1.1523e-08, 3.9217e-09,
        2.4559e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 82, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0032, 0.0027, 0.9832, 0.0025, 0.0032, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 82, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0035, 0.0025, 0.0039, 0.9801, 0.0033, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.077

[Epoch: 82, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0049, 0.0046, 0.9778, 0.0032, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 82, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.8629e-03, 5.2858e-03, 3.5754e-03, 5.7410e-07, 9.7810e-01, 5.1372e-03,
        6.0369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 83, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0435e-01, 3.3871e-07, 7.8142e-07, 4.2093e-10, 1.0547e-08, 4.1701e-09,
        1.9565e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.309

[Epoch: 83, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0029, 0.0024, 0.9835, 0.0030, 0.0030, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 83, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0030, 0.0038, 0.9785, 0.0033, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.046

[Epoch: 83, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0054, 0.0052, 0.9757, 0.0038, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 83, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.6470e-03, 4.4315e-03, 3.0829e-03, 5.9215e-07, 9.8232e-01, 4.2479e-03,
        4.2746e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 84, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5193e-01, 2.9384e-07, 7.5074e-07, 3.7500e-10, 1.2246e-08, 2.5741e-09,
        2.4807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.260

[Epoch: 84, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0028, 0.9828, 0.0027, 0.0027, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 84, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0029, 0.0035, 0.9794, 0.0032, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 84, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0042, 0.0038, 0.9798, 0.0029, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 84, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.3823e-03, 3.7520e-03, 2.4074e-03, 6.6902e-07, 9.8477e-01, 3.4148e-03,
        4.2748e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.193

[Epoch: 85, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9111e-01, 2.1933e-07, 6.2218e-07, 2.5759e-10, 5.6597e-09, 1.9624e-09,
        2.0889e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.300

[Epoch: 85, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0029, 0.0030, 0.9826, 0.0026, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 85, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0033, 0.0036, 0.0032, 0.9806, 0.0030, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.038

[Epoch: 85, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0043, 0.0039, 0.9796, 0.0032, 0.0031, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 85, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.7465e-03, 4.4062e-03, 3.6575e-03, 4.6887e-07, 9.8125e-01, 4.0239e-03,
        4.9165e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.196

[Epoch: 86, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5539e-01, 2.7723e-07, 5.7133e-07, 3.5014e-10, 1.0723e-08, 3.0234e-09,
        2.4460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.282

[Epoch: 86, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0027, 0.9818, 0.0035, 0.0028, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 86, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0041, 0.0031, 0.0043, 0.9770, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.078

[Epoch: 86, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0040, 0.0029, 0.9824, 0.0026, 0.0030, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 86, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.4982e-03, 3.8809e-03, 2.7767e-03, 1.0441e-06, 9.8347e-01, 4.1731e-03,
        4.2015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.170

[Epoch: 87, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9828e-01, 2.0163e-07, 6.0365e-07, 3.0662e-10, 7.7160e-09, 3.3776e-09,
        2.0172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.308

[Epoch: 87, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0043, 0.0025, 0.9789, 0.0038, 0.0041, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 87, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0030, 0.0024, 0.0026, 0.9845, 0.0033, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.032

[Epoch: 87, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0046, 0.0039, 0.9783, 0.0034, 0.0028, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 87, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.8785e-03, 4.4285e-03, 2.9415e-03, 4.7200e-07, 9.8328e-01, 3.7297e-03,
        3.7453e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.202

[Epoch: 88, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.4324e-01, 3.2980e-07, 9.9471e-07, 3.0029e-10, 9.7021e-09, 2.2962e-09,
        2.5676e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.267

[Epoch: 88, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0022, 0.0025, 0.9849, 0.0032, 0.0024, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 88, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0029, 0.0036, 0.0045, 0.9776, 0.0043, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 88, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0045, 0.0039, 0.9769, 0.0034, 0.0043, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 88, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.3976e-03, 4.7867e-03, 4.4154e-03, 1.0950e-06, 9.7847e-01, 4.7491e-03,
        5.1768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.199

[Epoch: 89, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.3131e-01, 2.3679e-07, 6.1034e-07, 2.9842e-10, 1.0738e-08, 1.1881e-08,
        1.6868e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.302

[Epoch: 89, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0026, 0.0029, 0.9825, 0.0032, 0.0028, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 89, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0034, 0.0036, 0.0031, 0.9816, 0.0026, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.035

[Epoch: 89, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0048, 0.0026, 0.9794, 0.0046, 0.0027, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 89, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.0570e-03, 4.1035e-03, 4.2234e-03, 6.6947e-07, 9.7822e-01, 5.4704e-03,
        5.9265e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.159

[Epoch: 90, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([6.8262e-01, 4.7186e-07, 5.4155e-07, 1.9378e-09, 7.2153e-08, 3.6208e-08,
        3.1738e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.236

[Epoch: 90, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0041, 0.0037, 0.9771, 0.0032, 0.0050, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 90, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0021, 0.0063, 0.0047, 0.9759, 0.0044, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.074

[Epoch: 90, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.0029, 0.9824, 0.0021, 0.0027, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 90, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.7291e-03, 3.8982e-03, 2.4164e-03, 5.4808e-07, 9.8410e-01, 3.7734e-03,
        4.0790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.237

[Epoch: 91, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.4582e-01, 4.0266e-07, 5.9570e-07, 2.4839e-09, 1.2917e-08, 3.2379e-08,
        1.5418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.327

[Epoch: 91, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0030, 0.0035, 0.9833, 0.0031, 0.0021, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 91, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0030, 0.0037, 0.0052, 0.9774, 0.0039, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 0.004

[Epoch: 91, batch: 160/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0050, 0.0034, 0.9764, 0.0035, 0.0043, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 91, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.2802e-03, 4.0568e-03, 3.7194e-03, 1.8612e-06, 9.8403e-01, 3.4350e-03,
        3.4802e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.147

[Epoch: 92, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.3887e-01, 6.6795e-07, 6.6362e-07, 1.8274e-09, 4.3252e-08, 4.2907e-08,
        2.6113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.295

[Epoch: 92, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.0028, 0.9797, 0.0038, 0.0040, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 92, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0035, 0.0036, 0.0036, 0.9820, 0.0028, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.078

[Epoch: 92, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0051, 0.0032, 0.9792, 0.0040, 0.0027, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 92, batch: 200/203] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.1690e-03, 4.1959e-03, 4.2399e-03, 1.0324e-06, 9.8000e-01, 4.3124e-03,
        5.0840e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.162

[Epoch: 93, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9069e-01, 7.1071e-07, 6.3227e-07, 5.9982e-09, 3.3339e-08, 1.8696e-08,
        2.0931e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.250

[Epoch: 93, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0044, 0.0034, 0.9796, 0.0031, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 93, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0042, 0.0033, 0.0035, 0.9803, 0.0036, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.032

[Epoch: 93, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.0033, 0.9810, 0.0031, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 93, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.2563e-03, 4.2353e-03, 3.5980e-03, 1.0570e-06, 9.8094e-01, 4.1615e-03,
        4.8119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.204

[Epoch: 94, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6316e-01, 8.0367e-07, 6.0408e-07, 1.5141e-09, 3.8050e-08, 5.0567e-08,
        2.3684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.298

[Epoch: 94, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0032, 0.0027, 0.9821, 0.0031, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 94, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0039, 0.0031, 0.0054, 0.9779, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 94, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0044, 0.0047, 0.9784, 0.0033, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.002

[Epoch: 94, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.7147e-03, 3.0168e-03, 3.2263e-03, 1.5350e-06, 9.8398e-01, 4.8496e-03,
        3.2095e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.199

[Epoch: 95, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8457e-01, 6.3476e-07, 6.2974e-07, 1.6161e-09, 1.7223e-08, 1.6730e-08,
        2.1543e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.257

[Epoch: 95, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.0034, 0.9796, 0.0039, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 95, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0033, 0.0035, 0.0037, 0.9809, 0.0027, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.063

[Epoch: 95, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0048, 0.0043, 0.9765, 0.0033, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 95, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7558e-03, 4.6757e-03, 3.7750e-03, 1.2220e-06, 9.7849e-01, 4.5251e-03,
        5.7760e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 96, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6925e-01, 7.2616e-07, 5.8341e-07, 2.8491e-09, 3.2987e-08, 2.4121e-08,
        2.3075e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 96, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0038, 0.9794, 0.0033, 0.0039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 96, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0043, 0.0048, 0.0049, 0.9762, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.026

[Epoch: 96, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0058, 0.0040, 0.9769, 0.0025, 0.0031, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 96, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.1092e-03, 3.4818e-03, 4.1913e-03, 6.2933e-07, 9.8401e-01, 2.7571e-03,
        3.4505e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 97, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8726e-01, 5.2696e-07, 3.7416e-07, 4.4047e-10, 9.4391e-09, 4.9375e-09,
        2.1274e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.296

[Epoch: 97, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0029, 0.0030, 0.9824, 0.0026, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 97, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0036, 0.0046, 0.0054, 0.9748, 0.0042, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 97, batch: 160/203] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0054, 0.0031, 0.0034, 0.9779, 0.0048, 0.0029, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 97, batch: 200/203] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.6381e-03, 2.8788e-03, 2.7134e-03, 2.4512e-06, 9.8832e-01, 2.2142e-03,
        2.2331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.220

[Epoch: 98, batch: 40/203] total loss per batch: 0.504
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7143e-01, 3.7492e-07, 2.5092e-07, 3.1740e-09, 2.0294e-08, 6.7514e-08,
        2.2857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 98, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0033, 0.9777, 0.0040, 0.0044, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 98, batch: 120/203] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0025, 0.0039, 0.0036, 0.9795, 0.0015, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 98, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0040, 0.0054, 0.9737, 0.0081, 0.0021, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 98, batch: 200/203] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([1.8342e-03, 2.1618e-03, 3.7966e-03, 3.9016e-06, 9.8083e-01, 4.3552e-03,
        7.0158e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.145

[Epoch: 99, batch: 40/203] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.2398e-01, 1.2890e-06, 4.9858e-07, 9.5379e-09, 9.3427e-08, 1.1024e-06,
        2.7602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.346

[Epoch: 99, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0038, 0.0035, 0.9798, 0.0022, 0.0038, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 99, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0023, 0.0047, 0.0051, 0.9762, 0.0031, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.072

[Epoch: 99, batch: 160/203] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0040, 0.0029, 0.9825, 0.0029, 0.0029, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 99, batch: 200/203] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0934e-03, 3.2877e-03, 3.2700e-03, 5.1764e-06, 9.7989e-01, 4.0934e-03,
        6.3630e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.162

[Epoch: 100, batch: 40/203] total loss per batch: 0.510
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7862e-01, 2.6107e-07, 9.6530e-08, 4.9207e-10, 3.5070e-09, 1.2759e-08,
        2.2138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 100, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0030, 0.0022, 0.9845, 0.0016, 0.0032, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 100, batch: 120/203] total loss per batch: 0.526
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0025, 0.0108, 0.0049, 0.9738, 0.0018, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.027

[Epoch: 100, batch: 160/203] total loss per batch: 0.521
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0040, 0.0018, 0.9809, 0.0055, 0.0024, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 100, batch: 200/203] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4040e-03, 4.0244e-03, 3.5393e-03, 6.0269e-06, 9.7659e-01, 4.5161e-03,
        7.9186e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.162

[Epoch: 101, batch: 40/203] total loss per batch: 0.508
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9433e-01, 3.2454e-06, 6.9083e-07, 2.4217e-08, 2.2527e-08, 2.8543e-07,
        2.0566e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.218

[Epoch: 101, batch: 80/203] total loss per batch: 0.494
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0025, 0.0027, 0.9838, 0.0023, 0.0040, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 101, batch: 120/203] total loss per batch: 0.521
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0036, 0.0045, 0.0056, 0.9759, 0.0042, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.020

[Epoch: 101, batch: 160/203] total loss per batch: 0.521
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0034, 0.9808, 0.0038, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 101, batch: 200/203] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7448e-03, 4.1180e-03, 3.2198e-03, 1.6948e-06, 9.8014e-01, 4.0321e-03,
        5.7441e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.172

[Epoch: 102, batch: 40/203] total loss per batch: 0.503
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8693e-01, 7.1802e-07, 5.4617e-07, 7.4880e-09, 1.1555e-08, 3.8389e-08,
        2.1307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 102, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0023, 0.9833, 0.0028, 0.0036, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.019

[Epoch: 102, batch: 120/203] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0021, 0.0043, 0.0057, 0.9782, 0.0042, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.064

[Epoch: 102, batch: 160/203] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0035, 0.0032, 0.9812, 0.0043, 0.0027, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 102, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8976e-03, 4.3886e-03, 3.2285e-03, 1.4813e-06, 9.7826e-01, 4.3204e-03,
        6.9082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.165

[Epoch: 103, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8561e-01, 4.6457e-07, 3.4336e-07, 4.4999e-09, 5.0835e-09, 4.9329e-08,
        2.1439e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.278

[Epoch: 103, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0024, 0.9823, 0.0031, 0.0039, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 103, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0022, 0.0042, 0.0045, 0.9798, 0.0039, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 103, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0037, 0.0033, 0.9805, 0.0043, 0.0027, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 103, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6441e-03, 3.8354e-03, 3.1246e-03, 1.4017e-06, 9.7993e-01, 4.4978e-03,
        5.9712e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.170

[Epoch: 104, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7721e-01, 4.6720e-07, 3.3488e-07, 4.6285e-09, 5.1480e-09, 3.9160e-08,
        2.2279e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.279

[Epoch: 104, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0024, 0.9820, 0.0031, 0.0040, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 104, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0023, 0.0040, 0.0046, 0.9797, 0.0038, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 104, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.0034, 0.9802, 0.0043, 0.0028, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 104, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5961e-03, 3.5953e-03, 2.9721e-03, 1.1920e-06, 9.8130e-01, 4.0961e-03,
        5.4432e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.171

[Epoch: 105, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7662e-01, 4.6249e-07, 3.3419e-07, 4.2666e-09, 5.0473e-09, 3.3413e-08,
        2.2338e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.278

[Epoch: 105, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0025, 0.9816, 0.0031, 0.0040, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 105, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0024, 0.0038, 0.0043, 0.9801, 0.0036, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 105, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0034, 0.9798, 0.0043, 0.0030, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 105, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5271e-03, 3.5852e-03, 2.8979e-03, 1.0623e-06, 9.8179e-01, 4.0105e-03,
        5.1855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.177

[Epoch: 106, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7509e-01, 4.1071e-07, 3.1268e-07, 3.8199e-09, 4.6892e-09, 3.1372e-08,
        2.2490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.279

[Epoch: 106, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0026, 0.9814, 0.0032, 0.0040, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 106, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0025, 0.0037, 0.0043, 0.9799, 0.0037, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 106, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0035, 0.9796, 0.0044, 0.0031, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 106, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5091e-03, 3.5267e-03, 2.8423e-03, 9.4703e-07, 9.8227e-01, 3.8007e-03,
        5.0483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.179

[Epoch: 107, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7600e-01, 3.9691e-07, 3.0002e-07, 3.6021e-09, 4.5680e-09, 3.0026e-08,
        2.2400e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.284

[Epoch: 107, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0027, 0.9811, 0.0032, 0.0041, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 107, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0025, 0.0036, 0.0042, 0.9801, 0.0035, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.051

[Epoch: 107, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0034, 0.9795, 0.0043, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 107, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.4832e-03, 3.5193e-03, 2.8372e-03, 8.3116e-07, 9.8263e-01, 3.8026e-03,
        4.7269e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.183

[Epoch: 108, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7358e-01, 3.7251e-07, 2.9464e-07, 3.4231e-09, 4.5772e-09, 2.9628e-08,
        2.2642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 108, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0028, 0.9808, 0.0033, 0.0041, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 108, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0036, 0.0042, 0.9796, 0.0037, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 108, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0035, 0.9794, 0.0043, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 108, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5259e-03, 3.4054e-03, 2.7636e-03, 7.3755e-07, 9.8306e-01, 3.5947e-03,
        4.6495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.183

[Epoch: 109, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7641e-01, 3.5297e-07, 2.7810e-07, 3.2266e-09, 4.2002e-09, 2.8511e-08,
        2.2359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 109, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0029, 0.9805, 0.0033, 0.0041, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 109, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0035, 0.0039, 0.9804, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 109, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0035, 0.9792, 0.0042, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 109, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5108e-03, 3.3660e-03, 2.7782e-03, 6.7054e-07, 9.8336e-01, 3.6770e-03,
        4.3042e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 110, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7202e-01, 3.2649e-07, 2.5571e-07, 2.7057e-09, 3.9033e-09, 2.4360e-08,
        2.2798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 110, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0029, 0.9807, 0.0033, 0.0039, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 110, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0028, 0.0033, 0.0039, 0.9800, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 110, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0035, 0.9793, 0.0043, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 110, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5294e-03, 3.4904e-03, 2.8010e-03, 6.3421e-07, 9.8334e-01, 3.4628e-03,
        4.3794e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.183

[Epoch: 111, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7612e-01, 3.1749e-07, 2.4540e-07, 2.4000e-09, 3.4014e-09, 2.2061e-08,
        2.2388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.287

[Epoch: 111, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0036, 0.0031, 0.9798, 0.0034, 0.0041, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 111, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.0035, 0.0038, 0.9796, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 111, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0034, 0.9797, 0.0040, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 111, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7325e-03, 3.3865e-03, 2.8899e-03, 5.6764e-07, 9.8299e-01, 3.8275e-03,
        4.1727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 112, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7315e-01, 3.2031e-07, 2.4762e-07, 2.2288e-09, 3.5014e-09, 2.0369e-08,
        2.2685e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.274

[Epoch: 112, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0029, 0.9812, 0.0031, 0.0039, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 112, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0034, 0.0037, 0.9802, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.048

[Epoch: 112, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.0034, 0.9796, 0.0041, 0.0032, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 112, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.4140e-03, 3.7420e-03, 2.6716e-03, 5.5311e-07, 9.8381e-01, 3.4451e-03,
        3.9158e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 113, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7188e-01, 3.2347e-07, 2.5995e-07, 1.9368e-09, 3.4122e-09, 2.2196e-08,
        2.2812e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.291

[Epoch: 113, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0030, 0.9809, 0.0031, 0.0036, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 113, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.0032, 0.0047, 0.9787, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.063

[Epoch: 113, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.0037, 0.9785, 0.0040, 0.0035, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 113, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8308e-03, 3.2145e-03, 2.9245e-03, 4.7773e-07, 9.8354e-01, 3.2678e-03,
        4.2205e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 114, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7852e-01, 4.1410e-07, 2.5277e-07, 1.5900e-09, 2.8126e-09, 1.7982e-08,
        2.2148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.255

[Epoch: 114, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0030, 0.9807, 0.0032, 0.0039, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 114, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0030, 0.0027, 0.9823, 0.0031, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.039

[Epoch: 114, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.0032, 0.9793, 0.0040, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 114, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6958e-03, 4.3919e-03, 2.7203e-03, 6.7606e-07, 9.8247e-01, 3.6733e-03,
        4.0492e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.184

[Epoch: 115, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7600e-01, 3.4529e-07, 2.3826e-07, 1.8528e-09, 4.6188e-09, 2.9969e-08,
        2.2400e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.309

[Epoch: 115, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0033, 0.9806, 0.0029, 0.0038, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 115, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0037, 0.0037, 0.0046, 0.9779, 0.0038, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.064

[Epoch: 115, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0039, 0.0034, 0.9795, 0.0034, 0.0032, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 115, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0580e-03, 4.3751e-03, 3.6139e-03, 5.0225e-07, 9.8127e-01, 4.4116e-03,
        3.2720e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.178

[Epoch: 116, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6448e-01, 5.6364e-07, 2.9404e-07, 2.3564e-09, 5.0597e-09, 2.4015e-08,
        2.3552e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.276

[Epoch: 116, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0036, 0.0030, 0.9804, 0.0033, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 116, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0042, 0.0038, 0.0041, 0.9780, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 116, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.0040, 0.9780, 0.0042, 0.0038, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 116, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8589e-03, 3.3419e-03, 2.8469e-03, 8.0288e-07, 9.8365e-01, 3.5738e-03,
        3.7324e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.205

[Epoch: 117, batch: 40/203] total loss per batch: 0.502
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7775e-01, 2.6456e-07, 1.9540e-07, 1.5874e-09, 5.3938e-09, 2.0989e-08,
        2.2225e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.282

[Epoch: 117, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.0035, 0.9799, 0.0029, 0.0039, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 117, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0042, 0.0040, 0.9777, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.068

[Epoch: 117, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.0037, 0.9781, 0.0034, 0.0033, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 117, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6950e-03, 3.8701e-03, 4.2036e-03, 3.0967e-07, 9.8151e-01, 3.6161e-03,
        4.1004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.181

[Epoch: 118, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8051e-01, 5.2097e-07, 2.7290e-07, 1.2437e-09, 4.3130e-09, 2.0275e-08,
        2.1949e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.291

[Epoch: 118, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0031, 0.9813, 0.0029, 0.0035, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 118, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0029, 0.0039, 0.9814, 0.0026, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.045

[Epoch: 118, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.0046, 0.9779, 0.0037, 0.0040, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 118, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2378e-03, 4.4431e-03, 3.5299e-03, 7.1113e-07, 9.8160e-01, 3.2455e-03,
        3.9436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.178

[Epoch: 119, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7806e-01, 3.1278e-07, 1.4768e-07, 4.6516e-10, 3.2213e-09, 2.3921e-08,
        2.2194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.289

[Epoch: 119, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0041, 0.0036, 0.9793, 0.0033, 0.0037, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 119, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0026, 0.0036, 0.9806, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 119, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.0033, 0.9792, 0.0035, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 119, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5404e-03, 3.0434e-03, 3.2245e-03, 5.8675e-07, 9.8377e-01, 4.2585e-03,
        3.1577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 120, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6082e-01, 4.0821e-07, 2.3681e-07, 1.0709e-09, 4.8907e-09, 2.6849e-08,
        2.3918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.264

[Epoch: 120, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0035, 0.9803, 0.0027, 0.0038, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 120, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0035, 0.0038, 0.9786, 0.0032, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 120, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.0037, 0.9802, 0.0032, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 120, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6163e-03, 3.8460e-03, 3.4415e-03, 4.1509e-07, 9.8288e-01, 3.5527e-03,
        3.6585e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.198

[Epoch: 121, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9526e-01, 3.1617e-07, 1.8456e-07, 7.5907e-10, 3.6281e-09, 1.8105e-08,
        2.0474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.302

[Epoch: 121, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0033, 0.0032, 0.9819, 0.0030, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 121, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0035, 0.0035, 0.9797, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 121, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0039, 0.0041, 0.9756, 0.0047, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 121, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6316e-03, 3.7696e-03, 2.6632e-03, 5.8434e-07, 9.8469e-01, 3.1340e-03,
        3.1142e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.188

[Epoch: 122, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5820e-01, 4.6113e-07, 2.1947e-07, 8.7762e-10, 5.4002e-09, 2.2912e-08,
        2.4180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.273

[Epoch: 122, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0038, 0.9796, 0.0029, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 122, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0030, 0.0041, 0.0045, 0.9784, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.064

[Epoch: 122, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0033, 0.9815, 0.0032, 0.0026, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 122, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0411e-03, 3.5671e-03, 3.9852e-03, 6.2922e-07, 9.8052e-01, 3.8492e-03,
        5.0372e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.170

[Epoch: 123, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7677e-01, 4.2822e-07, 2.3957e-07, 1.0784e-09, 4.4618e-09, 2.3703e-08,
        2.2323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.295

[Epoch: 123, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0037, 0.0037, 0.9786, 0.0035, 0.0039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 123, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0032, 0.0034, 0.0037, 0.9798, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 123, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0036, 0.9794, 0.0034, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 123, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0644e-03, 3.6508e-03, 3.2337e-03, 3.5972e-07, 9.8325e-01, 3.5099e-03,
        3.2906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.233

[Epoch: 124, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7829e-01, 4.4156e-07, 1.9816e-07, 5.8136e-10, 3.5472e-09, 1.0094e-08,
        2.2171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.277

[Epoch: 124, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0040, 0.0039, 0.9795, 0.0030, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 124, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0029, 0.0039, 0.0039, 0.9781, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 124, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0046, 0.0033, 0.9771, 0.0039, 0.0042, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 124, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0097e-03, 3.7353e-03, 2.5104e-03, 1.2956e-06, 9.8383e-01, 3.5800e-03,
        3.3354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.160

[Epoch: 125, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5178e-01, 3.8520e-07, 1.9734e-07, 1.0446e-09, 6.3683e-09, 1.8590e-08,
        2.4822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.265

[Epoch: 125, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0031, 0.9808, 0.0029, 0.0039, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 125, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0029, 0.0030, 0.0032, 0.9829, 0.0026, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.083

[Epoch: 125, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0035, 0.9792, 0.0036, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 125, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5329e-03, 4.0296e-03, 3.9330e-03, 6.9563e-07, 9.7905e-01, 4.1332e-03,
        5.3157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.172

[Epoch: 126, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.0498e-01, 3.5413e-07, 2.4627e-07, 1.3065e-09, 7.3792e-09, 1.8684e-08,
        1.9502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.321

[Epoch: 126, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0039, 0.0034, 0.9796, 0.0027, 0.0038, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 126, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0038, 0.0030, 0.0031, 0.9810, 0.0028, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.041

[Epoch: 126, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.0034, 0.9791, 0.0034, 0.0037, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 126, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.4279e-03, 2.8866e-03, 3.0283e-03, 5.7666e-07, 9.8580e-01, 3.5411e-03,
        2.3164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.196

[Epoch: 127, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5586e-01, 3.7883e-07, 3.2340e-07, 7.3174e-10, 6.4336e-09, 1.0586e-08,
        2.4414e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.257

[Epoch: 127, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0036, 0.0035, 0.9808, 0.0030, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 127, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0039, 0.0056, 0.9764, 0.0041, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 127, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.0032, 0.9788, 0.0041, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 127, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9884e-03, 3.0787e-03, 3.2899e-03, 8.1493e-07, 9.8377e-01, 3.4280e-03,
        3.4473e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.179

[Epoch: 128, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8899e-01, 4.3537e-07, 2.5752e-07, 1.2557e-09, 4.3415e-09, 1.0688e-08,
        2.1101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.289

[Epoch: 128, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0037, 0.9797, 0.0033, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 128, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0029, 0.0038, 0.0030, 0.9823, 0.0027, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.069

[Epoch: 128, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0041, 0.9784, 0.0036, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 128, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8996e-03, 4.0331e-03, 3.1398e-03, 9.1470e-07, 9.8270e-01, 3.3323e-03,
        3.8927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.206

[Epoch: 129, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7716e-01, 4.5500e-07, 2.6465e-07, 8.8203e-10, 6.2632e-09, 1.9976e-08,
        2.2284e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.303

[Epoch: 129, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0043, 0.0034, 0.9785, 0.0025, 0.0042, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 129, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0037, 0.0038, 0.9779, 0.0034, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.044

[Epoch: 129, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0044, 0.9782, 0.0034, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 129, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8332e-03, 3.8669e-03, 4.1410e-03, 4.9177e-07, 9.8108e-01, 3.9276e-03,
        4.1550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 130, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7468e-01, 5.3822e-07, 2.4090e-07, 5.7607e-10, 2.0228e-09, 3.8852e-09,
        2.2532e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.260

[Epoch: 130, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0031, 0.9819, 0.0027, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.045

[Epoch: 130, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0028, 0.0035, 0.0039, 0.9800, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.037

[Epoch: 130, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0026, 0.9813, 0.0036, 0.0028, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 130, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3573e-03, 3.5772e-03, 3.1832e-03, 5.3809e-07, 9.8272e-01, 3.5019e-03,
        3.6639e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.187

[Epoch: 131, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.4306e-01, 8.4884e-07, 4.3268e-07, 2.2538e-09, 1.8171e-08, 1.6476e-08,
        2.5693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.311

[Epoch: 131, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.0032, 0.9808, 0.0027, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 131, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0028, 0.0031, 0.0031, 0.9814, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 131, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0043, 0.0046, 0.9753, 0.0045, 0.0033, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 131, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1741e-03, 3.2151e-03, 2.5048e-03, 6.7186e-07, 9.8559e-01, 2.9951e-03,
        2.5153e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.175

[Epoch: 132, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([8.2016e-01, 4.1317e-07, 2.0733e-07, 5.6829e-10, 3.2972e-09, 8.5643e-09,
        1.7984e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.268

[Epoch: 132, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0036, 0.0033, 0.9796, 0.0032, 0.0031, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 132, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0036, 0.0046, 0.9792, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.060

[Epoch: 132, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0034, 0.9791, 0.0034, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 132, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0272e-03, 4.2472e-03, 3.2704e-03, 8.6977e-07, 9.8072e-01, 4.3590e-03,
        4.3769e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.193

[Epoch: 133, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6055e-01, 4.6640e-07, 2.8933e-07, 9.7185e-10, 4.3520e-09, 3.7135e-09,
        2.3945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 133, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0036, 0.9803, 0.0029, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 133, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0039, 0.0049, 0.9750, 0.0048, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.071

[Epoch: 133, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0028, 0.0033, 0.9820, 0.0033, 0.0025, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 133, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.8608e-03, 3.4930e-03, 3.5889e-03, 3.1799e-07, 9.8079e-01, 4.1350e-03,
        4.1366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.214

[Epoch: 134, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5946e-01, 5.6522e-07, 2.4173e-07, 1.4740e-09, 6.0163e-09, 8.7321e-09,
        2.4053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.282

[Epoch: 134, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.0029, 0.9811, 0.0029, 0.0039, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 134, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0029, 0.0040, 0.0035, 0.9806, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 134, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0041, 0.0046, 0.9752, 0.0046, 0.0038, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 134, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8115e-03, 3.2808e-03, 2.8934e-03, 8.1831e-07, 9.8487e-01, 3.0720e-03,
        3.0689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.173

[Epoch: 135, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9086e-01, 5.7930e-07, 2.2860e-07, 1.3081e-09, 5.6637e-09, 1.3235e-08,
        2.0914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.288

[Epoch: 135, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0035, 0.0029, 0.9802, 0.0033, 0.0035, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 135, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0023, 0.0024, 0.0031, 0.9846, 0.0025, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 135, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.0032, 0.9806, 0.0030, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 135, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6736e-03, 3.7735e-03, 3.4544e-03, 5.8217e-07, 9.8272e-01, 3.1317e-03,
        4.2429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 136, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.4570e-01, 6.2212e-07, 2.5162e-07, 8.3011e-10, 3.9988e-09, 5.8401e-09,
        2.5430e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.284

[Epoch: 136, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0043, 0.0037, 0.9787, 0.0036, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 136, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0046, 0.0044, 0.0046, 0.9759, 0.0041, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.041

[Epoch: 136, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0043, 0.0030, 0.9818, 0.0037, 0.0024, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 136, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.7753e-03, 3.4109e-03, 3.7786e-03, 4.8175e-07, 9.8115e-01, 3.4238e-03,
        3.4621e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.189

[Epoch: 137, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9790e-01, 5.5113e-07, 2.0401e-07, 1.1604e-09, 6.7797e-09, 5.4541e-09,
        2.0210e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.294

[Epoch: 137, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.0027, 0.9829, 0.0025, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 137, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0033, 0.0038, 0.0036, 0.9806, 0.0028, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 137, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0043, 0.0048, 0.9747, 0.0043, 0.0041, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 137, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9382e-03, 3.1368e-03, 3.1668e-03, 8.7566e-07, 9.8346e-01, 3.3680e-03,
        3.9280e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 138, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7081e-01, 6.3134e-07, 4.3919e-07, 1.6250e-09, 5.1660e-09, 5.0410e-09,
        2.2919e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.255

[Epoch: 138, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0027, 0.9818, 0.0032, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 138, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0028, 0.0027, 0.0033, 0.9819, 0.0026, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.070

[Epoch: 138, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0031, 0.0030, 0.9823, 0.0030, 0.0032, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 138, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1600e-03, 3.6530e-03, 3.5852e-03, 8.0508e-07, 9.8121e-01, 3.3748e-03,
        5.0194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.202

[Epoch: 139, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6713e-01, 4.2320e-07, 1.8991e-07, 4.4478e-10, 2.5496e-09, 2.1843e-09,
        2.3286e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.274

[Epoch: 139, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0047, 0.0036, 0.9763, 0.0040, 0.0038, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 139, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0033, 0.0034, 0.9786, 0.0045, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 139, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0040, 0.9799, 0.0036, 0.0030, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 139, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7079e-03, 2.7162e-03, 3.4860e-03, 4.1271e-07, 9.8432e-01, 2.9393e-03,
        2.8308e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.177

[Epoch: 140, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8299e-01, 6.5683e-07, 4.0216e-07, 1.8504e-09, 1.0340e-08, 7.7405e-09,
        2.1701e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.317

[Epoch: 140, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0028, 0.9807, 0.0031, 0.0038, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 140, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0049, 0.0039, 0.0036, 0.9781, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.072

[Epoch: 140, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0041, 0.0036, 0.9775, 0.0041, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 140, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.4059e-03, 3.4494e-03, 2.8221e-03, 4.5993e-07, 9.8577e-01, 2.4677e-03,
        3.0879e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.184

[Epoch: 141, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6534e-01, 4.1482e-07, 2.7088e-07, 1.1151e-09, 5.0376e-09, 6.4003e-09,
        2.3466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.262

[Epoch: 141, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0029, 0.9808, 0.0034, 0.0038, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 141, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0030, 0.0034, 0.9810, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.043

[Epoch: 141, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.0034, 0.9788, 0.0032, 0.0040, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 141, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5279e-03, 3.0525e-03, 3.4030e-03, 6.8702e-07, 9.8243e-01, 3.7380e-03,
        3.8510e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.205

[Epoch: 142, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7180e-01, 6.1944e-07, 2.2835e-07, 6.8266e-10, 4.7754e-09, 5.8818e-09,
        2.2820e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.290

[Epoch: 142, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0035, 0.9793, 0.0037, 0.0028, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 142, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0041, 0.0039, 0.9782, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 142, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0028, 0.0029, 0.9840, 0.0029, 0.0022, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 142, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9980e-03, 2.3064e-03, 3.7070e-03, 6.1655e-07, 9.8529e-01, 2.8806e-03,
        2.8167e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.195

[Epoch: 143, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7343e-01, 5.2238e-07, 2.2592e-07, 1.2666e-09, 8.6531e-09, 1.2907e-08,
        2.2657e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.290

[Epoch: 143, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.0031, 0.9820, 0.0027, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 143, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0040, 0.0031, 0.0039, 0.9782, 0.0039, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.062

[Epoch: 143, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0044, 0.0034, 0.9775, 0.0042, 0.0036, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 143, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7512e-03, 3.1015e-03, 2.7385e-03, 4.7171e-07, 9.8567e-01, 2.5575e-03,
        3.1773e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 144, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7528e-01, 4.5540e-07, 2.4934e-07, 7.1597e-10, 5.4110e-09, 6.0983e-09,
        2.2472e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 144, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0029, 0.9815, 0.0031, 0.0036, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 144, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0049, 0.0033, 0.9768, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 144, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0035, 0.0043, 0.9754, 0.0043, 0.0043, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 144, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3584e-03, 4.1612e-03, 3.8944e-03, 6.6443e-07, 9.8023e-01, 4.0557e-03,
        4.2949e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.189

[Epoch: 145, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8000e-01, 7.3990e-07, 2.4183e-07, 8.0884e-10, 4.9558e-09, 1.1059e-08,
        2.2000e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 145, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0033, 0.0035, 0.9781, 0.0042, 0.0037, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 145, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0033, 0.0033, 0.9821, 0.0029, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.041

[Epoch: 145, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0038, 0.9802, 0.0034, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 145, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2541e-03, 2.6326e-03, 3.9740e-03, 3.1492e-07, 9.8297e-01, 3.5845e-03,
        3.5799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.193

[Epoch: 146, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5932e-01, 5.3502e-07, 3.0755e-07, 1.0466e-09, 4.3582e-09, 4.4228e-09,
        2.4068e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.286

[Epoch: 146, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0029, 0.0029, 0.9814, 0.0030, 0.0037, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 146, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.0035, 0.0038, 0.9791, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.073

[Epoch: 146, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0039, 0.0030, 0.9813, 0.0030, 0.0032, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 146, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.1140e-03, 2.9331e-03, 2.6606e-03, 6.4980e-07, 9.8754e-01, 2.4210e-03,
        2.3270e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.217

[Epoch: 147, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9098e-01, 8.0748e-07, 3.8281e-07, 1.7760e-09, 8.2641e-09, 1.8992e-08,
        2.0902e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 147, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0033, 0.9801, 0.0032, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 147, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.0036, 0.0036, 0.9783, 0.0032, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 147, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0036, 0.9789, 0.0033, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 147, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.6263e-03, 2.5950e-03, 2.9423e-03, 4.6487e-07, 9.8384e-01, 3.4635e-03,
        3.5292e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 148, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.4356e-01, 4.1431e-07, 1.7434e-07, 7.8592e-10, 4.3019e-09, 5.3663e-09,
        2.5644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 148, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.0035, 0.9789, 0.0034, 0.0042, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 148, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.0036, 0.0038, 0.9775, 0.0037, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.066

[Epoch: 148, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0043, 0.0054, 0.9767, 0.0038, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 148, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4449e-03, 3.5826e-03, 4.2659e-03, 8.3218e-07, 9.7866e-01, 4.7003e-03,
        4.3470e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.173

[Epoch: 149, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9295e-01, 6.1694e-07, 1.7703e-07, 1.0309e-09, 8.3100e-09, 2.5563e-08,
        2.0705e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.298

[Epoch: 149, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0032, 0.0027, 0.9824, 0.0028, 0.0035, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 149, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0026, 0.0030, 0.0034, 0.9819, 0.0028, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.041

[Epoch: 149, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0042, 0.9796, 0.0030, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 149, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2379e-03, 3.9227e-03, 3.5582e-03, 5.5107e-07, 9.8289e-01, 3.3686e-03,
        3.0271e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.207

[Epoch: 150, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6929e-01, 6.8329e-07, 3.4264e-07, 5.1905e-10, 3.5428e-09, 2.7452e-09,
        2.3071e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 150, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0033, 0.9807, 0.0036, 0.0028, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 150, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0039, 0.0032, 0.9804, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 150, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0035, 0.9794, 0.0032, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 150, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.5548e-03, 2.8334e-03, 2.4053e-03, 7.6673e-07, 9.8676e-01, 2.6904e-03,
        2.7604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.220

[Epoch: 151, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7351e-01, 7.7776e-07, 2.7610e-07, 1.7767e-09, 9.1276e-09, 2.4218e-08,
        2.2649e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.265

[Epoch: 151, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0032, 0.9798, 0.0036, 0.0039, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 151, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0036, 0.0048, 0.9772, 0.0034, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 151, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0044, 0.0036, 0.9785, 0.0032, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 151, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3716e-03, 3.3272e-03, 3.5445e-03, 5.4758e-07, 9.8162e-01, 4.2775e-03,
        3.8623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.176

[Epoch: 152, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7588e-01, 3.3118e-07, 1.5994e-07, 3.9017e-10, 3.4167e-09, 4.0771e-09,
        2.2412e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.310

[Epoch: 152, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0029, 0.0031, 0.9814, 0.0029, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 152, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.0035, 0.0037, 0.9795, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 152, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0032, 0.9804, 0.0030, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 152, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.9533e-03, 3.6161e-03, 3.2490e-03, 6.0308e-07, 9.8350e-01, 3.3848e-03,
        3.2940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.175

[Epoch: 153, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6824e-01, 4.4213e-07, 1.9216e-07, 4.1159e-10, 3.8125e-09, 4.5914e-09,
        2.3176e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.276

[Epoch: 153, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.0035, 0.9798, 0.0032, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 153, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0030, 0.0033, 0.0034, 0.9800, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 153, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0040, 0.0037, 0.9786, 0.0031, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 153, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1223e-03, 3.0594e-03, 3.1481e-03, 5.7478e-07, 9.8342e-01, 3.7136e-03,
        3.5390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 154, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7855e-01, 6.1651e-07, 2.2193e-07, 9.2416e-10, 5.3400e-09, 7.6575e-09,
        2.2145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 154, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0033, 0.0034, 0.9800, 0.0034, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 154, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0034, 0.0033, 0.9798, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.051

[Epoch: 154, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.0038, 0.9790, 0.0033, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 154, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0663e-03, 3.3701e-03, 3.2718e-03, 5.0164e-07, 9.8336e-01, 3.4716e-03,
        3.4548e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.200

[Epoch: 155, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7226e-01, 4.4523e-07, 1.8402e-07, 5.2093e-10, 3.9419e-09, 5.0548e-09,
        2.2774e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 155, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0029, 0.0030, 0.9814, 0.0032, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 155, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0032, 0.0033, 0.9803, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.061

[Epoch: 155, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0037, 0.9787, 0.0034, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 155, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1066e-03, 3.4988e-03, 3.2705e-03, 4.3902e-07, 9.8363e-01, 3.1231e-03,
        3.3708e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 156, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6936e-01, 4.9671e-07, 2.0615e-07, 5.3333e-10, 4.6549e-09, 6.3547e-09,
        2.3064e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.288

[Epoch: 156, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0032, 0.9813, 0.0031, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 156, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.0029, 0.0030, 0.9824, 0.0028, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.059

[Epoch: 156, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0035, 0.9804, 0.0030, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 156, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2461e-03, 3.1862e-03, 2.9915e-03, 4.4991e-07, 9.8407e-01, 3.3058e-03,
        3.2026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.189

[Epoch: 157, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7210e-01, 5.5782e-07, 2.0725e-07, 4.9019e-10, 4.0426e-09, 5.7586e-09,
        2.2790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.285

[Epoch: 157, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0034, 0.9808, 0.0032, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 157, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0034, 0.0033, 0.0036, 0.9794, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 157, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0033, 0.9816, 0.0030, 0.0030, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 157, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7277e-03, 3.3816e-03, 3.3397e-03, 3.8351e-07, 9.8347e-01, 3.4777e-03,
        3.6030e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.200

[Epoch: 158, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7898e-01, 4.2944e-07, 1.4678e-07, 4.9367e-10, 3.9259e-09, 4.4213e-09,
        2.2102e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 158, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0036, 0.9798, 0.0030, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 158, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0040, 0.0043, 0.0047, 0.9749, 0.0040, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 158, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0037, 0.0035, 0.9796, 0.0034, 0.0032, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 158, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5485e-03, 3.5700e-03, 3.2151e-03, 4.4845e-07, 9.8326e-01, 3.3233e-03,
        3.0787e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 159, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7398e-01, 4.8886e-07, 1.7278e-07, 5.6188e-10, 4.6761e-09, 6.4732e-09,
        2.2602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 159, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0032, 0.9805, 0.0036, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 159, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0039, 0.0043, 0.9781, 0.0040, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 159, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.0039, 0.9781, 0.0040, 0.0036, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 159, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4737e-03, 3.3958e-03, 3.2387e-03, 4.9318e-07, 9.8322e-01, 3.7062e-03,
        2.9665e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.186

[Epoch: 160, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5881e-01, 4.3881e-07, 1.6194e-07, 4.3752e-10, 4.8669e-09, 1.0352e-08,
        2.4119e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.290

[Epoch: 160, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0032, 0.0036, 0.9778, 0.0038, 0.0041, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 160, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0023, 0.0020, 0.9842, 0.0027, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.078

[Epoch: 160, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0046, 0.0045, 0.9766, 0.0032, 0.0037, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 160, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.0159e-03, 3.4354e-03, 3.7460e-03, 2.6534e-07, 9.8080e-01, 3.6198e-03,
        4.3846e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.191

[Epoch: 161, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9233e-01, 5.0794e-07, 1.5639e-07, 2.7951e-10, 4.7394e-09, 3.8605e-09,
        2.0767e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.295

[Epoch: 161, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0039, 0.0037, 0.9797, 0.0028, 0.0040, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 161, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0027, 0.0024, 0.0030, 0.9837, 0.0022, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.041

[Epoch: 161, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0029, 0.0026, 0.9837, 0.0028, 0.0028, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 161, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7078e-03, 2.8474e-03, 3.3960e-03, 3.4805e-07, 9.8535e-01, 2.8517e-03,
        2.8495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.203

[Epoch: 162, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8342e-01, 4.6784e-07, 1.9509e-07, 7.3667e-10, 8.6425e-09, 4.6762e-09,
        2.1658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.258

[Epoch: 162, batch: 80/203] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0027, 0.0032, 0.9808, 0.0033, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 162, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0043, 0.0043, 0.0065, 0.9733, 0.0041, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.045

[Epoch: 162, batch: 160/203] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0030, 0.9811, 0.0031, 0.0028, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 162, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5864e-03, 3.3956e-03, 3.2528e-03, 7.3305e-07, 9.8238e-01, 3.7972e-03,
        3.5876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.181

[Epoch: 163, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5212e-01, 6.1285e-07, 2.1050e-07, 1.2087e-09, 1.9695e-08, 2.0959e-08,
        2.4788e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.285

[Epoch: 163, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0031, 0.0033, 0.9788, 0.0036, 0.0040, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 163, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0036, 0.0030, 0.0040, 0.9793, 0.0029, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.071

[Epoch: 163, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0045, 0.0034, 0.9780, 0.0035, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 163, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7647e-03, 3.1662e-03, 4.1217e-03, 4.6772e-07, 9.8145e-01, 3.6592e-03,
        3.8347e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 164, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8562e-01, 5.5854e-07, 1.8328e-07, 5.5021e-10, 1.3179e-08, 8.5429e-09,
        2.1438e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.294

[Epoch: 164, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.0033, 0.9795, 0.0035, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 164, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.0032, 0.0041, 0.9803, 0.0025, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 164, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0033, 0.9797, 0.0035, 0.0037, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 164, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3136e-03, 2.8166e-03, 3.4018e-03, 3.6061e-07, 9.8424e-01, 3.2051e-03,
        3.0273e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 165, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7011e-01, 5.7240e-07, 1.9289e-07, 4.6559e-10, 1.2975e-08, 6.1444e-09,
        2.2989e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 165, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0029, 0.9799, 0.0035, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 165, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0034, 0.0039, 0.9795, 0.0031, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 165, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0029, 0.9801, 0.0035, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 165, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.0861e-03, 2.6587e-03, 3.0465e-03, 6.0498e-07, 9.8471e-01, 3.3773e-03,
        3.1195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.195

[Epoch: 166, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7274e-01, 4.6514e-07, 1.6103e-07, 5.5213e-10, 1.1393e-08, 4.8220e-09,
        2.2726e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 166, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0030, 0.9804, 0.0034, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 166, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0034, 0.0035, 0.9800, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.049

[Epoch: 166, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0032, 0.9795, 0.0035, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 166, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1689e-03, 3.0197e-03, 3.3813e-03, 3.2723e-07, 9.8352e-01, 3.3302e-03,
        3.5776e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.197

[Epoch: 167, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7302e-01, 4.9604e-07, 1.6296e-07, 6.1007e-10, 1.1331e-08, 5.5372e-09,
        2.2698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.293

[Epoch: 167, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0031, 0.9801, 0.0034, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 167, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0031, 0.0031, 0.0036, 0.9806, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 167, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0036, 0.0032, 0.9799, 0.0033, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 167, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3355e-03, 3.3446e-03, 3.2981e-03, 3.6871e-07, 9.8338e-01, 3.2123e-03,
        3.4262e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 168, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7284e-01, 5.1467e-07, 1.6469e-07, 4.5743e-10, 1.0373e-08, 4.3823e-09,
        2.2716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.273

[Epoch: 168, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0032, 0.9798, 0.0035, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 168, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0031, 0.0032, 0.0033, 0.9808, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 168, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0032, 0.9802, 0.0033, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 168, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2888e-03, 2.8739e-03, 3.3225e-03, 2.9224e-07, 9.8416e-01, 3.3080e-03,
        3.0480e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.197

[Epoch: 169, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6846e-01, 4.4198e-07, 1.7139e-07, 5.1707e-10, 1.0960e-08, 5.4565e-09,
        2.3154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.290

[Epoch: 169, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0032, 0.9797, 0.0034, 0.0038, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 169, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0034, 0.0033, 0.0036, 0.9798, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 169, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0034, 0.9798, 0.0034, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 169, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2822e-03, 3.0041e-03, 3.1568e-03, 3.2134e-07, 9.8414e-01, 3.2059e-03,
        3.2108e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 170, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8362e-01, 4.1289e-07, 1.4322e-07, 4.5066e-10, 7.7731e-09, 3.7040e-09,
        2.1638e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.289

[Epoch: 170, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.0030, 0.9802, 0.0035, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 170, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0032, 0.0030, 0.9811, 0.0030, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 170, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0037, 0.0034, 0.9791, 0.0034, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 170, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4351e-03, 3.5784e-03, 3.6808e-03, 2.7045e-07, 9.8223e-01, 3.5296e-03,
        3.5450e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.187

[Epoch: 171, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6897e-01, 4.2748e-07, 1.0248e-07, 2.5181e-10, 6.2921e-09, 2.6075e-09,
        2.3103e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.278

[Epoch: 171, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0032, 0.9803, 0.0034, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 171, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.0035, 0.0037, 0.9787, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 171, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0034, 0.0035, 0.9801, 0.0033, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 171, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3636e-03, 3.6215e-03, 3.0474e-03, 3.4035e-07, 9.8331e-01, 3.3963e-03,
        3.2576e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.191

[Epoch: 172, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8086e-01, 3.3925e-07, 1.2620e-07, 3.4142e-10, 7.0451e-09, 3.4879e-09,
        2.1914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.296

[Epoch: 172, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0034, 0.9796, 0.0033, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 172, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0034, 0.0034, 0.9795, 0.0032, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 172, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0032, 0.9809, 0.0035, 0.0028, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 172, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3538e-03, 3.1537e-03, 3.5386e-03, 3.9845e-07, 9.8305e-01, 3.5222e-03,
        3.3835e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.181

[Epoch: 173, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5588e-01, 4.9842e-07, 1.5331e-07, 4.0051e-10, 8.3002e-09, 2.1559e-09,
        2.4412e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.260

[Epoch: 173, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0037, 0.9785, 0.0036, 0.0041, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 173, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0038, 0.0037, 0.9790, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.053

[Epoch: 173, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0034, 0.9803, 0.0031, 0.0030, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 173, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7967e-03, 2.9557e-03, 2.6882e-03, 2.1953e-07, 9.8467e-01, 3.4297e-03,
        3.4577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.211

[Epoch: 174, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.9843e-01, 7.1391e-07, 2.2007e-07, 6.4973e-10, 1.0961e-08, 1.1096e-08,
        2.0157e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.282

[Epoch: 174, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0031, 0.9784, 0.0047, 0.0032, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 174, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0038, 0.0047, 0.0046, 0.9737, 0.0045, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.045

[Epoch: 174, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0037, 0.0038, 0.9782, 0.0040, 0.0035, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 174, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8932e-03, 3.9871e-03, 3.4208e-03, 4.4876e-07, 9.8379e-01, 2.8394e-03,
        3.0648e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.209

[Epoch: 175, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6187e-01, 5.7181e-07, 1.5357e-07, 3.4938e-10, 5.4375e-09, 1.7993e-09,
        2.3813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.329

[Epoch: 175, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0032, 0.9797, 0.0029, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 175, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0033, 0.0032, 0.0035, 0.9822, 0.0033, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.058

[Epoch: 175, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0042, 0.0040, 0.9765, 0.0039, 0.0036, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 175, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5878e-03, 3.9652e-03, 3.9337e-03, 4.2289e-07, 9.8163e-01, 3.1010e-03,
        3.7850e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.170

[Epoch: 176, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6792e-01, 3.5333e-07, 1.5203e-07, 2.4331e-10, 6.7971e-09, 5.8238e-09,
        2.3208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.261

[Epoch: 176, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0027, 0.0029, 0.9839, 0.0026, 0.0027, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 176, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0028, 0.0028, 0.0030, 0.9826, 0.0024, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.057

[Epoch: 176, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0033, 0.0032, 0.9812, 0.0029, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 176, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.3646e-03, 3.4984e-03, 3.9813e-03, 5.9931e-07, 9.8214e-01, 3.7517e-03,
        3.2604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.178

[Epoch: 177, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7979e-01, 5.9383e-07, 1.2166e-07, 3.8665e-10, 1.1761e-08, 5.7388e-09,
        2.2021e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.287

[Epoch: 177, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0032, 0.9804, 0.0036, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 177, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0032, 0.0039, 0.0035, 0.9778, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 177, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.0037, 0.9782, 0.0037, 0.0039, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 177, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.3682e-03, 2.6259e-03, 2.6657e-03, 4.0933e-07, 9.8673e-01, 2.7106e-03,
        2.9000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.215

[Epoch: 178, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6554e-01, 6.0917e-07, 1.2847e-07, 3.7209e-10, 5.3412e-09, 4.9868e-09,
        2.3446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.269

[Epoch: 178, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0040, 0.0036, 0.9779, 0.0037, 0.0037, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 178, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0038, 0.0039, 0.0034, 0.9789, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.060

[Epoch: 178, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0034, 0.9794, 0.0037, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 178, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.1732e-03, 3.5607e-03, 3.1418e-03, 4.1312e-07, 9.8330e-01, 3.2363e-03,
        3.5887e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.169

[Epoch: 179, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8209e-01, 6.4395e-07, 1.5894e-07, 5.5552e-10, 9.3342e-09, 4.2642e-09,
        2.1791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.297

[Epoch: 179, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0031, 0.9819, 0.0027, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 179, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.0039, 0.0039, 0.9778, 0.0035, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.061

[Epoch: 179, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0038, 0.0037, 0.9782, 0.0040, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 179, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4041e-03, 3.4386e-03, 3.3105e-03, 4.4251e-07, 9.8273e-01, 3.3648e-03,
        3.7558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.200

[Epoch: 180, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.5924e-01, 7.2789e-07, 1.6530e-07, 5.6884e-10, 1.1275e-08, 7.7798e-09,
        2.4076e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 180, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0030, 0.9814, 0.0030, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 180, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0029, 0.0029, 0.0028, 0.9818, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 180, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0039, 0.0035, 0.9787, 0.0036, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 180, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4602e-03, 3.7070e-03, 3.4266e-03, 3.2650e-07, 9.8242e-01, 3.3314e-03,
        3.6580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.200

[Epoch: 181, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8067e-01, 3.4834e-07, 8.8515e-08, 2.5835e-10, 4.6714e-09, 2.0797e-09,
        2.1933e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.271

[Epoch: 181, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0034, 0.9799, 0.0037, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 181, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0027, 0.0032, 0.0028, 0.9821, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.045

[Epoch: 181, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0030, 0.9814, 0.0030, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 181, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7440e-03, 3.5332e-03, 3.0046e-03, 4.1760e-07, 9.8460e-01, 3.2229e-03,
        2.8904e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.176

[Epoch: 182, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6858e-01, 6.2401e-07, 1.4404e-07, 6.2177e-10, 9.7887e-09, 8.1921e-09,
        2.3142e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.298

[Epoch: 182, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0033, 0.9802, 0.0032, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 182, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0041, 0.0044, 0.9781, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 182, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0033, 0.9802, 0.0036, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 182, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.7092e-03, 2.9721e-03, 2.7119e-03, 5.6713e-07, 9.8618e-01, 2.6525e-03,
        2.7745e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.186

[Epoch: 183, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8131e-01, 5.8986e-07, 1.2385e-07, 3.5648e-10, 5.6960e-09, 3.6888e-09,
        2.1868e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.278

[Epoch: 183, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0030, 0.9810, 0.0032, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 183, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0042, 0.0045, 0.0049, 0.9743, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.075

[Epoch: 183, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0039, 0.0034, 0.9777, 0.0044, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 183, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.4416e-03, 3.6881e-03, 3.3480e-03, 3.3937e-07, 9.8145e-01, 3.9914e-03,
        4.0777e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.204

[Epoch: 184, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6421e-01, 3.0395e-07, 8.7196e-08, 3.8833e-10, 5.9356e-09, 2.3944e-09,
        2.3579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 184, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0033, 0.9814, 0.0030, 0.0034, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 184, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0031, 0.0030, 0.0020, 0.9830, 0.0035, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.038

[Epoch: 184, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.0042, 0.9788, 0.0033, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 184, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5222e-03, 3.8068e-03, 3.6924e-03, 3.1684e-07, 9.8200e-01, 3.4556e-03,
        3.5224e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.208

[Epoch: 185, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8408e-01, 4.1810e-07, 8.4023e-08, 2.8347e-10, 6.6455e-09, 3.1095e-09,
        2.1592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.266

[Epoch: 185, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0043, 0.0040, 0.9777, 0.0036, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 185, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0035, 0.0042, 0.9795, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.045

[Epoch: 185, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0031, 0.0033, 0.9821, 0.0029, 0.0034, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 185, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8384e-03, 3.5000e-03, 3.0193e-03, 4.0912e-07, 9.8423e-01, 3.8964e-03,
        2.5200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.193

[Epoch: 186, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6787e-01, 5.8243e-07, 2.2775e-07, 8.7909e-10, 9.1823e-09, 7.6526e-09,
        2.3213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.310

[Epoch: 186, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.0030, 0.9797, 0.0035, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 186, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0038, 0.0036, 0.9795, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.070

[Epoch: 186, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0032, 0.9795, 0.0040, 0.0034, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 186, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.6607e-03, 4.0576e-03, 3.1678e-03, 2.9215e-07, 9.8282e-01, 3.0123e-03,
        4.2785e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.173

[Epoch: 187, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7648e-01, 6.6306e-07, 1.4064e-07, 1.0444e-09, 7.5195e-09, 4.4062e-09,
        2.2352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.274

[Epoch: 187, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0028, 0.0030, 0.9820, 0.0030, 0.0033, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 187, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0027, 0.0028, 0.9815, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.056

[Epoch: 187, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0038, 0.0039, 0.9760, 0.0045, 0.0038, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 187, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.5010e-03, 3.7888e-03, 3.8000e-03, 4.1015e-07, 9.8149e-01, 3.8905e-03,
        3.5304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 188, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7448e-01, 3.5239e-07, 8.5842e-08, 3.4638e-10, 5.8644e-09, 5.6802e-09,
        2.2552e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.271

[Epoch: 188, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0038, 0.9801, 0.0031, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 188, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0032, 0.0046, 0.0042, 0.9758, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 188, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.0031, 0.9826, 0.0028, 0.0030, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 188, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.2108e-03, 2.4940e-03, 3.2985e-03, 3.0107e-07, 9.8416e-01, 3.8325e-03,
        3.0003e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.190

[Epoch: 189, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8571e-01, 3.6603e-07, 1.4545e-07, 6.4011e-10, 6.8860e-09, 4.8926e-09,
        2.1429e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.296

[Epoch: 189, batch: 80/203] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0041, 0.0037, 0.9776, 0.0041, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 189, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0032, 0.0032, 0.9784, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 189, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0036, 0.9788, 0.0036, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 189, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([2.8718e-03, 3.5306e-03, 3.6840e-03, 6.3229e-07, 9.8263e-01, 3.7388e-03,
        3.5394e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.211

[Epoch: 190, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6093e-01, 5.7821e-07, 1.2241e-07, 6.0093e-10, 5.3053e-09, 3.3356e-09,
        2.3907e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.275

[Epoch: 190, batch: 80/203] total loss per batch: 0.492
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0036, 0.0029, 0.9807, 0.0030, 0.0033, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 190, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0035, 0.0036, 0.0035, 0.9786, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.071

[Epoch: 190, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0040, 0.0035, 0.9780, 0.0038, 0.0030, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 190, batch: 200/203] total loss per batch: 0.531
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.3627e-03, 3.4937e-03, 4.3220e-03, 4.4442e-07, 9.7980e-01, 4.1829e-03,
        3.8347e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.180

[Epoch: 191, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6308e-01, 5.7794e-07, 1.7800e-07, 4.7882e-10, 2.4245e-09, 1.5731e-09,
        2.3692e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.285

[Epoch: 191, batch: 80/203] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0032, 0.9804, 0.0032, 0.0038, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 191, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0029, 0.0033, 0.9815, 0.0031, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 191, batch: 160/203] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0034, 0.9775, 0.0039, 0.0035, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 191, batch: 200/203] total loss per batch: 0.532
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.0828e-03, 3.3991e-03, 3.3694e-03, 6.8390e-07, 9.8244e-01, 3.0552e-03,
        3.6495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.177

[Epoch: 192, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8061e-01, 8.8887e-07, 1.3328e-07, 1.4278e-09, 1.3527e-09, 2.6109e-09,
        2.1939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.264

[Epoch: 192, batch: 80/203] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0032, 0.0041, 0.9770, 0.0038, 0.0030, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 192, batch: 120/203] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0045, 0.0040, 0.9787, 0.0034, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.050

[Epoch: 192, batch: 160/203] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.0026, 0.9824, 0.0029, 0.0030, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 192, batch: 200/203] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([5.3307e-03, 2.2960e-03, 3.5970e-03, 1.3173e-06, 9.7995e-01, 4.3783e-03,
        4.4445e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.179

[Epoch: 193, batch: 40/203] total loss per batch: 0.501
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.8424e-01, 8.1837e-07, 1.3508e-07, 2.5207e-09, 3.2120e-09, 1.0486e-08,
        2.1576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.291

[Epoch: 193, batch: 80/203] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0038, 0.9775, 0.0039, 0.0054, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 193, batch: 120/203] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0040, 0.0042, 0.9773, 0.0043, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.042

[Epoch: 193, batch: 160/203] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0035, 0.0031, 0.9800, 0.0032, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 193, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.5680e-03, 2.5848e-03, 3.5860e-03, 1.1300e-06, 9.8197e-01, 3.3855e-03,
        3.9078e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.194

[Epoch: 194, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.6362e-01, 8.5106e-07, 1.0749e-07, 1.6313e-09, 3.1699e-09, 8.7849e-09,
        2.3638e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.281

[Epoch: 194, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0035, 0.9789, 0.0036, 0.0040, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 194, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.0042, 0.0041, 0.9770, 0.0041, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.047

[Epoch: 194, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.0030, 0.9805, 0.0033, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 194, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.4690e-03, 2.8063e-03, 3.5695e-03, 7.9447e-07, 9.8224e-01, 3.2576e-03,
        3.6600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 195, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7389e-01, 7.7833e-07, 1.1232e-07, 1.5510e-09, 3.1260e-09, 8.8778e-09,
        2.2611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.280

[Epoch: 195, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0036, 0.9790, 0.0036, 0.0039, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 195, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.0040, 0.0040, 0.9775, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.052

[Epoch: 195, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.0030, 0.9806, 0.0033, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 195, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.2576e-03, 2.8982e-03, 3.5996e-03, 7.3020e-07, 9.8231e-01, 3.3004e-03,
        3.6289e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 196, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7173e-01, 7.3720e-07, 1.0865e-07, 1.4275e-09, 2.9859e-09, 7.9063e-09,
        2.2827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.282

[Epoch: 196, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0036, 0.9791, 0.0036, 0.0039, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 196, batch: 120/203] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0039, 0.0039, 0.9780, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 196, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0031, 0.9807, 0.0033, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 196, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.1264e-03, 2.9995e-03, 3.5590e-03, 6.6144e-07, 9.8242e-01, 3.3011e-03,
        3.5906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 197, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7231e-01, 6.9237e-07, 1.0419e-07, 1.3699e-09, 2.8550e-09, 7.3089e-09,
        2.2769e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 197, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0036, 0.9792, 0.0035, 0.0039, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 197, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.0038, 0.0037, 0.9785, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 197, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0031, 0.9806, 0.0033, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 197, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([4.0322e-03, 3.0675e-03, 3.5397e-03, 6.2359e-07, 9.8253e-01, 3.2979e-03,
        3.5347e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 198, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7365e-01, 6.5365e-07, 9.7644e-08, 1.2440e-09, 2.6899e-09, 6.6668e-09,
        2.2635e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 198, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0037, 0.9792, 0.0035, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 198, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0037, 0.0036, 0.9789, 0.0036, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 198, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0031, 0.9805, 0.0033, 0.0030, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 198, batch: 200/203] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.9362e-03, 3.1467e-03, 3.4898e-03, 5.7008e-07, 9.8260e-01, 3.3126e-03,
        3.5165e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 199, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7385e-01, 6.1661e-07, 9.3330e-08, 1.1950e-09, 2.5723e-09, 6.1580e-09,
        2.2615e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 199, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0037, 0.9793, 0.0035, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 199, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0037, 0.0035, 0.9791, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.054

[Epoch: 199, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0031, 0.9805, 0.0033, 0.0030, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 199, batch: 200/203] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.8611e-03, 3.1514e-03, 3.4761e-03, 5.4767e-07, 9.8275e-01, 3.2997e-03,
        3.4644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

[Epoch: 200, batch: 40/203] total loss per batch: 0.500
Policy (actual, predicted): 0 0
Policy data: tensor([0.7733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267])
Policy pred: tensor([7.7331e-01, 5.8479e-07, 8.7459e-08, 1.0879e-09, 2.4218e-09, 5.7583e-09,
        2.2669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.282 0.283

[Epoch: 200, batch: 80/203] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0032, 0.0037, 0.9794, 0.0034, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 200, batch: 120/203] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0032, 0.0037, 0.0034, 0.9796, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.054 -0.055

[Epoch: 200, batch: 160/203] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0035, 0.0032, 0.9803, 0.0033, 0.0030, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 200, batch: 200/203] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0000, 0.9833, 0.0033, 0.0033])
Policy pred: tensor([3.7899e-03, 3.2003e-03, 3.4173e-03, 5.1190e-07, 9.8289e-01, 3.2985e-03,
        3.3986e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.192 -0.192

