Training set samples: 6871
Batch size: 32
[Epoch: 1, batch: 43/215] total loss per batch: 2.133
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1601, 0.1489, 0.1208, 0.1187, 0.1382, 0.1813, 0.1320],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.036

[Epoch: 1, batch: 86/215] total loss per batch: 2.076
Policy (actual, predicted): 5 6
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([0.1501, 0.1541, 0.1081, 0.1191, 0.1421, 0.1612, 0.1653],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.022

[Epoch: 1, batch: 129/215] total loss per batch: 2.062
Policy (actual, predicted): 3 4
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1532, 0.1483, 0.1411, 0.1515, 0.1573, 0.1248, 0.1239],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.018

[Epoch: 1, batch: 172/215] total loss per batch: 2.039
Policy (actual, predicted): 6 0
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.1811, 0.1707, 0.1147, 0.1088, 0.1431, 0.1512, 0.1304],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.033

[Epoch: 1, batch: 215/215] total loss per batch: 2.026
Policy (actual, predicted): 2 1
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0597, 0.3350, 0.1531, 0.1899, 0.0930, 0.0437, 0.1256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.011

[Epoch: 2, batch: 43/215] total loss per batch: 1.944
Policy (actual, predicted): 5 0
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.2199, 0.1449, 0.1354, 0.1641, 0.1543, 0.0940, 0.0874],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 2, batch: 86/215] total loss per batch: 1.911
Policy (actual, predicted): 5 2
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([0.0605, 0.0122, 0.4478, 0.0494, 0.0093, 0.1858, 0.2349],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.034

[Epoch: 2, batch: 129/215] total loss per batch: 1.878
Policy (actual, predicted): 3 4
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1788, 0.1464, 0.1426, 0.1574, 0.1933, 0.0854, 0.0960],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.002

[Epoch: 2, batch: 172/215] total loss per batch: 1.878
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.0118, 0.3484, 0.2165, 0.0593, 0.1343, 0.0028, 0.2269],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.046

[Epoch: 2, batch: 215/215] total loss per batch: 1.833
Policy (actual, predicted): 2 1
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0473, 0.2498, 0.1097, 0.1545, 0.2323, 0.0924, 0.1140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 3, batch: 43/215] total loss per batch: 1.806
Policy (actual, predicted): 5 3
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1281, 0.1381, 0.1108, 0.2070, 0.1880, 0.1124, 0.1157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.020

[Epoch: 3, batch: 86/215] total loss per batch: 1.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([0.0252, 0.0059, 0.2365, 0.0477, 0.0098, 0.4978, 0.1770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.037

[Epoch: 3, batch: 129/215] total loss per batch: 1.819
Policy (actual, predicted): 3 1
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1676, 0.1870, 0.1419, 0.1798, 0.1418, 0.0856, 0.0963],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 3, batch: 172/215] total loss per batch: 1.825
Policy (actual, predicted): 6 4
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.0074, 0.2559, 0.1503, 0.0642, 0.2641, 0.0012, 0.2569],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.032

[Epoch: 3, batch: 215/215] total loss per batch: 1.779
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0976, 0.1555, 0.2023, 0.1729, 0.2021, 0.0975, 0.0721],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.011

[Epoch: 4, batch: 43/215] total loss per batch: 1.761
Policy (actual, predicted): 5 3
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1268, 0.1451, 0.0886, 0.2593, 0.1296, 0.1775, 0.0731],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.012

[Epoch: 4, batch: 86/215] total loss per batch: 1.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.8830e-04, 2.5599e-03, 3.4141e-01, 1.1212e-03, 2.5683e-04, 6.5347e-01,
        5.9721e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.070

[Epoch: 4, batch: 129/215] total loss per batch: 1.768
Policy (actual, predicted): 3 2
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1330, 0.1644, 0.2015, 0.1505, 0.1623, 0.1028, 0.0854],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 4, batch: 172/215] total loss per batch: 1.796
Policy (actual, predicted): 6 4
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.0094, 0.2624, 0.1480, 0.0636, 0.2833, 0.0004, 0.2328],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.049

[Epoch: 4, batch: 215/215] total loss per batch: 1.745
Policy (actual, predicted): 2 4
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0442, 0.2106, 0.1204, 0.2281, 0.2643, 0.0748, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 5, batch: 43/215] total loss per batch: 1.721
Policy (actual, predicted): 5 3
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1414, 0.1747, 0.0797, 0.2375, 0.1084, 0.1321, 0.1261],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.040

[Epoch: 5, batch: 86/215] total loss per batch: 1.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.9124e-04, 7.8298e-04, 1.5127e-01, 1.6290e-04, 5.2398e-05, 8.4698e-01,
        4.6984e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.092

[Epoch: 5, batch: 129/215] total loss per batch: 1.719
Policy (actual, predicted): 3 2
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1078, 0.1213, 0.2355, 0.1992, 0.1578, 0.0875, 0.0909],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.002

[Epoch: 5, batch: 172/215] total loss per batch: 1.757
Policy (actual, predicted): 6 4
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.0100, 0.2414, 0.1835, 0.0911, 0.3106, 0.0006, 0.1629],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.098

[Epoch: 5, batch: 215/215] total loss per batch: 1.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0524, 0.1011, 0.3369, 0.1300, 0.2249, 0.0605, 0.0943],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.043

[Epoch: 6, batch: 43/215] total loss per batch: 1.669
Policy (actual, predicted): 5 3
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1918, 0.1922, 0.0725, 0.2025, 0.0972, 0.1037, 0.1400],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.019

[Epoch: 6, batch: 86/215] total loss per batch: 1.653
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1817e-04, 9.2621e-05, 9.5837e-02, 4.7505e-05, 6.4758e-06, 9.0363e-01,
        1.7108e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.047

[Epoch: 6, batch: 129/215] total loss per batch: 1.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0988, 0.0872, 0.2196, 0.2933, 0.1667, 0.0585, 0.0759],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.050

[Epoch: 6, batch: 172/215] total loss per batch: 1.719
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([0.0059, 0.2624, 0.1752, 0.0979, 0.2262, 0.0005, 0.2320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.078

[Epoch: 6, batch: 215/215] total loss per batch: 1.634
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0108, 0.1106, 0.3406, 0.1360, 0.3199, 0.0344, 0.0477],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.017

[Epoch: 7, batch: 43/215] total loss per batch: 1.606
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1318, 0.2791, 0.0628, 0.2605, 0.0815, 0.0772, 0.1071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.014

[Epoch: 7, batch: 86/215] total loss per batch: 1.597
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1064e-04, 1.1259e-04, 1.4569e-01, 1.3756e-05, 3.8175e-06, 8.5392e-01,
        4.4499e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 0.091

[Epoch: 7, batch: 129/215] total loss per batch: 1.611
Policy (actual, predicted): 3 2
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0978, 0.0831, 0.2733, 0.2552, 0.1506, 0.0654, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.011

[Epoch: 7, batch: 172/215] total loss per batch: 1.677
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.7862e-03, 2.8213e-01, 2.0357e-01, 1.2512e-01, 1.8169e-01, 2.3592e-04,
        2.0347e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.083

[Epoch: 7, batch: 215/215] total loss per batch: 1.568
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0173, 0.0995, 0.5002, 0.0913, 0.2113, 0.0556, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.170

[Epoch: 8, batch: 43/215] total loss per batch: 1.542
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1319, 0.3252, 0.0813, 0.2136, 0.0698, 0.0890, 0.0890],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.026

[Epoch: 8, batch: 86/215] total loss per batch: 1.539
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1506e-04, 2.9366e-04, 3.9204e-01, 1.2173e-05, 1.6935e-05, 6.0740e-01,
        1.5739e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 0.176

[Epoch: 8, batch: 129/215] total loss per batch: 1.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0619, 0.1104, 0.2541, 0.2670, 0.1760, 0.0694, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.025

[Epoch: 8, batch: 172/215] total loss per batch: 1.621
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.3321e-03, 2.6992e-01, 2.1084e-01, 9.7287e-02, 1.3920e-01, 1.3339e-04,
        2.7828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.011

[Epoch: 8, batch: 215/215] total loss per batch: 1.511
Policy (actual, predicted): 2 4
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0555, 0.1397, 0.0685, 0.2160, 0.3826, 0.1166, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.022

[Epoch: 9, batch: 43/215] total loss per batch: 1.479
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0877, 0.3770, 0.0623, 0.2073, 0.0986, 0.0987, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.006

[Epoch: 9, batch: 86/215] total loss per batch: 1.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.6032e-05, 3.7822e-05, 5.4238e-02, 6.5055e-06, 1.4487e-05, 9.4563e-01,
        1.4350e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.040

[Epoch: 9, batch: 129/215] total loss per batch: 1.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0514, 0.0681, 0.1985, 0.4491, 0.1539, 0.0343, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.008

[Epoch: 9, batch: 172/215] total loss per batch: 1.540
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.1098e-03, 2.3596e-01, 2.7973e-01, 1.0083e-01, 6.1788e-02, 6.9119e-05,
        3.2051e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 -0.002

[Epoch: 9, batch: 215/215] total loss per batch: 1.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0307, 0.0883, 0.5633, 0.0480, 0.1543, 0.0131, 0.1024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 10, batch: 43/215] total loss per batch: 1.413
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.1188, 0.4584, 0.0551, 0.1248, 0.1073, 0.0302, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.005

[Epoch: 10, batch: 86/215] total loss per batch: 1.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.9376e-04, 3.4981e-04, 5.9998e-02, 5.3045e-05, 3.8128e-05, 9.3914e-01,
        2.4298e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.515

[Epoch: 10, batch: 129/215] total loss per batch: 1.416
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0532, 0.0134, 0.1325, 0.6623, 0.1026, 0.0187, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.054

[Epoch: 10, batch: 172/215] total loss per batch: 1.465
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.8903e-04, 3.8284e-01, 2.3576e-01, 1.3122e-01, 4.2139e-02, 2.4095e-05,
        2.0733e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.272

[Epoch: 10, batch: 215/215] total loss per batch: 1.363
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0172, 0.0227, 0.7401, 0.0248, 0.0622, 0.0068, 0.1261],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 11, batch: 43/215] total loss per batch: 1.328
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0796, 0.4945, 0.0261, 0.1168, 0.1576, 0.0447, 0.0807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.026

[Epoch: 11, batch: 86/215] total loss per batch: 1.356
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.1251e-04, 2.6221e-04, 1.9982e-01, 8.5666e-06, 1.6520e-05, 7.9957e-01,
        7.2119e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.888

[Epoch: 11, batch: 129/215] total loss per batch: 1.339
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0734, 0.0308, 0.1108, 0.6260, 0.1218, 0.0179, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.033

[Epoch: 11, batch: 172/215] total loss per batch: 1.379
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.4714e-04, 3.1532e-01, 2.0137e-01, 1.3499e-01, 6.0098e-02, 2.0926e-05,
        2.8796e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.257

[Epoch: 11, batch: 215/215] total loss per batch: 1.291
Policy (actual, predicted): 2 6
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0272, 0.0670, 0.1357, 0.1077, 0.0972, 0.2215, 0.3436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.016

[Epoch: 12, batch: 43/215] total loss per batch: 1.268
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0855, 0.3818, 0.0571, 0.1151, 0.1982, 0.0437, 0.1186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.014

[Epoch: 12, batch: 86/215] total loss per batch: 1.284
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.0677e-05, 1.7664e-03, 4.3118e-01, 5.9782e-06, 3.8609e-04, 5.6652e-01,
        6.7721e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.868

[Epoch: 12, batch: 129/215] total loss per batch: 1.276
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0382, 0.0058, 0.0729, 0.7926, 0.0676, 0.0137, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.001

[Epoch: 12, batch: 172/215] total loss per batch: 1.321
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.4155e-04, 2.9396e-01, 2.5362e-01, 1.3492e-01, 5.9778e-02, 6.6299e-05,
        2.5742e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.231

[Epoch: 12, batch: 215/215] total loss per batch: 1.244
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0434, 0.0755, 0.6628, 0.0290, 0.0460, 0.0116, 0.1317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 13, batch: 43/215] total loss per batch: 1.213
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0876, 0.4443, 0.0789, 0.1019, 0.0961, 0.1098, 0.0814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.001

[Epoch: 13, batch: 86/215] total loss per batch: 1.218
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.9215e-05, 2.0609e-04, 4.1037e-02, 2.0629e-05, 4.9732e-05, 9.5860e-01,
        5.6940e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.879

[Epoch: 13, batch: 129/215] total loss per batch: 1.213
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0356, 0.0218, 0.0430, 0.7556, 0.1099, 0.0223, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.004

[Epoch: 13, batch: 172/215] total loss per batch: 1.246
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.7989e-05, 2.4079e-01, 1.2003e-01, 1.4859e-01, 2.0611e-01, 6.0865e-05,
        2.8433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.409

[Epoch: 13, batch: 215/215] total loss per batch: 1.181
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0271, 0.5485, 0.3090, 0.0715, 0.0187, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.058

[Epoch: 14, batch: 43/215] total loss per batch: 1.153
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0291, 0.6757, 0.0457, 0.0503, 0.0723, 0.1098, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.001

[Epoch: 14, batch: 86/215] total loss per batch: 1.171
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.6263e-05, 1.3966e-04, 3.5441e-01, 3.2201e-05, 2.5454e-05, 6.4487e-01,
        4.8544e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.892

[Epoch: 14, batch: 129/215] total loss per batch: 1.174
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0329, 0.0091, 0.0919, 0.7534, 0.0880, 0.0158, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.009

[Epoch: 14, batch: 172/215] total loss per batch: 1.195
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.6788e-05, 3.3229e-01, 2.8572e-01, 8.6676e-02, 4.4707e-02, 7.2393e-05,
        2.5047e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.156

[Epoch: 14, batch: 215/215] total loss per batch: 1.131
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0671, 0.0735, 0.7004, 0.0295, 0.0500, 0.0077, 0.0717],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 15, batch: 43/215] total loss per batch: 1.121
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0519, 0.5601, 0.0321, 0.0557, 0.1257, 0.1247, 0.0498],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.002

[Epoch: 15, batch: 86/215] total loss per batch: 1.132
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3118e-06, 1.2782e-05, 3.6877e-04, 3.6037e-06, 4.3078e-06, 9.9955e-01,
        6.1935e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.893

[Epoch: 15, batch: 129/215] total loss per batch: 1.130
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0427, 0.0170, 0.1249, 0.7020, 0.0818, 0.0214, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.010

[Epoch: 15, batch: 172/215] total loss per batch: 1.142
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.3314e-04, 1.8582e-01, 2.2359e-01, 6.0698e-02, 1.9253e-01, 2.2935e-04,
        3.3630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.100

[Epoch: 15, batch: 215/215] total loss per batch: 1.094
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.0062, 0.6068, 0.0321, 0.0495, 0.0238, 0.2787],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.003

[Epoch: 16, batch: 43/215] total loss per batch: 1.085
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0542, 0.5626, 0.0494, 0.0865, 0.0288, 0.1921, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.012

[Epoch: 16, batch: 86/215] total loss per batch: 1.079
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.4746e-05, 3.1768e-05, 6.9618e-03, 4.6301e-06, 1.4802e-06, 9.9296e-01,
        2.4458e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.876

[Epoch: 16, batch: 129/215] total loss per batch: 1.078
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0502, 0.0360, 0.0942, 0.6922, 0.0787, 0.0409, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.006

[Epoch: 16, batch: 172/215] total loss per batch: 1.116
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.4611e-04, 3.5544e-01, 1.8593e-01, 1.2087e-01, 1.0525e-01, 2.6184e-04,
        2.3150e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.124

[Epoch: 16, batch: 215/215] total loss per batch: 1.064
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0081, 0.0578, 0.5079, 0.0815, 0.2578, 0.0182, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.015

[Epoch: 17, batch: 43/215] total loss per batch: 1.032
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0622, 0.4466, 0.0429, 0.0904, 0.0407, 0.2931, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.014

[Epoch: 17, batch: 86/215] total loss per batch: 1.056
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7503e-06, 4.3777e-05, 1.7454e-02, 5.3959e-07, 1.5508e-05, 9.8248e-01,
        5.0997e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.889

[Epoch: 17, batch: 129/215] total loss per batch: 1.049
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0693, 0.0312, 0.0863, 0.5867, 0.1584, 0.0583, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.019

[Epoch: 17, batch: 172/215] total loss per batch: 1.080
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9784e-04, 3.0645e-01, 2.6781e-01, 1.1674e-01, 5.4760e-02, 2.1692e-05,
        2.5402e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.083

[Epoch: 17, batch: 215/215] total loss per batch: 0.999
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0019, 0.0019, 0.8582, 0.1004, 0.0104, 0.0088, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.010

[Epoch: 18, batch: 43/215] total loss per batch: 1.003
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0573, 0.5671, 0.0565, 0.0617, 0.0387, 0.1919, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.003

[Epoch: 18, batch: 86/215] total loss per batch: 1.017
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.9516e-06, 1.3990e-04, 1.1752e-02, 7.7642e-06, 1.0207e-04, 9.8799e-01,
        6.5631e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.905

[Epoch: 18, batch: 129/215] total loss per batch: 1.006
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0572, 0.0211, 0.0688, 0.6228, 0.1807, 0.0421, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.004

[Epoch: 18, batch: 172/215] total loss per batch: 1.034
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3260e-05, 1.7803e-01, 6.9686e-02, 4.4821e-02, 1.8064e-01, 3.2192e-05,
        5.2678e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.198

[Epoch: 18, batch: 215/215] total loss per batch: 0.986
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0169, 0.0180, 0.6921, 0.0490, 0.0166, 0.0183, 0.1891],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 19, batch: 43/215] total loss per batch: 0.997
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0452, 0.5842, 0.0152, 0.0880, 0.0536, 0.1979, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.017

[Epoch: 19, batch: 86/215] total loss per batch: 0.997
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3738e-06, 5.0322e-05, 6.1430e-03, 2.7491e-06, 5.6740e-07, 9.9377e-01,
        3.3590e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.884

[Epoch: 19, batch: 129/215] total loss per batch: 0.984
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0285, 0.0474, 0.0593, 0.7039, 0.1059, 0.0508, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.000

[Epoch: 19, batch: 172/215] total loss per batch: 1.006
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.9022e-05, 7.0424e-01, 9.6981e-02, 4.7938e-02, 9.5808e-03, 5.6933e-06,
        1.4118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.044

[Epoch: 19, batch: 215/215] total loss per batch: 0.960
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0012, 0.0021, 0.9258, 0.0506, 0.0092, 0.0031, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 20, batch: 43/215] total loss per batch: 0.968
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0264, 0.2443, 0.0174, 0.0231, 0.0209, 0.6438, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.015

[Epoch: 20, batch: 86/215] total loss per batch: 0.973
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3728e-05, 5.1781e-05, 2.6931e-02, 3.3315e-06, 3.3418e-05, 9.7293e-01,
        3.7322e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.951

[Epoch: 20, batch: 129/215] total loss per batch: 0.959
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0412, 0.0295, 0.0163, 0.6538, 0.1931, 0.0617, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.005

[Epoch: 20, batch: 172/215] total loss per batch: 0.979
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6162e-05, 2.1285e-01, 1.5830e-01, 1.5011e-01, 8.2986e-02, 7.5849e-06,
        3.9572e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 20, batch: 215/215] total loss per batch: 0.951
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0191, 0.0159, 0.5563, 0.2717, 0.0685, 0.0300, 0.0385],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 21, batch: 43/215] total loss per batch: 0.952
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0419, 0.7720, 0.0130, 0.0193, 0.0315, 0.0867, 0.0356],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.005

[Epoch: 21, batch: 86/215] total loss per batch: 0.946
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.1888e-05, 5.4447e-04, 1.0483e-01, 1.2668e-05, 1.6970e-05, 8.9451e-01,
        1.5533e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.916

[Epoch: 21, batch: 129/215] total loss per batch: 0.947
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0267, 0.0239, 0.0401, 0.7627, 0.1138, 0.0288, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.003

[Epoch: 21, batch: 172/215] total loss per batch: 0.956
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.5194e-05, 4.9933e-01, 1.6921e-01, 2.2959e-02, 2.5288e-02, 2.7395e-05,
        2.8310e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 21, batch: 215/215] total loss per batch: 0.935
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0053, 0.6476, 0.1868, 0.0083, 0.0071, 0.1370],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 22, batch: 43/215] total loss per batch: 0.922
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0107, 0.1916, 0.0166, 0.0392, 0.0240, 0.6926, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.001

[Epoch: 22, batch: 86/215] total loss per batch: 0.939
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.0203e-05, 5.2380e-05, 2.1693e-03, 2.4477e-06, 3.1897e-05, 9.9766e-01,
        1.2924e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.895

[Epoch: 22, batch: 129/215] total loss per batch: 0.930
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0433, 0.0190, 0.0189, 0.6886, 0.1885, 0.0294, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 -0.015

[Epoch: 22, batch: 172/215] total loss per batch: 0.950
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.7865e-05, 4.3393e-01, 8.9890e-02, 4.4566e-02, 3.3807e-02, 1.8368e-05,
        3.9771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.045

[Epoch: 22, batch: 215/215] total loss per batch: 0.907
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0145, 0.7865, 0.0934, 0.0616, 0.0084, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.007

[Epoch: 23, batch: 43/215] total loss per batch: 0.910
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0540, 0.4617, 0.0314, 0.0413, 0.0643, 0.3202, 0.0270],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.009

[Epoch: 23, batch: 86/215] total loss per batch: 0.917
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.8236e-05, 2.3925e-04, 3.5939e-02, 6.1621e-05, 1.1600e-06, 9.6369e-01,
        2.9464e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.804

[Epoch: 23, batch: 129/215] total loss per batch: 0.920
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0391, 0.0205, 0.0333, 0.7884, 0.0595, 0.0554, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 23, batch: 172/215] total loss per batch: 0.930
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3968e-04, 1.4506e-01, 6.8643e-02, 6.0928e-02, 2.7863e-01, 1.6445e-04,
        4.4643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.044

[Epoch: 23, batch: 215/215] total loss per batch: 0.899
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0268, 0.0138, 0.4478, 0.3797, 0.0167, 0.0248, 0.0903],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.007

[Epoch: 24, batch: 43/215] total loss per batch: 0.899
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0223, 0.2614, 0.0174, 0.0212, 0.0041, 0.6370, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.003

[Epoch: 24, batch: 86/215] total loss per batch: 0.909
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5498e-05, 4.7467e-05, 1.1266e-02, 2.9255e-06, 3.8105e-05, 9.8858e-01,
        5.1135e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.904

[Epoch: 24, batch: 129/215] total loss per batch: 0.908
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0687, 0.0321, 0.0295, 0.6010, 0.2149, 0.0476, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.002

[Epoch: 24, batch: 172/215] total loss per batch: 0.914
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.0497e-05, 4.9032e-01, 1.0686e-01, 8.2537e-02, 4.6938e-02, 1.3983e-05,
        2.7331e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.048

[Epoch: 24, batch: 215/215] total loss per batch: 0.887
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0277, 0.0124, 0.8180, 0.1166, 0.0163, 0.0020, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.008

[Epoch: 25, batch: 43/215] total loss per batch: 0.889
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0209, 0.6990, 0.0138, 0.0145, 0.0146, 0.2182, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.006

[Epoch: 25, batch: 86/215] total loss per batch: 0.900
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.7979e-05, 1.4111e-04, 1.9250e-02, 7.2594e-06, 1.7063e-06, 9.8052e-01,
        2.7610e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.849

[Epoch: 25, batch: 129/215] total loss per batch: 0.892
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0429, 0.0282, 0.0321, 0.6259, 0.1536, 0.1114, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 25, batch: 172/215] total loss per batch: 0.913
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.6726e-05, 2.1898e-01, 9.3148e-02, 9.3370e-02, 1.6112e-01, 3.5580e-05,
        4.3329e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.034

[Epoch: 25, batch: 215/215] total loss per batch: 0.885
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0167, 0.0311, 0.5173, 0.1512, 0.0250, 0.0622, 0.1966],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.006

[Epoch: 26, batch: 43/215] total loss per batch: 0.880
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0103, 0.3450, 0.0134, 0.0234, 0.0067, 0.5732, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.007

[Epoch: 26, batch: 86/215] total loss per batch: 0.883
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.8023e-05, 2.2553e-04, 8.9769e-02, 2.2872e-05, 3.1591e-05, 9.0977e-01,
        8.2034e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.935

[Epoch: 26, batch: 129/215] total loss per batch: 0.888
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1704, 0.0376, 0.0145, 0.5496, 0.1145, 0.0998, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.010

[Epoch: 26, batch: 172/215] total loss per batch: 0.903
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.2262e-05, 4.2584e-01, 1.0065e-01, 6.6859e-02, 5.7705e-02, 3.0579e-05,
        3.4884e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.037

[Epoch: 26, batch: 215/215] total loss per batch: 0.879
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0154, 0.0293, 0.7352, 0.1558, 0.0219, 0.0146, 0.0277],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 27, batch: 43/215] total loss per batch: 0.863
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0489, 0.5295, 0.0474, 0.0346, 0.0455, 0.2549, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.003

[Epoch: 27, batch: 86/215] total loss per batch: 0.884
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2486e-05, 6.1077e-06, 2.0686e-02, 4.5563e-06, 2.6630e-06, 9.7928e-01,
        4.7622e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.904

[Epoch: 27, batch: 129/215] total loss per batch: 0.873
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0264, 0.0105, 0.0122, 0.7642, 0.1394, 0.0347, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.001

[Epoch: 27, batch: 172/215] total loss per batch: 0.891
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.3948e-05, 4.8671e-01, 8.0554e-02, 6.0870e-02, 8.4184e-02, 1.2439e-05,
        2.8758e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.041

[Epoch: 27, batch: 215/215] total loss per batch: 0.856
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0388, 0.0182, 0.6361, 0.1630, 0.0095, 0.0206, 0.1138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 28, batch: 43/215] total loss per batch: 0.867
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0196, 0.3397, 0.0139, 0.0481, 0.0150, 0.5185, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.000

[Epoch: 28, batch: 86/215] total loss per batch: 0.878
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0345e-04, 1.2693e-04, 8.9272e-03, 3.7720e-06, 2.3885e-05, 9.9080e-01,
        1.4618e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.741

[Epoch: 28, batch: 129/215] total loss per batch: 0.875
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0660, 0.0256, 0.0187, 0.7121, 0.1103, 0.0636, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 28, batch: 172/215] total loss per batch: 0.886
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.8244e-05, 1.9437e-01, 1.0902e-01, 8.4043e-02, 1.0935e-01, 4.2044e-05,
        5.0309e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 28, batch: 215/215] total loss per batch: 0.847
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0080, 0.7990, 0.1600, 0.0069, 0.0061, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 29, batch: 43/215] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0252, 0.1585, 0.0157, 0.0265, 0.0273, 0.7077, 0.0389],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.004

[Epoch: 29, batch: 86/215] total loss per batch: 0.862
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.5619e-05, 1.3964e-04, 1.3318e-01, 3.8225e-06, 8.2381e-06, 8.6658e-01,
        3.2516e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.933

[Epoch: 29, batch: 129/215] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0601, 0.0120, 0.0344, 0.6657, 0.1802, 0.0406, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 29, batch: 172/215] total loss per batch: 0.870
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2112e-04, 3.5945e-01, 1.1075e-01, 5.9523e-02, 3.5390e-02, 2.9890e-05,
        4.3473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.031

[Epoch: 29, batch: 215/215] total loss per batch: 0.846
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0184, 0.0160, 0.7405, 0.1064, 0.0109, 0.0285, 0.0794],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.013

[Epoch: 30, batch: 43/215] total loss per batch: 0.857
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0182, 0.4859, 0.0313, 0.0283, 0.0170, 0.3581, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.001

[Epoch: 30, batch: 86/215] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.5966e-05, 5.3249e-05, 3.0138e-02, 3.4900e-06, 1.2960e-06, 9.6973e-01,
        2.3624e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.872

[Epoch: 30, batch: 129/215] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0514, 0.0055, 0.0213, 0.7627, 0.1075, 0.0463, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.019

[Epoch: 30, batch: 172/215] total loss per batch: 0.871
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3380e-04, 4.8068e-01, 5.7267e-02, 4.3049e-02, 9.6258e-02, 2.8760e-05,
        3.2258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.031

[Epoch: 30, batch: 215/215] total loss per batch: 0.842
Policy (actual, predicted): 2 3
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0196, 0.0168, 0.4481, 0.4567, 0.0286, 0.0179, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 31, batch: 43/215] total loss per batch: 0.850
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0151, 0.6323, 0.0097, 0.0291, 0.0375, 0.2187, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.005

[Epoch: 31, batch: 86/215] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.3472e-05, 9.1009e-05, 9.3532e-03, 6.0875e-06, 6.0883e-06, 9.9046e-01,
        1.9050e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.874

[Epoch: 31, batch: 129/215] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0758, 0.0309, 0.0126, 0.6836, 0.1129, 0.0764, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 31, batch: 172/215] total loss per batch: 0.860
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.9381e-05, 4.3437e-01, 8.3442e-02, 2.5994e-02, 3.0503e-02, 2.1192e-05,
        4.2563e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.041

[Epoch: 31, batch: 215/215] total loss per batch: 0.840
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0228, 0.0706, 0.7143, 0.1158, 0.0130, 0.0214, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 32, batch: 43/215] total loss per batch: 0.836
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0094, 0.1700, 0.0154, 0.0119, 0.0130, 0.7683, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.004

[Epoch: 32, batch: 86/215] total loss per batch: 0.849
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.5913e-05, 3.6979e-05, 1.9602e-02, 3.1979e-06, 6.1762e-07, 9.8026e-01,
        1.1071e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.936

[Epoch: 32, batch: 129/215] total loss per batch: 0.844
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0624, 0.0153, 0.0096, 0.6971, 0.1456, 0.0610, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 32, batch: 172/215] total loss per batch: 0.859
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.4589e-04, 3.1015e-01, 7.3065e-02, 5.7887e-02, 2.7880e-01, 4.5897e-05,
        2.7951e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.057

[Epoch: 32, batch: 215/215] total loss per batch: 0.830
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0269, 0.0127, 0.4616, 0.4033, 0.0149, 0.0506, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 33, batch: 43/215] total loss per batch: 0.834
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0110, 0.6152, 0.0183, 0.0165, 0.0196, 0.2918, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.004

[Epoch: 33, batch: 86/215] total loss per batch: 0.846
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.8189e-05, 1.9007e-04, 2.1187e-02, 4.3402e-06, 1.6421e-05, 9.7856e-01,
        2.1224e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.913

[Epoch: 33, batch: 129/215] total loss per batch: 0.842
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0603, 0.0114, 0.0131, 0.7581, 0.1044, 0.0470, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 33, batch: 172/215] total loss per batch: 0.864
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.7684e-05, 5.4709e-01, 7.5644e-02, 3.5866e-02, 2.0202e-02, 8.5064e-06,
        3.2116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.048

[Epoch: 33, batch: 215/215] total loss per batch: 0.835
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0018, 0.0176, 0.8580, 0.0915, 0.0150, 0.0046, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 34, batch: 43/215] total loss per batch: 0.833
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0206, 0.1111, 0.0146, 0.0280, 0.0108, 0.7850, 0.0299],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.007

[Epoch: 34, batch: 86/215] total loss per batch: 0.841
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.6360e-05, 1.5312e-04, 1.2034e-01, 1.3768e-05, 6.0368e-07, 8.7922e-01,
        1.8059e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.953

[Epoch: 34, batch: 129/215] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1020, 0.0178, 0.0206, 0.6468, 0.1388, 0.0632, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.011

[Epoch: 34, batch: 172/215] total loss per batch: 0.853
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.5306e-05, 2.2514e-01, 7.4531e-02, 6.0869e-02, 1.0326e-01, 9.3392e-05,
        5.3603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.045

[Epoch: 34, batch: 215/215] total loss per batch: 0.827
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0307, 0.8475, 0.0624, 0.0092, 0.0131, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 35, batch: 43/215] total loss per batch: 0.829
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0064, 0.4127, 0.0118, 0.0212, 0.0147, 0.5129, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 0.000

[Epoch: 35, batch: 86/215] total loss per batch: 0.837
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0277e-05, 5.1951e-05, 5.5856e-03, 5.3155e-06, 4.2568e-06, 9.9434e-01,
        6.2769e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.840

[Epoch: 35, batch: 129/215] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0645, 0.0093, 0.0125, 0.7070, 0.1324, 0.0636, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 35, batch: 172/215] total loss per batch: 0.842
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.8704e-05, 6.1908e-01, 5.2937e-02, 7.4268e-02, 1.8581e-02, 7.6571e-06,
        2.3510e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.042

[Epoch: 35, batch: 215/215] total loss per batch: 0.822
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0107, 0.0177, 0.6645, 0.2489, 0.0277, 0.0182, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 36, batch: 43/215] total loss per batch: 0.824
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0165, 0.6925, 0.0314, 0.0177, 0.0073, 0.1989, 0.0357],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.005

[Epoch: 36, batch: 86/215] total loss per batch: 0.830
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2344e-05, 9.0797e-06, 1.6612e-02, 2.4271e-06, 7.3777e-07, 9.8333e-01,
        1.2024e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.979

[Epoch: 36, batch: 129/215] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0643, 0.0147, 0.0216, 0.6875, 0.1606, 0.0407, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.007

[Epoch: 36, batch: 172/215] total loss per batch: 0.835
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.6992e-05, 2.3007e-01, 1.5706e-01, 5.5817e-02, 4.4569e-02, 5.9074e-05,
        5.1237e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.090

[Epoch: 36, batch: 215/215] total loss per batch: 0.813
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0494, 0.4371, 0.4196, 0.0153, 0.0137, 0.0580],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.010

[Epoch: 37, batch: 43/215] total loss per batch: 0.820
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0127, 0.1358, 0.0155, 0.0261, 0.0305, 0.7507, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.005

[Epoch: 37, batch: 86/215] total loss per batch: 0.830
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.6399e-05, 2.7224e-04, 3.0704e-02, 8.3102e-07, 1.3282e-05, 9.6894e-01,
        1.1239e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 37, batch: 129/215] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0605, 0.0074, 0.0074, 0.7596, 0.1201, 0.0357, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 37, batch: 172/215] total loss per batch: 0.838
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0156e-05, 2.4763e-01, 6.3448e-02, 3.0540e-02, 8.9966e-02, 1.4546e-05,
        5.6839e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 37, batch: 215/215] total loss per batch: 0.812
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.0369, 0.9058, 0.0381, 0.0069, 0.0059, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 38, batch: 43/215] total loss per batch: 0.819
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0196, 0.2956, 0.0195, 0.0416, 0.0162, 0.5688, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.007

[Epoch: 38, batch: 86/215] total loss per batch: 0.824
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.6418e-05, 1.1832e-05, 2.7007e-02, 2.6469e-06, 2.6477e-07, 9.7292e-01,
        3.5815e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.961

[Epoch: 38, batch: 129/215] total loss per batch: 0.827
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0858, 0.0127, 0.0151, 0.6575, 0.1409, 0.0743, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 38, batch: 172/215] total loss per batch: 0.841
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6410e-04, 4.7913e-01, 5.0164e-02, 3.4383e-02, 9.1475e-03, 2.3786e-05,
        4.2698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.040

[Epoch: 38, batch: 215/215] total loss per batch: 0.815
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.0287, 0.7024, 0.1801, 0.0245, 0.0174, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 39, batch: 43/215] total loss per batch: 0.815
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0120, 0.3266, 0.0131, 0.0288, 0.0277, 0.5665, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.007

[Epoch: 39, batch: 86/215] total loss per batch: 0.823
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.3563e-06, 2.9220e-05, 1.3963e-02, 4.4075e-07, 3.5636e-06, 9.8598e-01,
        1.5031e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.933

[Epoch: 39, batch: 129/215] total loss per batch: 0.823
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0553, 0.0136, 0.0159, 0.7091, 0.1636, 0.0310, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.017

[Epoch: 39, batch: 172/215] total loss per batch: 0.838
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.5541e-05, 4.3588e-01, 1.3279e-01, 2.9707e-02, 1.1084e-01, 4.8466e-05,
        2.9067e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.060

[Epoch: 39, batch: 215/215] total loss per batch: 0.811
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.0389, 0.5005, 0.3991, 0.0201, 0.0182, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 40, batch: 43/215] total loss per batch: 0.811
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0167, 0.5784, 0.0376, 0.0232, 0.0147, 0.3114, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.009

[Epoch: 40, batch: 86/215] total loss per batch: 0.822
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7319e-05, 1.3344e-05, 4.7957e-03, 3.3860e-06, 1.2384e-06, 9.9514e-01,
        3.1729e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.984

[Epoch: 40, batch: 129/215] total loss per batch: 0.818
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1038, 0.0065, 0.0141, 0.6924, 0.1508, 0.0233, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 40, batch: 172/215] total loss per batch: 0.835
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.5462e-05, 4.2517e-01, 5.7013e-02, 6.3509e-02, 4.5876e-02, 2.0389e-05,
        4.0833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.180

[Epoch: 40, batch: 215/215] total loss per batch: 0.808
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0217, 0.0303, 0.7276, 0.1583, 0.0277, 0.0098, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 41, batch: 43/215] total loss per batch: 0.812
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0229, 0.2101, 0.0113, 0.0175, 0.0262, 0.6867, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.008

[Epoch: 41, batch: 86/215] total loss per batch: 0.823
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0673e-05, 3.6338e-05, 5.0514e-02, 1.3065e-06, 1.7520e-06, 9.4943e-01,
        8.0245e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.949

[Epoch: 41, batch: 129/215] total loss per batch: 0.816
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0692, 0.0112, 0.0124, 0.7424, 0.1081, 0.0481, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 41, batch: 172/215] total loss per batch: 0.830
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.7913e-05, 2.6153e-01, 9.6791e-02, 3.6776e-02, 7.5379e-02, 1.4327e-05,
        5.2944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.037

[Epoch: 41, batch: 215/215] total loss per batch: 0.805
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0134, 0.0242, 0.7635, 0.1371, 0.0089, 0.0277, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 42, batch: 43/215] total loss per batch: 0.815
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0156, 0.2931, 0.0213, 0.0345, 0.0185, 0.5910, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.010

[Epoch: 42, batch: 86/215] total loss per batch: 0.822
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.3880e-05, 5.8409e-05, 2.5156e-02, 1.2290e-06, 2.0326e-06, 9.7474e-01,
        2.3356e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.943

[Epoch: 42, batch: 129/215] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0837, 0.0141, 0.0132, 0.6767, 0.1583, 0.0464, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 42, batch: 172/215] total loss per batch: 0.828
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0689e-04, 5.9381e-01, 5.9474e-02, 2.6654e-02, 1.1747e-01, 1.6108e-05,
        2.0247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.052

[Epoch: 42, batch: 215/215] total loss per batch: 0.804
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0279, 0.7959, 0.1456, 0.0123, 0.0071, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 43, batch: 43/215] total loss per batch: 0.814
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0179, 0.3982, 0.0144, 0.0185, 0.0186, 0.5221, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.005

[Epoch: 43, batch: 86/215] total loss per batch: 0.823
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7648e-05, 2.4001e-05, 5.3823e-03, 2.7431e-06, 9.4577e-07, 9.9456e-01,
        1.3305e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.980

[Epoch: 43, batch: 129/215] total loss per batch: 0.822
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0921, 0.0109, 0.0173, 0.7188, 0.1071, 0.0479, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.004

[Epoch: 43, batch: 172/215] total loss per batch: 0.829
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.7467e-05, 2.7684e-01, 6.1024e-02, 5.2263e-02, 5.8978e-02, 2.6348e-05,
        5.5082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.039

[Epoch: 43, batch: 215/215] total loss per batch: 0.807
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0652, 0.7621, 0.1165, 0.0143, 0.0152, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 44, batch: 43/215] total loss per batch: 0.811
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0203, 0.4370, 0.0167, 0.0454, 0.0376, 0.4207, 0.0222],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.011

[Epoch: 44, batch: 86/215] total loss per batch: 0.820
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.0575e-05, 8.9108e-05, 2.8869e-02, 4.4796e-07, 2.0241e-06, 9.7098e-01,
        1.5513e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.963

[Epoch: 44, batch: 129/215] total loss per batch: 0.816
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0839, 0.0170, 0.0155, 0.7032, 0.1235, 0.0492, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 44, batch: 172/215] total loss per batch: 0.824
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9979e-04, 4.6204e-01, 6.7065e-02, 2.2912e-02, 6.5819e-02, 9.7761e-06,
        3.8195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.048

[Epoch: 44, batch: 215/215] total loss per batch: 0.805
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0263, 0.4781, 0.4147, 0.0333, 0.0188, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 45, batch: 43/215] total loss per batch: 0.810
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.1956, 0.0098, 0.0231, 0.0188, 0.6979, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.010

[Epoch: 45, batch: 86/215] total loss per batch: 0.811
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.0604e-05, 2.9597e-05, 1.4500e-02, 9.8110e-07, 2.6522e-06, 9.8542e-01,
        2.8625e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.945

[Epoch: 45, batch: 129/215] total loss per batch: 0.813
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0842, 0.0076, 0.0118, 0.7273, 0.1225, 0.0392, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 45, batch: 172/215] total loss per batch: 0.822
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.4106e-05, 4.1813e-01, 8.2588e-02, 4.7106e-02, 6.3047e-02, 1.2179e-05,
        3.8909e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.078

[Epoch: 45, batch: 215/215] total loss per batch: 0.803
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0030, 0.0134, 0.8735, 0.0804, 0.0080, 0.0103, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 46, batch: 43/215] total loss per batch: 0.804
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0279, 0.5509, 0.0254, 0.0196, 0.0152, 0.3243, 0.0367],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.013

[Epoch: 46, batch: 86/215] total loss per batch: 0.808
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5933e-05, 4.1771e-05, 1.9275e-02, 7.2473e-07, 3.0090e-06, 9.8065e-01,
        1.6184e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.978

[Epoch: 46, batch: 129/215] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0778, 0.0118, 0.0139, 0.7103, 0.1478, 0.0322, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.008

[Epoch: 46, batch: 172/215] total loss per batch: 0.816
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.3216e-05, 4.7535e-01, 5.3291e-02, 2.2802e-02, 6.6836e-02, 1.3334e-05,
        3.8166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.082

[Epoch: 46, batch: 215/215] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0600, 0.5811, 0.3131, 0.0183, 0.0112, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 47, batch: 43/215] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0226, 0.3576, 0.0101, 0.0308, 0.0210, 0.5443, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.010

[Epoch: 47, batch: 86/215] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.0948e-05, 8.5741e-05, 3.2330e-02, 2.3773e-06, 2.3399e-06, 9.6754e-01,
        2.1045e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.973

[Epoch: 47, batch: 129/215] total loss per batch: 0.804
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0804, 0.0153, 0.0091, 0.7071, 0.1206, 0.0536, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 47, batch: 172/215] total loss per batch: 0.817
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.6781e-05, 3.7088e-01, 4.6671e-02, 2.7348e-02, 5.5186e-02, 1.9108e-05,
        4.9986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.101

[Epoch: 47, batch: 215/215] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0315, 0.7627, 0.1391, 0.0162, 0.0134, 0.0306],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 48, batch: 43/215] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0137, 0.1879, 0.0155, 0.0192, 0.0149, 0.7215, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 48, batch: 86/215] total loss per batch: 0.809
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.0397e-05, 7.0758e-05, 2.7202e-02, 2.5676e-06, 1.9912e-06, 9.7267e-01,
        1.4661e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.933

[Epoch: 48, batch: 129/215] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0914, 0.0121, 0.0083, 0.7284, 0.1210, 0.0291, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 48, batch: 172/215] total loss per batch: 0.816
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2593e-04, 3.8154e-01, 8.7427e-02, 3.2304e-02, 5.1033e-02, 7.5025e-06,
        4.4756e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 48, batch: 215/215] total loss per batch: 0.795
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0201, 0.1596, 0.5351, 0.2404, 0.0102, 0.0243, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 49, batch: 43/215] total loss per batch: 0.799
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0179, 0.6223, 0.0132, 0.0241, 0.0126, 0.3031, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.010

[Epoch: 49, batch: 86/215] total loss per batch: 0.811
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.2683e-06, 1.8890e-05, 5.1487e-03, 3.1512e-07, 6.4702e-07, 9.9482e-01,
        6.1528e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.943

[Epoch: 49, batch: 129/215] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1051, 0.0093, 0.0082, 0.7286, 0.1077, 0.0284, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 49, batch: 172/215] total loss per batch: 0.817
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6842e-05, 4.8400e-01, 6.0356e-02, 6.9999e-02, 5.8459e-02, 2.2406e-05,
        3.2714e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.180

[Epoch: 49, batch: 215/215] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0154, 0.7929, 0.1447, 0.0139, 0.0146, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 50, batch: 43/215] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0194, 0.2017, 0.0215, 0.0208, 0.0104, 0.7072, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.013

[Epoch: 50, batch: 86/215] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2597e-05, 5.3879e-05, 1.2480e-02, 2.7203e-06, 1.3231e-06, 9.8743e-01,
        2.0108e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 50, batch: 129/215] total loss per batch: 0.804
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1083, 0.0231, 0.0102, 0.7329, 0.0912, 0.0262, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 50, batch: 172/215] total loss per batch: 0.817
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.6565e-05, 3.9538e-01, 8.4872e-02, 2.0000e-02, 8.6712e-02, 1.2013e-05,
        4.1294e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 50, batch: 215/215] total loss per batch: 0.794
Policy (actual, predicted): 2 3
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0156, 0.0316, 0.4248, 0.4688, 0.0155, 0.0276, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 51, batch: 43/215] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0202, 0.4069, 0.0139, 0.0325, 0.0197, 0.4909, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.012

[Epoch: 51, batch: 86/215] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.5034e-06, 2.2440e-05, 7.6593e-03, 4.7373e-07, 1.8992e-06, 9.9230e-01,
        1.1444e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.980

[Epoch: 51, batch: 129/215] total loss per batch: 0.797
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0798, 0.0074, 0.0150, 0.7131, 0.1441, 0.0327, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 51, batch: 172/215] total loss per batch: 0.812
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.4139e-05, 3.3965e-01, 4.6830e-02, 2.3681e-02, 3.5910e-02, 4.7463e-06,
        5.5389e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.078

[Epoch: 51, batch: 215/215] total loss per batch: 0.784
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0207, 0.8432, 0.0983, 0.0129, 0.0124, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 52, batch: 43/215] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0102, 0.4466, 0.0125, 0.0188, 0.0181, 0.4800, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.013

[Epoch: 52, batch: 86/215] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.2908e-05, 1.6215e-04, 6.1120e-03, 5.3922e-07, 2.3876e-06, 9.9370e-01,
        2.3532e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.974

[Epoch: 52, batch: 129/215] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0940, 0.0077, 0.0088, 0.6905, 0.1531, 0.0375, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.017

[Epoch: 52, batch: 172/215] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.4098e-05, 4.0336e-01, 4.8567e-02, 4.6014e-02, 7.7222e-02, 1.9140e-05,
        4.2476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 52, batch: 215/215] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0412, 0.6478, 0.2508, 0.0158, 0.0189, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 53, batch: 43/215] total loss per batch: 0.770
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0157, 0.3045, 0.0150, 0.0170, 0.0148, 0.6054, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 53, batch: 86/215] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3013e-05, 2.7356e-05, 9.9205e-03, 7.0434e-07, 1.2499e-06, 9.9003e-01,
        6.8954e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 53, batch: 129/215] total loss per batch: 0.776
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0911, 0.0071, 0.0106, 0.6976, 0.1378, 0.0459, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 53, batch: 172/215] total loss per batch: 0.787
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.7832e-05, 4.5459e-01, 7.0898e-02, 2.3833e-02, 4.6088e-02, 8.1921e-06,
        4.0454e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.068

[Epoch: 53, batch: 215/215] total loss per batch: 0.762
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0016, 0.0135, 0.7657, 0.1912, 0.0124, 0.0100, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 54, batch: 43/215] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0129, 0.3882, 0.0227, 0.0180, 0.0271, 0.5124, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 54, batch: 86/215] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.3041e-06, 2.6021e-05, 6.3768e-03, 3.8535e-07, 1.1343e-06, 9.9358e-01,
        3.1716e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 54, batch: 129/215] total loss per batch: 0.776
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0864, 0.0068, 0.0075, 0.7037, 0.1537, 0.0348, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 54, batch: 172/215] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.9828e-05, 3.8854e-01, 4.3349e-02, 3.1327e-02, 3.8683e-02, 1.5243e-05,
        4.9804e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 54, batch: 215/215] total loss per batch: 0.766
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0375, 0.6921, 0.2159, 0.0219, 0.0136, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 55, batch: 43/215] total loss per batch: 0.771
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0113, 0.3966, 0.0108, 0.0208, 0.0149, 0.5311, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 55, batch: 86/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2919e-05, 2.4379e-05, 1.4968e-02, 4.8294e-07, 1.1482e-06, 9.8497e-01,
        2.3552e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 55, batch: 129/215] total loss per batch: 0.780
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1073, 0.0102, 0.0075, 0.6678, 0.1502, 0.0487, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 55, batch: 172/215] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.1444e-05, 3.5894e-01, 7.5584e-02, 2.9551e-02, 3.7511e-02, 3.6298e-06,
        4.9836e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.077

[Epoch: 55, batch: 215/215] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0639, 0.5709, 0.3266, 0.0125, 0.0154, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 56, batch: 43/215] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0152, 0.3195, 0.0200, 0.0165, 0.0136, 0.5880, 0.0273],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 56, batch: 86/215] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.4106e-06, 9.9317e-06, 5.3740e-03, 3.5426e-07, 7.2789e-07, 9.9461e-01,
        4.0978e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 56, batch: 129/215] total loss per batch: 0.784
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0910, 0.0070, 0.0067, 0.7437, 0.1140, 0.0292, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 56, batch: 172/215] total loss per batch: 0.793
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.4833e-05, 5.1689e-01, 6.5445e-02, 3.7694e-02, 3.7035e-02, 1.3622e-05,
        3.4288e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.045

[Epoch: 56, batch: 215/215] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0248, 0.7973, 0.1430, 0.0176, 0.0067, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 57, batch: 43/215] total loss per batch: 0.779
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0168, 0.5070, 0.0107, 0.0249, 0.0136, 0.4117, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.017

[Epoch: 57, batch: 86/215] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.6625e-06, 8.8751e-06, 9.2526e-03, 4.4770e-07, 1.9829e-06, 9.9069e-01,
        4.0779e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 57, batch: 129/215] total loss per batch: 0.789
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1121, 0.0090, 0.0061, 0.6659, 0.1519, 0.0492, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.010

[Epoch: 57, batch: 172/215] total loss per batch: 0.798
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.8829e-05, 4.2964e-01, 5.7291e-02, 2.7290e-02, 7.4243e-02, 5.8948e-06,
        4.1147e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.076

[Epoch: 57, batch: 215/215] total loss per batch: 0.771
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0155, 0.0503, 0.6337, 0.2576, 0.0139, 0.0147, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 58, batch: 43/215] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0136, 0.2298, 0.0136, 0.0148, 0.0161, 0.6835, 0.0285],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 58, batch: 86/215] total loss per batch: 0.790
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.5860e-06, 2.2786e-05, 2.1488e-02, 3.3176e-07, 9.3681e-07, 9.7848e-01,
        7.8802e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.982

[Epoch: 58, batch: 129/215] total loss per batch: 0.791
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0840, 0.0043, 0.0065, 0.7403, 0.1363, 0.0220, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.010

[Epoch: 58, batch: 172/215] total loss per batch: 0.797
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6681e-05, 3.7468e-01, 5.8465e-02, 2.5007e-02, 3.1050e-02, 3.4481e-06,
        5.1077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.083

[Epoch: 58, batch: 215/215] total loss per batch: 0.770
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0330, 0.7280, 0.2034, 0.0102, 0.0109, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 59, batch: 43/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0183, 0.4045, 0.0244, 0.0359, 0.0314, 0.4666, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 59, batch: 86/215] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.5146e-06, 5.2955e-06, 3.0108e-03, 1.1457e-07, 8.2827e-07, 9.9696e-01,
        1.5802e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.977

[Epoch: 59, batch: 129/215] total loss per batch: 0.790
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0784, 0.0120, 0.0066, 0.7349, 0.1297, 0.0323, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 59, batch: 172/215] total loss per batch: 0.795
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.2222e-05, 4.6759e-01, 7.3812e-02, 4.0481e-02, 6.5644e-02, 2.2091e-05,
        3.5235e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.043

[Epoch: 59, batch: 215/215] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0297, 0.7762, 0.1606, 0.0091, 0.0099, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 60, batch: 43/215] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0170, 0.3659, 0.0098, 0.0176, 0.0032, 0.5770, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.014

[Epoch: 60, batch: 86/215] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3536e-05, 1.0299e-05, 2.6586e-02, 7.0300e-07, 7.1629e-07, 9.7338e-01,
        9.1996e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.979

[Epoch: 60, batch: 129/215] total loss per batch: 0.788
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0920, 0.0037, 0.0052, 0.7163, 0.1446, 0.0321, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.019

[Epoch: 60, batch: 172/215] total loss per batch: 0.794
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9574e-05, 3.5932e-01, 5.5837e-02, 2.9996e-02, 4.5162e-02, 6.2637e-06,
        5.0966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.045

[Epoch: 60, batch: 215/215] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0829, 0.6064, 0.2668, 0.0187, 0.0117, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 61, batch: 43/215] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0198, 0.4479, 0.0162, 0.0156, 0.0279, 0.4523, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 61, batch: 86/215] total loss per batch: 0.788
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.6298e-06, 2.2652e-05, 1.0471e-02, 1.4814e-07, 8.8881e-07, 9.8950e-01,
        3.3369e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.976

[Epoch: 61, batch: 129/215] total loss per batch: 0.781
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0858, 0.0175, 0.0078, 0.6732, 0.1687, 0.0401, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 61, batch: 172/215] total loss per batch: 0.795
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.7191e-05, 4.9261e-01, 6.8528e-02, 2.8093e-02, 7.4846e-02, 7.0146e-06,
        3.3586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.095

[Epoch: 61, batch: 215/215] total loss per batch: 0.770
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0249, 0.7808, 0.1714, 0.0066, 0.0077, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 62, batch: 43/215] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0164, 0.1728, 0.0118, 0.0162, 0.0033, 0.7702, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 62, batch: 86/215] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3026e-05, 1.4472e-05, 4.9529e-03, 3.1257e-07, 5.9363e-07, 9.9502e-01,
        3.5605e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.978

[Epoch: 62, batch: 129/215] total loss per batch: 0.781
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0985, 0.0073, 0.0057, 0.7215, 0.1300, 0.0316, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 62, batch: 172/215] total loss per batch: 0.791
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.7279e-05, 3.6751e-01, 5.8323e-02, 3.2275e-02, 4.4457e-02, 1.0407e-05,
        4.9738e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 62, batch: 215/215] total loss per batch: 0.768
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0657, 0.6793, 0.2088, 0.0257, 0.0071, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 63, batch: 43/215] total loss per batch: 0.772
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0277, 0.5202, 0.0174, 0.0319, 0.0063, 0.3881, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.014

[Epoch: 63, batch: 86/215] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.2027e-06, 2.4625e-05, 1.3603e-02, 8.0537e-08, 1.4652e-06, 9.8636e-01,
        7.2987e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.979

[Epoch: 63, batch: 129/215] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0853, 0.0057, 0.0077, 0.7192, 0.1447, 0.0306, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.009

[Epoch: 63, batch: 172/215] total loss per batch: 0.794
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.2251e-05, 3.4213e-01, 3.5188e-02, 2.6967e-02, 3.2843e-02, 8.2104e-06,
        5.6280e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.110

[Epoch: 63, batch: 215/215] total loss per batch: 0.771
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0277, 0.6250, 0.3021, 0.0099, 0.0180, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 64, batch: 43/215] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0157, 0.3233, 0.0210, 0.0188, 0.0161, 0.5898, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 64, batch: 86/215] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.9229e-06, 4.4340e-05, 1.7593e-02, 2.7700e-07, 5.7271e-07, 9.8234e-01,
        1.1528e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 64, batch: 129/215] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0916, 0.0091, 0.0135, 0.7141, 0.1351, 0.0310, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 64, batch: 172/215] total loss per batch: 0.793
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.9014e-06, 5.3136e-01, 6.7781e-02, 3.1923e-02, 6.5055e-02, 6.0298e-06,
        3.0386e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 64, batch: 215/215] total loss per batch: 0.770
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0356, 0.7420, 0.1742, 0.0265, 0.0072, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 65, batch: 43/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0189, 0.3165, 0.0152, 0.0167, 0.0077, 0.5986, 0.0265],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 65, batch: 86/215] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.7498e-06, 3.5542e-05, 1.3303e-02, 3.6667e-07, 2.9575e-06, 9.8664e-01,
        3.8124e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.979

[Epoch: 65, batch: 129/215] total loss per batch: 0.781
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1033, 0.0058, 0.0045, 0.7200, 0.1247, 0.0329, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.010

[Epoch: 65, batch: 172/215] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.5018e-05, 3.0095e-01, 4.6203e-02, 2.3660e-02, 3.2952e-02, 3.7901e-06,
        5.9620e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.066

[Epoch: 65, batch: 215/215] total loss per batch: 0.768
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0582, 0.6665, 0.2319, 0.0109, 0.0132, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 66, batch: 43/215] total loss per batch: 0.770
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0104, 0.3559, 0.0164, 0.0218, 0.0110, 0.5679, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 66, batch: 86/215] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.8756e-06, 5.1556e-06, 3.9847e-03, 1.1554e-07, 5.8357e-07, 9.9599e-01,
        1.0072e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.967

[Epoch: 66, batch: 129/215] total loss per batch: 0.780
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0851, 0.0078, 0.0082, 0.7263, 0.1333, 0.0341, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 66, batch: 172/215] total loss per batch: 0.788
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.1120e-05, 4.9430e-01, 6.5826e-02, 4.6652e-02, 8.5279e-02, 5.6101e-06,
        3.0789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.118

[Epoch: 66, batch: 215/215] total loss per batch: 0.766
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0351, 0.6524, 0.2476, 0.0293, 0.0180, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 67, batch: 43/215] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0120, 0.3400, 0.0143, 0.0162, 0.0108, 0.5742, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.015

[Epoch: 67, batch: 86/215] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1503e-05, 5.1291e-05, 9.4413e-03, 2.9145e-07, 3.6442e-06, 9.9049e-01,
        4.4022e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.974

[Epoch: 67, batch: 129/215] total loss per batch: 0.779
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1051, 0.0068, 0.0069, 0.6959, 0.1358, 0.0376, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 67, batch: 172/215] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6640e-05, 3.3446e-01, 4.9842e-02, 2.7513e-02, 2.5440e-02, 6.4853e-06,
        5.6272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.060

[Epoch: 67, batch: 215/215] total loss per batch: 0.763
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0018, 0.0313, 0.7655, 0.1653, 0.0192, 0.0103, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 68, batch: 43/215] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0130, 0.3405, 0.0210, 0.0244, 0.0212, 0.5698, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 68, batch: 86/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.8821e-06, 8.4649e-06, 4.6575e-03, 5.0299e-08, 3.0489e-07, 9.9532e-01,
        1.4569e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.967

[Epoch: 68, batch: 129/215] total loss per batch: 0.778
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0951, 0.0081, 0.0070, 0.7126, 0.1258, 0.0459, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.005

[Epoch: 68, batch: 172/215] total loss per batch: 0.792
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.6071e-05, 4.2141e-01, 7.5275e-02, 2.9367e-02, 6.3863e-02, 1.0565e-05,
        4.0998e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.101

[Epoch: 68, batch: 215/215] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0381, 0.7109, 0.2195, 0.0122, 0.0063, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 69, batch: 43/215] total loss per batch: 0.771
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0081, 0.5028, 0.0111, 0.0130, 0.0043, 0.4420, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 69, batch: 86/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.0689e-06, 7.2490e-06, 6.4789e-03, 8.4511e-08, 1.4929e-06, 9.9351e-01,
        2.3867e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.972

[Epoch: 69, batch: 129/215] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0719, 0.0076, 0.0057, 0.7741, 0.0998, 0.0356, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 69, batch: 172/215] total loss per batch: 0.793
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9643e-05, 4.7295e-01, 4.6447e-02, 2.8946e-02, 2.6439e-02, 2.5227e-06,
        4.2520e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.035

[Epoch: 69, batch: 215/215] total loss per batch: 0.768
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0311, 0.6972, 0.2290, 0.0161, 0.0158, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.003

[Epoch: 70, batch: 43/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0128, 0.2244, 0.0207, 0.0215, 0.0087, 0.6967, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 70, batch: 86/215] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.8905e-06, 2.0804e-05, 7.5134e-03, 3.6741e-07, 5.2380e-07, 9.9244e-01,
        1.4107e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.980

[Epoch: 70, batch: 129/215] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0737, 0.0033, 0.0100, 0.6871, 0.1822, 0.0359, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 70, batch: 172/215] total loss per batch: 0.795
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.8401e-05, 3.7103e-01, 8.0229e-02, 4.2283e-02, 9.7668e-02, 4.8631e-06,
        4.0870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.104

[Epoch: 70, batch: 215/215] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0332, 0.6995, 0.2304, 0.0086, 0.0104, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 71, batch: 43/215] total loss per batch: 0.774
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0138, 0.5488, 0.0172, 0.0183, 0.0047, 0.3777, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 71, batch: 86/215] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.0231e-06, 5.0039e-06, 1.2368e-02, 9.7045e-08, 1.3442e-06, 9.8762e-01,
        2.4804e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.975

[Epoch: 71, batch: 129/215] total loss per batch: 0.780
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0872, 0.0064, 0.0059, 0.7350, 0.1175, 0.0420, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 71, batch: 172/215] total loss per batch: 0.790
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6613e-05, 4.9277e-01, 4.9635e-02, 3.1969e-02, 4.5147e-02, 5.0367e-06,
        3.8045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.051

[Epoch: 71, batch: 215/215] total loss per batch: 0.767
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0508, 0.7311, 0.1815, 0.0116, 0.0121, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 72, batch: 43/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0135, 0.2446, 0.0228, 0.0180, 0.0064, 0.6766, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 72, batch: 86/215] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.6689e-06, 3.9551e-06, 6.1891e-03, 1.3514e-07, 5.6701e-07, 9.9379e-01,
        6.5690e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 72, batch: 129/215] total loss per batch: 0.778
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0993, 0.0056, 0.0057, 0.6729, 0.1825, 0.0280, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 72, batch: 172/215] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.6904e-05, 3.1344e-01, 8.4787e-02, 3.3608e-02, 8.0300e-02, 6.3446e-06,
        4.8779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.047

[Epoch: 72, batch: 215/215] total loss per batch: 0.765
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0576, 0.6853, 0.2175, 0.0140, 0.0101, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 73, batch: 43/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0204, 0.4524, 0.0203, 0.0132, 0.0051, 0.4799, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 73, batch: 86/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.4681e-06, 2.1294e-05, 2.0894e-02, 9.4762e-08, 7.4658e-07, 9.7908e-01,
        1.2184e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 73, batch: 129/215] total loss per batch: 0.776
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0901, 0.0069, 0.0072, 0.7417, 0.1186, 0.0316, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 73, batch: 172/215] total loss per batch: 0.785
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8967e-05, 4.9810e-01, 4.7707e-02, 2.6036e-02, 2.4311e-02, 3.0310e-06,
        4.0383e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.050

[Epoch: 73, batch: 215/215] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0170, 0.7202, 0.2344, 0.0086, 0.0101, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 74, batch: 43/215] total loss per batch: 0.771
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0113, 0.2482, 0.0197, 0.0171, 0.0034, 0.6770, 0.0233],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 74, batch: 86/215] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.0222e-06, 9.1874e-06, 1.2055e-02, 6.8067e-08, 8.2477e-07, 9.8792e-01,
        7.5693e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 74, batch: 129/215] total loss per batch: 0.776
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0956, 0.0075, 0.0063, 0.6926, 0.1631, 0.0286, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 74, batch: 172/215] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.7609e-05, 3.3794e-01, 9.5729e-02, 3.4057e-02, 9.0159e-02, 5.8966e-06,
        4.4206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.092

[Epoch: 74, batch: 215/215] total loss per batch: 0.763
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0796, 0.5640, 0.3143, 0.0122, 0.0124, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 75, batch: 43/215] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0134, 0.4072, 0.0160, 0.0250, 0.0091, 0.5163, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 75, batch: 86/215] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.1633e-06, 1.5516e-05, 6.2301e-03, 2.0714e-07, 3.2879e-07, 9.9374e-01,
        2.8269e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.956

[Epoch: 75, batch: 129/215] total loss per batch: 0.775
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0827, 0.0049, 0.0110, 0.7292, 0.1241, 0.0419, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.011

[Epoch: 75, batch: 172/215] total loss per batch: 0.786
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.5520e-05, 4.4054e-01, 6.9344e-02, 3.3658e-02, 4.2778e-02, 4.9324e-06,
        4.1362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.043

[Epoch: 75, batch: 215/215] total loss per batch: 0.762
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0022, 0.0284, 0.7789, 0.1620, 0.0125, 0.0096, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 76, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0098, 0.3413, 0.0206, 0.0171, 0.0074, 0.5927, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 76, batch: 86/215] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5533e-05, 1.8599e-05, 1.9682e-02, 1.7246e-07, 1.0545e-06, 9.8027e-01,
        8.6314e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 76, batch: 129/215] total loss per batch: 0.775
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0959, 0.0098, 0.0039, 0.6798, 0.1733, 0.0322, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 76, batch: 172/215] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.5484e-05, 3.6927e-01, 5.3145e-02, 2.8933e-02, 3.5298e-02, 3.0325e-06,
        5.1334e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.077

[Epoch: 76, batch: 215/215] total loss per batch: 0.762
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0343, 0.8086, 0.1266, 0.0108, 0.0091, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 77, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0115, 0.4871, 0.0100, 0.0334, 0.0064, 0.4307, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 77, batch: 86/215] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0490e-05, 1.0233e-05, 1.4066e-02, 1.4846e-07, 4.6034e-07, 9.8591e-01,
        4.0003e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.973

[Epoch: 77, batch: 129/215] total loss per batch: 0.775
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0925, 0.0046, 0.0094, 0.7356, 0.1123, 0.0405, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.014

[Epoch: 77, batch: 172/215] total loss per batch: 0.787
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.9901e-05, 4.2336e-01, 8.4364e-02, 3.2721e-02, 5.4583e-02, 4.0672e-06,
        4.0492e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 77, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0393, 0.5795, 0.3165, 0.0317, 0.0193, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 78, batch: 43/215] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0192, 0.2004, 0.0227, 0.0206, 0.0052, 0.7231, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.020

[Epoch: 78, batch: 86/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.4456e-06, 3.3832e-06, 5.4430e-03, 3.2907e-08, 2.8747e-07, 9.9455e-01,
        2.2845e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.984

[Epoch: 78, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0820, 0.0031, 0.0062, 0.7034, 0.1738, 0.0258, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 78, batch: 172/215] total loss per batch: 0.785
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3712e-05, 5.1545e-01, 4.8864e-02, 2.4789e-02, 3.5825e-02, 3.0472e-06,
        3.7506e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.065

[Epoch: 78, batch: 215/215] total loss per batch: 0.763
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0268, 0.7351, 0.2094, 0.0083, 0.0095, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 79, batch: 43/215] total loss per batch: 0.769
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0104, 0.5835, 0.0069, 0.0147, 0.0058, 0.3669, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 79, batch: 86/215] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.4918e-06, 1.9905e-05, 1.0130e-02, 6.8804e-08, 7.8928e-07, 9.8984e-01,
        4.9681e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 79, batch: 129/215] total loss per batch: 0.773
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1045, 0.0046, 0.0060, 0.7118, 0.1358, 0.0309, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.019

[Epoch: 79, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.3808e-05, 2.7115e-01, 6.4503e-02, 2.4273e-02, 5.3781e-02, 2.5579e-06,
        5.8625e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.058

[Epoch: 79, batch: 215/215] total loss per batch: 0.763
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0584, 0.6412, 0.2646, 0.0129, 0.0113, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 80, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0186, 0.2273, 0.0147, 0.0155, 0.0095, 0.6928, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.019

[Epoch: 80, batch: 86/215] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.3289e-06, 1.6426e-05, 8.1553e-03, 6.4300e-08, 8.5437e-07, 9.9182e-01,
        3.1826e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.975

[Epoch: 80, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0823, 0.0047, 0.0091, 0.7396, 0.1272, 0.0320, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.015

[Epoch: 80, batch: 172/215] total loss per batch: 0.785
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.2629e-05, 5.4678e-01, 5.5724e-02, 3.0338e-02, 4.2581e-02, 4.6389e-06,
        3.2452e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 80, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0291, 0.8537, 0.0940, 0.0081, 0.0058, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 81, batch: 43/215] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0130, 0.3845, 0.0169, 0.0222, 0.0051, 0.5454, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.021

[Epoch: 81, batch: 86/215] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.3273e-06, 1.7531e-05, 2.0744e-02, 1.0730e-07, 6.1993e-07, 9.7923e-01,
        3.6196e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 81, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1024, 0.0062, 0.0086, 0.6985, 0.1477, 0.0309, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.012

[Epoch: 81, batch: 172/215] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9908e-05, 3.5670e-01, 7.4795e-02, 3.1252e-02, 6.0984e-02, 1.6396e-06,
        4.7625e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 81, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0543, 0.6013, 0.2951, 0.0228, 0.0111, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 82, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0137, 0.2783, 0.0088, 0.0134, 0.0050, 0.6643, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.020

[Epoch: 82, batch: 86/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.8644e-06, 6.2188e-06, 4.9621e-03, 1.2880e-07, 4.8518e-07, 9.9502e-01,
        6.6569e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 82, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0870, 0.0019, 0.0077, 0.7056, 0.1575, 0.0342, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.000

[Epoch: 82, batch: 172/215] total loss per batch: 0.786
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.5126e-05, 4.6463e-01, 5.7229e-02, 2.4487e-02, 4.5168e-02, 8.2358e-06,
        4.0844e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.045

[Epoch: 82, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0538, 0.7277, 0.1660, 0.0186, 0.0126, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 83, batch: 43/215] total loss per batch: 0.768
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.5353, 0.0170, 0.0303, 0.0040, 0.3897, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.021

[Epoch: 83, batch: 86/215] total loss per batch: 0.774
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.9154e-06, 4.5815e-06, 7.4966e-03, 8.7111e-08, 3.8001e-07, 9.9249e-01,
        3.1646e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.972

[Epoch: 83, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0961, 0.0045, 0.0114, 0.7261, 0.1142, 0.0420, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 83, batch: 172/215] total loss per batch: 0.786
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.9907e-05, 4.1743e-01, 8.7800e-02, 3.1173e-02, 6.2741e-02, 4.4195e-06,
        4.0081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.066

[Epoch: 83, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0360, 0.7573, 0.1803, 0.0116, 0.0067, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 84, batch: 43/215] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0149, 0.2276, 0.0203, 0.0158, 0.0035, 0.6938, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.018

[Epoch: 84, batch: 86/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3328e-05, 2.7781e-05, 7.8743e-03, 3.0862e-07, 8.0539e-07, 9.9207e-01,
        1.0008e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 84, batch: 129/215] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0886, 0.0045, 0.0039, 0.7081, 0.1633, 0.0254, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.015

[Epoch: 84, batch: 172/215] total loss per batch: 0.784
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.8481e-05, 2.8429e-01, 5.3221e-02, 3.0944e-02, 4.0037e-02, 6.7956e-06,
        5.9148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 84, batch: 215/215] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0268, 0.7143, 0.2097, 0.0136, 0.0118, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 85, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0079, 0.4706, 0.0124, 0.0240, 0.0074, 0.4737, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.021

[Epoch: 85, batch: 86/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0195e-05, 1.1243e-05, 1.5588e-02, 3.5461e-08, 5.2516e-07, 9.8439e-01,
        3.6638e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.975

[Epoch: 85, batch: 129/215] total loss per batch: 0.773
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0941, 0.0097, 0.0105, 0.7058, 0.1424, 0.0332, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 85, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.5862e-05, 5.5198e-01, 6.1910e-02, 2.1963e-02, 2.4574e-02, 1.7612e-06,
        3.3955e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 85, batch: 215/215] total loss per batch: 0.759
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0024, 0.0316, 0.5559, 0.3819, 0.0103, 0.0073, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 86, batch: 43/215] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0165, 0.3103, 0.0210, 0.0184, 0.0022, 0.6181, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 86, batch: 86/215] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1876e-05, 2.6381e-05, 1.9297e-02, 2.7837e-07, 1.4442e-06, 9.8065e-01,
        1.2950e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 86, batch: 129/215] total loss per batch: 0.773
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0976, 0.0049, 0.0047, 0.7169, 0.1471, 0.0242, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 86, batch: 172/215] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.1836e-05, 3.0343e-01, 8.0155e-02, 3.5681e-02, 9.0515e-02, 6.8286e-06,
        4.9019e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.055

[Epoch: 86, batch: 215/215] total loss per batch: 0.758
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0427, 0.8035, 0.1145, 0.0109, 0.0103, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 87, batch: 43/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0096, 0.3391, 0.0173, 0.0307, 0.0151, 0.5621, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.021

[Epoch: 87, batch: 86/215] total loss per batch: 0.774
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.0927e-06, 5.9162e-06, 9.8956e-03, 7.3264e-08, 7.4491e-07, 9.9009e-01,
        4.2654e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 87, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0714, 0.0039, 0.0059, 0.7223, 0.1515, 0.0400, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 87, batch: 172/215] total loss per batch: 0.781
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.9188e-05, 3.8630e-01, 6.2028e-02, 2.6750e-02, 3.0577e-02, 3.4879e-06,
        4.9428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.058

[Epoch: 87, batch: 215/215] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0338, 0.7429, 0.1916, 0.0136, 0.0068, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 88, batch: 43/215] total loss per batch: 0.764
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0163, 0.5621, 0.0098, 0.0208, 0.0050, 0.3782, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 88, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.0422e-06, 4.6760e-06, 8.1108e-03, 1.5454e-07, 2.9900e-07, 9.9187e-01,
        3.0282e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.971

[Epoch: 88, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1054, 0.0046, 0.0091, 0.7221, 0.1226, 0.0312, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.042

[Epoch: 88, batch: 172/215] total loss per batch: 0.781
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.0029e-05, 4.3223e-01, 5.8734e-02, 3.1293e-02, 5.5186e-02, 2.2145e-06,
        4.2253e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.040

[Epoch: 88, batch: 215/215] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.0415, 0.7073, 0.2155, 0.0144, 0.0095, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 89, batch: 43/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0138, 0.1420, 0.0210, 0.0191, 0.0044, 0.7849, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 89, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3847e-06, 1.0786e-05, 1.0885e-02, 7.4443e-08, 2.1233e-06, 9.8910e-01,
        2.2401e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 89, batch: 129/215] total loss per batch: 0.772
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0885, 0.0078, 0.0065, 0.7128, 0.1361, 0.0428, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 89, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.6459e-05, 4.0726e-01, 7.4464e-02, 3.1147e-02, 4.3618e-02, 5.8037e-06,
        4.4345e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 89, batch: 215/215] total loss per batch: 0.758
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0488, 0.6313, 0.2745, 0.0180, 0.0101, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 90, batch: 43/215] total loss per batch: 0.764
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0104, 0.5142, 0.0133, 0.0295, 0.0116, 0.4119, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.014

[Epoch: 90, batch: 86/215] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.2234e-06, 1.5477e-05, 1.7964e-02, 1.3750e-07, 1.1932e-06, 9.8201e-01,
        2.7972e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.982

[Epoch: 90, batch: 129/215] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0924, 0.0053, 0.0090, 0.7234, 0.1379, 0.0267, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 90, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.2991e-05, 3.8305e-01, 7.6000e-02, 2.6466e-02, 7.9664e-02, 2.4136e-06,
        4.3474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.092

[Epoch: 90, batch: 215/215] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0768, 0.7660, 0.1196, 0.0146, 0.0086, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 91, batch: 43/215] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.3480, 0.0130, 0.0253, 0.0061, 0.5721, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.022

[Epoch: 91, batch: 86/215] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.7289e-06, 4.1758e-06, 3.6670e-03, 4.3001e-08, 4.8604e-07, 9.9632e-01,
        2.2839e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.972

[Epoch: 91, batch: 129/215] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1024, 0.0078, 0.0057, 0.6932, 0.1510, 0.0345, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 91, batch: 172/215] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.0891e-05, 3.7112e-01, 4.9826e-02, 2.3729e-02, 3.0626e-02, 2.2737e-06,
        5.2468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 91, batch: 215/215] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.0191, 0.7252, 0.2134, 0.0159, 0.0112, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 92, batch: 43/215] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.2353, 0.0113, 0.0198, 0.0040, 0.6992, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.021

[Epoch: 92, batch: 86/215] total loss per batch: 0.774
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1189e-05, 6.6649e-06, 4.9397e-03, 3.8135e-08, 1.0704e-06, 9.9504e-01,
        2.6165e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 92, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0943, 0.0041, 0.0072, 0.6958, 0.1593, 0.0321, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.016

[Epoch: 92, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.1907e-05, 4.8205e-01, 8.1687e-02, 4.4331e-02, 1.1207e-01, 2.6954e-06,
        2.7977e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.113

[Epoch: 92, batch: 215/215] total loss per batch: 0.758
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0558, 0.7224, 0.1809, 0.0142, 0.0117, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 93, batch: 43/215] total loss per batch: 0.762
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0112, 0.5285, 0.0134, 0.0380, 0.0101, 0.3841, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.020

[Epoch: 93, batch: 86/215] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5970e-06, 7.6909e-06, 8.6449e-03, 4.4666e-08, 9.6426e-07, 9.9134e-01,
        5.8367e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.975

[Epoch: 93, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0915, 0.0056, 0.0053, 0.7463, 0.1085, 0.0361, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 93, batch: 172/215] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3198e-05, 3.2386e-01, 5.7271e-02, 2.3148e-02, 2.5823e-02, 1.1858e-06,
        5.6987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 93, batch: 215/215] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0394, 0.5972, 0.3248, 0.0138, 0.0090, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 94, batch: 43/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0107, 0.2595, 0.0141, 0.0194, 0.0034, 0.6583, 0.0347],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.022

[Epoch: 94, batch: 86/215] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.9624e-06, 1.2854e-05, 5.8819e-03, 1.0312e-07, 4.7871e-07, 9.9409e-01,
        2.8997e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 94, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0811, 0.0037, 0.0044, 0.7578, 0.1248, 0.0216, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 94, batch: 172/215] total loss per batch: 0.779
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.1235e-05, 4.3812e-01, 9.5476e-02, 3.5416e-02, 1.0477e-01, 5.4028e-06,
        3.2619e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 94, batch: 215/215] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.0260, 0.7623, 0.1760, 0.0177, 0.0081, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 95, batch: 43/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0166, 0.3921, 0.0163, 0.0168, 0.0032, 0.5495, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.022

[Epoch: 95, batch: 86/215] total loss per batch: 0.774
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.9887e-06, 3.3155e-06, 6.2977e-03, 4.7170e-08, 2.8440e-07, 9.9369e-01,
        4.4398e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 95, batch: 129/215] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1071, 0.0072, 0.0058, 0.6893, 0.1413, 0.0451, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 95, batch: 172/215] total loss per batch: 0.780
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.0399e-05, 3.9298e-01, 4.7391e-02, 2.6892e-02, 1.8603e-02, 2.7722e-06,
        5.1410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 95, batch: 215/215] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0401, 0.7572, 0.1675, 0.0118, 0.0090, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 96, batch: 43/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0144, 0.3518, 0.0214, 0.0362, 0.0071, 0.5432, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.016

[Epoch: 96, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.4841e-06, 1.1948e-05, 1.7729e-02, 9.9447e-08, 9.8326e-07, 9.8225e-01,
        4.3023e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.982

[Epoch: 96, batch: 129/215] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0964, 0.0087, 0.0052, 0.7158, 0.1378, 0.0298, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 96, batch: 172/215] total loss per batch: 0.779
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3274e-05, 4.7270e-01, 8.6189e-02, 3.0005e-02, 4.7087e-02, 3.9563e-06,
        3.6400e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.116

[Epoch: 96, batch: 215/215] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0380, 0.5708, 0.3546, 0.0152, 0.0124, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 97, batch: 43/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0131, 0.4508, 0.0127, 0.0203, 0.0079, 0.4701, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 97, batch: 86/215] total loss per batch: 0.771
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3709e-06, 6.5603e-06, 5.5051e-03, 4.1931e-08, 1.2356e-06, 9.9448e-01,
        2.0983e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 97, batch: 129/215] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0960, 0.0059, 0.0053, 0.6886, 0.1646, 0.0339, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 97, batch: 172/215] total loss per batch: 0.778
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.7955e-05, 4.5116e-01, 6.3225e-02, 2.3886e-02, 4.2398e-02, 1.4757e-06,
        4.1931e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 97, batch: 215/215] total loss per batch: 0.754
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0441, 0.7413, 0.1691, 0.0242, 0.0086, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 98, batch: 43/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0070, 0.1964, 0.0188, 0.0177, 0.0048, 0.7472, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.022

[Epoch: 98, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1107e-05, 3.9667e-06, 9.1757e-03, 1.0298e-07, 9.1105e-07, 9.9079e-01,
        1.6839e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 98, batch: 129/215] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1028, 0.0081, 0.0076, 0.7236, 0.1261, 0.0264, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 98, batch: 172/215] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9287e-05, 3.0620e-01, 4.8969e-02, 2.5511e-02, 2.9909e-02, 4.4311e-06,
        5.8939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.101

[Epoch: 98, batch: 215/215] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0381, 0.7417, 0.1889, 0.0139, 0.0087, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 99, batch: 43/215] total loss per batch: 0.761
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0195, 0.5584, 0.0141, 0.0280, 0.0065, 0.3650, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 99, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.5001e-06, 1.1908e-05, 7.2413e-03, 5.1179e-08, 6.9203e-07, 9.9273e-01,
        7.8590e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 99, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0897, 0.0034, 0.0073, 0.6871, 0.1592, 0.0490, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 99, batch: 172/215] total loss per batch: 0.779
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.7712e-05, 4.6673e-01, 8.0501e-02, 3.1125e-02, 4.5697e-02, 2.1624e-06,
        3.7593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.059

[Epoch: 99, batch: 215/215] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0364, 0.5791, 0.3296, 0.0267, 0.0138, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 100, batch: 43/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0108, 0.2465, 0.0144, 0.0146, 0.0027, 0.6838, 0.0273],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 100, batch: 86/215] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.4629e-06, 1.2103e-05, 8.6571e-03, 5.8414e-08, 4.8694e-07, 9.9132e-01,
        1.7872e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.973

[Epoch: 100, batch: 129/215] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1053, 0.0101, 0.0074, 0.6731, 0.1622, 0.0339, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 100, batch: 172/215] total loss per batch: 0.778
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.4654e-05, 4.5044e-01, 5.6718e-02, 2.1585e-02, 3.4241e-02, 3.4700e-06,
        4.3700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 100, batch: 215/215] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0248, 0.8459, 0.0942, 0.0192, 0.0057, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 101, batch: 43/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0105, 0.4110, 0.0139, 0.0197, 0.0062, 0.5185, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.025

[Epoch: 101, batch: 86/215] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.4681e-06, 1.6444e-05, 6.2600e-03, 2.6125e-08, 4.7390e-07, 9.9371e-01,
        9.4221e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.984

[Epoch: 101, batch: 129/215] total loss per batch: 0.767
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0748, 0.0036, 0.0037, 0.7407, 0.1465, 0.0281, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 101, batch: 172/215] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.7284e-05, 4.0109e-01, 6.0698e-02, 3.8405e-02, 4.5840e-02, 3.0223e-06,
        4.5394e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 101, batch: 215/215] total loss per batch: 0.751
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0272, 0.7235, 0.2196, 0.0109, 0.0081, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 102, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0103, 0.3838, 0.0177, 0.0273, 0.0052, 0.5398, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.026

[Epoch: 102, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3649e-05, 2.8348e-05, 1.2157e-02, 5.9985e-08, 6.7050e-07, 9.8780e-01,
        3.9109e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.977

[Epoch: 102, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0971, 0.0047, 0.0056, 0.6949, 0.1608, 0.0316, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 102, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3604e-05, 3.8954e-01, 5.8316e-02, 2.3952e-02, 4.6771e-02, 4.1360e-06,
        4.8141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.059

[Epoch: 102, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0365, 0.6623, 0.2552, 0.0261, 0.0070, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.003

[Epoch: 103, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0071, 0.3607, 0.0115, 0.0190, 0.0033, 0.5876, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.026

[Epoch: 103, batch: 86/215] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.5805e-06, 8.3784e-06, 3.7633e-03, 1.2285e-08, 2.3991e-07, 9.9622e-01,
        3.9437e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 103, batch: 129/215] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0865, 0.0040, 0.0050, 0.7421, 0.1308, 0.0273, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 103, batch: 172/215] total loss per batch: 0.766
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.7092e-05, 4.3439e-01, 6.3778e-02, 3.0941e-02, 5.0357e-02, 2.5996e-06,
        4.2052e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.058

[Epoch: 103, batch: 215/215] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0478, 0.7429, 0.1770, 0.0116, 0.0074, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 104, batch: 43/215] total loss per batch: 0.750
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0117, 0.3781, 0.0146, 0.0214, 0.0044, 0.5531, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.026

[Epoch: 104, batch: 86/215] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.8515e-06, 1.4052e-05, 9.7290e-03, 4.6048e-08, 3.9007e-07, 9.9024e-01,
        2.8168e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 104, batch: 129/215] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1203, 0.0047, 0.0057, 0.6892, 0.1419, 0.0337, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 104, batch: 172/215] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.7974e-06, 3.7194e-01, 5.7899e-02, 2.6528e-02, 4.3464e-02, 3.6711e-06,
        5.0016e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 104, batch: 215/215] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.0392, 0.6662, 0.2580, 0.0220, 0.0075, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 105, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0088, 0.3803, 0.0155, 0.0277, 0.0046, 0.5522, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 105, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.0331e-06, 7.9385e-06, 5.5126e-03, 1.0384e-08, 2.2506e-07, 9.9447e-01,
        2.1741e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 105, batch: 129/215] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0902, 0.0050, 0.0059, 0.7117, 0.1486, 0.0328, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 105, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.9906e-05, 4.6085e-01, 6.4478e-02, 2.3411e-02, 4.2137e-02, 2.8449e-06,
        4.0911e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.081

[Epoch: 105, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0471, 0.7529, 0.1743, 0.0117, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 106, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0090, 0.3576, 0.0107, 0.0184, 0.0020, 0.5852, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.026

[Epoch: 106, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.2950e-06, 5.9428e-06, 8.9475e-03, 1.8101e-08, 1.6813e-07, 9.9104e-01,
        2.8209e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 106, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0904, 0.0046, 0.0053, 0.7157, 0.1473, 0.0321, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 106, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.6609e-06, 3.6026e-01, 6.0943e-02, 3.0850e-02, 4.1274e-02, 1.5799e-06,
        5.0666e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 106, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0350, 0.6821, 0.2510, 0.0170, 0.0061, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 107, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0124, 0.3568, 0.0128, 0.0232, 0.0066, 0.5733, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 107, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.2959e-06, 1.6726e-05, 7.4253e-03, 1.5185e-08, 4.8394e-07, 9.9255e-01,
        1.5260e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.978

[Epoch: 107, batch: 129/215] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0928, 0.0053, 0.0091, 0.6919, 0.1643, 0.0311, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 107, batch: 172/215] total loss per batch: 0.772
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2399e-05, 4.9733e-01, 5.0026e-02, 1.8628e-02, 5.2867e-02, 1.5937e-06,
        3.8114e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.121

[Epoch: 107, batch: 215/215] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0415, 0.7073, 0.2184, 0.0169, 0.0061, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 108, batch: 43/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0119, 0.4055, 0.0149, 0.0220, 0.0031, 0.5297, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 108, batch: 86/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.3710e-06, 3.6973e-06, 3.9510e-03, 2.8149e-08, 2.3739e-07, 9.9604e-01,
        2.3506e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 108, batch: 129/215] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0902, 0.0055, 0.0036, 0.7083, 0.1580, 0.0301, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 108, batch: 172/215] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.5084e-05, 3.8182e-01, 6.7563e-02, 3.6011e-02, 5.2337e-02, 3.1652e-06,
        4.6225e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.044

[Epoch: 108, batch: 215/215] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0308, 0.7106, 0.2293, 0.0146, 0.0059, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 109, batch: 43/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0091, 0.3365, 0.0098, 0.0184, 0.0034, 0.6121, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 109, batch: 86/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.5928e-06, 8.9513e-06, 1.0142e-02, 2.9914e-08, 3.2722e-07, 9.8984e-01,
        1.8607e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 109, batch: 129/215] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0998, 0.0066, 0.0064, 0.6806, 0.1586, 0.0428, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 109, batch: 172/215] total loss per batch: 0.774
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.5042e-05, 4.3033e-01, 6.2378e-02, 2.2689e-02, 6.5391e-02, 2.3214e-06,
        4.1919e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.105

[Epoch: 109, batch: 215/215] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0443, 0.7151, 0.2061, 0.0146, 0.0076, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 110, batch: 43/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0097, 0.4096, 0.0186, 0.0197, 0.0028, 0.5289, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 110, batch: 86/215] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2662e-06, 6.1323e-06, 5.0066e-03, 1.4246e-08, 1.7261e-07, 9.9498e-01,
        1.9130e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 110, batch: 129/215] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1020, 0.0056, 0.0058, 0.7195, 0.1293, 0.0339, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 110, batch: 172/215] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6778e-05, 3.9861e-01, 6.6966e-02, 2.5848e-02, 4.9530e-02, 4.2302e-06,
        4.5903e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.060

[Epoch: 110, batch: 215/215] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0370, 0.7047, 0.2269, 0.0159, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 111, batch: 43/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0084, 0.3795, 0.0106, 0.0193, 0.0053, 0.5550, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 111, batch: 86/215] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.1803e-06, 1.7554e-06, 5.5599e-03, 3.9640e-08, 1.2480e-07, 9.9443e-01,
        1.1803e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 111, batch: 129/215] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0792, 0.0056, 0.0076, 0.6935, 0.1727, 0.0360, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.047

[Epoch: 111, batch: 172/215] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.0024e-05, 4.0798e-01, 5.1659e-02, 1.7442e-02, 3.7733e-02, 1.5607e-06,
        4.8516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.053

[Epoch: 111, batch: 215/215] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0547, 0.6707, 0.2341, 0.0174, 0.0094, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 112, batch: 43/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0067, 0.3228, 0.0107, 0.0185, 0.0031, 0.6256, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 112, batch: 86/215] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.9394e-06, 6.4217e-06, 8.1954e-03, 1.8638e-08, 3.8817e-07, 9.9179e-01,
        2.4790e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 112, batch: 129/215] total loss per batch: 0.762
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1058, 0.0056, 0.0041, 0.7120, 0.1368, 0.0319, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 112, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.0698e-05, 4.2444e-01, 8.0365e-02, 2.6488e-02, 5.2753e-02, 2.5428e-06,
        4.1593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.068

[Epoch: 112, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0499, 0.7068, 0.2150, 0.0111, 0.0059, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 113, batch: 43/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.4211, 0.0095, 0.0163, 0.0038, 0.5304, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 113, batch: 86/215] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.7529e-06, 3.0090e-06, 1.0904e-02, 3.0851e-08, 2.4420e-07, 9.8908e-01,
        2.1797e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 113, batch: 129/215] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0912, 0.0072, 0.0076, 0.7057, 0.1514, 0.0318, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 113, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2331e-05, 4.3386e-01, 5.4966e-02, 2.1687e-02, 4.6148e-02, 9.2741e-07,
        4.4333e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 113, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0030, 0.0313, 0.7464, 0.1979, 0.0097, 0.0068, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 114, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0106, 0.3738, 0.0120, 0.0230, 0.0043, 0.5577, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 114, batch: 86/215] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.3090e-06, 6.3350e-06, 3.1911e-03, 2.6410e-08, 5.2561e-07, 9.9679e-01,
        1.5211e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 114, batch: 129/215] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0928, 0.0055, 0.0043, 0.7188, 0.1399, 0.0331, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.013

[Epoch: 114, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.0424e-05, 3.6195e-01, 6.0425e-02, 2.4926e-02, 4.5574e-02, 3.2301e-06,
        5.0711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 114, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0460, 0.7138, 0.2017, 0.0151, 0.0066, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 115, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0106, 0.3728, 0.0096, 0.0166, 0.0050, 0.5733, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.029

[Epoch: 115, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.7706e-06, 1.6487e-05, 1.0249e-02, 1.7372e-08, 9.5900e-07, 9.8973e-01,
        1.2364e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 115, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0969, 0.0055, 0.0058, 0.6874, 0.1623, 0.0359, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 115, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3963e-05, 4.0919e-01, 6.6807e-02, 2.8076e-02, 5.5008e-02, 1.4066e-06,
        4.4090e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 115, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.0381, 0.7238, 0.2084, 0.0156, 0.0061, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 116, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0100, 0.3681, 0.0121, 0.0279, 0.0038, 0.5592, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.024

[Epoch: 116, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2236e-06, 8.3384e-06, 4.2331e-03, 2.0279e-08, 5.3886e-07, 9.9575e-01,
        1.6109e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 116, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0934, 0.0059, 0.0054, 0.7114, 0.1501, 0.0282, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.023

[Epoch: 116, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8929e-05, 4.1562e-01, 5.2839e-02, 1.7632e-02, 4.9190e-02, 1.8641e-06,
        4.6470e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 116, batch: 215/215] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0432, 0.5939, 0.3243, 0.0159, 0.0094, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 117, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0092, 0.3415, 0.0109, 0.0169, 0.0032, 0.6101, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 117, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.5240e-06, 9.1670e-06, 7.6867e-03, 2.3671e-08, 5.3709e-07, 9.9230e-01,
        1.4944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.974

[Epoch: 117, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0917, 0.0037, 0.0059, 0.7130, 0.1471, 0.0337, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 117, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3014e-05, 4.2535e-01, 6.7606e-02, 2.7163e-02, 6.5519e-02, 1.5579e-06,
        4.1435e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 117, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0299, 0.7950, 0.1471, 0.0141, 0.0052, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 118, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.4092, 0.0113, 0.0265, 0.0074, 0.5052, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.023

[Epoch: 118, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.6631e-06, 6.0874e-06, 5.4096e-03, 2.4087e-08, 4.5842e-07, 9.9458e-01,
        3.2641e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 118, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1043, 0.0093, 0.0052, 0.6947, 0.1494, 0.0320, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 118, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6867e-05, 4.6028e-01, 5.1231e-02, 2.3368e-02, 3.5493e-02, 2.0982e-06,
        4.2961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 118, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0410, 0.7075, 0.2175, 0.0132, 0.0076, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 119, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0102, 0.2860, 0.0125, 0.0219, 0.0035, 0.6584, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 119, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3864e-06, 3.8174e-06, 2.7613e-03, 1.5912e-08, 2.3000e-07, 9.9723e-01,
        1.4926e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 119, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0815, 0.0081, 0.0058, 0.7070, 0.1507, 0.0430, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 119, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3772e-05, 3.5570e-01, 5.9955e-02, 2.3441e-02, 5.2564e-02, 1.5015e-06,
        5.0833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 119, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0349, 0.6645, 0.2717, 0.0126, 0.0077, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 120, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0134, 0.5123, 0.0111, 0.0162, 0.0075, 0.4306, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 120, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.2728e-06, 8.5430e-06, 8.4001e-03, 4.2389e-08, 1.1570e-06, 9.9158e-01,
        2.6896e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 120, batch: 129/215] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0882, 0.0044, 0.0060, 0.7386, 0.1386, 0.0206, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 120, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.7371e-05, 4.7932e-01, 6.9196e-02, 2.7942e-02, 4.8924e-02, 3.4780e-06,
        3.7460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.076

[Epoch: 120, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.0422, 0.7663, 0.1509, 0.0155, 0.0078, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 121, batch: 43/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0129, 0.2486, 0.0169, 0.0181, 0.0033, 0.6805, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.030

[Epoch: 121, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.0682e-06, 5.9807e-06, 8.7000e-03, 3.5407e-08, 4.3694e-07, 9.9129e-01,
        2.9895e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 121, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1022, 0.0053, 0.0061, 0.6950, 0.1545, 0.0304, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 121, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8761e-05, 3.2667e-01, 6.9633e-02, 2.6038e-02, 5.1522e-02, 2.0034e-06,
        5.2611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 121, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0353, 0.6865, 0.2428, 0.0167, 0.0070, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 122, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0120, 0.4528, 0.0105, 0.0198, 0.0050, 0.4897, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 122, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.7387e-06, 6.4261e-06, 5.2303e-03, 3.5248e-08, 8.8497e-07, 9.9475e-01,
        1.7535e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 122, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0898, 0.0048, 0.0070, 0.6996, 0.1550, 0.0403, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 122, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2202e-05, 4.3525e-01, 5.1413e-02, 2.4751e-02, 5.1943e-02, 1.8418e-06,
        4.3663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 122, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0335, 0.6985, 0.2312, 0.0167, 0.0085, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 123, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0141, 0.3032, 0.0161, 0.0200, 0.0033, 0.6289, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.030

[Epoch: 123, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.3859e-06, 4.2320e-06, 1.1544e-02, 2.8038e-08, 2.0792e-07, 9.8845e-01,
        1.2852e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 123, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1112, 0.0048, 0.0070, 0.7010, 0.1382, 0.0302, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 123, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.3969e-05, 3.7141e-01, 7.5931e-02, 2.7743e-02, 4.2313e-02, 2.5574e-06,
        4.8257e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 123, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0350, 0.7162, 0.2081, 0.0213, 0.0054, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 124, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0129, 0.3303, 0.0096, 0.0211, 0.0029, 0.6045, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.029

[Epoch: 124, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.5451e-06, 4.7950e-06, 4.2867e-03, 4.3387e-08, 3.0857e-07, 9.9570e-01,
        2.6105e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 124, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0833, 0.0053, 0.0060, 0.7112, 0.1473, 0.0438, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 124, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.1618e-05, 4.1837e-01, 5.7908e-02, 2.0561e-02, 5.2031e-02, 2.5134e-06,
        4.5110e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.068

[Epoch: 124, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0474, 0.7286, 0.1923, 0.0162, 0.0076, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 125, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0112, 0.4824, 0.0141, 0.0244, 0.0079, 0.4495, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.028

[Epoch: 125, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.0420e-06, 4.0519e-06, 6.1137e-03, 1.8799e-08, 4.0390e-07, 9.9387e-01,
        6.4278e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 125, batch: 129/215] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0961, 0.0069, 0.0057, 0.7102, 0.1485, 0.0266, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.055

[Epoch: 125, batch: 172/215] total loss per batch: 0.771
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6294e-05, 4.5398e-01, 6.4918e-02, 3.1070e-02, 3.7825e-02, 2.4720e-06,
        4.1219e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.082

[Epoch: 125, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0383, 0.6935, 0.2374, 0.0151, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 126, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0081, 0.3059, 0.0084, 0.0209, 0.0030, 0.6324, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.026

[Epoch: 126, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.7102e-06, 5.5586e-06, 8.2545e-03, 2.7542e-08, 6.2722e-07, 9.9174e-01,
        1.2951e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 126, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1044, 0.0056, 0.0052, 0.6996, 0.1338, 0.0467, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 126, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3254e-05, 3.7375e-01, 4.6239e-02, 1.7414e-02, 5.4232e-02, 2.1519e-06,
        5.0834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 126, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0373, 0.7015, 0.2289, 0.0154, 0.0074, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 127, batch: 43/215] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0108, 0.3172, 0.0105, 0.0135, 0.0039, 0.6363, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 127, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.4434e-06, 3.3500e-06, 4.2924e-03, 1.1409e-08, 4.3844e-07, 9.9569e-01,
        4.9776e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.984

[Epoch: 127, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0882, 0.0046, 0.0084, 0.7153, 0.1549, 0.0242, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 127, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.2990e-05, 5.3175e-01, 6.7962e-02, 2.8161e-02, 4.4489e-02, 1.8570e-06,
        3.2762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.078

[Epoch: 127, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0630, 0.6468, 0.2548, 0.0195, 0.0057, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 128, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0102, 0.4935, 0.0113, 0.0367, 0.0052, 0.4181, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.029

[Epoch: 128, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.5711e-06, 5.7354e-06, 6.2839e-03, 2.6866e-08, 2.6043e-07, 9.9371e-01,
        1.0106e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 128, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1093, 0.0078, 0.0059, 0.6871, 0.1505, 0.0331, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.022

[Epoch: 128, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0640e-05, 3.3513e-01, 4.8872e-02, 1.6506e-02, 4.9770e-02, 2.0641e-06,
        5.4971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.079

[Epoch: 128, batch: 215/215] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0312, 0.7792, 0.1632, 0.0104, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 129, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0138, 0.2491, 0.0164, 0.0150, 0.0040, 0.6921, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.030

[Epoch: 129, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.1227e-06, 4.1830e-06, 6.4463e-03, 1.2176e-08, 1.2400e-06, 9.9354e-01,
        1.7131e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 129, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0877, 0.0028, 0.0065, 0.7096, 0.1543, 0.0348, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 129, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6155e-05, 4.2876e-01, 6.6550e-02, 2.6621e-02, 5.8004e-02, 1.2330e-06,
        4.2004e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 129, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0460, 0.7016, 0.2213, 0.0138, 0.0069, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 130, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0099, 0.4484, 0.0114, 0.0201, 0.0043, 0.4940, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.032

[Epoch: 130, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.8807e-06, 8.2645e-06, 7.8686e-03, 4.4180e-08, 2.8432e-07, 9.9211e-01,
        4.3012e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 130, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0987, 0.0061, 0.0063, 0.6978, 0.1499, 0.0365, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 130, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6248e-05, 4.7333e-01, 4.4199e-02, 1.8275e-02, 3.6256e-02, 2.0377e-06,
        4.2792e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.083

[Epoch: 130, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0416, 0.6716, 0.2438, 0.0193, 0.0092, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 131, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0119, 0.2904, 0.0107, 0.0141, 0.0027, 0.6603, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 131, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.7506e-06, 2.5672e-06, 1.7723e-02, 1.2247e-08, 6.9692e-07, 9.8227e-01,
        4.1775e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 131, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0920, 0.0031, 0.0076, 0.7181, 0.1375, 0.0363, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.048

[Epoch: 131, batch: 172/215] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3123e-05, 3.5518e-01, 6.5278e-02, 2.5643e-02, 7.2987e-02, 2.2202e-06,
        4.8089e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 131, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0404, 0.7359, 0.1880, 0.0161, 0.0067, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 132, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0124, 0.4446, 0.0103, 0.0190, 0.0072, 0.4945, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.029

[Epoch: 132, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.7144e-06, 5.2258e-06, 4.7949e-03, 3.8300e-08, 6.2119e-07, 9.9519e-01,
        1.7762e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.993

[Epoch: 132, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0908, 0.0067, 0.0066, 0.6997, 0.1571, 0.0348, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 132, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3067e-05, 4.1094e-01, 4.1392e-02, 1.9247e-02, 2.4921e-02, 9.3012e-07,
        5.0349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 132, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0420, 0.6977, 0.2318, 0.0129, 0.0054, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 133, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0097, 0.3339, 0.0103, 0.0171, 0.0024, 0.6168, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 133, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.6146e-06, 3.7553e-06, 3.8330e-03, 2.4990e-08, 2.5756e-07, 9.9616e-01,
        1.1688e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 133, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0912, 0.0046, 0.0062, 0.7057, 0.1592, 0.0281, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 133, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.6523e-05, 4.1212e-01, 8.1397e-02, 2.3298e-02, 7.4994e-02, 2.2378e-06,
        4.0815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 133, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0431, 0.7391, 0.1903, 0.0119, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 134, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0151, 0.3644, 0.0143, 0.0245, 0.0058, 0.5645, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 134, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.4786e-06, 6.4440e-06, 1.3202e-02, 3.2661e-08, 4.7520e-07, 9.8678e-01,
        6.7050e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.993

[Epoch: 134, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1004, 0.0051, 0.0043, 0.7151, 0.1310, 0.0386, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 134, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.9248e-06, 3.9469e-01, 3.7647e-02, 1.9663e-02, 3.3117e-02, 1.2340e-06,
        5.1488e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.060

[Epoch: 134, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0478, 0.6559, 0.2583, 0.0188, 0.0077, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.030

[Epoch: 135, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0121, 0.3650, 0.0151, 0.0241, 0.0048, 0.5617, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 135, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.0733e-06, 5.3635e-06, 6.4481e-03, 2.7524e-08, 5.2900e-07, 9.9354e-01,
        6.6371e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 135, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0813, 0.0048, 0.0059, 0.7346, 0.1381, 0.0316, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 135, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6927e-05, 4.6722e-01, 6.1678e-02, 2.4959e-02, 6.0424e-02, 1.3623e-06,
        3.8569e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 135, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0408, 0.7456, 0.1863, 0.0121, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.007

[Epoch: 136, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0086, 0.3545, 0.0106, 0.0156, 0.0045, 0.5945, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.031

[Epoch: 136, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.2106e-06, 4.0394e-06, 1.5452e-02, 3.0411e-08, 6.1741e-07, 9.8453e-01,
        7.9306e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 136, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1055, 0.0047, 0.0045, 0.6904, 0.1552, 0.0338, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 136, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.6374e-06, 3.9635e-01, 5.0692e-02, 1.9909e-02, 5.2767e-02, 1.9968e-06,
        4.8027e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.062

[Epoch: 136, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0329, 0.6394, 0.2918, 0.0177, 0.0073, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.039

[Epoch: 137, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0137, 0.4182, 0.0151, 0.0219, 0.0048, 0.5064, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 137, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.9743e-06, 4.7010e-06, 6.7578e-03, 1.1709e-08, 5.6869e-07, 9.9323e-01,
        5.6532e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 137, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0900, 0.0056, 0.0067, 0.7149, 0.1413, 0.0365, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 137, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.3603e-05, 4.5110e-01, 6.0207e-02, 2.4496e-02, 5.5899e-02, 2.3352e-06,
        4.0827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.066

[Epoch: 137, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0487, 0.7000, 0.2127, 0.0161, 0.0074, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 138, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0106, 0.3561, 0.0107, 0.0166, 0.0045, 0.5899, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.032

[Epoch: 138, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.8314e-06, 1.7605e-06, 1.5673e-02, 2.0815e-08, 5.9600e-07, 9.8432e-01,
        9.0536e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.980

[Epoch: 138, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0900, 0.0061, 0.0030, 0.7079, 0.1605, 0.0264, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 138, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3434e-05, 3.9925e-01, 4.7136e-02, 2.1044e-02, 2.9376e-02, 9.3241e-07,
        5.0318e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 138, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0505, 0.7640, 0.1603, 0.0116, 0.0045, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 139, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0096, 0.3531, 0.0097, 0.0208, 0.0040, 0.5813, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 139, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([4.1331e-06, 7.8720e-06, 5.7140e-03, 1.7842e-08, 2.8869e-07, 9.9427e-01,
        6.0430e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 139, batch: 129/215] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1046, 0.0046, 0.0042, 0.6937, 0.1536, 0.0334, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 139, batch: 172/215] total loss per batch: 0.769
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.9842e-05, 4.5622e-01, 7.2802e-02, 2.5094e-02, 7.3230e-02, 2.0866e-06,
        3.7263e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 139, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0345, 0.7479, 0.1840, 0.0131, 0.0075, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 140, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0154, 0.4338, 0.0133, 0.0217, 0.0058, 0.4968, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 140, batch: 86/215] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([8.7953e-06, 1.6634e-06, 6.8437e-03, 3.7769e-08, 3.3566e-07, 9.9314e-01,
        4.5592e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 140, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1008, 0.0085, 0.0052, 0.7152, 0.1360, 0.0297, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.033

[Epoch: 140, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3118e-05, 3.8768e-01, 5.5019e-02, 2.4716e-02, 3.2905e-02, 1.1250e-06,
        4.9967e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 140, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0502, 0.6023, 0.3058, 0.0224, 0.0073, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 141, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0066, 0.2799, 0.0071, 0.0110, 0.0036, 0.6832, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 141, batch: 86/215] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7376e-06, 6.1159e-06, 1.0012e-02, 1.3092e-08, 8.4683e-07, 9.8998e-01,
        1.3070e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 141, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0793, 0.0040, 0.0042, 0.7192, 0.1564, 0.0315, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 141, batch: 172/215] total loss per batch: 0.767
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.6097e-05, 4.2124e-01, 7.1594e-02, 2.2779e-02, 6.6545e-02, 1.7911e-06,
        4.1781e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.065

[Epoch: 141, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0401, 0.7364, 0.1967, 0.0114, 0.0060, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 142, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0162, 0.4690, 0.0147, 0.0237, 0.0073, 0.4410, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 142, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.8155e-06, 3.1096e-06, 9.0277e-03, 3.0947e-08, 3.5709e-07, 9.9096e-01,
        4.0219e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 142, batch: 129/215] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0929, 0.0067, 0.0040, 0.7221, 0.1377, 0.0317, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 142, batch: 172/215] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.1308e-05, 3.8410e-01, 5.9875e-02, 2.6198e-02, 4.0212e-02, 1.4369e-06,
        4.8960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.076

[Epoch: 142, batch: 215/215] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0527, 0.7156, 0.1952, 0.0161, 0.0083, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 143, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0052, 0.3423, 0.0086, 0.0178, 0.0041, 0.6063, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.027

[Epoch: 143, batch: 86/215] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.4355e-06, 5.5623e-06, 6.5688e-03, 2.9971e-08, 7.7602e-07, 9.9342e-01,
        7.1091e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 143, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0971, 0.0042, 0.0060, 0.7020, 0.1461, 0.0397, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 143, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.2920e-05, 4.9414e-01, 5.6290e-02, 2.4634e-02, 5.3416e-02, 1.2954e-06,
        3.7149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 143, batch: 215/215] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0309, 0.7232, 0.2157, 0.0142, 0.0054, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 144, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0092, 0.3364, 0.0121, 0.0180, 0.0036, 0.5956, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.035

[Epoch: 144, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.6858e-06, 1.4300e-06, 4.7006e-03, 1.2599e-08, 1.8881e-07, 9.9529e-01,
        5.7345e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 144, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0964, 0.0056, 0.0030, 0.7167, 0.1434, 0.0287, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.041

[Epoch: 144, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.1607e-05, 3.4892e-01, 4.5027e-02, 2.3175e-02, 4.5998e-02, 1.7206e-06,
        5.3687e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.092

[Epoch: 144, batch: 215/215] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0533, 0.6818, 0.2297, 0.0165, 0.0072, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.028

[Epoch: 145, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.4373, 0.0128, 0.0232, 0.0086, 0.4923, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.032

[Epoch: 145, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3273e-06, 4.3667e-06, 8.5945e-03, 1.6446e-08, 7.7247e-07, 9.9140e-01,
        7.7093e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 145, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0981, 0.0084, 0.0041, 0.6977, 0.1489, 0.0386, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.031

[Epoch: 145, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2457e-05, 5.0084e-01, 6.1508e-02, 2.0716e-02, 5.5948e-02, 1.3797e-06,
        3.6097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 145, batch: 215/215] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.0403, 0.6953, 0.2324, 0.0155, 0.0068, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 146, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0068, 0.2718, 0.0103, 0.0169, 0.0033, 0.6693, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 146, batch: 86/215] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.5201e-06, 7.7128e-06, 6.2236e-03, 2.2429e-08, 1.6477e-06, 9.9376e-01,
        1.1385e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.983

[Epoch: 146, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0847, 0.0040, 0.0056, 0.7068, 0.1639, 0.0302, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 146, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8243e-05, 3.5378e-01, 4.6657e-02, 1.7351e-02, 4.1231e-02, 1.0799e-06,
        5.4096e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.062

[Epoch: 146, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0297, 0.7282, 0.2141, 0.0121, 0.0059, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 147, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0108, 0.4799, 0.0128, 0.0191, 0.0048, 0.4632, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 147, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.3700e-06, 3.3265e-06, 4.7088e-03, 1.4797e-08, 7.8441e-07, 9.9528e-01,
        9.0125e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 147, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1069, 0.0079, 0.0049, 0.6938, 0.1417, 0.0392, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 147, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.8794e-06, 4.6312e-01, 6.5260e-02, 2.4626e-02, 5.2840e-02, 5.5365e-07,
        3.9415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.075

[Epoch: 147, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0387, 0.7177, 0.2197, 0.0111, 0.0051, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 148, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0077, 0.2838, 0.0093, 0.0187, 0.0021, 0.6642, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 148, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([6.6010e-06, 1.1153e-05, 7.1272e-03, 2.7222e-08, 1.3895e-06, 9.9285e-01,
        9.6548e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.982

[Epoch: 148, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0958, 0.0047, 0.0067, 0.7010, 0.1467, 0.0384, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 148, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8948e-05, 3.4479e-01, 4.3915e-02, 2.1638e-02, 4.4448e-02, 2.6228e-06,
        5.4519e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.055

[Epoch: 148, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0440, 0.7369, 0.1877, 0.0136, 0.0059, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.012

[Epoch: 149, batch: 43/215] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0117, 0.4608, 0.0118, 0.0218, 0.0109, 0.4723, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 149, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2667e-06, 3.2048e-06, 6.0085e-03, 2.0856e-08, 6.9853e-07, 9.9398e-01,
        4.6626e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 149, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0898, 0.0073, 0.0054, 0.7079, 0.1429, 0.0408, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 149, batch: 172/215] total loss per batch: 0.768
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.2392e-05, 5.0657e-01, 8.8972e-02, 2.8032e-02, 6.3453e-02, 7.6137e-07,
        3.1296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.075

[Epoch: 149, batch: 215/215] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0452, 0.6702, 0.2512, 0.0168, 0.0070, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 150, batch: 43/215] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0082, 0.3388, 0.0146, 0.0218, 0.0038, 0.5972, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 150, batch: 86/215] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([5.1785e-06, 3.8995e-06, 7.3093e-03, 2.2358e-08, 6.2849e-07, 9.9268e-01,
        6.7432e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 150, batch: 129/215] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0947, 0.0050, 0.0058, 0.7125, 0.1495, 0.0276, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 150, batch: 172/215] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.6001e-06, 3.6702e-01, 4.1066e-02, 2.0209e-02, 4.2513e-02, 6.9395e-07,
        5.2918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.054

[Epoch: 150, batch: 215/215] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0395, 0.7414, 0.1920, 0.0104, 0.0063, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 151, batch: 43/215] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0098, 0.3671, 0.0115, 0.0181, 0.0066, 0.5703, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 151, batch: 86/215] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.5592e-06, 4.8890e-06, 6.7937e-03, 1.7374e-08, 1.0882e-06, 9.9320e-01,
        6.0807e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 151, batch: 129/215] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1096, 0.0064, 0.0060, 0.7004, 0.1398, 0.0321, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 151, batch: 172/215] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.6062e-05, 4.0558e-01, 7.5283e-02, 2.3215e-02, 9.0120e-02, 2.0019e-06,
        4.0578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 151, batch: 215/215] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0450, 0.6721, 0.2434, 0.0205, 0.0066, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.025

[Epoch: 152, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0106, 0.3897, 0.0121, 0.0248, 0.0062, 0.5448, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.035

[Epoch: 152, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1432e-06, 3.2515e-06, 9.6142e-03, 9.2082e-09, 5.4078e-07, 9.9038e-01,
        5.8361e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 152, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0872, 0.0054, 0.0052, 0.7158, 0.1506, 0.0321, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.020

[Epoch: 152, batch: 172/215] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.8010e-06, 4.4166e-01, 5.2009e-02, 2.2420e-02, 3.7324e-02, 5.8026e-07,
        4.4658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 152, batch: 215/215] total loss per batch: 0.739
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0423, 0.6935, 0.2371, 0.0127, 0.0054, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 153, batch: 43/215] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0109, 0.3445, 0.0153, 0.0192, 0.0065, 0.5900, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 153, batch: 86/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1302e-06, 2.0411e-06, 3.5481e-03, 7.9683e-09, 4.4990e-07, 9.9645e-01,
        6.8668e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 153, batch: 129/215] total loss per batch: 0.753
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0947, 0.0055, 0.0052, 0.7116, 0.1457, 0.0329, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 153, batch: 172/215] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.0807e-06, 3.9469e-01, 4.9153e-02, 1.9310e-02, 5.4418e-02, 1.4088e-06,
        4.8242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 153, batch: 215/215] total loss per batch: 0.739
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0355, 0.7738, 0.1643, 0.0121, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 154, batch: 43/215] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0110, 0.3833, 0.0115, 0.0229, 0.0058, 0.5518, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 154, batch: 86/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.8468e-06, 2.4346e-06, 6.3013e-03, 6.7197e-09, 4.7033e-07, 9.9369e-01,
        3.2877e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 154, batch: 129/215] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0948, 0.0066, 0.0055, 0.7010, 0.1531, 0.0350, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.025

[Epoch: 154, batch: 172/215] total loss per batch: 0.761
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.3874e-06, 4.4246e-01, 5.9440e-02, 2.0615e-02, 5.3394e-02, 6.2757e-07,
        4.2409e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 154, batch: 215/215] total loss per batch: 0.738
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0436, 0.6527, 0.2739, 0.0151, 0.0057, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 155, batch: 43/215] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0101, 0.3634, 0.0151, 0.0215, 0.0078, 0.5659, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 155, batch: 86/215] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3179e-06, 1.7532e-06, 5.9079e-03, 7.8963e-09, 3.4579e-07, 9.9409e-01,
        5.8524e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 155, batch: 129/215] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0898, 0.0049, 0.0053, 0.7135, 0.1493, 0.0331, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 155, batch: 172/215] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.4147e-06, 4.0185e-01, 5.2660e-02, 2.1385e-02, 4.4108e-02, 1.1834e-06,
        4.7999e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 155, batch: 215/215] total loss per batch: 0.739
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0403, 0.7359, 0.1950, 0.0140, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 156, batch: 43/215] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0105, 0.3508, 0.0089, 0.0188, 0.0038, 0.5952, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 156, batch: 86/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1295e-06, 2.3746e-06, 5.6948e-03, 6.9020e-09, 6.2049e-07, 9.9430e-01,
        4.9870e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 156, batch: 129/215] total loss per batch: 0.753
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0929, 0.0069, 0.0044, 0.7002, 0.1564, 0.0352, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 156, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.7946e-06, 4.3849e-01, 5.2844e-02, 1.9958e-02, 5.2003e-02, 5.6157e-07,
        4.3670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.072

[Epoch: 156, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0407, 0.6912, 0.2349, 0.0176, 0.0056, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 157, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0092, 0.3996, 0.0114, 0.0198, 0.0086, 0.5292, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 157, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2976e-06, 2.3388e-06, 8.7013e-03, 9.4728e-09, 3.4320e-07, 9.9129e-01,
        3.9223e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 157, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0951, 0.0046, 0.0068, 0.7101, 0.1468, 0.0321, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 157, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.5663e-06, 3.8503e-01, 6.9707e-02, 2.2629e-02, 5.3194e-02, 1.8870e-06,
        4.6943e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.076

[Epoch: 157, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0393, 0.7085, 0.2221, 0.0156, 0.0050, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 158, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0115, 0.3376, 0.0123, 0.0215, 0.0040, 0.6042, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.035

[Epoch: 158, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.6116e-06, 3.9346e-06, 4.3841e-03, 5.9323e-09, 5.6413e-07, 9.9561e-01,
        1.3893e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 158, batch: 129/215] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0900, 0.0067, 0.0044, 0.7074, 0.1517, 0.0347, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.029

[Epoch: 158, batch: 172/215] total loss per batch: 0.765
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.5932e-06, 4.5271e-01, 4.8853e-02, 2.2713e-02, 4.9753e-02, 4.8201e-07,
        4.2596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 158, batch: 215/215] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0432, 0.7241, 0.1983, 0.0168, 0.0062, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 159, batch: 43/215] total loss per batch: 0.750
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0075, 0.3776, 0.0069, 0.0171, 0.0046, 0.5712, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 159, batch: 86/215] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1353e-06, 3.1784e-06, 8.3511e-03, 9.7028e-09, 3.8818e-07, 9.9164e-01,
        5.1331e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 159, batch: 129/215] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0977, 0.0071, 0.0079, 0.7074, 0.1432, 0.0323, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 159, batch: 172/215] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.9062e-06, 3.9204e-01, 6.1854e-02, 2.1222e-02, 5.5884e-02, 1.5100e-06,
        4.6900e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.073

[Epoch: 159, batch: 215/215] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0382, 0.6867, 0.2491, 0.0134, 0.0054, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 160, batch: 43/215] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0103, 0.4383, 0.0144, 0.0256, 0.0096, 0.4858, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.029

[Epoch: 160, batch: 86/215] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.3781e-07, 2.4070e-06, 5.2141e-03, 1.1598e-08, 4.1779e-07, 9.9478e-01,
        1.7636e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 160, batch: 129/215] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0869, 0.0049, 0.0054, 0.7163, 0.1526, 0.0302, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 160, batch: 172/215] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.6240e-06, 4.0090e-01, 5.1919e-02, 2.0119e-02, 5.4169e-02, 1.0420e-06,
        4.7288e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 160, batch: 215/215] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.0357, 0.7269, 0.1980, 0.0187, 0.0068, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.027

[Epoch: 161, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0107, 0.2679, 0.0100, 0.0213, 0.0037, 0.6753, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 161, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.9569e-06, 2.5039e-06, 6.9293e-03, 1.1019e-08, 6.2966e-07, 9.9306e-01,
        1.1990e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.985

[Epoch: 161, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0990, 0.0057, 0.0049, 0.7175, 0.1360, 0.0327, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 161, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.1785e-05, 4.6125e-01, 5.8974e-02, 1.9292e-02, 5.1306e-02, 1.1733e-06,
        4.0917e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 161, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0432, 0.7016, 0.2278, 0.0132, 0.0067, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 162, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0117, 0.4200, 0.0146, 0.0237, 0.0053, 0.5035, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 162, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5001e-06, 1.8361e-06, 3.9462e-03, 5.4765e-09, 2.8732e-07, 9.9605e-01,
        8.5139e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 162, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0911, 0.0058, 0.0057, 0.7128, 0.1481, 0.0324, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 162, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.9266e-06, 3.6774e-01, 5.5181e-02, 1.9721e-02, 6.7985e-02, 1.5263e-06,
        4.8936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 162, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0450, 0.6840, 0.2344, 0.0179, 0.0065, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 163, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0111, 0.3895, 0.0087, 0.0215, 0.0053, 0.5517, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 163, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5725e-06, 1.6364e-06, 4.8974e-03, 1.0067e-08, 2.6779e-07, 9.9510e-01,
        1.6046e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 163, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1003, 0.0079, 0.0049, 0.7069, 0.1405, 0.0342, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 163, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.7808e-06, 4.5066e-01, 5.1194e-02, 1.9636e-02, 3.5455e-02, 7.6192e-07,
        4.4305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 163, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0026, 0.0275, 0.7744, 0.1738, 0.0132, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.008

[Epoch: 164, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0094, 0.3041, 0.0088, 0.0170, 0.0032, 0.6445, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 164, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2491e-06, 2.5833e-06, 5.1932e-03, 7.8211e-09, 6.9797e-07, 9.9480e-01,
        5.9636e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 164, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0851, 0.0055, 0.0049, 0.7078, 0.1562, 0.0369, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 164, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.2257e-06, 4.2005e-01, 5.4205e-02, 1.7353e-02, 5.8945e-02, 1.0637e-06,
        4.4944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.061

[Epoch: 164, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0563, 0.6169, 0.2904, 0.0167, 0.0073, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 165, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 1
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0109, 0.4804, 0.0107, 0.0239, 0.0070, 0.4500, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.032

[Epoch: 165, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.0429e-06, 2.1246e-06, 7.8669e-03, 1.2567e-08, 4.5222e-07, 9.9213e-01,
        9.3091e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 165, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0896, 0.0040, 0.0058, 0.7238, 0.1390, 0.0316, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 165, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.8269e-06, 4.3625e-01, 5.7571e-02, 2.3952e-02, 4.5090e-02, 6.3109e-07,
        4.3713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 165, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0342, 0.7290, 0.2081, 0.0129, 0.0060, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 166, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0114, 0.2485, 0.0098, 0.0171, 0.0033, 0.6946, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 166, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.4936e-06, 3.1758e-06, 4.1776e-03, 6.7965e-09, 6.0075e-07, 9.9582e-01,
        4.3241e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 166, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0952, 0.0043, 0.0042, 0.7199, 0.1317, 0.0399, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.018

[Epoch: 166, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.1800e-06, 4.1855e-01, 6.2799e-02, 1.8078e-02, 5.6913e-02, 1.0624e-06,
        4.4365e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.059

[Epoch: 166, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0403, 0.7281, 0.1995, 0.0154, 0.0062, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 167, batch: 43/215] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0130, 0.4505, 0.0130, 0.0275, 0.0091, 0.4737, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 167, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7936e-06, 1.6137e-06, 7.0702e-03, 9.5670e-09, 2.8033e-07, 9.9293e-01,
        6.0186e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 167, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0912, 0.0041, 0.0055, 0.7199, 0.1432, 0.0311, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 167, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.8562e-06, 4.2146e-01, 4.7982e-02, 1.9603e-02, 3.9172e-02, 7.7309e-07,
        4.7177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 167, batch: 215/215] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0395, 0.7558, 0.1777, 0.0128, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 168, batch: 43/215] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0156, 0.3088, 0.0128, 0.0162, 0.0046, 0.6256, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.030

[Epoch: 168, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1352e-06, 2.5625e-06, 4.8871e-03, 5.6658e-09, 1.1324e-06, 9.9511e-01,
        4.6811e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 168, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0980, 0.0052, 0.0041, 0.7116, 0.1476, 0.0296, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 168, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0914e-05, 4.0075e-01, 6.3971e-02, 2.0381e-02, 6.3521e-02, 1.2125e-06,
        4.5137e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.062

[Epoch: 168, batch: 215/215] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0415, 0.6342, 0.2855, 0.0189, 0.0070, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 169, batch: 43/215] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0103, 0.3771, 0.0124, 0.0225, 0.0046, 0.5619, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 169, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1860e-06, 3.8523e-06, 5.7204e-03, 9.1344e-09, 3.7054e-07, 9.9427e-01,
        6.9024e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 169, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1010, 0.0052, 0.0058, 0.7046, 0.1401, 0.0378, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 169, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.9083e-06, 4.1316e-01, 4.0298e-02, 2.0370e-02, 3.8413e-02, 9.1570e-07,
        4.8775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.058

[Epoch: 169, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0402, 0.7552, 0.1773, 0.0141, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 170, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0155, 0.3893, 0.0114, 0.0226, 0.0060, 0.5409, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.035

[Epoch: 170, batch: 86/215] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.3378e-06, 3.3471e-06, 9.4299e-03, 1.3311e-08, 4.4507e-07, 9.9056e-01,
        1.2135e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.994

[Epoch: 170, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0851, 0.0046, 0.0041, 0.7179, 0.1571, 0.0273, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 170, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.4695e-05, 4.2190e-01, 8.0435e-02, 2.2074e-02, 7.8581e-02, 2.0253e-06,
        3.9699e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.065

[Epoch: 170, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0417, 0.7139, 0.2127, 0.0150, 0.0062, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 171, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0070, 0.3285, 0.0077, 0.0170, 0.0046, 0.6115, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 171, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.8049e-06, 7.7029e-06, 6.8392e-03, 2.0816e-08, 1.1191e-06, 9.9315e-01,
        1.7713e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 171, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1011, 0.0055, 0.0056, 0.6981, 0.1421, 0.0417, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 171, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.1147e-06, 3.9761e-01, 3.7343e-02, 2.2543e-02, 3.6595e-02, 9.0542e-07,
        5.0590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 171, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0322, 0.7193, 0.2181, 0.0154, 0.0061, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 172, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0133, 0.4400, 0.0144, 0.0218, 0.0062, 0.4945, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 172, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.5785e-06, 1.2139e-06, 5.6199e-03, 7.1067e-09, 2.2277e-07, 9.9438e-01,
        8.9336e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.994

[Epoch: 172, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0906, 0.0037, 0.0053, 0.7159, 0.1470, 0.0320, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 172, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.1440e-05, 4.5539e-01, 6.7309e-02, 2.1088e-02, 6.8337e-02, 1.0097e-06,
        3.8786e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.061

[Epoch: 172, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0498, 0.6899, 0.2305, 0.0151, 0.0047, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 173, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0097, 0.3150, 0.0099, 0.0158, 0.0039, 0.6357, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 173, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.0121e-06, 4.3850e-06, 5.9753e-03, 1.3427e-08, 9.3301e-07, 9.9402e-01,
        1.5353e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 173, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0939, 0.0078, 0.0043, 0.7169, 0.1384, 0.0344, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.027

[Epoch: 173, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.8556e-06, 3.6617e-01, 4.3408e-02, 1.8035e-02, 4.8880e-02, 9.7710e-07,
        5.2349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 173, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0331, 0.7117, 0.2242, 0.0150, 0.0067, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 174, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0101, 0.4188, 0.0118, 0.0233, 0.0054, 0.5175, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 174, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7763e-06, 1.7703e-06, 6.1308e-03, 5.0711e-09, 1.8839e-07, 9.9386e-01,
        6.3181e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 174, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0927, 0.0037, 0.0058, 0.7001, 0.1596, 0.0325, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.045

[Epoch: 174, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0190e-05, 4.7307e-01, 6.2805e-02, 2.6983e-02, 5.9471e-02, 1.1206e-06,
        3.7766e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.059

[Epoch: 174, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0488, 0.7019, 0.2199, 0.0155, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 175, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.3563, 0.0093, 0.0174, 0.0043, 0.5913, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 175, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.5990e-06, 2.7449e-06, 4.7442e-03, 1.3180e-08, 5.8643e-07, 9.9525e-01,
        9.5779e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 175, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0897, 0.0058, 0.0045, 0.7175, 0.1440, 0.0341, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 175, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.9792e-06, 3.5872e-01, 3.9788e-02, 1.6889e-02, 4.8785e-02, 8.3154e-07,
        5.3580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 175, batch: 215/215] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0373, 0.6815, 0.2497, 0.0138, 0.0054, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 176, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0098, 0.3689, 0.0119, 0.0187, 0.0062, 0.5655, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 176, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.6488e-06, 2.3348e-06, 4.4261e-03, 6.6204e-09, 2.9304e-07, 9.9557e-01,
        1.4847e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 176, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1020, 0.0061, 0.0054, 0.6839, 0.1610, 0.0354, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 176, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([9.3783e-06, 4.8163e-01, 5.8182e-02, 2.2541e-02, 4.4677e-02, 7.8426e-07,
        3.9296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.058

[Epoch: 176, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0489, 0.7219, 0.2000, 0.0152, 0.0054, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 177, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0089, 0.3654, 0.0122, 0.0216, 0.0046, 0.5744, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 177, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2517e-06, 2.9000e-06, 6.4299e-03, 1.6778e-08, 6.3907e-07, 9.9356e-01,
        9.8192e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 177, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0963, 0.0044, 0.0055, 0.6989, 0.1541, 0.0359, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.048

[Epoch: 177, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3587e-05, 3.7285e-01, 6.4910e-02, 2.0493e-02, 6.1685e-02, 8.2566e-07,
        4.8005e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 177, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0394, 0.7006, 0.2269, 0.0147, 0.0062, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 178, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0097, 0.3570, 0.0098, 0.0194, 0.0059, 0.5783, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 178, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3067e-06, 2.4918e-06, 4.5135e-03, 4.8876e-09, 4.4317e-07, 9.9548e-01,
        2.3132e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 178, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0914, 0.0075, 0.0047, 0.7155, 0.1465, 0.0306, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.021

[Epoch: 178, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.7700e-06, 4.3765e-01, 4.7065e-02, 2.1663e-02, 3.2859e-02, 9.4494e-07,
        4.6076e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 178, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0312, 0.7857, 0.1596, 0.0112, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 179, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0082, 0.3815, 0.0135, 0.0207, 0.0057, 0.5612, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 179, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.8457e-06, 2.2871e-06, 4.9786e-03, 9.7057e-09, 4.8673e-07, 9.9502e-01,
        5.4050e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 179, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0853, 0.0041, 0.0054, 0.7210, 0.1408, 0.0375, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 179, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0676e-05, 4.0395e-01, 7.4408e-02, 2.2154e-02, 7.4722e-02, 8.8372e-07,
        4.2476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 179, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0482, 0.6247, 0.2917, 0.0193, 0.0069, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 180, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0110, 0.3687, 0.0096, 0.0200, 0.0049, 0.5656, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.036

[Epoch: 180, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.8699e-07, 2.1422e-06, 6.1667e-03, 3.2441e-09, 3.9141e-07, 9.9383e-01,
        1.4297e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 180, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0997, 0.0076, 0.0054, 0.7135, 0.1418, 0.0283, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 180, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.9882e-06, 4.4473e-01, 4.3534e-02, 2.3061e-02, 3.5555e-02, 1.2069e-06,
        4.5311e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.060

[Epoch: 180, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0312, 0.7419, 0.2004, 0.0125, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 181, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0083, 0.3654, 0.0110, 0.0177, 0.0046, 0.5808, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 181, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3569e-06, 1.2625e-06, 3.0377e-03, 3.1176e-09, 3.0955e-07, 9.9696e-01,
        5.7246e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 181, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0817, 0.0040, 0.0055, 0.7253, 0.1427, 0.0354, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.028

[Epoch: 181, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.4777e-06, 4.0395e-01, 7.0665e-02, 2.0855e-02, 5.7236e-02, 9.5586e-07,
        4.4728e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 181, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0515, 0.7057, 0.2044, 0.0201, 0.0072, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 182, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0115, 0.3211, 0.0121, 0.0205, 0.0071, 0.6135, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 182, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3936e-06, 2.3614e-06, 7.9132e-03, 4.8924e-09, 6.2911e-07, 9.9208e-01,
        8.4683e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 182, batch: 129/215] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0953, 0.0056, 0.0046, 0.7165, 0.1425, 0.0312, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 182, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0151e-05, 4.0387e-01, 4.8659e-02, 2.1963e-02, 4.5520e-02, 1.4215e-06,
        4.7998e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 182, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0252, 0.7304, 0.2202, 0.0115, 0.0045, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 183, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.4095, 0.0102, 0.0175, 0.0053, 0.5319, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 183, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([7.4218e-07, 1.1617e-06, 2.6661e-03, 2.5087e-09, 2.0311e-07, 9.9733e-01,
        5.1579e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.992

[Epoch: 183, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0867, 0.0054, 0.0049, 0.7251, 0.1407, 0.0323, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.032

[Epoch: 183, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.8952e-06, 4.2890e-01, 6.0897e-02, 2.2434e-02, 4.7844e-02, 6.6375e-07,
        4.3992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.068

[Epoch: 183, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0567, 0.6528, 0.2549, 0.0185, 0.0074, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 184, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0084, 0.3425, 0.0107, 0.0179, 0.0064, 0.6014, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.038

[Epoch: 184, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3552e-06, 3.5522e-06, 5.5446e-03, 8.5980e-09, 9.1307e-07, 9.9445e-01,
        3.7749e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 184, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0958, 0.0067, 0.0058, 0.6993, 0.1506, 0.0367, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.044

[Epoch: 184, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.8280e-06, 4.1267e-01, 5.4080e-02, 2.4882e-02, 4.9343e-02, 1.0438e-06,
        4.5902e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 184, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0323, 0.7277, 0.2145, 0.0113, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 185, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0120, 0.3752, 0.0099, 0.0196, 0.0063, 0.5600, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 185, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3469e-06, 1.8717e-06, 5.6699e-03, 5.6045e-09, 5.2172e-07, 9.9433e-01,
        6.1738e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 185, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0954, 0.0058, 0.0051, 0.7017, 0.1492, 0.0359, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 185, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.5203e-06, 4.2377e-01, 5.5041e-02, 2.0898e-02, 4.3359e-02, 6.9758e-07,
        4.5693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.074

[Epoch: 185, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0487, 0.7125, 0.2022, 0.0180, 0.0068, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 186, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0089, 0.4164, 0.0104, 0.0206, 0.0054, 0.5295, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.033

[Epoch: 186, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.3628e-06, 1.7162e-06, 6.5997e-03, 6.5946e-09, 5.1767e-07, 9.9340e-01,
        6.5538e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 186, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0999, 0.0052, 0.0064, 0.6863, 0.1606, 0.0369, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 186, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.3978e-05, 4.0888e-01, 5.9983e-02, 2.5124e-02, 5.5069e-02, 1.1457e-06,
        4.5093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 186, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0370, 0.7132, 0.2178, 0.0161, 0.0061, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 187, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0143, 0.3141, 0.0108, 0.0221, 0.0064, 0.6183, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 187, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2305e-06, 1.9812e-06, 4.4420e-03, 7.0324e-09, 8.5107e-07, 9.9555e-01,
        7.2914e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 187, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1022, 0.0050, 0.0042, 0.6841, 0.1625, 0.0373, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 187, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([4.1389e-06, 3.9715e-01, 5.8154e-02, 2.1560e-02, 3.9639e-02, 7.6820e-07,
        4.8350e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 187, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0417, 0.6922, 0.2358, 0.0155, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 188, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0105, 0.4014, 0.0086, 0.0193, 0.0046, 0.5429, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 188, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5397e-06, 2.4327e-06, 6.0693e-03, 7.5423e-09, 5.6817e-07, 9.9393e-01,
        8.6844e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 188, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0951, 0.0045, 0.0044, 0.6953, 0.1622, 0.0343, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 188, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([2.8084e-05, 4.2991e-01, 6.7001e-02, 2.4900e-02, 5.6632e-02, 1.8341e-06,
        4.2153e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 188, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0443, 0.7456, 0.1783, 0.0164, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 189, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0109, 0.3752, 0.0125, 0.0224, 0.0041, 0.5631, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 189, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.6151e-06, 1.3225e-06, 4.0022e-03, 7.7991e-09, 4.5523e-07, 9.9599e-01,
        7.7953e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 189, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0982, 0.0041, 0.0060, 0.7065, 0.1461, 0.0342, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.040

[Epoch: 189, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([3.2533e-06, 4.2652e-01, 5.4051e-02, 1.9640e-02, 4.2783e-02, 4.0768e-07,
        4.5700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 189, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.0387, 0.6819, 0.2501, 0.0148, 0.0053, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 190, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.3722, 0.0097, 0.0207, 0.0064, 0.5650, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 190, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.7525e-06, 3.8740e-06, 5.5664e-03, 8.0915e-09, 6.2569e-07, 9.9443e-01,
        6.0891e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.981

[Epoch: 190, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0890, 0.0047, 0.0036, 0.7277, 0.1378, 0.0327, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.024

[Epoch: 190, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.8598e-05, 3.9385e-01, 5.8646e-02, 2.2641e-02, 5.5852e-02, 1.0981e-06,
        4.6899e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 190, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0436, 0.7054, 0.2162, 0.0181, 0.0059, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 191, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0109, 0.3469, 0.0116, 0.0212, 0.0044, 0.5887, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 191, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.8316e-07, 1.2117e-06, 5.2592e-03, 4.2589e-09, 3.1231e-07, 9.9474e-01,
        6.5709e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 191, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0927, 0.0042, 0.0060, 0.7191, 0.1406, 0.0329, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 191, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.3688e-06, 4.3370e-01, 5.2561e-02, 1.8791e-02, 4.4074e-02, 6.8089e-07,
        4.5087e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.070

[Epoch: 191, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0271, 0.7470, 0.1985, 0.0127, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 192, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0101, 0.3973, 0.0106, 0.0207, 0.0059, 0.5366, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.034

[Epoch: 192, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([3.2704e-06, 3.7360e-06, 6.1277e-03, 7.8065e-09, 5.9822e-07, 9.9386e-01,
        4.6734e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.987

[Epoch: 192, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0983, 0.0066, 0.0051, 0.7096, 0.1365, 0.0379, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.034

[Epoch: 192, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([6.7842e-06, 4.5381e-01, 4.8383e-02, 2.1288e-02, 5.0219e-02, 6.2352e-07,
        4.2630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.067

[Epoch: 192, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0612, 0.6421, 0.2620, 0.0204, 0.0053, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 193, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0076, 0.3634, 0.0092, 0.0206, 0.0035, 0.5852, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 193, batch: 86/215] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.1379e-06, 8.6637e-07, 5.6159e-03, 4.0801e-09, 2.3052e-07, 9.9438e-01,
        1.5617e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.984

[Epoch: 193, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0843, 0.0054, 0.0054, 0.7126, 0.1573, 0.0302, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.037

[Epoch: 193, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0399e-05, 3.7913e-01, 6.6337e-02, 1.9729e-02, 5.3109e-02, 1.0347e-06,
        4.8168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.056

[Epoch: 193, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0288, 0.7766, 0.1696, 0.0107, 0.0043, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 194, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0122, 0.3792, 0.0108, 0.0167, 0.0046, 0.5603, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.037

[Epoch: 194, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([9.9296e-07, 2.2446e-06, 6.7354e-03, 6.6021e-09, 5.8929e-07, 9.9326e-01,
        3.4050e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.993

[Epoch: 194, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1028, 0.0062, 0.0061, 0.6909, 0.1491, 0.0392, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.035

[Epoch: 194, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.6118e-06, 4.3454e-01, 5.9172e-02, 2.4390e-02, 4.9055e-02, 5.4488e-07,
        4.3284e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.075

[Epoch: 194, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0361, 0.6619, 0.2715, 0.0147, 0.0054, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 195, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0077, 0.3646, 0.0094, 0.0189, 0.0050, 0.5801, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 195, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2789e-06, 9.1372e-07, 4.0560e-03, 3.3982e-09, 2.5388e-07, 9.9594e-01,
        1.0388e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.990

[Epoch: 195, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0860, 0.0061, 0.0050, 0.7085, 0.1614, 0.0287, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.036

[Epoch: 195, batch: 172/215] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0666e-05, 3.9938e-01, 5.9783e-02, 2.0203e-02, 5.7732e-02, 1.2811e-06,
        4.6289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.063

[Epoch: 195, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0445, 0.7078, 0.2196, 0.0138, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 196, batch: 43/215] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0094, 0.3750, 0.0103, 0.0198, 0.0058, 0.5577, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 196, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.0792e-06, 3.2309e-06, 6.9596e-03, 6.6065e-09, 7.0567e-07, 9.9303e-01,
        6.2809e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.991

[Epoch: 196, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.1003, 0.0060, 0.0046, 0.7078, 0.1435, 0.0337, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.039

[Epoch: 196, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([1.0008e-05, 4.0485e-01, 5.9903e-02, 2.4972e-02, 4.5581e-02, 1.0435e-06,
        4.6468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.071

[Epoch: 196, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0331, 0.7563, 0.1838, 0.0124, 0.0056, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 197, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0101, 0.3757, 0.0100, 0.0208, 0.0051, 0.5658, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 197, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.2765e-06, 6.6753e-07, 5.3379e-03, 3.0921e-09, 3.4033e-07, 9.9466e-01,
        1.1748e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.989

[Epoch: 197, batch: 129/215] total loss per batch: 0.753
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0868, 0.0061, 0.0062, 0.7156, 0.1479, 0.0318, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.030

[Epoch: 197, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.4095e-06, 4.0930e-01, 4.9230e-02, 2.0666e-02, 5.1847e-02, 7.4549e-07,
        4.6895e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.064

[Epoch: 197, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0474, 0.6381, 0.2817, 0.0164, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 198, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0093, 0.3289, 0.0119, 0.0183, 0.0049, 0.6141, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.039

[Epoch: 198, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.1800e-06, 3.7667e-06, 7.6970e-03, 1.4547e-08, 7.5053e-07, 9.9230e-01,
        9.5239e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 198, batch: 129/215] total loss per batch: 0.753
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0983, 0.0046, 0.0046, 0.7059, 0.1480, 0.0327, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.038

[Epoch: 198, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 1
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([7.4840e-06, 4.5454e-01, 6.3867e-02, 2.4279e-02, 4.9067e-02, 1.3756e-06,
        4.0823e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.077

[Epoch: 198, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0532, 0.7165, 0.1984, 0.0140, 0.0063, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 199, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0133, 0.3904, 0.0110, 0.0190, 0.0050, 0.5450, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 199, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([1.5355e-06, 1.4402e-06, 4.5662e-03, 2.8240e-09, 3.4484e-07, 9.9543e-01,
        5.2628e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.988

[Epoch: 199, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0953, 0.0059, 0.0061, 0.7120, 0.1396, 0.0367, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.026

[Epoch: 199, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([8.1761e-06, 3.8759e-01, 5.0519e-02, 1.7620e-02, 5.5887e-02, 5.8614e-07,
        4.8838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.069

[Epoch: 199, batch: 215/215] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0369, 0.7189, 0.2160, 0.0125, 0.0063, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 200, batch: 43/215] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.3700, 0.0100, 0.0200, 0.0050, 0.5700, 0.0150])
Policy pred: tensor([0.0098, 0.3725, 0.0114, 0.0239, 0.0047, 0.5642, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.053 -0.040

[Epoch: 200, batch: 86/215] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0000, 0.0050, 0.0000, 0.0000, 0.9950, 0.0000])
Policy pred: tensor([2.5469e-06, 2.7702e-06, 7.1172e-03, 1.3620e-08, 3.6367e-07, 9.9288e-01,
        4.0029e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.995 -0.986

[Epoch: 200, batch: 129/215] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0950, 0.0050, 0.0050, 0.7100, 0.1450, 0.0350, 0.0050])
Policy pred: tensor([0.0904, 0.0044, 0.0057, 0.7127, 0.1509, 0.0295, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.031 0.043

[Epoch: 200, batch: 172/215] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0000, 0.4200, 0.0550, 0.0200, 0.0500, 0.0000, 0.4550])
Policy pred: tensor([5.4565e-06, 4.2578e-01, 5.5261e-02, 2.0726e-02, 5.2030e-02, 8.7693e-07,
        4.4620e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.058 0.076

[Epoch: 200, batch: 215/215] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0400, 0.7100, 0.2200, 0.0150, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0465, 0.6956, 0.2235, 0.0174, 0.0052, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

