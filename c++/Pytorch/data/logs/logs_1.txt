Training set samples: 6486
Batch size: 32
[Epoch: 1, batch: 40/203] total loss per batch: 1.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0065, 0.9050, 0.0185, 0.0251, 0.0261, 0.0032, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 1, batch: 80/203] total loss per batch: 1.414
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.9916e-01, 3.3017e-04, 1.3238e-02, 1.3337e-04, 3.8800e-04, 2.7286e-01,
        3.1389e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 0.188

[Epoch: 1, batch: 120/203] total loss per batch: 1.441
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.4652e-03, 4.9447e-01, 2.7302e-01, 5.2642e-05, 3.2558e-03, 2.1739e-01,
        8.3476e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 0.017

[Epoch: 1, batch: 160/203] total loss per batch: 1.301
Policy (actual, predicted): 4 6
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.1947e-02, 2.7415e-01, 3.3102e-05, 7.3802e-02, 7.4937e-02, 2.5469e-02,
        5.2966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 1, batch: 200/203] total loss per batch: 1.385
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0113, 0.4360, 0.0922, 0.0064, 0.0049, 0.1687, 0.2805],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 2, batch: 40/203] total loss per batch: 1.158
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8890e-03, 9.5012e-01, 1.0846e-02, 5.9009e-03, 2.4735e-02, 4.6768e-04,
        6.0396e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.046

[Epoch: 2, batch: 80/203] total loss per batch: 1.126
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([6.1026e-01, 7.0043e-06, 2.3180e-04, 7.3807e-06, 2.2951e-05, 1.3228e-01,
        2.5719e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.208

[Epoch: 2, batch: 120/203] total loss per batch: 1.124
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.0950e-04, 8.2673e-01, 8.2238e-02, 1.9509e-06, 2.9571e-04, 8.6126e-02,
        3.9952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 0.369

[Epoch: 2, batch: 160/203] total loss per batch: 1.023
Policy (actual, predicted): 4 6
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.1786e-02, 2.0161e-01, 6.4117e-06, 7.4605e-02, 2.6670e-01, 1.4377e-02,
        4.3091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.019

[Epoch: 2, batch: 200/203] total loss per batch: 1.118
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0151, 0.4369, 0.1027, 0.0064, 0.0025, 0.0836, 0.3528],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 3, batch: 40/203] total loss per batch: 0.971
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.5863e-03, 9.5628e-01, 4.3117e-03, 5.7495e-03, 2.8385e-02, 3.0920e-04,
        3.3810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 3, batch: 80/203] total loss per batch: 0.961
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([7.3570e-01, 5.7351e-08, 6.2286e-06, 3.2169e-06, 2.6539e-06, 6.8301e-02,
        1.9599e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.543

[Epoch: 3, batch: 120/203] total loss per batch: 0.938
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4115e-04, 9.3042e-01, 3.9436e-02, 2.7612e-07, 4.5237e-05, 2.9288e-02,
        6.6967e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.254

[Epoch: 3, batch: 160/203] total loss per batch: 0.895
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.7786e-02, 7.2722e-02, 7.5278e-06, 1.5255e-01, 4.8126e-01, 2.3190e-02,
        2.5248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.004

[Epoch: 3, batch: 200/203] total loss per batch: 0.955
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0055, 0.7376, 0.0787, 0.0032, 0.0015, 0.0324, 0.1410],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 4, batch: 40/203] total loss per batch: 0.904
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([8.8082e-04, 9.7051e-01, 8.4219e-03, 6.2507e-03, 1.1162e-02, 5.0500e-04,
        2.2671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 4, batch: 80/203] total loss per batch: 0.900
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.6219e-01, 2.7655e-08, 4.6060e-07, 3.2449e-06, 8.0512e-07, 1.2058e-02,
        5.2574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.665

[Epoch: 4, batch: 120/203] total loss per batch: 0.861
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.5433e-04, 9.4247e-01, 2.6172e-02, 2.2073e-08, 4.6492e-06, 3.0852e-02,
        2.4498e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.811

[Epoch: 4, batch: 160/203] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.4201e-03, 9.5592e-02, 1.2488e-05, 5.0722e-02, 7.7393e-01, 1.0419e-02,
        6.1905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.017

[Epoch: 4, batch: 200/203] total loss per batch: 0.908
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.5742, 0.0863, 0.0049, 0.0014, 0.0436, 0.2841],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 5, batch: 40/203] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0010, 0.9773, 0.0077, 0.0029, 0.0084, 0.0011, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.006

[Epoch: 5, batch: 80/203] total loss per batch: 0.873
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9487e-01, 7.3211e-07, 4.1200e-05, 8.5924e-05, 1.3544e-05, 6.9355e-02,
        7.3563e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.396

[Epoch: 5, batch: 120/203] total loss per batch: 0.843
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4710e-05, 9.7056e-01, 6.0906e-03, 1.8581e-08, 2.5471e-06, 2.3320e-02,
        1.4920e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 5, batch: 160/203] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.0733e-02, 8.0583e-02, 1.5443e-05, 9.3478e-02, 7.2225e-01, 1.0186e-02,
        8.2751e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.005

[Epoch: 5, batch: 200/203] total loss per batch: 0.888
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0111, 0.3115, 0.0828, 0.0049, 0.0018, 0.1137, 0.4742],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.032

[Epoch: 6, batch: 40/203] total loss per batch: 0.863
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.6842e-03, 9.6402e-01, 3.7714e-03, 2.0120e-03, 2.2710e-02, 8.1789e-04,
        4.9819e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 6, batch: 80/203] total loss per batch: 0.854
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([5.1807e-01, 1.0592e-07, 8.0138e-06, 2.3438e-05, 4.1678e-06, 9.4815e-02,
        3.8708e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.717

[Epoch: 6, batch: 120/203] total loss per batch: 0.832
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.3627e-04, 9.5296e-01, 1.1186e-02, 1.4548e-08, 4.7795e-06, 3.5517e-02,
        9.7297e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.965

[Epoch: 6, batch: 160/203] total loss per batch: 0.822
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([9.1573e-03, 1.0816e-01, 2.2857e-05, 5.2068e-02, 5.7603e-01, 8.0180e-03,
        2.4654e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.013

[Epoch: 6, batch: 200/203] total loss per batch: 0.859
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0142, 0.7246, 0.0594, 0.0042, 0.0022, 0.0475, 0.1479],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 7, batch: 40/203] total loss per batch: 0.845
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([5.1909e-04, 9.7975e-01, 2.6938e-03, 3.1525e-03, 1.0635e-02, 1.1037e-03,
        2.1467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 7, batch: 80/203] total loss per batch: 0.841
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.9876e-03, 1.2507e-07, 3.1091e-05, 1.8453e-05, 8.7936e-06, 1.2570e-02,
        9.8338e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.497

[Epoch: 7, batch: 120/203] total loss per batch: 0.816
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.6713e-04, 9.5744e-01, 2.4565e-02, 4.8612e-08, 4.7960e-06, 1.7813e-02,
        1.1201e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.859

[Epoch: 7, batch: 160/203] total loss per batch: 0.808
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([9.0589e-03, 4.9509e-02, 1.6476e-05, 6.5078e-02, 7.0911e-01, 1.7863e-02,
        1.4936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.014

[Epoch: 7, batch: 200/203] total loss per batch: 0.846
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0113, 0.4037, 0.0801, 0.0037, 0.0023, 0.0958, 0.4032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 8, batch: 40/203] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.9721, 0.0043, 0.0012, 0.0171, 0.0014, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.028

[Epoch: 8, batch: 80/203] total loss per batch: 0.823
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([9.8091e-01, 2.8314e-08, 6.4185e-06, 6.1688e-07, 1.3771e-06, 9.1268e-04,
        1.8165e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.489

[Epoch: 8, batch: 120/203] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.3683e-04, 9.6537e-01, 1.6911e-02, 6.1858e-09, 1.9156e-06, 1.7569e-02,
        1.6517e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.926

[Epoch: 8, batch: 160/203] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.3623e-03, 8.9609e-02, 3.2478e-05, 2.2412e-02, 8.3588e-01, 4.6047e-03,
        3.9099e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.014

[Epoch: 8, batch: 200/203] total loss per batch: 0.835
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0084, 0.4137, 0.0781, 0.0049, 0.0022, 0.1008, 0.3918],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 9, batch: 40/203] total loss per batch: 0.825
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.9767, 0.0056, 0.0014, 0.0095, 0.0019, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.027

[Epoch: 9, batch: 80/203] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1506e-01, 3.1740e-07, 4.7832e-05, 7.8036e-06, 4.2207e-05, 4.1888e-02,
        7.4295e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.114

[Epoch: 9, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.8794e-05, 9.4926e-01, 2.8118e-02, 7.5899e-09, 1.2656e-06, 2.2595e-02,
        7.5915e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.943

[Epoch: 9, batch: 160/203] total loss per batch: 0.792
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.2729e-02, 1.2384e-01, 1.8155e-05, 3.9796e-02, 6.2142e-01, 6.7304e-03,
        1.9547e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.001

[Epoch: 9, batch: 200/203] total loss per batch: 0.828
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0088, 0.6133, 0.0717, 0.0063, 0.0031, 0.0737, 0.2231],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 10, batch: 40/203] total loss per batch: 0.820
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.9747, 0.0033, 0.0025, 0.0148, 0.0012, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 10, batch: 80/203] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.5644e-01, 1.9835e-08, 5.4225e-05, 5.0910e-05, 6.7255e-06, 1.0886e-02,
        5.3256e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.792

[Epoch: 10, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.7001e-04, 9.7143e-01, 1.5183e-02, 4.5127e-09, 3.6756e-06, 1.3204e-02,
        8.6863e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.870

[Epoch: 10, batch: 160/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.6071e-02, 6.5349e-02, 5.0465e-05, 3.1672e-02, 8.1898e-01, 1.3215e-02,
        5.4666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.004

[Epoch: 10, batch: 200/203] total loss per batch: 0.825
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0093, 0.5226, 0.1102, 0.0060, 0.0031, 0.0437, 0.3051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 11, batch: 40/203] total loss per batch: 0.819
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.9622, 0.0083, 0.0045, 0.0172, 0.0026, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 11, batch: 80/203] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.2588e-01, 4.1868e-08, 4.3674e-05, 4.3464e-05, 6.8461e-06, 5.8672e-03,
        8.6816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.615

[Epoch: 11, batch: 120/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.3631e-05, 9.2033e-01, 2.9648e-02, 2.0863e-08, 2.9189e-06, 4.9977e-02,
        1.6442e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.919

[Epoch: 11, batch: 160/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([9.8306e-03, 9.4889e-02, 2.5669e-05, 4.4820e-02, 7.3495e-01, 7.9742e-03,
        1.0751e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.001

[Epoch: 11, batch: 200/203] total loss per batch: 0.820
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0244, 0.3963, 0.0869, 0.0076, 0.0040, 0.0928, 0.3880],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 12, batch: 40/203] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.9757, 0.0043, 0.0012, 0.0101, 0.0028, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 12, batch: 80/203] total loss per batch: 0.799
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.6606e-01, 2.4141e-08, 1.4859e-05, 2.3368e-05, 3.4487e-06, 5.8359e-03,
        6.2806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.614

[Epoch: 12, batch: 120/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0978e-04, 9.7234e-01, 1.3196e-02, 5.5192e-09, 1.6186e-06, 1.4342e-02,
        7.5425e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.844

[Epoch: 12, batch: 160/203] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.2236e-02, 1.0962e-01, 7.1599e-05, 3.7205e-02, 7.2103e-01, 1.3324e-02,
        1.0651e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.003

[Epoch: 12, batch: 200/203] total loss per batch: 0.816
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0055, 0.6602, 0.0660, 0.0028, 0.0017, 0.0279, 0.2361],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 13, batch: 40/203] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9644, 0.0063, 0.0029, 0.0183, 0.0025, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 13, batch: 80/203] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.0763e-01, 1.3161e-08, 5.6572e-06, 1.2658e-05, 1.5573e-06, 6.4246e-03,
        8.8593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.699

[Epoch: 13, batch: 120/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.8856e-05, 9.6897e-01, 1.6532e-02, 1.2363e-08, 2.9584e-06, 1.4469e-02,
        1.1478e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.918

[Epoch: 13, batch: 160/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.1973e-03, 7.4736e-02, 2.5469e-05, 1.8794e-02, 7.7523e-01, 6.5487e-03,
        1.1747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.014

[Epoch: 13, batch: 200/203] total loss per batch: 0.818
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0134, 0.3784, 0.0975, 0.0118, 0.0042, 0.0851, 0.4095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 14, batch: 40/203] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.9770, 0.0032, 0.0037, 0.0103, 0.0014, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 14, batch: 80/203] total loss per batch: 0.803
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([5.7523e-01, 6.8653e-08, 4.6397e-05, 3.9452e-06, 5.1656e-06, 4.2485e-03,
        4.2047e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.558

[Epoch: 14, batch: 120/203] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4964e-04, 9.3043e-01, 1.6766e-02, 1.2308e-08, 3.0485e-06, 5.2648e-02,
        4.2944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.972

[Epoch: 14, batch: 160/203] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.5439e-02, 1.5525e-01, 1.9584e-04, 2.3614e-02, 6.4915e-01, 1.2396e-02,
        1.4396e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.010

[Epoch: 14, batch: 200/203] total loss per batch: 0.818
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0057, 0.7464, 0.0577, 0.0028, 0.0024, 0.0283, 0.1567],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 15, batch: 40/203] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.9748, 0.0029, 0.0019, 0.0132, 0.0019, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.005

[Epoch: 15, batch: 80/203] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.9046e-02, 6.2168e-09, 5.1261e-06, 5.6343e-06, 6.1087e-07, 3.9592e-03,
        9.5698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.720

[Epoch: 15, batch: 120/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.7275e-05, 9.7253e-01, 1.6493e-02, 1.9924e-08, 3.0233e-06, 1.0905e-02,
        1.0216e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.869

[Epoch: 15, batch: 160/203] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.7904e-02, 6.7365e-02, 4.5364e-05, 1.9725e-02, 8.1256e-01, 7.4970e-03,
        7.4899e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.011

[Epoch: 15, batch: 200/203] total loss per batch: 0.814
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0114, 0.4007, 0.0413, 0.0114, 0.0041, 0.0734, 0.4577],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 16, batch: 40/203] total loss per batch: 0.810
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.9701, 0.0076, 0.0064, 0.0108, 0.0017, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 16, batch: 80/203] total loss per batch: 0.799
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8582e-01, 6.3271e-09, 1.4042e-06, 1.4364e-05, 1.6172e-06, 9.0521e-03,
        8.0511e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.518

[Epoch: 16, batch: 120/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.0569e-04, 9.5207e-01, 1.6836e-02, 1.3859e-08, 6.2123e-06, 3.0852e-02,
        3.3108e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.834

[Epoch: 16, batch: 160/203] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.9026e-02, 2.1478e-01, 1.7175e-04, 3.4174e-02, 5.6886e-01, 1.4825e-02,
        1.4816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.009

[Epoch: 16, batch: 200/203] total loss per batch: 0.812
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0067, 0.3744, 0.0697, 0.0043, 0.0039, 0.0637, 0.4774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 17, batch: 40/203] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.9655, 0.0032, 0.0036, 0.0137, 0.0027, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 17, batch: 80/203] total loss per batch: 0.796
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.2194e-01, 5.0302e-09, 2.6603e-06, 6.0206e-07, 6.6754e-07, 1.2162e-02,
        6.6590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.624

[Epoch: 17, batch: 120/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.5388e-05, 9.4872e-01, 3.4320e-02, 6.8947e-09, 5.0460e-07, 1.6934e-02,
        3.0739e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.970

[Epoch: 17, batch: 160/203] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8539e-03, 1.1506e-02, 1.2353e-05, 5.9260e-03, 9.3615e-01, 4.4231e-03,
        3.7132e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 17, batch: 200/203] total loss per batch: 0.812
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0087, 0.7801, 0.0566, 0.0056, 0.0034, 0.0377, 0.1079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 18, batch: 40/203] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.9571, 0.0056, 0.0044, 0.0240, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 18, batch: 80/203] total loss per batch: 0.795
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.0095e-01, 1.4581e-09, 1.5259e-07, 8.1030e-07, 5.2942e-07, 4.6636e-03,
        8.9439e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.658

[Epoch: 18, batch: 120/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3367e-05, 9.7897e-01, 1.1730e-02, 5.9136e-09, 3.5037e-06, 9.2475e-03,
        2.5162e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.960

[Epoch: 18, batch: 160/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.5644e-02, 1.5609e-01, 1.0131e-04, 1.6483e-02, 5.9422e-01, 1.4451e-02,
        2.0301e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.015

[Epoch: 18, batch: 200/203] total loss per batch: 0.809
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0084, 0.3569, 0.1088, 0.0054, 0.0030, 0.0607, 0.4569],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 19, batch: 40/203] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0047, 0.9679, 0.0026, 0.0028, 0.0148, 0.0022, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 19, batch: 80/203] total loss per batch: 0.793
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.2668e-01, 2.1341e-08, 8.6697e-07, 7.7559e-07, 8.4818e-07, 9.2874e-03,
        5.6403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.696

[Epoch: 19, batch: 120/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0521e-05, 9.5761e-01, 1.3753e-02, 3.9215e-09, 7.8203e-07, 2.8621e-02,
        4.2573e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.931

[Epoch: 19, batch: 160/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.4127e-03, 4.0231e-02, 9.0452e-05, 1.1422e-02, 8.8639e-01, 9.2703e-03,
        4.5181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.000

[Epoch: 19, batch: 200/203] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0109, 0.6461, 0.0621, 0.0036, 0.0030, 0.0397, 0.2347],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 20, batch: 40/203] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.9767, 0.0016, 0.0033, 0.0108, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 20, batch: 80/203] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([6.7608e-02, 5.8355e-09, 1.5077e-06, 1.6804e-06, 1.7240e-06, 1.1937e-02,
        9.2045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.559

[Epoch: 20, batch: 120/203] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.5492e-04, 9.7128e-01, 1.8654e-02, 2.3935e-08, 2.6344e-06, 9.9073e-03,
        5.1468e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 20, batch: 160/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.4298e-02, 1.5757e-01, 6.2792e-05, 2.2241e-02, 6.8621e-01, 1.6259e-02,
        1.0337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.004

[Epoch: 20, batch: 200/203] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0133, 0.4661, 0.0879, 0.0041, 0.0033, 0.1214, 0.3039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 21, batch: 40/203] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9779, 0.0023, 0.0033, 0.0085, 0.0022, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.028

[Epoch: 21, batch: 80/203] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3700e-01, 4.2623e-09, 8.7399e-07, 6.3231e-07, 1.2544e-06, 1.5142e-02,
        7.4786e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.751

[Epoch: 21, batch: 120/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9919e-05, 9.6524e-01, 1.0942e-02, 2.8032e-08, 2.8862e-06, 2.3783e-02,
        6.0559e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.881

[Epoch: 21, batch: 160/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.0015e-02, 1.0581e-01, 6.4861e-05, 2.3774e-02, 5.8810e-01, 1.3451e-02,
        2.5879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.008

[Epoch: 21, batch: 200/203] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0098, 0.5595, 0.0565, 0.0037, 0.0035, 0.0646, 0.3024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 22, batch: 40/203] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.9817, 0.0020, 0.0018, 0.0077, 0.0036, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.031

[Epoch: 22, batch: 80/203] total loss per batch: 0.791
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.7916e-01, 2.1380e-09, 1.3032e-06, 3.7618e-07, 4.8381e-07, 7.5026e-03,
        7.1333e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.676

[Epoch: 22, batch: 120/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.5670e-05, 9.4807e-01, 3.2596e-02, 8.9493e-09, 2.3277e-06, 1.9249e-02,
        3.5109e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.950

[Epoch: 22, batch: 160/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.2364e-03, 7.2632e-02, 4.7655e-05, 5.6138e-03, 8.7195e-01, 9.0500e-03,
        3.3473e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.016

[Epoch: 22, batch: 200/203] total loss per batch: 0.807
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0187, 0.4164, 0.0726, 0.0035, 0.0034, 0.0441, 0.4413],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 23, batch: 40/203] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9680, 0.0054, 0.0026, 0.0139, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 23, batch: 80/203] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([9.7297e-02, 9.2277e-09, 6.2347e-07, 3.6293e-06, 2.5561e-07, 2.0624e-02,
        8.8207e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.574

[Epoch: 23, batch: 120/203] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.5787e-05, 9.6989e-01, 1.5297e-02, 1.9779e-08, 1.6511e-06, 1.4753e-02,
        1.1155e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.982

[Epoch: 23, batch: 160/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.1476e-02, 1.0617e-01, 6.9694e-05, 8.3030e-03, 7.6834e-01, 4.6391e-03,
        1.0100e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.002

[Epoch: 23, batch: 200/203] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0064, 0.5787, 0.0767, 0.0037, 0.0024, 0.0392, 0.2929],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 24, batch: 40/203] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9733, 0.0019, 0.0030, 0.0146, 0.0022, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 24, batch: 80/203] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.9026e-01, 1.1115e-08, 3.4842e-06, 2.1934e-06, 1.5407e-06, 5.8650e-03,
        7.0387e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.718

[Epoch: 24, batch: 120/203] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.1053e-05, 9.6893e-01, 1.5407e-02, 6.9841e-09, 3.9085e-06, 1.5612e-02,
        1.0190e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.952

[Epoch: 24, batch: 160/203] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.7119e-03, 6.0644e-02, 4.9841e-05, 5.9999e-03, 7.7361e-01, 1.0386e-02,
        1.4060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.018

[Epoch: 24, batch: 200/203] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0158, 0.4734, 0.0427, 0.0060, 0.0061, 0.0769, 0.3791],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 25, batch: 40/203] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9791, 0.0020, 0.0037, 0.0065, 0.0025, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.026

[Epoch: 25, batch: 80/203] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7521e-01, 1.5097e-09, 5.2325e-07, 1.5239e-06, 1.6849e-07, 3.7034e-03,
        8.2109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.639

[Epoch: 25, batch: 120/203] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0021e-04, 9.5344e-01, 2.6539e-02, 6.3996e-08, 2.4739e-06, 1.9917e-02,
        5.5558e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.811

[Epoch: 25, batch: 160/203] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.2127e-02, 1.4852e-01, 6.1996e-05, 8.3086e-03, 7.7154e-01, 6.6067e-03,
        5.2834e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.017

[Epoch: 25, batch: 200/203] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0175, 0.5103, 0.0705, 0.0036, 0.0052, 0.0706, 0.3223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 26, batch: 40/203] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.9788, 0.0015, 0.0046, 0.0092, 0.0016, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.040

[Epoch: 26, batch: 80/203] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2701e-01, 1.6459e-08, 2.5881e-06, 8.4930e-07, 9.4215e-07, 1.8796e-03,
        7.7111e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.668

[Epoch: 26, batch: 120/203] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.1249e-04, 9.7548e-01, 1.6135e-02, 4.0139e-09, 5.4693e-06, 8.2674e-03,
        2.7457e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.985

[Epoch: 26, batch: 160/203] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.1155e-02, 1.0935e-01, 3.4105e-05, 7.9332e-03, 6.9135e-01, 9.1595e-03,
        1.7101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.016

[Epoch: 26, batch: 200/203] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0052, 0.6790, 0.0694, 0.0016, 0.0022, 0.0260, 0.2166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 27, batch: 40/203] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.9764, 0.0017, 0.0038, 0.0096, 0.0023, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 27, batch: 80/203] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.2953e-01, 1.1994e-08, 5.7027e-06, 2.2067e-05, 3.5012e-07, 1.3585e-02,
        6.5686e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.667

[Epoch: 27, batch: 120/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3541e-05, 9.3063e-01, 3.7924e-02, 2.5415e-08, 4.6504e-06, 3.1392e-02,
        4.2808e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.975

[Epoch: 27, batch: 160/203] total loss per batch: 0.766
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.4965e-03, 6.9264e-02, 4.9588e-05, 1.1532e-02, 8.6941e-01, 4.0067e-03,
        4.0244e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.013

[Epoch: 27, batch: 200/203] total loss per batch: 0.800
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0159, 0.2898, 0.0651, 0.0029, 0.0057, 0.1151, 0.5054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 28, batch: 40/203] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9802, 0.0021, 0.0023, 0.0090, 0.0012, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.033

[Epoch: 28, batch: 80/203] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([9.5129e-03, 1.3680e-09, 1.4381e-07, 6.8017e-07, 5.9751e-08, 2.6161e-03,
        9.8787e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.670

[Epoch: 28, batch: 120/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.5470e-04, 9.6581e-01, 1.4936e-02, 2.0141e-08, 2.9490e-06, 1.9094e-02,
        3.0163e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.978

[Epoch: 28, batch: 160/203] total loss per batch: 0.764
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.6272e-02, 1.3072e-01, 4.0052e-05, 9.3758e-03, 6.6747e-01, 9.1756e-03,
        1.6694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.027

[Epoch: 28, batch: 200/203] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0049, 0.7583, 0.0579, 0.0032, 0.0038, 0.0173, 0.1546],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 29, batch: 40/203] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.9746, 0.0014, 0.0053, 0.0110, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 29, batch: 80/203] total loss per batch: 0.783
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([8.4274e-01, 2.7173e-07, 1.0733e-04, 1.1399e-05, 9.6886e-06, 8.7801e-03,
        1.4836e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.534

[Epoch: 29, batch: 120/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.3242e-06, 9.8113e-01, 7.6804e-03, 3.1630e-09, 9.0323e-07, 1.1172e-02,
        8.5693e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.968

[Epoch: 29, batch: 160/203] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.1508e-02, 9.0205e-02, 8.1554e-05, 1.8969e-02, 7.4702e-01, 1.0893e-02,
        1.2132e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.010

[Epoch: 29, batch: 200/203] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0132, 0.4015, 0.0990, 0.0055, 0.0053, 0.0889, 0.3866],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 30, batch: 40/203] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.9847, 0.0015, 0.0028, 0.0044, 0.0017, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 30, batch: 80/203] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.6882e-02, 9.7794e-10, 2.8371e-06, 1.1315e-06, 2.2047e-06, 1.9484e-02,
        9.4363e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.625

[Epoch: 30, batch: 120/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.1554e-04, 9.2743e-01, 4.7173e-02, 2.1069e-08, 2.8072e-06, 2.5169e-02,
        7.5399e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.977

[Epoch: 30, batch: 160/203] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.5925e-03, 7.6293e-02, 3.9751e-05, 5.5517e-03, 8.3553e-01, 4.9652e-03,
        7.2025e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.011

[Epoch: 30, batch: 200/203] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0106, 0.6604, 0.0352, 0.0034, 0.0036, 0.0638, 0.2229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 31, batch: 40/203] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9733, 0.0034, 0.0063, 0.0107, 0.0027, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 31, batch: 80/203] total loss per batch: 0.784
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([9.5823e-01, 1.8900e-09, 2.8057e-04, 7.6256e-06, 1.0172e-05, 7.3147e-03,
        3.4153e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.568

[Epoch: 31, batch: 120/203] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.0929e-05, 9.7910e-01, 2.9805e-03, 7.8207e-09, 3.1265e-07, 1.7892e-02,
        7.3001e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.982

[Epoch: 31, batch: 160/203] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.5117e-03, 1.4639e-01, 8.3240e-05, 8.4376e-03, 5.7279e-01, 4.7401e-03,
        2.6105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 31, batch: 200/203] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0072, 0.4557, 0.0819, 0.0014, 0.0029, 0.0373, 0.4137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 32, batch: 40/203] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9801, 0.0025, 0.0022, 0.0064, 0.0020, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.026

[Epoch: 32, batch: 80/203] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.6400e-01, 4.9604e-06, 4.2546e-04, 2.7082e-05, 1.3227e-03, 8.6168e-02,
        5.4805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 0.228

[Epoch: 32, batch: 120/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.9855e-05, 9.7460e-01, 7.4549e-03, 2.0052e-07, 1.4292e-06, 1.7867e-02,
        5.4654e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.828

[Epoch: 32, batch: 160/203] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.2939e-03, 3.2968e-02, 4.4818e-05, 4.7459e-03, 9.3213e-01, 7.8823e-03,
        1.3931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 32, batch: 200/203] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0114, 0.5447, 0.0387, 0.0025, 0.0032, 0.0921, 0.3074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 33, batch: 40/203] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.3031e-03, 9.8503e-01, 8.5134e-04, 5.2962e-03, 4.4106e-03, 1.0776e-03,
        2.0340e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 33, batch: 80/203] total loss per batch: 0.781
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.6312e-01, 7.4536e-09, 2.9413e-05, 1.9793e-07, 2.3515e-06, 3.4833e-03,
        8.3336e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.951

[Epoch: 33, batch: 120/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4516e-04, 9.8008e-01, 9.9955e-03, 7.5971e-09, 2.0611e-06, 9.7747e-03,
        1.2459e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.982

[Epoch: 33, batch: 160/203] total loss per batch: 0.763
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.7307e-02, 2.6223e-01, 9.6817e-05, 2.3244e-02, 5.1207e-01, 2.4447e-02,
        1.6061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 33, batch: 200/203] total loss per batch: 0.798
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0079, 0.3741, 0.0788, 0.0028, 0.0051, 0.0556, 0.4758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 34, batch: 40/203] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9792, 0.0025, 0.0033, 0.0040, 0.0042, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 34, batch: 80/203] total loss per batch: 0.780
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9901e-01, 2.7166e-09, 1.1528e-05, 2.9375e-07, 3.1849e-06, 5.6912e-03,
        7.9529e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.821

[Epoch: 34, batch: 120/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.2417e-05, 9.7095e-01, 1.6679e-02, 3.0470e-08, 1.0232e-06, 1.2347e-02,
        1.2750e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 34, batch: 160/203] total loss per batch: 0.763
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.1518e-03, 1.2687e-02, 1.0409e-05, 4.3873e-03, 9.3990e-01, 2.2466e-03,
        3.8613e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.012

[Epoch: 34, batch: 200/203] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0072, 0.6217, 0.0359, 0.0032, 0.0024, 0.0523, 0.2774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 35, batch: 40/203] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9807, 0.0014, 0.0033, 0.0066, 0.0018, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 35, batch: 80/203] total loss per batch: 0.781
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.6412e-01, 6.5986e-09, 2.5529e-05, 2.1411e-06, 4.2100e-06, 1.4703e-02,
        8.2115e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.624

[Epoch: 35, batch: 120/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.0720e-04, 9.2413e-01, 3.1742e-02, 7.9600e-08, 1.6804e-05, 4.3704e-02,
        4.0412e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.908

[Epoch: 35, batch: 160/203] total loss per batch: 0.762
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.5609e-02, 9.5952e-02, 8.7466e-05, 6.8754e-03, 7.3092e-01, 8.4327e-03,
        1.4213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 35, batch: 200/203] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0070, 0.4606, 0.0810, 0.0048, 0.0026, 0.0361, 0.4078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 36, batch: 40/203] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.9705, 0.0035, 0.0037, 0.0060, 0.0060, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.006

[Epoch: 36, batch: 80/203] total loss per batch: 0.781
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2885e-01, 2.6244e-08, 2.2960e-05, 1.0925e-06, 1.1465e-05, 9.1712e-03,
        7.6194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.667

[Epoch: 36, batch: 120/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.3671e-05, 9.8597e-01, 9.6612e-03, 9.4106e-09, 7.4590e-07, 4.3123e-03,
        1.6630e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.956

[Epoch: 36, batch: 160/203] total loss per batch: 0.762
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.6841e-03, 5.5255e-02, 3.7545e-05, 1.4886e-02, 8.2758e-01, 6.8307e-03,
        8.7730e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.011

[Epoch: 36, batch: 200/203] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0062, 0.5961, 0.0616, 0.0037, 0.0035, 0.0608, 0.2681],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 37, batch: 40/203] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([9.5274e-04, 9.8277e-01, 1.8075e-03, 5.2052e-03, 5.9540e-03, 1.5589e-03,
        1.7486e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 37, batch: 80/203] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3919e-01, 3.8079e-08, 3.9686e-05, 1.6660e-06, 3.3787e-06, 8.1049e-03,
        7.5266e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.794

[Epoch: 37, batch: 120/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.5701e-04, 9.4914e-01, 2.7120e-02, 1.9999e-07, 7.2858e-06, 2.3568e-02,
        4.7067e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.933

[Epoch: 37, batch: 160/203] total loss per batch: 0.762
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.4190e-02, 1.1732e-01, 3.9848e-05, 4.3507e-03, 7.2701e-01, 6.9465e-03,
        1.3014e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.013

[Epoch: 37, batch: 200/203] total loss per batch: 0.795
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0090, 0.3223, 0.0676, 0.0034, 0.0022, 0.0376, 0.5578],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 38, batch: 40/203] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9815, 0.0017, 0.0030, 0.0051, 0.0022, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 38, batch: 80/203] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.1010e-01, 1.0089e-08, 1.4596e-05, 7.4780e-07, 6.8574e-06, 7.3835e-03,
        8.8249e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.635

[Epoch: 38, batch: 120/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.2089e-05, 9.8010e-01, 9.4682e-03, 2.0380e-08, 1.3067e-06, 1.0359e-02,
        6.7347e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.974

[Epoch: 38, batch: 160/203] total loss per batch: 0.761
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.2759e-02, 1.1852e-01, 8.2615e-05, 1.6779e-02, 7.2021e-01, 1.3112e-02,
        1.1854e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 38, batch: 200/203] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0052, 0.7357, 0.0398, 0.0055, 0.0022, 0.0548, 0.1568],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 39, batch: 40/203] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9707, 0.0025, 0.0052, 0.0092, 0.0055, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 39, batch: 80/203] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.5770e-01, 3.6970e-08, 2.8543e-05, 1.2377e-06, 1.6903e-05, 2.3874e-02,
        5.1838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.663

[Epoch: 39, batch: 120/203] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.6707e-05, 9.5986e-01, 1.3703e-02, 6.4870e-08, 3.6173e-06, 2.6409e-02,
        2.4433e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.945

[Epoch: 39, batch: 160/203] total loss per batch: 0.759
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.5283e-03, 6.8142e-02, 2.5978e-05, 3.5582e-03, 8.5385e-01, 6.7018e-03,
        5.9194e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.013

[Epoch: 39, batch: 200/203] total loss per batch: 0.793
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0072, 0.2926, 0.1094, 0.0017, 0.0012, 0.0551, 0.5328],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 40, batch: 40/203] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9727, 0.0023, 0.0048, 0.0117, 0.0027, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 40, batch: 80/203] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([6.5739e-02, 6.6842e-09, 9.1924e-06, 2.2532e-06, 2.5491e-06, 7.6106e-03,
        9.2664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.737

[Epoch: 40, batch: 120/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0160e-05, 9.6603e-01, 1.6198e-02, 2.3078e-08, 1.3014e-06, 1.7718e-02,
        1.7075e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.896

[Epoch: 40, batch: 160/203] total loss per batch: 0.759
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.3926e-02, 9.8395e-02, 5.5383e-05, 1.6599e-02, 6.5662e-01, 1.2152e-02,
        2.0225e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.009

[Epoch: 40, batch: 200/203] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0074, 0.6196, 0.0681, 0.0090, 0.0053, 0.0809, 0.2097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 41, batch: 40/203] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.9721, 0.0041, 0.0062, 0.0046, 0.0023, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.005

[Epoch: 41, batch: 80/203] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.1410e-01, 1.0836e-08, 3.4867e-05, 7.1719e-07, 7.4641e-06, 1.4695e-02,
        6.7116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.630

[Epoch: 41, batch: 120/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.2891e-05, 9.7580e-01, 1.0060e-02, 8.8768e-08, 1.7014e-06, 1.4097e-02,
        3.3190e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.903

[Epoch: 41, batch: 160/203] total loss per batch: 0.759
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.6542e-03, 1.1438e-01, 5.4436e-05, 3.6359e-03, 8.1652e-01, 8.0864e-03,
        5.0668e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.000

[Epoch: 41, batch: 200/203] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0154, 0.4645, 0.0689, 0.0029, 0.0066, 0.0697, 0.3719],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 42, batch: 40/203] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9769, 0.0031, 0.0027, 0.0070, 0.0024, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 42, batch: 80/203] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.3526e-01, 1.7596e-08, 5.0821e-06, 4.6873e-07, 9.1695e-06, 8.5976e-03,
        8.5613e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.630

[Epoch: 42, batch: 120/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.3561e-04, 9.7343e-01, 1.7780e-02, 2.5088e-08, 2.2251e-06, 8.6485e-03,
        1.7749e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.958

[Epoch: 42, batch: 160/203] total loss per batch: 0.758
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.3010e-02, 1.2901e-01, 3.8297e-05, 1.0807e-02, 6.9912e-01, 9.9442e-03,
        1.3807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 42, batch: 200/203] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0065, 0.5483, 0.0821, 0.0085, 0.0031, 0.0480, 0.3034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 43, batch: 40/203] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9774, 0.0018, 0.0067, 0.0042, 0.0021, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.002

[Epoch: 43, batch: 80/203] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.0469e-01, 4.5917e-09, 1.7017e-05, 1.4120e-06, 2.1980e-06, 1.4239e-02,
        6.8105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.711

[Epoch: 43, batch: 120/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.7862e-05, 9.7854e-01, 1.1515e-02, 1.5318e-08, 2.2599e-06, 9.9004e-03,
        7.0098e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 43, batch: 160/203] total loss per batch: 0.758
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.0980e-03, 8.4870e-02, 8.0200e-05, 1.1161e-02, 8.2847e-01, 1.1265e-02,
        5.6055e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 43, batch: 200/203] total loss per batch: 0.810
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0084, 0.4632, 0.0375, 0.0008, 0.0012, 0.0307, 0.4583],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 44, batch: 40/203] total loss per batch: 0.846
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.9686, 0.0012, 0.0076, 0.0089, 0.0024, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 44, batch: 80/203] total loss per batch: 0.861
Policy (actual, predicted): 6 0
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([8.4924e-01, 1.2426e-07, 4.9886e-04, 8.6244e-06, 1.6524e-04, 6.6511e-02,
        8.3575e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.526

[Epoch: 44, batch: 120/203] total loss per batch: 0.825
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.5097e-03, 6.7002e-01, 1.9683e-01, 4.2158e-07, 8.4521e-06, 1.3151e-01,
        1.2048e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.832

[Epoch: 44, batch: 160/203] total loss per batch: 0.823
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.7192e-02, 9.4790e-02, 2.4065e-04, 1.5150e-02, 6.1937e-01, 4.2368e-03,
        2.4902e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.019

[Epoch: 44, batch: 200/203] total loss per batch: 0.862
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0070, 0.5740, 0.0581, 0.0028, 0.0056, 0.0660, 0.2864],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 45, batch: 40/203] total loss per batch: 0.849
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5537e-03, 9.7153e-01, 6.6779e-03, 8.8299e-04, 1.0986e-02, 2.8696e-03,
        3.4951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 45, batch: 80/203] total loss per batch: 0.834
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.6725e-01, 1.0686e-05, 8.5641e-06, 1.6477e-05, 3.5795e-05, 1.7882e-02,
        7.1480e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.412

[Epoch: 45, batch: 120/203] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0466e-03, 9.9189e-01, 6.4376e-03, 2.4855e-07, 3.5271e-06, 6.2038e-04,
        1.6045e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.893

[Epoch: 45, batch: 160/203] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.3222e-02, 1.5223e-01, 2.2954e-05, 9.5179e-03, 7.7205e-01, 7.4014e-03,
        4.5555e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 45, batch: 200/203] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0023, 0.5206, 0.0753, 0.0034, 0.0021, 0.0632, 0.3332],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 46, batch: 40/203] total loss per batch: 0.811
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9812, 0.0034, 0.0035, 0.0043, 0.0017, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 46, batch: 80/203] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.1936e-01, 7.4231e-08, 2.1551e-06, 6.6091e-06, 3.9907e-06, 3.1987e-03,
        8.7743e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.554

[Epoch: 46, batch: 120/203] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.4724e-03, 9.8098e-01, 1.0536e-02, 1.5293e-07, 2.7361e-06, 5.0055e-03,
        5.7858e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.923

[Epoch: 46, batch: 160/203] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.7466e-03, 1.2929e-01, 4.4832e-05, 1.1369e-02, 7.4066e-01, 8.5007e-03,
        1.0539e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.018

[Epoch: 46, batch: 200/203] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0078, 0.4600, 0.0384, 0.0029, 0.0028, 0.1010, 0.3871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 47, batch: 40/203] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9818, 0.0019, 0.0016, 0.0089, 0.0016, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 47, batch: 80/203] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.8971e-01, 2.4036e-08, 2.1439e-06, 9.0774e-07, 1.5531e-05, 1.1397e-02,
        4.9887e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.704

[Epoch: 47, batch: 120/203] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.9767e-03, 9.7148e-01, 9.4618e-03, 5.5272e-09, 6.9686e-07, 1.7079e-02,
        4.2714e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.970

[Epoch: 47, batch: 160/203] total loss per batch: 0.754
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.7493e-03, 8.1241e-02, 2.5889e-05, 1.3611e-02, 7.7822e-01, 1.3792e-02,
        1.0536e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 47, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0056, 0.5021, 0.0663, 0.0020, 0.0034, 0.0454, 0.3753],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 48, batch: 40/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9754, 0.0044, 0.0026, 0.0091, 0.0019, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 48, batch: 80/203] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([4.3371e-02, 2.1157e-07, 1.9702e-05, 2.2203e-05, 3.7157e-05, 9.2087e-03,
        9.4734e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.654

[Epoch: 48, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.6498e-03, 9.5783e-01, 2.7042e-02, 1.1606e-06, 4.2848e-06, 1.3472e-02,
        1.1042e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.915

[Epoch: 48, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.3559e-03, 1.2952e-01, 4.6839e-05, 1.0113e-02, 6.9909e-01, 1.2201e-02,
        1.4368e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 48, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0064, 0.4939, 0.0635, 0.0022, 0.0039, 0.0599, 0.3702],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 49, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9815, 0.0027, 0.0020, 0.0055, 0.0020, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 49, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3176e-01, 5.4647e-08, 1.1657e-05, 3.0435e-06, 3.5972e-05, 9.2106e-03,
        7.5898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.768

[Epoch: 49, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.3181e-04, 9.6451e-01, 1.8177e-02, 8.7580e-08, 1.0981e-06, 1.6779e-02,
        3.7299e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.961

[Epoch: 49, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.5286e-03, 7.4018e-02, 1.3299e-05, 5.5360e-03, 8.4401e-01, 7.3475e-03,
        6.3548e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 49, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0075, 0.5329, 0.0703, 0.0021, 0.0037, 0.0694, 0.3141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 50, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.9823, 0.0022, 0.0025, 0.0060, 0.0015, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 50, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1202e-01, 6.7254e-08, 1.8258e-05, 2.4812e-06, 1.9204e-05, 9.8735e-03,
        7.7806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 50, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.4255e-04, 9.6911e-01, 1.8911e-02, 7.7282e-08, 8.4936e-07, 1.1337e-02,
        2.3235e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.967

[Epoch: 50, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.9512e-03, 1.1521e-01, 2.9932e-05, 8.1179e-03, 6.8245e-01, 1.3630e-02,
        1.7461e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 50, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0049, 0.4691, 0.0542, 0.0019, 0.0026, 0.0488, 0.4185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 51, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9755, 0.0030, 0.0030, 0.0068, 0.0032, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 51, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8928e-01, 3.2438e-08, 6.5626e-06, 1.5662e-06, 9.9488e-06, 7.1289e-03,
        8.0358e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.655

[Epoch: 51, batch: 120/203] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6990e-04, 9.6549e-01, 2.0828e-02, 1.1900e-07, 9.6590e-07, 1.3207e-02,
        3.2105e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 51, batch: 160/203] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.3950e-03, 7.3374e-02, 1.4203e-05, 5.5044e-03, 8.6354e-01, 6.5221e-03,
        4.6646e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 51, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0043, 0.6234, 0.0636, 0.0031, 0.0033, 0.0582, 0.2441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 52, batch: 40/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.9774, 0.0031, 0.0028, 0.0070, 0.0021, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 52, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1247e-01, 3.2761e-08, 8.4042e-06, 1.2621e-06, 6.9153e-06, 6.3909e-03,
        7.8112e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.713

[Epoch: 52, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.5117e-04, 9.5834e-01, 2.2951e-02, 8.7595e-08, 9.4732e-07, 1.8155e-02,
        2.8911e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.949

[Epoch: 52, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.2856e-03, 1.3381e-01, 3.0966e-05, 8.9260e-03, 6.5950e-01, 1.3224e-02,
        1.7722e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 52, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0051, 0.3884, 0.0735, 0.0026, 0.0035, 0.0586, 0.4682],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 53, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9787, 0.0028, 0.0024, 0.0053, 0.0027, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 53, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0375e-01, 3.1552e-08, 6.8535e-06, 1.1493e-06, 7.4354e-06, 7.3712e-03,
        7.8887e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.661

[Epoch: 53, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.3295e-04, 9.6429e-01, 2.0176e-02, 8.0742e-08, 9.4922e-07, 1.4904e-02,
        3.2269e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 53, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.6246e-03, 8.2471e-02, 1.1655e-05, 3.8983e-03, 8.5811e-01, 7.1952e-03,
        4.3692e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 53, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.6343, 0.0554, 0.0024, 0.0030, 0.0531, 0.2472],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 54, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9778, 0.0027, 0.0025, 0.0066, 0.0021, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 54, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8970e-01, 1.9206e-08, 5.8692e-06, 8.7626e-07, 4.0593e-06, 6.1789e-03,
        8.0411e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.643

[Epoch: 54, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.9247e-04, 9.5909e-01, 2.6872e-02, 8.9389e-08, 1.1583e-06, 1.3647e-02,
        4.5597e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 54, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.0718e-03, 9.5350e-02, 1.8699e-05, 6.3259e-03, 7.0877e-01, 8.0754e-03,
        1.7538e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 54, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.4259, 0.0837, 0.0022, 0.0035, 0.0482, 0.4310],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 55, batch: 40/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9809, 0.0027, 0.0028, 0.0048, 0.0023, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 55, batch: 80/203] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2368e-01, 4.0920e-08, 5.9022e-06, 1.2028e-06, 5.8149e-06, 7.3790e-03,
        7.6893e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.706

[Epoch: 55, batch: 120/203] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6370e-04, 9.7281e-01, 1.2283e-02, 5.4241e-08, 4.1005e-07, 1.4441e-02,
        3.5122e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.931

[Epoch: 55, batch: 160/203] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.4480e-03, 1.1573e-01, 9.2284e-06, 4.3103e-03, 8.1405e-01, 8.5040e-03,
        5.1949e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 55, batch: 200/203] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0068, 0.5863, 0.0379, 0.0028, 0.0035, 0.0612, 0.3016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 56, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0070, 0.9677, 0.0038, 0.0032, 0.0084, 0.0036, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 56, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8538e-01, 1.1778e-08, 1.8864e-06, 4.4743e-07, 3.6516e-06, 5.0944e-03,
        8.0952e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.672

[Epoch: 56, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8581e-04, 9.5470e-01, 2.5680e-02, 8.8332e-08, 1.1960e-06, 1.9327e-02,
        7.7843e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.907

[Epoch: 56, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.5386e-03, 8.1802e-02, 1.5988e-05, 7.6382e-03, 7.2908e-01, 8.5336e-03,
        1.6840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.017

[Epoch: 56, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0064, 0.4903, 0.0857, 0.0025, 0.0040, 0.0411, 0.3700],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 57, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.9748, 0.0024, 0.0034, 0.0083, 0.0029, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 57, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3088e-01, 5.9281e-08, 6.2532e-06, 1.1429e-06, 4.5244e-06, 6.6320e-03,
        7.6247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.674

[Epoch: 57, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.0345e-04, 9.6967e-01, 1.8300e-02, 2.2942e-08, 4.0482e-07, 1.1626e-02,
        4.3386e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.941

[Epoch: 57, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.1370e-03, 1.4139e-01, 2.3572e-05, 5.1963e-03, 7.6056e-01, 1.0977e-02,
        7.3710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.012

[Epoch: 57, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0052, 0.5182, 0.0341, 0.0028, 0.0032, 0.0663, 0.3701],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 58, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.9755, 0.0036, 0.0027, 0.0047, 0.0041, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 58, batch: 80/203] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8252e-01, 7.2437e-09, 3.3806e-06, 4.8296e-07, 2.1044e-06, 3.5788e-03,
        8.1390e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.717

[Epoch: 58, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9913e-04, 9.5726e-01, 2.2277e-02, 6.9372e-08, 2.0823e-06, 2.0159e-02,
        1.1434e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.856

[Epoch: 58, batch: 160/203] total loss per batch: 0.751
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.5627e-03, 7.2301e-02, 2.4530e-05, 7.2356e-03, 7.6285e-01, 6.5840e-03,
        1.4645e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 58, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0064, 0.4525, 0.1290, 0.0031, 0.0044, 0.0490, 0.3556],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 59, batch: 40/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.9790, 0.0021, 0.0027, 0.0063, 0.0017, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 59, batch: 80/203] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3331e-01, 2.3821e-08, 7.8975e-06, 1.2864e-06, 4.7272e-06, 1.0824e-02,
        7.5585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.656

[Epoch: 59, batch: 120/203] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.5008e-04, 9.7323e-01, 1.4448e-02, 2.9519e-08, 5.9137e-07, 1.2070e-02,
        2.0565e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.966

[Epoch: 59, batch: 160/203] total loss per batch: 0.752
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.8471e-03, 1.0187e-01, 2.1571e-05, 5.9865e-03, 7.9867e-01, 9.5015e-03,
        7.8103e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 59, batch: 200/203] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.6863, 0.0248, 0.0022, 0.0026, 0.0512, 0.2285],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 60, batch: 40/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9812, 0.0033, 0.0024, 0.0036, 0.0024, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 60, batch: 80/203] total loss per batch: 0.768
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8628e-01, 1.1157e-08, 4.5333e-06, 1.2403e-06, 1.4393e-06, 4.0745e-03,
        8.0964e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.710

[Epoch: 60, batch: 120/203] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.1027e-04, 9.7510e-01, 1.4436e-02, 4.8637e-08, 6.0231e-07, 1.0352e-02,
        3.9491e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.924

[Epoch: 60, batch: 160/203] total loss per batch: 0.752
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.1317e-03, 1.0166e-01, 3.6884e-05, 6.5255e-03, 7.1833e-01, 8.3153e-03,
        1.5700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 60, batch: 200/203] total loss per batch: 0.785
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0081, 0.3345, 0.0745, 0.0037, 0.0047, 0.0976, 0.4770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 61, batch: 40/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9787, 0.0015, 0.0021, 0.0085, 0.0014, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 61, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2849e-01, 2.1097e-08, 9.4799e-06, 1.2730e-06, 5.3007e-06, 5.0514e-03,
        7.6644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.629

[Epoch: 61, batch: 120/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9115e-04, 9.6439e-01, 1.5134e-02, 5.8591e-08, 6.2143e-07, 2.0187e-02,
        4.2559e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.950

[Epoch: 61, batch: 160/203] total loss per batch: 0.751
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.9530e-03, 1.1805e-01, 4.0516e-05, 9.7314e-03, 7.4476e-01, 1.0620e-02,
        1.0984e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 61, batch: 200/203] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.6174, 0.0537, 0.0020, 0.0020, 0.0349, 0.2865],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 62, batch: 40/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9789, 0.0031, 0.0030, 0.0041, 0.0024, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 62, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5859e-01, 1.5775e-08, 3.9185e-06, 1.0234e-06, 1.3035e-06, 3.1428e-03,
        8.3826e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.678

[Epoch: 62, batch: 120/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.7529e-04, 9.7459e-01, 1.6081e-02, 6.7790e-08, 1.6576e-06, 9.0548e-03,
        4.0313e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.915

[Epoch: 62, batch: 160/203] total loss per batch: 0.751
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.1795e-03, 8.0407e-02, 4.0769e-05, 6.7304e-03, 8.1290e-01, 5.6995e-03,
        8.9043e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 62, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0059, 0.4073, 0.0590, 0.0021, 0.0036, 0.0790, 0.4431],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 63, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.9827, 0.0025, 0.0038, 0.0039, 0.0025, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 63, batch: 80/203] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.9020e-01, 9.4217e-09, 5.1236e-06, 1.4147e-07, 3.9149e-06, 5.3616e-03,
        7.0443e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.683

[Epoch: 63, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4028e-04, 9.7660e-01, 1.2356e-02, 7.0915e-08, 4.8897e-07, 1.0898e-02,
        4.9689e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 63, batch: 160/203] total loss per batch: 0.751
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.4754e-03, 1.0813e-01, 3.2389e-05, 8.5619e-03, 7.6618e-01, 1.1568e-02,
        1.0104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 63, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0048, 0.6156, 0.0665, 0.0039, 0.0032, 0.0375, 0.2685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 64, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.9706, 0.0023, 0.0044, 0.0077, 0.0025, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 64, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.0914e-01, 1.4222e-08, 9.4303e-06, 1.2293e-06, 1.6108e-06, 2.9668e-03,
        8.8788e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.633

[Epoch: 64, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.5413e-04, 9.5025e-01, 2.9905e-02, 1.0146e-07, 1.5966e-06, 1.9386e-02,
        1.4208e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.924

[Epoch: 64, batch: 160/203] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.6725e-03, 9.4802e-02, 3.5946e-05, 8.9956e-03, 7.4925e-01, 8.6975e-03,
        1.3255e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 64, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0058, 0.4015, 0.0441, 0.0027, 0.0048, 0.0773, 0.4638],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 65, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.9819, 0.0030, 0.0023, 0.0044, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 65, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.8609e-01, 2.4355e-08, 7.5640e-06, 4.0647e-07, 4.5566e-06, 9.9234e-03,
        7.0398e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.689

[Epoch: 65, batch: 120/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.4437e-04, 9.7588e-01, 1.2228e-02, 3.6110e-08, 5.3547e-07, 1.1747e-02,
        6.4028e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.965

[Epoch: 65, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.5477e-03, 8.9897e-02, 2.5362e-05, 1.0905e-02, 7.7583e-01, 5.2560e-03,
        1.1154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 65, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0059, 0.6087, 0.0965, 0.0026, 0.0040, 0.0626, 0.2197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 66, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0066, 0.9704, 0.0040, 0.0051, 0.0047, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 66, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.1850e-01, 9.1714e-09, 5.6246e-06, 6.3960e-07, 1.9524e-06, 1.6753e-03,
        8.7981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.691

[Epoch: 66, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.2600e-04, 9.5857e-01, 2.0362e-02, 5.3847e-08, 6.3780e-07, 2.0839e-02,
        4.6951e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.951

[Epoch: 66, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.2917e-03, 1.0209e-01, 4.1933e-05, 8.3410e-03, 7.5294e-01, 8.6883e-03,
        1.2061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 66, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0079, 0.3939, 0.0512, 0.0045, 0.0038, 0.0507, 0.4880],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 67, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9745, 0.0031, 0.0047, 0.0086, 0.0028, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 67, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.4799e-01, 1.8653e-08, 1.6613e-05, 1.2652e-07, 6.9554e-06, 1.2063e-02,
        6.3992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.646

[Epoch: 67, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6688e-04, 9.6505e-01, 2.0374e-02, 1.1113e-07, 8.6779e-07, 1.4104e-02,
        3.8150e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.836

[Epoch: 67, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.3120e-03, 1.1725e-01, 3.9057e-05, 3.8770e-03, 7.6630e-01, 8.2469e-03,
        9.8972e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 67, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0078, 0.5999, 0.0525, 0.0025, 0.0063, 0.0543, 0.2767],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 68, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.9887, 0.0012, 0.0023, 0.0020, 0.0022, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 68, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([7.4456e-02, 5.0919e-09, 5.9407e-06, 9.4062e-07, 2.5557e-06, 2.1354e-03,
        9.2340e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.654

[Epoch: 68, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.6276e-04, 9.6930e-01, 1.5196e-02, 8.1011e-08, 6.6285e-07, 1.5342e-02,
        9.0491e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.958

[Epoch: 68, batch: 160/203] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.9589e-03, 9.4661e-02, 6.9417e-05, 4.9644e-03, 7.9534e-01, 7.2368e-03,
        9.1769e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 68, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0048, 0.5761, 0.0570, 0.0035, 0.0024, 0.0631, 0.2931],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 69, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9803, 0.0023, 0.0042, 0.0057, 0.0014, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 69, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.5875e-01, 6.9618e-09, 2.8699e-05, 4.5815e-07, 4.0788e-06, 6.2478e-03,
        7.3497e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.666

[Epoch: 69, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.6473e-04, 9.8183e-01, 1.0078e-02, 4.3215e-08, 2.8599e-07, 7.9256e-03,
        2.9113e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.954

[Epoch: 69, batch: 160/203] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8737e-03, 1.5893e-01, 2.9227e-05, 5.1358e-03, 7.0426e-01, 7.6464e-03,
        1.1912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 69, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0073, 0.3613, 0.0482, 0.0023, 0.0033, 0.0435, 0.5342],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 70, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0057, 0.9695, 0.0026, 0.0030, 0.0070, 0.0040, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 70, batch: 80/203] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.3848e-01, 5.2746e-09, 1.0914e-05, 1.7942e-06, 4.6890e-06, 4.6535e-03,
        8.5685e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.673

[Epoch: 70, batch: 120/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.8192e-04, 9.5251e-01, 2.2108e-02, 1.0870e-07, 7.1794e-07, 2.5195e-02,
        3.8923e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.893

[Epoch: 70, batch: 160/203] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2490e-03, 6.4133e-02, 3.8047e-05, 2.4264e-03, 8.0232e-01, 5.5770e-03,
        1.2126e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 70, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.6874, 0.0653, 0.0031, 0.0030, 0.0404, 0.1968],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 71, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9831, 0.0024, 0.0027, 0.0037, 0.0024, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 71, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.8330e-01, 2.8581e-08, 2.0438e-05, 7.1206e-07, 7.4901e-06, 8.4755e-03,
        7.0819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.755

[Epoch: 71, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.8010e-04, 9.6833e-01, 1.0394e-02, 1.1407e-07, 3.8711e-07, 2.1096e-02,
        9.0500e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.968

[Epoch: 71, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0259e-03, 1.3709e-01, 3.7555e-05, 5.0880e-03, 7.1537e-01, 7.9288e-03,
        1.2946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 71, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0080, 0.4060, 0.0696, 0.0041, 0.0044, 0.0715, 0.4364],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 72, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0064, 0.9652, 0.0035, 0.0059, 0.0073, 0.0037, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 72, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5651e-01, 8.7231e-09, 3.9492e-06, 5.6757e-07, 1.6985e-06, 4.6962e-03,
        8.3879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.468

[Epoch: 72, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.7090e-04, 9.6952e-01, 1.7580e-02, 8.8429e-08, 1.1285e-06, 1.2624e-02,
        7.0912e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 72, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.1951e-03, 8.0046e-02, 3.5255e-05, 3.6516e-03, 8.2668e-01, 7.3284e-03,
        7.7067e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 72, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0066, 0.5451, 0.0760, 0.0047, 0.0050, 0.0500, 0.3126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 73, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9864, 0.0013, 0.0035, 0.0017, 0.0022, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 73, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1291e-01, 5.3630e-09, 1.0787e-05, 1.3336e-06, 3.5218e-06, 5.6332e-03,
        7.8144e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.872

[Epoch: 73, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.3387e-04, 9.7039e-01, 1.1857e-02, 6.7769e-08, 6.4670e-07, 1.7515e-02,
        4.2911e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.935

[Epoch: 73, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.3779e-03, 1.4042e-01, 1.6167e-04, 5.3932e-03, 6.6982e-01, 7.3258e-03,
        1.6850e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 73, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0027, 0.5129, 0.0437, 0.0021, 0.0027, 0.0670, 0.3689],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 74, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0055, 0.9661, 0.0038, 0.0035, 0.0112, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 74, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1414e-01, 4.4245e-09, 9.8195e-06, 1.8971e-06, 6.6315e-06, 6.8609e-03,
        7.7898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.783

[Epoch: 74, batch: 120/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0363e-04, 9.7105e-01, 1.8070e-02, 6.8085e-08, 4.4823e-07, 1.0771e-02,
        9.4396e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 74, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.4132e-03, 6.5454e-02, 5.9283e-05, 5.5174e-03, 8.2148e-01, 7.2903e-03,
        9.4788e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 74, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0064, 0.5291, 0.0691, 0.0042, 0.0040, 0.0523, 0.3349],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 75, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9850, 0.0017, 0.0039, 0.0027, 0.0017, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 75, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9888e-01, 3.0154e-08, 1.1652e-05, 3.3163e-06, 2.8023e-06, 6.5602e-03,
        7.9455e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.640

[Epoch: 75, batch: 120/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0683e-04, 9.6379e-01, 1.4136e-02, 7.4490e-08, 8.7535e-07, 2.1969e-02,
        7.2910e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 75, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.1363e-03, 8.8592e-02, 8.1364e-05, 6.4067e-03, 7.6206e-01, 6.1126e-03,
        1.2961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 75, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0053, 0.5028, 0.0574, 0.0031, 0.0037, 0.0640, 0.3637],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 76, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9802, 0.0026, 0.0024, 0.0036, 0.0038, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 76, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0214e-01, 1.4888e-08, 1.0980e-05, 1.9541e-06, 4.2471e-06, 4.9502e-03,
        7.9290e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.768

[Epoch: 76, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.3302e-05, 9.6655e-01, 1.8204e-02, 2.3102e-08, 5.0976e-07, 1.5157e-02,
        3.8979e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.939

[Epoch: 76, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.5473e-03, 9.0770e-02, 4.3545e-05, 4.8875e-03, 8.0461e-01, 4.9291e-03,
        9.0208e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 76, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.5037, 0.0931, 0.0017, 0.0029, 0.0299, 0.3641],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 77, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9836, 0.0023, 0.0039, 0.0028, 0.0022, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 77, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3955e-01, 3.8870e-08, 2.3595e-05, 1.8796e-06, 2.3242e-06, 7.3703e-03,
        7.5305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.687

[Epoch: 77, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.7840e-05, 9.7581e-01, 9.9311e-03, 5.2910e-08, 5.4997e-07, 1.4199e-02,
        6.1363e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.902

[Epoch: 77, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.9075e-03, 1.3212e-01, 5.2252e-05, 4.0674e-03, 7.1686e-01, 8.2090e-03,
        1.3178e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 77, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.5587, 0.0453, 0.0036, 0.0046, 0.0955, 0.2879],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 78, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.9807, 0.0023, 0.0044, 0.0046, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 78, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5823e-01, 2.7044e-08, 9.8993e-06, 1.1486e-06, 3.7518e-06, 2.3264e-03,
        8.3943e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.715

[Epoch: 78, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.8605e-04, 9.3453e-01, 3.4263e-02, 1.1731e-07, 9.2853e-07, 3.1025e-02,
        3.4573e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.923

[Epoch: 78, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2654e-03, 6.2791e-02, 4.2400e-05, 3.1156e-03, 8.5484e-01, 5.3992e-03,
        6.9548e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 78, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.3576, 0.0559, 0.0027, 0.0045, 0.0787, 0.4967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 79, batch: 40/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9788, 0.0027, 0.0032, 0.0027, 0.0047, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 79, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2116e-01, 1.4480e-08, 2.7269e-05, 2.3482e-06, 4.7766e-06, 8.4773e-03,
        7.7032e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.628

[Epoch: 79, batch: 120/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([9.2305e-05, 9.7584e-01, 1.0529e-02, 1.3954e-07, 1.8634e-06, 1.3532e-02,
        4.1466e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.805

[Epoch: 79, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.5827e-03, 1.8694e-01, 1.5963e-04, 1.0994e-02, 5.3136e-01, 1.2993e-02,
        2.5097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 79, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0029, 0.6987, 0.0534, 0.0040, 0.0043, 0.0489, 0.1878],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 80, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9744, 0.0027, 0.0066, 0.0065, 0.0028, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 80, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8622e-01, 1.9875e-08, 1.1448e-05, 1.8022e-06, 3.0798e-06, 3.2275e-03,
        8.1054e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.643

[Epoch: 80, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0701e-04, 9.6948e-01, 1.1334e-02, 6.9176e-08, 3.2677e-07, 1.9075e-02,
        1.0364e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.986

[Epoch: 80, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.3892e-03, 5.3433e-02, 4.0664e-05, 4.2151e-03, 9.0572e-01, 4.8981e-03,
        2.6299e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 80, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.4143, 0.0508, 0.0024, 0.0057, 0.0587, 0.4645],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 81, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.9841, 0.0023, 0.0024, 0.0034, 0.0027, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 81, batch: 80/203] total loss per batch: 0.765
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2499e-01, 1.3692e-08, 2.0060e-05, 2.5282e-06, 5.1795e-06, 7.6502e-03,
        7.6733e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.740

[Epoch: 81, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.3944e-05, 9.7779e-01, 1.3797e-02, 7.1548e-08, 9.0627e-07, 8.3900e-03,
        3.6129e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.979

[Epoch: 81, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([1.3342e-02, 1.0672e-01, 4.1513e-05, 1.3579e-02, 7.1700e-01, 1.0315e-02,
        1.3901e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 81, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.6112, 0.0620, 0.0035, 0.0043, 0.0499, 0.2653],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 82, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.9718, 0.0036, 0.0041, 0.0059, 0.0034, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 82, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8769e-01, 6.3620e-09, 2.6451e-05, 2.1098e-06, 1.8544e-06, 6.2629e-03,
        8.0602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.581

[Epoch: 82, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.5560e-05, 9.5393e-01, 2.0105e-02, 1.5448e-07, 7.0048e-07, 2.5881e-02,
        1.6528e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.985

[Epoch: 82, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.6267e-03, 1.1771e-01, 8.0729e-05, 7.4596e-03, 7.8139e-01, 9.3766e-03,
        7.6361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 82, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0036, 0.4722, 0.0752, 0.0018, 0.0037, 0.0658, 0.3776],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 83, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9755, 0.0035, 0.0049, 0.0038, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 83, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2736e-01, 2.8398e-08, 7.1939e-06, 2.3435e-06, 2.1507e-06, 4.5798e-03,
        7.6805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.773

[Epoch: 83, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.2137e-05, 9.8434e-01, 7.8608e-03, 6.2369e-08, 3.8305e-07, 7.7544e-03,
        5.3786e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.975

[Epoch: 83, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.8305e-03, 7.6421e-02, 3.8552e-05, 3.9855e-03, 8.0779e-01, 4.0346e-03,
        1.0190e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 83, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.4649, 0.0423, 0.0029, 0.0025, 0.0590, 0.4245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 84, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9776, 0.0026, 0.0034, 0.0053, 0.0032, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 84, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5564e-01, 9.9765e-09, 1.9028e-05, 7.8515e-07, 5.0069e-06, 3.4387e-03,
        8.4089e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.654

[Epoch: 84, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.0771e-05, 9.6945e-01, 1.6193e-02, 5.9544e-08, 4.1959e-07, 1.4334e-02,
        5.5921e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.967

[Epoch: 84, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.3022e-03, 1.6842e-01, 1.4287e-04, 7.6747e-03, 6.3837e-01, 8.4650e-03,
        1.6863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 84, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0045, 0.5575, 0.0872, 0.0051, 0.0047, 0.0809, 0.2601],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 85, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9828, 0.0019, 0.0037, 0.0032, 0.0027, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 85, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.7008e-01, 6.3320e-08, 1.8495e-05, 1.4609e-06, 2.9608e-06, 4.1422e-03,
        6.2575e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.715

[Epoch: 85, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.6033e-05, 9.5749e-01, 2.3185e-02, 1.0137e-07, 1.0760e-06, 1.9269e-02,
        6.7559e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.969

[Epoch: 85, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8684e-03, 5.3396e-02, 2.7419e-05, 3.3250e-03, 8.8818e-01, 4.8542e-03,
        4.5346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 85, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0057, 0.5097, 0.0304, 0.0022, 0.0046, 0.0835, 0.3638],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 86, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9741, 0.0032, 0.0053, 0.0059, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 86, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([6.6759e-02, 3.8302e-08, 9.8920e-06, 2.5644e-06, 8.1148e-06, 2.2423e-03,
        9.3098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 86, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.4182e-05, 9.5635e-01, 1.4668e-02, 2.5034e-07, 7.5052e-07, 2.8934e-02,
        2.3587e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.975

[Epoch: 86, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.2623e-03, 1.1369e-01, 6.3793e-05, 5.8767e-03, 6.9011e-01, 5.5470e-03,
        1.7846e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 86, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5416, 0.0879, 0.0056, 0.0043, 0.0361, 0.3216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 87, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9817, 0.0032, 0.0031, 0.0031, 0.0026, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 87, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4476e-01, 3.3190e-08, 3.9028e-05, 2.2012e-06, 6.0191e-06, 7.1120e-03,
        7.4808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.660

[Epoch: 87, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.3166e-04, 9.6334e-01, 1.8252e-02, 7.9852e-08, 9.6226e-07, 1.8271e-02,
        1.0144e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.961

[Epoch: 87, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.9275e-03, 9.3760e-02, 4.8387e-05, 5.8941e-03, 8.1439e-01, 6.5087e-03,
        7.1468e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 87, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0043, 0.5130, 0.0288, 0.0028, 0.0044, 0.0490, 0.3977],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 88, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.9696, 0.0053, 0.0036, 0.0041, 0.0072, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 88, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4877e-01, 3.9351e-08, 2.7002e-05, 7.5255e-07, 2.0010e-06, 7.8920e-03,
        7.4331e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 88, batch: 120/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.5145e-05, 9.4653e-01, 2.5143e-02, 1.8933e-07, 1.7741e-06, 2.8240e-02,
        3.9000e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.958

[Epoch: 88, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.7817e-03, 1.1924e-01, 5.8000e-05, 4.1716e-03, 7.3057e-01, 7.4167e-03,
        1.3276e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 88, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.4513, 0.0922, 0.0028, 0.0047, 0.0659, 0.3785],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 89, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9728, 0.0037, 0.0073, 0.0040, 0.0033, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 89, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.3766e-01, 2.9916e-08, 1.1552e-05, 1.7974e-06, 1.4167e-05, 3.8260e-03,
        8.5849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.738

[Epoch: 89, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.3715e-05, 9.6790e-01, 2.1970e-02, 1.4440e-07, 1.3574e-06, 1.0055e-02,
        6.4121e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.908

[Epoch: 89, batch: 160/203] total loss per batch: 0.748
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.4841e-03, 9.7856e-02, 6.9200e-05, 6.4159e-03, 7.8696e-01, 5.3326e-03,
        9.5886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 89, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0030, 0.6671, 0.0591, 0.0049, 0.0031, 0.0395, 0.2232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 90, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9800, 0.0029, 0.0025, 0.0042, 0.0037, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 90, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.6821e-01, 2.1280e-08, 3.6066e-05, 4.6738e-07, 1.7530e-06, 2.9767e-03,
        7.2877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.602

[Epoch: 90, batch: 120/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.9504e-05, 9.7591e-01, 8.1811e-03, 4.3141e-08, 4.8704e-07, 1.5890e-02,
        4.8856e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 90, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.7059e-03, 1.0876e-01, 3.3592e-05, 5.5312e-03, 7.6945e-01, 7.3778e-03,
        1.0415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 90, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.2648, 0.0573, 0.0022, 0.0037, 0.0769, 0.5906],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 91, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9777, 0.0037, 0.0038, 0.0040, 0.0030, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 91, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.4374e-01, 2.0552e-08, 7.8366e-06, 1.0760e-06, 4.8791e-06, 4.4481e-03,
        8.5180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.736

[Epoch: 91, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0647e-05, 9.7210e-01, 1.7589e-02, 1.6521e-07, 6.6281e-07, 1.0258e-02,
        6.2161e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.940

[Epoch: 91, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.0322e-03, 9.2222e-02, 5.4543e-05, 5.8866e-03, 7.4097e-01, 4.9623e-03,
        1.4987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 91, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.7813, 0.0369, 0.0017, 0.0029, 0.0330, 0.1415],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 92, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9801, 0.0024, 0.0020, 0.0034, 0.0034, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 92, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2572e-01, 4.5680e-08, 2.5046e-05, 2.9728e-06, 5.4066e-06, 7.9329e-03,
        7.6632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.635

[Epoch: 92, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0563e-05, 9.8137e-01, 7.9336e-03, 8.7788e-08, 4.4399e-07, 1.0650e-02,
        2.2210e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.939

[Epoch: 92, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.6122e-03, 1.0052e-01, 3.9908e-05, 1.0444e-02, 7.7911e-01, 6.4792e-03,
        9.8796e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 92, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.4688, 0.0733, 0.0027, 0.0043, 0.0427, 0.4047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 93, batch: 40/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9738, 0.0033, 0.0051, 0.0055, 0.0044, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 93, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0631e-01, 4.8846e-08, 1.5327e-05, 1.8435e-06, 7.4924e-06, 4.5180e-03,
        7.8915e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.704

[Epoch: 93, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.4112e-05, 9.5912e-01, 1.9763e-02, 2.7168e-07, 2.0502e-06, 2.1043e-02,
        9.8386e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.925

[Epoch: 93, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.5623e-03, 9.1203e-02, 3.7458e-05, 4.4735e-03, 8.0784e-01, 4.8913e-03,
        8.5994e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 93, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0051, 0.4701, 0.0643, 0.0032, 0.0062, 0.0685, 0.3826],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 94, batch: 40/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9811, 0.0033, 0.0036, 0.0024, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 94, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9968e-01, 1.0102e-07, 2.4224e-05, 3.6555e-06, 5.7340e-06, 5.0527e-03,
        7.9524e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.654

[Epoch: 94, batch: 120/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.8235e-05, 9.3500e-01, 3.4168e-02, 2.4410e-07, 1.4293e-06, 3.0745e-02,
        1.5883e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.941

[Epoch: 94, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.6020e-03, 1.0605e-01, 5.8154e-05, 7.0696e-03, 6.5586e-01, 7.7415e-03,
        2.1662e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 94, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0057, 0.5804, 0.0513, 0.0040, 0.0062, 0.0506, 0.3017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 95, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.9622, 0.0040, 0.0089, 0.0086, 0.0063, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 95, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7458e-01, 3.3369e-08, 1.2356e-05, 1.0524e-06, 3.1602e-06, 4.4168e-03,
        8.2099e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.658

[Epoch: 95, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.9451e-05, 9.6674e-01, 1.2713e-02, 3.5038e-07, 9.0399e-07, 2.0510e-02,
        1.4198e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.952

[Epoch: 95, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.1009e-03, 1.0113e-01, 7.4392e-05, 5.8119e-03, 8.1330e-01, 6.5999e-03,
        6.8982e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 95, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0053, 0.4472, 0.0553, 0.0064, 0.0056, 0.1142, 0.3660],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 96, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.9807, 0.0033, 0.0028, 0.0024, 0.0034, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 96, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.5359e-01, 4.6786e-08, 2.2001e-05, 8.1081e-07, 4.8704e-06, 5.4725e-03,
        7.4091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.731

[Epoch: 96, batch: 120/203] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9882e-05, 9.7797e-01, 1.0715e-02, 7.5616e-08, 6.3495e-07, 1.1288e-02,
        5.5758e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.925

[Epoch: 96, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([8.3486e-03, 1.0766e-01, 4.7619e-05, 6.1924e-03, 7.8512e-01, 8.3308e-03,
        8.4299e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 96, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0045, 0.5471, 0.0559, 0.0030, 0.0047, 0.0426, 0.3421],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 97, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9765, 0.0024, 0.0034, 0.0048, 0.0045, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 97, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.1917e-01, 2.2964e-08, 1.5159e-05, 6.0580e-07, 3.4313e-06, 2.0035e-03,
        8.7880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.604

[Epoch: 97, batch: 120/203] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.2194e-05, 9.5674e-01, 1.8208e-02, 1.6096e-07, 4.9568e-07, 2.4984e-02,
        3.3279e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.937

[Epoch: 97, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2979e-03, 1.4187e-01, 6.0919e-05, 7.9020e-03, 6.9206e-01, 7.1092e-03,
        1.4670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 97, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0061, 0.5000, 0.0582, 0.0045, 0.0053, 0.0655, 0.3605],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 98, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9750, 0.0040, 0.0031, 0.0043, 0.0041, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 98, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.8137e-01, 5.0714e-08, 1.4478e-05, 8.3490e-07, 8.3468e-06, 7.5011e-03,
        7.1111e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.715

[Epoch: 98, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.9185e-05, 9.7741e-01, 1.1880e-02, 8.5450e-08, 4.8475e-07, 1.0649e-02,
        3.2153e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.897

[Epoch: 98, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7587e-03, 6.9516e-02, 2.6964e-05, 4.1470e-03, 8.4884e-01, 6.4873e-03,
        6.7221e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 98, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.5035, 0.0721, 0.0021, 0.0039, 0.0477, 0.3670],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 99, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9794, 0.0024, 0.0027, 0.0042, 0.0030, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 99, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.3475e-01, 2.6806e-08, 1.4784e-05, 8.2092e-07, 1.8708e-06, 2.5568e-03,
        8.6267e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.642

[Epoch: 99, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.2304e-04, 9.6078e-01, 1.7923e-02, 2.9451e-07, 6.9039e-07, 2.1168e-02,
        6.7747e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.911

[Epoch: 99, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.4404e-03, 9.9415e-02, 2.9737e-05, 7.9675e-03, 7.1414e-01, 7.5935e-03,
        1.6442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 99, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0043, 0.6239, 0.0578, 0.0026, 0.0052, 0.0437, 0.2624],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 100, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9818, 0.0019, 0.0030, 0.0062, 0.0022, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 100, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.8830e-01, 9.3147e-08, 2.9982e-05, 1.4480e-06, 4.8864e-06, 5.9930e-03,
        7.0567e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.759

[Epoch: 100, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.7553e-05, 9.5124e-01, 2.9462e-02, 2.6377e-07, 2.5902e-06, 1.9239e-02,
        6.8158e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.933

[Epoch: 100, batch: 160/203] total loss per batch: 0.747
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.3223e-03, 1.2021e-01, 7.6218e-05, 7.7533e-03, 7.6906e-01, 5.7758e-03,
        9.1802e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 100, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.3718, 0.0555, 0.0034, 0.0041, 0.0607, 0.5007],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 101, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.9714, 0.0038, 0.0035, 0.0040, 0.0059, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 101, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8202e-01, 1.8376e-08, 1.1623e-05, 4.2381e-07, 2.8648e-06, 3.6025e-03,
        8.1436e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.620

[Epoch: 101, batch: 120/203] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.1788e-04, 9.7495e-01, 1.2767e-02, 1.3437e-07, 4.2263e-07, 1.2162e-02,
        1.3476e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.938

[Epoch: 101, batch: 160/203] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.1875e-03, 9.3763e-02, 6.7462e-05, 5.6161e-03, 7.8495e-01, 7.4118e-03,
        1.0100e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 101, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.5717, 0.0624, 0.0048, 0.0064, 0.0783, 0.2709],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 102, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9806, 0.0032, 0.0027, 0.0038, 0.0025, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 102, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7593e-01, 2.8674e-08, 9.3263e-06, 3.6311e-07, 1.9918e-06, 2.4511e-03,
        8.2161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.766

[Epoch: 102, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.8859e-05, 9.7408e-01, 1.4163e-02, 9.3973e-08, 1.3578e-06, 1.1687e-02,
        2.4289e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.924

[Epoch: 102, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.4045e-03, 9.4600e-02, 5.1149e-05, 4.3426e-03, 7.6887e-01, 6.9493e-03,
        1.1878e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 102, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.5072, 0.0565, 0.0030, 0.0051, 0.0646, 0.3599],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 103, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9793, 0.0028, 0.0036, 0.0039, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 103, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.5705e-01, 1.9631e-08, 1.0411e-05, 4.1622e-07, 3.8827e-06, 4.7920e-03,
        7.3815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.632

[Epoch: 103, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.9502e-05, 9.6506e-01, 1.6901e-02, 8.8001e-08, 5.5113e-07, 1.7951e-02,
        2.8436e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.918

[Epoch: 103, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0033e-03, 9.3082e-02, 2.7962e-05, 3.4112e-03, 7.9769e-01, 5.0213e-03,
        9.6766e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 103, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5315, 0.0494, 0.0026, 0.0029, 0.0592, 0.3513],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 104, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9808, 0.0031, 0.0033, 0.0038, 0.0025, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 104, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5542e-01, 2.5340e-08, 6.9421e-06, 6.2371e-07, 2.8637e-06, 2.3548e-03,
        8.4222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.736

[Epoch: 104, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.7491e-05, 9.6976e-01, 1.3362e-02, 8.0377e-08, 6.8751e-07, 1.6802e-02,
        1.1519e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.900

[Epoch: 104, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0523e-03, 1.1514e-01, 2.7305e-05, 3.1743e-03, 7.3151e-01, 5.9491e-03,
        1.3915e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 104, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.5295, 0.0623, 0.0029, 0.0040, 0.0558, 0.3420],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 105, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9799, 0.0028, 0.0041, 0.0041, 0.0028, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 105, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4239e-01, 3.0589e-08, 8.6022e-06, 4.4994e-07, 2.4066e-06, 3.8063e-03,
        7.5380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.677

[Epoch: 105, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.1726e-04, 9.5475e-01, 2.3097e-02, 1.3871e-07, 9.5892e-07, 2.2033e-02,
        3.1999e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 105, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0824e-03, 8.8641e-02, 3.1510e-05, 3.3517e-03, 8.0218e-01, 6.4740e-03,
        9.5238e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 105, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.4808, 0.0542, 0.0031, 0.0042, 0.0588, 0.3951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 106, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9809, 0.0031, 0.0033, 0.0028, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 106, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.6291e-01, 1.4845e-08, 7.0632e-06, 5.6193e-07, 2.2439e-06, 3.1717e-03,
        8.3391e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.700

[Epoch: 106, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.7611e-05, 9.6603e-01, 1.7825e-02, 8.6789e-08, 9.6787e-07, 1.6084e-02,
        2.9660e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 106, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.0441e-03, 1.2080e-01, 3.0221e-05, 3.4139e-03, 7.3859e-01, 7.5253e-03,
        1.2360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 106, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0049, 0.5533, 0.0765, 0.0032, 0.0048, 0.0625, 0.2948],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 107, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0047, 0.9762, 0.0031, 0.0045, 0.0036, 0.0038, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 107, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.5218e-01, 1.8674e-08, 8.9099e-06, 4.4785e-07, 2.1169e-06, 3.5080e-03,
        7.4430e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.658

[Epoch: 107, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0155e-05, 9.6986e-01, 1.4035e-02, 6.4784e-08, 4.9740e-07, 1.6054e-02,
        5.0340e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.947

[Epoch: 107, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9298e-03, 9.3798e-02, 2.8920e-05, 3.8318e-03, 7.8626e-01, 5.8363e-03,
        1.0632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 107, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0048, 0.4953, 0.0420, 0.0045, 0.0053, 0.0610, 0.3873],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 108, batch: 40/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9810, 0.0031, 0.0029, 0.0037, 0.0038, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 108, batch: 80/203] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5753e-01, 1.0421e-08, 7.0109e-06, 8.8839e-07, 1.7540e-06, 2.5801e-03,
        8.3988e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.651

[Epoch: 108, batch: 120/203] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.7085e-05, 9.8060e-01, 8.7104e-03, 2.2260e-08, 1.4375e-07, 1.0660e-02,
        6.7375e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.879

[Epoch: 108, batch: 160/203] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0912e-03, 1.1054e-01, 1.8482e-05, 3.0401e-03, 7.6311e-01, 4.9448e-03,
        1.1425e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 108, batch: 200/203] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.5199, 0.0591, 0.0024, 0.0038, 0.0547, 0.3563],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 109, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9767, 0.0023, 0.0039, 0.0034, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 109, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4474e-01, 4.8127e-08, 8.6478e-06, 4.0123e-07, 4.9836e-06, 3.1146e-03,
        7.5213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.742

[Epoch: 109, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.0767e-04, 9.4976e-01, 2.5093e-02, 7.4677e-08, 1.1144e-06, 2.5035e-02,
        7.2665e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.938

[Epoch: 109, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0521e-03, 1.0292e-01, 5.0237e-05, 5.4465e-03, 7.8256e-01, 4.8510e-03,
        9.9123e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 109, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0027, 0.5190, 0.0509, 0.0027, 0.0035, 0.0517, 0.3694],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 110, batch: 40/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9799, 0.0029, 0.0041, 0.0045, 0.0036, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 110, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7596e-01, 6.4518e-08, 1.4796e-05, 8.1257e-07, 3.6442e-06, 3.0676e-03,
        8.2096e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.623

[Epoch: 110, batch: 120/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.7478e-05, 9.5928e-01, 1.6687e-02, 8.5958e-08, 9.7882e-07, 2.3991e-02,
        8.3623e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.925

[Epoch: 110, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.2764e-03, 1.0326e-01, 4.5053e-05, 6.4121e-03, 7.3347e-01, 6.4623e-03,
        1.4507e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 110, batch: 200/203] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.5376, 0.0528, 0.0040, 0.0039, 0.0534, 0.3430],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 111, batch: 40/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9810, 0.0031, 0.0036, 0.0034, 0.0035, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 111, batch: 80/203] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9376e-01, 3.7085e-08, 1.3506e-05, 6.3690e-07, 2.8582e-06, 2.6214e-03,
        8.0360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.760

[Epoch: 111, batch: 120/203] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([9.5406e-05, 9.6665e-01, 1.7780e-02, 1.1360e-07, 3.0951e-06, 1.5475e-02,
        1.0039e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.849

[Epoch: 111, batch: 160/203] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.0264e-03, 1.0905e-01, 8.0074e-05, 3.4896e-03, 7.8384e-01, 8.0761e-03,
        8.9434e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 111, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0052, 0.4519, 0.0844, 0.0032, 0.0060, 0.0725, 0.3767],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 112, batch: 40/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9764, 0.0031, 0.0045, 0.0047, 0.0041, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 112, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1461e-01, 2.5807e-08, 6.6388e-06, 7.0810e-07, 1.9806e-06, 3.7223e-03,
        7.8165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.709

[Epoch: 112, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9868e-05, 9.6808e-01, 1.4604e-02, 5.5321e-08, 1.1330e-06, 1.7285e-02,
        7.6239e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.956

[Epoch: 112, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.4873e-03, 1.0821e-01, 3.0106e-05, 4.4267e-03, 7.6749e-01, 5.6359e-03,
        1.0972e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 112, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0029, 0.6050, 0.0486, 0.0017, 0.0028, 0.0551, 0.2838],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 113, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.9754, 0.0031, 0.0048, 0.0044, 0.0036, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 113, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0360e-01, 1.9378e-08, 1.6710e-05, 1.0495e-06, 3.6766e-06, 4.0566e-03,
        7.9233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.649

[Epoch: 113, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.3099e-05, 9.7801e-01, 1.0106e-02, 5.3978e-08, 4.8759e-07, 1.1857e-02,
        2.0163e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.958

[Epoch: 113, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.3887e-03, 9.2686e-02, 3.6199e-05, 5.5662e-03, 7.3612e-01, 5.7499e-03,
        1.5545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 113, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0041, 0.4217, 0.0554, 0.0030, 0.0032, 0.0639, 0.4487],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 114, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9837, 0.0022, 0.0036, 0.0025, 0.0035, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 114, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8817e-01, 2.0217e-08, 6.3294e-06, 4.6929e-07, 2.0779e-06, 4.6519e-03,
        8.0717e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 114, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.3655e-05, 9.6591e-01, 1.9694e-02, 4.6598e-08, 6.3104e-07, 1.4344e-02,
        4.0400e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.932

[Epoch: 114, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2287e-03, 1.0315e-01, 1.9026e-05, 4.2453e-03, 8.1705e-01, 5.5660e-03,
        6.5742e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 114, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.5719, 0.0714, 0.0025, 0.0038, 0.0524, 0.2940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 115, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9806, 0.0027, 0.0041, 0.0031, 0.0031, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 115, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0650e-01, 2.7071e-08, 1.3714e-05, 7.2659e-07, 2.4160e-06, 2.9750e-03,
        7.9051e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.672

[Epoch: 115, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.2290e-05, 9.6581e-01, 1.6556e-02, 6.3835e-08, 9.0646e-07, 1.7604e-02,
        1.1561e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.917

[Epoch: 115, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.2121e-03, 9.6398e-02, 4.8773e-05, 4.5756e-03, 7.0715e-01, 7.0775e-03,
        1.7954e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 115, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0052, 0.4529, 0.0500, 0.0027, 0.0044, 0.0775, 0.4071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 116, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9772, 0.0033, 0.0051, 0.0039, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 116, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0717e-01, 1.3808e-08, 6.8967e-06, 5.8133e-07, 1.1452e-06, 4.9965e-03,
        7.8783e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.714

[Epoch: 116, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.3299e-05, 9.6265e-01, 2.1157e-02, 1.4672e-07, 5.6093e-07, 1.6156e-02,
        6.9919e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 116, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2760e-03, 1.1853e-01, 3.8872e-05, 4.0359e-03, 7.9845e-01, 6.9091e-03,
        6.7759e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 116, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0047, 0.5869, 0.0925, 0.0026, 0.0045, 0.0496, 0.2592],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 117, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9804, 0.0025, 0.0030, 0.0031, 0.0036, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 117, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1276e-01, 1.8258e-08, 8.3337e-06, 6.2302e-07, 2.3263e-06, 3.1697e-03,
        7.8406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.613

[Epoch: 117, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.7987e-05, 9.6434e-01, 1.2421e-02, 4.4411e-08, 2.8823e-07, 2.3198e-02,
        2.2752e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.918

[Epoch: 117, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.1600e-03, 9.3359e-02, 3.2709e-05, 4.3870e-03, 7.6198e-01, 5.2633e-03,
        1.2982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 117, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0032, 0.4518, 0.0328, 0.0023, 0.0031, 0.0597, 0.4471],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 118, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9832, 0.0027, 0.0028, 0.0025, 0.0028, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 118, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7731e-01, 1.8980e-08, 6.2677e-06, 4.7496e-07, 1.4560e-06, 3.8259e-03,
        8.1886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.732

[Epoch: 118, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8205e-05, 9.6801e-01, 1.7129e-02, 8.8161e-08, 5.1652e-07, 1.4837e-02,
        5.4427e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.928

[Epoch: 118, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8267e-03, 1.2243e-01, 3.3744e-05, 4.3605e-03, 7.7317e-01, 4.7650e-03,
        9.0419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 118, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.6208, 0.0618, 0.0034, 0.0044, 0.0585, 0.2475],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 119, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9772, 0.0033, 0.0039, 0.0046, 0.0030, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 119, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4865e-01, 2.3214e-08, 1.1891e-05, 5.2930e-07, 2.8157e-06, 4.1247e-03,
        7.4721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.685

[Epoch: 119, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([9.6717e-05, 9.5789e-01, 2.3289e-02, 1.2800e-07, 1.1343e-06, 1.8722e-02,
        4.1844e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.922

[Epoch: 119, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.5044e-03, 8.3603e-02, 3.0946e-05, 3.5343e-03, 7.8157e-01, 5.1097e-03,
        1.2165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 119, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.4594, 0.0466, 0.0039, 0.0038, 0.0611, 0.4211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 120, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9743, 0.0028, 0.0060, 0.0037, 0.0043, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 120, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.4187e-01, 1.6928e-08, 2.8831e-06, 6.0530e-07, 1.2491e-06, 3.9153e-03,
        8.5420e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.728

[Epoch: 120, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.2055e-04, 9.6952e-01, 1.3234e-02, 1.5603e-07, 9.6751e-07, 1.7127e-02,
        1.2636e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.943

[Epoch: 120, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([7.0045e-03, 1.2114e-01, 5.2138e-05, 4.7738e-03, 6.9948e-01, 6.9374e-03,
        1.6061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 120, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.5599, 0.0792, 0.0031, 0.0035, 0.0511, 0.2997],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.026

[Epoch: 121, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9783, 0.0027, 0.0025, 0.0047, 0.0030, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 121, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([3.2725e-01, 1.1072e-08, 9.1702e-06, 4.8633e-07, 1.8581e-06, 4.4440e-03,
        6.6829e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.679

[Epoch: 121, batch: 120/203] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.2368e-05, 9.6026e-01, 2.3196e-02, 5.4163e-08, 1.1096e-06, 1.6460e-02,
        8.7913e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.838

[Epoch: 121, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9255e-03, 8.4252e-02, 2.4755e-05, 2.9505e-03, 8.6002e-01, 6.4358e-03,
        4.2389e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 121, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0065, 0.4758, 0.0440, 0.0022, 0.0056, 0.0583, 0.4077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 122, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9784, 0.0032, 0.0053, 0.0028, 0.0032, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 122, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([9.9663e-02, 2.0402e-08, 2.5162e-06, 5.0307e-07, 3.9792e-06, 2.9817e-03,
        8.9735e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.655

[Epoch: 122, batch: 120/203] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.4242e-05, 9.7347e-01, 1.3246e-02, 8.3939e-08, 5.4975e-07, 1.3235e-02,
        5.9511e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.970

[Epoch: 122, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.2709e-03, 1.3736e-01, 3.3499e-05, 3.0784e-03, 6.3945e-01, 7.0334e-03,
        2.0778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.050

[Epoch: 122, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.5215, 0.0652, 0.0040, 0.0043, 0.0703, 0.3292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 123, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9806, 0.0029, 0.0031, 0.0032, 0.0026, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 123, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3133e-01, 2.1741e-08, 1.0808e-05, 6.9749e-07, 2.5648e-06, 4.3908e-03,
        7.6426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.683

[Epoch: 123, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.7063e-05, 9.7295e-01, 8.3885e-03, 3.5944e-08, 3.1094e-07, 1.8641e-02,
        1.6934e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.964

[Epoch: 123, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.6161e-03, 7.0540e-02, 2.7362e-05, 3.0708e-03, 8.7182e-01, 5.5066e-03,
        4.6419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 123, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.5128, 0.0552, 0.0023, 0.0041, 0.0778, 0.3434],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 124, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9820, 0.0025, 0.0042, 0.0029, 0.0026, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 124, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8327e-01, 1.6044e-08, 6.0584e-06, 3.6450e-07, 2.4199e-06, 5.2431e-03,
        8.1147e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.691

[Epoch: 124, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.3316e-05, 9.4978e-01, 3.4189e-02, 1.2999e-07, 1.2134e-06, 1.5967e-02,
        7.8518e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.964

[Epoch: 124, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.5431e-03, 1.1928e-01, 4.5726e-05, 6.0834e-03, 7.1786e-01, 8.7855e-03,
        1.4141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 124, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.5183, 0.0717, 0.0021, 0.0031, 0.0312, 0.3695],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 125, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.9744, 0.0042, 0.0032, 0.0041, 0.0042, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 125, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3049e-01, 7.2482e-08, 7.0478e-06, 4.3226e-07, 2.7850e-06, 3.4531e-03,
        7.6605e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.722

[Epoch: 125, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.2828e-05, 9.7454e-01, 9.7549e-03, 6.7845e-08, 5.5376e-07, 1.5650e-02,
        2.1719e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.963

[Epoch: 125, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.9753e-03, 9.9108e-02, 4.4769e-05, 4.7082e-03, 8.0331e-01, 7.3095e-03,
        8.0544e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 125, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0042, 0.5096, 0.0496, 0.0042, 0.0047, 0.0676, 0.3601],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 126, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9829, 0.0020, 0.0034, 0.0025, 0.0037, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 126, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0329e-01, 2.7972e-08, 1.5234e-05, 3.1207e-07, 2.5199e-06, 4.1274e-03,
        7.9256e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.639

[Epoch: 126, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.6966e-05, 9.6608e-01, 1.6095e-02, 9.4804e-08, 4.9379e-07, 1.7789e-02,
        4.6798e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.960

[Epoch: 126, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.7145e-03, 9.6093e-02, 4.2091e-05, 3.6785e-03, 7.2924e-01, 7.9752e-03,
        1.5726e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 126, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0054, 0.5070, 0.0690, 0.0026, 0.0040, 0.0683, 0.3436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 127, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9766, 0.0036, 0.0042, 0.0033, 0.0053, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 127, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7478e-01, 2.8907e-08, 7.1948e-06, 4.1647e-07, 2.8902e-06, 3.3805e-03,
        8.2183e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.653

[Epoch: 127, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.9504e-05, 9.6453e-01, 1.7208e-02, 5.0815e-08, 3.8946e-07, 1.8187e-02,
        4.3209e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.910

[Epoch: 127, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.5964e-03, 1.1830e-01, 3.2644e-05, 3.8762e-03, 7.9269e-01, 7.6180e-03,
        7.3886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 127, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.5602, 0.0452, 0.0028, 0.0048, 0.0612, 0.3218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 128, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9841, 0.0019, 0.0030, 0.0029, 0.0026, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 128, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0987e-01, 2.8273e-08, 9.9657e-06, 5.6558e-07, 2.8740e-06, 4.2281e-03,
        7.8589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.697

[Epoch: 128, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.0892e-05, 9.7634e-01, 1.0965e-02, 7.0049e-08, 3.3540e-07, 1.2655e-02,
        2.7434e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.963

[Epoch: 128, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9602e-03, 8.0850e-02, 2.6001e-05, 4.2769e-03, 7.7516e-01, 6.9307e-03,
        1.2879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 128, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.4911, 0.0551, 0.0025, 0.0033, 0.0484, 0.3963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 129, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.9826, 0.0028, 0.0034, 0.0030, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 129, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1265e-01, 6.5450e-08, 1.3111e-05, 4.4212e-07, 3.1634e-06, 3.4494e-03,
        7.8389e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.644

[Epoch: 129, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.5556e-05, 9.5632e-01, 2.2486e-02, 9.7994e-08, 7.8477e-07, 2.1122e-02,
        6.7742e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 129, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0868e-03, 1.4111e-01, 4.8446e-05, 4.1029e-03, 7.1414e-01, 6.7664e-03,
        1.2874e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 129, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.5652, 0.0643, 0.0040, 0.0038, 0.0639, 0.2942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 130, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9832, 0.0018, 0.0036, 0.0025, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 130, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9533e-01, 2.5899e-08, 9.6727e-06, 7.8483e-07, 1.5706e-06, 4.4804e-03,
        8.0018e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.745

[Epoch: 130, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.4344e-05, 9.6954e-01, 1.4928e-02, 6.2055e-08, 3.3769e-07, 1.5474e-02,
        2.3984e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.928

[Epoch: 130, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0126e-03, 6.8377e-02, 2.9337e-05, 5.5867e-03, 8.2636e-01, 7.2904e-03,
        8.7346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 130, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0042, 0.4634, 0.0592, 0.0027, 0.0042, 0.0610, 0.4052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 131, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.9839, 0.0029, 0.0034, 0.0027, 0.0026, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 131, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1186e-01, 2.6364e-08, 4.2181e-06, 2.2782e-07, 2.5829e-06, 3.2615e-03,
        7.8487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.653

[Epoch: 131, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.6832e-05, 9.5766e-01, 2.0691e-02, 9.1589e-08, 6.2463e-07, 2.1581e-02,
        2.2959e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 131, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8936e-03, 1.1839e-01, 5.4547e-05, 5.3120e-03, 7.3427e-01, 7.5353e-03,
        1.2955e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 131, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0047, 0.5743, 0.0631, 0.0028, 0.0047, 0.0473, 0.3031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.026

[Epoch: 132, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9748, 0.0045, 0.0040, 0.0031, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 132, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8603e-01, 1.3287e-08, 5.0308e-06, 5.7124e-07, 1.7663e-06, 3.2898e-03,
        8.1067e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.695

[Epoch: 132, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.4149e-05, 9.7215e-01, 1.3904e-02, 7.5559e-08, 5.4769e-07, 1.3896e-02,
        4.6153e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.933

[Epoch: 132, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.6575e-03, 1.0283e-01, 3.0102e-05, 3.3107e-03, 7.8512e-01, 5.8767e-03,
        9.9177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 132, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.4271, 0.0665, 0.0059, 0.0042, 0.0804, 0.4120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 133, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9789, 0.0036, 0.0030, 0.0045, 0.0026, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 133, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3836e-01, 1.8932e-08, 4.3135e-06, 3.2844e-07, 2.7803e-06, 4.7831e-03,
        7.5685e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.663

[Epoch: 133, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.4421e-05, 9.6766e-01, 1.3518e-02, 6.0931e-08, 3.3523e-07, 1.8784e-02,
        1.7618e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.944

[Epoch: 133, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.9551e-03, 1.0726e-01, 4.6609e-05, 2.9449e-03, 7.1503e-01, 6.0848e-03,
        1.6368e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 133, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5953, 0.0464, 0.0022, 0.0034, 0.0449, 0.3050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 134, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9815, 0.0026, 0.0042, 0.0040, 0.0024, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 134, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5948e-01, 3.3211e-08, 4.7362e-06, 5.9574e-07, 1.9128e-06, 3.1935e-03,
        8.3732e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.694

[Epoch: 134, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.9238e-05, 9.7152e-01, 1.3977e-02, 7.6640e-08, 7.1274e-07, 1.4443e-02,
        3.2901e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.941

[Epoch: 134, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.7363e-03, 1.1283e-01, 3.5737e-05, 4.2262e-03, 8.0823e-01, 6.0532e-03,
        6.3887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 134, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.4508, 0.0585, 0.0038, 0.0046, 0.0629, 0.4150],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 135, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9752, 0.0038, 0.0039, 0.0045, 0.0041, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 135, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2939e-01, 2.8915e-08, 1.0251e-05, 8.1946e-07, 3.5313e-06, 5.3931e-03,
        7.6521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.674

[Epoch: 135, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([9.9237e-05, 9.6446e-01, 1.7973e-02, 8.8797e-08, 1.2464e-06, 1.7469e-02,
        7.5679e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.911

[Epoch: 135, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9597e-03, 9.1318e-02, 5.8379e-05, 3.1756e-03, 7.4587e-01, 6.2851e-03,
        1.4933e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 135, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0047, 0.5282, 0.0739, 0.0036, 0.0044, 0.0632, 0.3220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 136, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9780, 0.0030, 0.0055, 0.0036, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 136, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9372e-01, 4.9686e-08, 7.5296e-06, 6.1843e-07, 1.7934e-06, 3.8113e-03,
        8.0246e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.714

[Epoch: 136, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.2951e-05, 9.6166e-01, 2.1732e-02, 1.0092e-07, 1.0497e-06, 1.6539e-02,
        1.5488e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.869

[Epoch: 136, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0504e-03, 1.1147e-01, 4.2531e-05, 3.7306e-03, 7.7855e-01, 6.7784e-03,
        9.4375e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 136, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0026, 0.5517, 0.0491, 0.0025, 0.0036, 0.0611, 0.3292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 137, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9790, 0.0032, 0.0033, 0.0032, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.027

[Epoch: 137, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8992e-01, 1.6866e-08, 5.3857e-06, 6.0983e-07, 3.1914e-06, 3.6840e-03,
        8.0639e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.677

[Epoch: 137, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.7570e-05, 9.7813e-01, 8.2846e-03, 3.2162e-08, 5.4911e-07, 1.3564e-02,
        4.3733e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.952

[Epoch: 137, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7971e-03, 1.0912e-01, 4.0258e-05, 4.2307e-03, 7.5906e-01, 5.4941e-03,
        1.1826e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 137, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0041, 0.4734, 0.0628, 0.0034, 0.0047, 0.0592, 0.3925],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 138, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9782, 0.0031, 0.0048, 0.0046, 0.0027, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 138, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3136e-01, 3.3846e-08, 5.4390e-06, 2.7621e-07, 3.9022e-06, 4.3296e-03,
        7.6430e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.686

[Epoch: 138, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.7109e-05, 9.6167e-01, 1.9577e-02, 5.2390e-08, 6.5580e-07, 1.8709e-02,
        6.3760e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.930

[Epoch: 138, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.8669e-03, 9.7228e-02, 3.6219e-05, 2.4474e-03, 7.8776e-01, 6.3000e-03,
        1.0337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 138, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0033, 0.5537, 0.0692, 0.0029, 0.0045, 0.0609, 0.3055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 139, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9797, 0.0036, 0.0035, 0.0040, 0.0027, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 139, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7292e-01, 1.3228e-08, 5.4093e-06, 8.9014e-07, 3.3004e-06, 4.7148e-03,
        8.2235e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.610

[Epoch: 139, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3537e-05, 9.7190e-01, 1.3211e-02, 6.0721e-08, 7.5544e-07, 1.4845e-02,
        1.5007e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.915

[Epoch: 139, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0896e-03, 1.1227e-01, 2.6561e-05, 3.2638e-03, 7.6912e-01, 6.8460e-03,
        1.0438e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 139, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.5331, 0.0475, 0.0029, 0.0035, 0.0511, 0.3584],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 140, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.9737, 0.0027, 0.0060, 0.0049, 0.0031, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 140, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.5197e-01, 4.2004e-08, 1.7523e-06, 3.8350e-07, 3.0017e-06, 3.2714e-03,
        7.4475e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.754

[Epoch: 140, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.5951e-05, 9.6114e-01, 1.9462e-02, 6.0441e-08, 3.3891e-07, 1.9356e-02,
        4.3108e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.940

[Epoch: 140, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([6.7117e-03, 1.1634e-01, 5.0127e-05, 6.7521e-03, 7.1225e-01, 7.4247e-03,
        1.5048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 140, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0032, 0.4960, 0.0633, 0.0040, 0.0042, 0.0678, 0.3615],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 141, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9813, 0.0035, 0.0033, 0.0026, 0.0025, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 141, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5760e-01, 9.5827e-09, 1.1486e-05, 8.4416e-07, 1.3451e-06, 3.8803e-03,
        8.3851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.687

[Epoch: 141, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.7336e-05, 9.5643e-01, 1.6942e-02, 1.1273e-07, 8.4108e-07, 2.6547e-02,
        4.3040e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.915

[Epoch: 141, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.8374e-03, 7.8592e-02, 3.1035e-05, 3.7205e-03, 8.3226e-01, 5.6060e-03,
        7.6949e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.046

[Epoch: 141, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0060, 0.4510, 0.0523, 0.0034, 0.0050, 0.0687, 0.4136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 142, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.9752, 0.0033, 0.0044, 0.0038, 0.0053, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 142, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2925e-01, 3.9805e-08, 4.6746e-06, 2.7765e-07, 2.2808e-06, 2.4736e-03,
        7.6827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.711

[Epoch: 142, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.8992e-05, 9.7069e-01, 1.8350e-02, 1.3106e-07, 2.0525e-06, 1.0872e-02,
        3.2807e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.949

[Epoch: 142, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.3658e-03, 1.3073e-01, 4.8752e-05, 4.1970e-03, 6.8533e-01, 6.7062e-03,
        1.6762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 142, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0018, 0.6172, 0.0739, 0.0023, 0.0025, 0.0515, 0.2507],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 143, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.9729, 0.0039, 0.0053, 0.0038, 0.0041, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 143, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7979e-01, 1.0985e-08, 9.8938e-06, 9.2362e-07, 2.7603e-06, 4.2833e-03,
        8.1592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.630

[Epoch: 143, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6856e-05, 9.6327e-01, 2.1073e-02, 9.5214e-08, 1.1225e-06, 1.5607e-02,
        1.4166e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.908

[Epoch: 143, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0082e-03, 9.1444e-02, 3.4722e-05, 5.0068e-03, 8.2398e-01, 7.1651e-03,
        6.8366e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 143, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0049, 0.4233, 0.0498, 0.0033, 0.0050, 0.0611, 0.4526],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 144, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.9846, 0.0016, 0.0027, 0.0028, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 144, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0493e-01, 3.7842e-08, 5.8978e-06, 5.9079e-07, 1.5581e-06, 3.0542e-03,
        7.9200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.722

[Epoch: 144, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([8.0402e-05, 9.6993e-01, 1.2313e-02, 4.7639e-08, 4.1532e-07, 1.7675e-02,
        2.4262e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.934

[Epoch: 144, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.5904e-03, 1.2017e-01, 3.3583e-05, 5.5544e-03, 7.4406e-01, 7.1375e-03,
        1.1845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 144, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5908, 0.0575, 0.0021, 0.0030, 0.0529, 0.2909],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 145, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9754, 0.0044, 0.0046, 0.0040, 0.0039, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 145, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2798e-01, 2.9063e-08, 9.6163e-06, 8.3125e-07, 2.6949e-06, 3.7210e-03,
        7.6829e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.679

[Epoch: 145, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.6314e-05, 9.5932e-01, 2.2036e-02, 1.5393e-07, 1.5839e-06, 1.8568e-02,
        7.4600e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.904

[Epoch: 145, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.5171e-03, 8.3225e-02, 2.9365e-05, 2.6218e-03, 7.8357e-01, 6.8917e-03,
        1.2014e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 145, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.4510, 0.0736, 0.0034, 0.0037, 0.0530, 0.4115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 146, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9840, 0.0016, 0.0031, 0.0030, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 146, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9643e-01, 5.8329e-08, 9.1311e-06, 1.2381e-06, 3.3170e-06, 4.8446e-03,
        7.9871e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.694

[Epoch: 146, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.2925e-05, 9.6135e-01, 1.3766e-02, 6.7499e-08, 1.0544e-06, 2.4819e-02,
        2.8721e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.900

[Epoch: 146, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.1414e-03, 1.1321e-01, 3.8796e-05, 4.3774e-03, 7.4217e-01, 6.3625e-03,
        1.2870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 146, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0025, 0.5750, 0.0531, 0.0027, 0.0035, 0.0568, 0.3064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 147, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9800, 0.0029, 0.0048, 0.0034, 0.0026, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 147, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9599e-01, 3.5188e-08, 6.0317e-06, 5.8343e-07, 2.8404e-06, 3.9661e-03,
        8.0004e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.698

[Epoch: 147, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.4759e-05, 9.6314e-01, 2.0747e-02, 6.4827e-08, 1.0891e-06, 1.6053e-02,
        4.8785e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.925

[Epoch: 147, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0716e-03, 9.3665e-02, 2.5769e-05, 3.7054e-03, 7.9290e-01, 6.6803e-03,
        9.8955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 147, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0062, 0.4316, 0.0638, 0.0035, 0.0056, 0.0751, 0.4143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 148, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9706, 0.0050, 0.0054, 0.0040, 0.0049, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 148, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0370e-01, 2.0229e-08, 8.3429e-06, 1.0618e-06, 3.0941e-06, 3.4711e-03,
        7.9281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.667

[Epoch: 148, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8859e-05, 9.7928e-01, 1.1087e-02, 8.1893e-08, 8.9399e-07, 9.6061e-03,
        4.6189e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.934

[Epoch: 148, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.4745e-03, 1.0802e-01, 4.2164e-05, 3.7767e-03, 7.6850e-01, 5.5919e-03,
        1.0960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 148, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.5759, 0.0583, 0.0038, 0.0047, 0.0703, 0.2833],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 149, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9773, 0.0044, 0.0028, 0.0039, 0.0039, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 149, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1974e-01, 1.2305e-08, 3.3218e-06, 5.3766e-07, 2.0885e-06, 3.8409e-03,
        7.7642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.666

[Epoch: 149, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.9380e-05, 9.6360e-01, 2.0656e-02, 3.2111e-08, 3.2922e-07, 1.5700e-02,
        9.7920e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.924

[Epoch: 149, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0730e-03, 1.1310e-01, 3.1878e-05, 3.6960e-03, 7.3817e-01, 7.3400e-03,
        1.3359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 149, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0029, 0.5080, 0.0532, 0.0020, 0.0030, 0.0478, 0.3831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 150, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.9741, 0.0040, 0.0048, 0.0037, 0.0039, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 150, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8106e-01, 4.1146e-08, 9.5090e-06, 9.9334e-07, 2.8349e-06, 2.6296e-03,
        8.1630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.711

[Epoch: 150, batch: 120/203] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.9426e-05, 9.7064e-01, 1.5339e-02, 4.6661e-08, 3.8516e-07, 1.3985e-02,
        2.6921e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 150, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7759e-03, 8.4377e-02, 3.9167e-05, 4.0591e-03, 8.1551e-01, 5.9151e-03,
        8.6321e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 150, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0042, 0.5118, 0.0641, 0.0035, 0.0037, 0.0643, 0.3484],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 151, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9809, 0.0030, 0.0036, 0.0035, 0.0037, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 151, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1279e-01, 2.2420e-08, 8.9451e-06, 1.5660e-06, 2.8521e-06, 3.8835e-03,
        7.8332e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.624

[Epoch: 151, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6214e-05, 9.5982e-01, 1.9263e-02, 1.0678e-07, 8.8084e-07, 2.0870e-02,
        2.7718e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.928

[Epoch: 151, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.6730e-03, 1.1760e-01, 3.8018e-05, 3.9480e-03, 7.1284e-01, 7.2414e-03,
        1.5365e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 151, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.5200, 0.0670, 0.0033, 0.0044, 0.0576, 0.3438],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 152, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9813, 0.0031, 0.0032, 0.0028, 0.0040, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 152, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2138e-01, 3.2800e-08, 8.7656e-06, 8.5017e-07, 2.0604e-06, 3.7920e-03,
        7.7482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.733

[Epoch: 152, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.9930e-05, 9.6577e-01, 1.8464e-02, 7.7622e-08, 7.9495e-07, 1.5703e-02,
        3.7943e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.879

[Epoch: 152, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.1446e-03, 1.1214e-01, 4.5799e-05, 4.2367e-03, 7.7453e-01, 7.3137e-03,
        9.6593e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 152, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.5140, 0.0581, 0.0028, 0.0039, 0.0503, 0.3672],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 153, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9788, 0.0035, 0.0039, 0.0032, 0.0036, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 153, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7691e-01, 1.3833e-08, 5.3948e-06, 8.4938e-07, 1.8839e-06, 3.2197e-03,
        8.1986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.640

[Epoch: 153, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.8919e-05, 9.6413e-01, 1.7712e-02, 6.1889e-08, 4.8864e-07, 1.8114e-02,
        2.7560e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.925

[Epoch: 153, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7503e-03, 9.5003e-02, 3.5766e-05, 3.5847e-03, 7.7165e-01, 6.8434e-03,
        1.1913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 153, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.5074, 0.0534, 0.0034, 0.0042, 0.0735, 0.3542],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 154, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9807, 0.0030, 0.0035, 0.0034, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 154, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1685e-01, 1.5852e-08, 5.7129e-06, 5.9885e-07, 1.6945e-06, 3.5401e-03,
        7.7961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.713

[Epoch: 154, batch: 120/203] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.7193e-05, 9.6783e-01, 1.6141e-02, 5.9948e-08, 6.3964e-07, 1.5980e-02,
        3.2733e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.924

[Epoch: 154, batch: 160/203] total loss per batch: 0.740
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4364e-03, 9.6375e-02, 2.6344e-05, 3.3950e-03, 7.8495e-01, 6.6903e-03,
        1.0512e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 154, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.5358, 0.0615, 0.0031, 0.0036, 0.0530, 0.3396],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 155, batch: 40/203] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9789, 0.0038, 0.0034, 0.0037, 0.0031, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 155, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0537e-01, 1.5936e-08, 5.3474e-06, 5.3870e-07, 1.7613e-06, 3.6742e-03,
        7.9095e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.681

[Epoch: 155, batch: 120/203] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3889e-05, 9.6242e-01, 1.9396e-02, 5.6469e-08, 7.3136e-07, 1.8141e-02,
        3.1593e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.915

[Epoch: 155, batch: 160/203] total loss per batch: 0.740
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.0949e-03, 1.0767e-01, 3.1354e-05, 4.1026e-03, 7.5666e-01, 7.1457e-03,
        1.2029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 155, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.5033, 0.0582, 0.0031, 0.0041, 0.0630, 0.3645],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 156, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9779, 0.0031, 0.0049, 0.0038, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 156, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8880e-01, 1.3755e-08, 5.2834e-06, 6.3899e-07, 1.3053e-06, 3.5766e-03,
        8.0762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.697

[Epoch: 156, batch: 120/203] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([6.4917e-05, 9.6961e-01, 1.5062e-02, 5.7607e-08, 6.1836e-07, 1.5259e-02,
        3.1091e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.913

[Epoch: 156, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7740e-03, 1.0357e-01, 2.8504e-05, 3.5548e-03, 7.6883e-01, 7.6258e-03,
        1.1262e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 156, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.5212, 0.0653, 0.0035, 0.0039, 0.0602, 0.3424],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 157, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.9759, 0.0039, 0.0035, 0.0043, 0.0038, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 157, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2604e-01, 1.3148e-08, 3.8564e-06, 4.4277e-07, 1.4645e-06, 3.5363e-03,
        7.7042e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.669

[Epoch: 157, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.7250e-05, 9.6788e-01, 1.8159e-02, 4.5238e-08, 4.9492e-07, 1.3934e-02,
        3.5177e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.903

[Epoch: 157, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.0846e-03, 9.0282e-02, 2.1778e-05, 2.1694e-03, 7.8594e-01, 5.4788e-03,
        1.1303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 157, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0046, 0.5210, 0.0527, 0.0030, 0.0040, 0.0635, 0.3512],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 158, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9793, 0.0028, 0.0033, 0.0043, 0.0031, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 158, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8184e-01, 7.5771e-09, 4.9981e-06, 6.6898e-07, 1.5977e-06, 3.9920e-03,
        8.1416e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.672

[Epoch: 158, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.2807e-05, 9.7078e-01, 1.2850e-02, 2.4648e-08, 2.0492e-07, 1.6352e-02,
        1.4094e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.928

[Epoch: 158, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.6976e-03, 1.1594e-01, 2.3110e-05, 3.5365e-03, 7.5541e-01, 7.6563e-03,
        1.1374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 158, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5158, 0.0609, 0.0024, 0.0035, 0.0520, 0.3623],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 159, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9780, 0.0033, 0.0037, 0.0036, 0.0033, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 159, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1335e-01, 7.7375e-09, 3.5537e-06, 3.2058e-07, 7.2245e-07, 3.9879e-03,
        7.8266e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 159, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.6806e-05, 9.6138e-01, 1.8171e-02, 2.9520e-08, 3.0461e-07, 2.0415e-02,
        2.3846e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.937

[Epoch: 159, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.9935e-03, 1.0343e-01, 2.1474e-05, 3.0359e-03, 7.7854e-01, 5.3986e-03,
        1.0658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 159, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0049, 0.5323, 0.0601, 0.0029, 0.0039, 0.0447, 0.3513],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 160, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9735, 0.0067, 0.0049, 0.0036, 0.0029, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 160, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9924e-01, 2.0752e-08, 7.6259e-06, 7.2019e-07, 2.6304e-06, 3.9967e-03,
        7.9675e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.649

[Epoch: 160, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.6034e-05, 9.7220e-01, 1.5014e-02, 3.5530e-08, 4.8163e-07, 1.2736e-02,
        7.0679e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.918

[Epoch: 160, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.4104e-03, 1.0667e-01, 3.3023e-05, 5.4538e-03, 7.3885e-01, 7.4436e-03,
        1.3713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 160, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5048, 0.0606, 0.0044, 0.0046, 0.0680, 0.3548],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 161, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9841, 0.0019, 0.0034, 0.0028, 0.0025, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 161, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9251e-01, 2.1722e-08, 4.1031e-06, 5.3581e-07, 1.2146e-06, 2.9691e-03,
        8.0451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.758

[Epoch: 161, batch: 120/203] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.7482e-05, 9.5836e-01, 1.8376e-02, 5.4189e-08, 5.8552e-07, 2.3233e-02,
        5.3165e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.934

[Epoch: 161, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.2431e-03, 1.0144e-01, 4.5831e-05, 5.8765e-03, 7.7782e-01, 9.2992e-03,
        1.0027e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 161, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.5050, 0.0658, 0.0029, 0.0046, 0.0608, 0.3568],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 162, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9774, 0.0035, 0.0038, 0.0041, 0.0043, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 162, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2531e-01, 2.5878e-08, 2.0519e-06, 5.5049e-07, 1.8726e-06, 3.5995e-03,
        7.7109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.673

[Epoch: 162, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9006e-05, 9.6221e-01, 2.6071e-02, 5.8344e-08, 6.4681e-07, 1.1692e-02,
        3.4341e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.888

[Epoch: 162, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9210e-03, 9.2077e-02, 2.7999e-05, 4.2346e-03, 7.7991e-01, 6.3984e-03,
        1.1343e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.042

[Epoch: 162, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0040, 0.5171, 0.0565, 0.0041, 0.0044, 0.0797, 0.3342],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 163, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.9806, 0.0038, 0.0028, 0.0031, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 163, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.7697e-01, 1.3296e-08, 2.5491e-06, 5.1185e-07, 1.1492e-06, 3.4678e-03,
        8.1956e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.654

[Epoch: 163, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.1120e-05, 9.7259e-01, 1.0178e-02, 8.5626e-08, 6.6143e-07, 1.7190e-02,
        4.1876e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.943

[Epoch: 163, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4184e-03, 9.6614e-02, 2.5254e-05, 4.2412e-03, 7.9311e-01, 7.3201e-03,
        9.5270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 163, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5551, 0.0541, 0.0018, 0.0029, 0.0391, 0.3441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 164, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.9784, 0.0023, 0.0039, 0.0048, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 164, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4369e-01, 5.9116e-08, 5.2296e-06, 7.7937e-07, 2.5119e-06, 5.4386e-03,
        7.5087e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.663

[Epoch: 164, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.6777e-05, 9.6100e-01, 2.2169e-02, 5.2942e-08, 4.2033e-07, 1.6791e-02,
        3.4355e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.904

[Epoch: 164, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.7172e-03, 1.0889e-01, 2.6929e-05, 4.1853e-03, 7.3445e-01, 7.0317e-03,
        1.4171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.042

[Epoch: 164, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0032, 0.4781, 0.0673, 0.0040, 0.0041, 0.0703, 0.3730],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 165, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9813, 0.0039, 0.0030, 0.0027, 0.0027, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 165, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.6578e-01, 2.4848e-08, 2.3713e-06, 4.9545e-07, 1.3272e-06, 2.4129e-03,
        8.3180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.729

[Epoch: 165, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.8897e-05, 9.6767e-01, 1.5290e-02, 4.8460e-08, 6.3767e-07, 1.6995e-02,
        2.5080e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.908

[Epoch: 165, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.6841e-03, 9.6110e-02, 2.8767e-05, 4.5355e-03, 7.9182e-01, 7.2055e-03,
        9.6621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 165, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0037, 0.5376, 0.0636, 0.0024, 0.0041, 0.0597, 0.3289],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 166, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.9798, 0.0027, 0.0047, 0.0033, 0.0035, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 166, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2434e-01, 3.3714e-08, 3.7042e-06, 7.4768e-07, 1.6175e-06, 4.2828e-03,
        7.7137e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.665

[Epoch: 166, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.6073e-05, 9.6717e-01, 1.4986e-02, 4.6752e-08, 4.5796e-07, 1.7813e-02,
        4.1780e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.910

[Epoch: 166, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.3990e-03, 1.0664e-01, 2.5018e-05, 2.7377e-03, 7.4473e-01, 7.6944e-03,
        1.3478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 166, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0033, 0.5110, 0.0516, 0.0027, 0.0043, 0.0692, 0.3579],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 167, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9787, 0.0034, 0.0034, 0.0041, 0.0032, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 167, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9614e-01, 8.9776e-09, 1.7183e-06, 5.3578e-07, 1.3308e-06, 3.2717e-03,
        8.0058e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.668

[Epoch: 167, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.1876e-05, 9.6929e-01, 1.5546e-02, 3.6427e-08, 3.5259e-07, 1.5142e-02,
        1.6614e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 167, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.1055e-03, 1.0302e-01, 3.0635e-05, 3.7655e-03, 7.9756e-01, 6.0149e-03,
        8.6501e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 167, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0043, 0.5300, 0.0650, 0.0035, 0.0036, 0.0563, 0.3373],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 168, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.9789, 0.0034, 0.0032, 0.0031, 0.0032, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 168, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9142e-01, 1.3483e-08, 4.0620e-06, 4.4318e-07, 9.8625e-07, 3.2627e-03,
        8.0531e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.679

[Epoch: 168, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9470e-05, 9.6932e-01, 1.3952e-02, 2.9767e-08, 3.5226e-07, 1.6697e-02,
        2.2646e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.908

[Epoch: 168, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.6136e-03, 1.0597e-01, 3.1858e-05, 3.8951e-03, 7.5327e-01, 6.8512e-03,
        1.2636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 168, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0039, 0.4885, 0.0617, 0.0033, 0.0045, 0.0634, 0.3748],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 169, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9841, 0.0024, 0.0033, 0.0025, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 169, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2022e-01, 2.1865e-08, 3.8656e-06, 4.2920e-07, 1.7445e-06, 3.0701e-03,
        7.7670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.698

[Epoch: 169, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0970e-05, 9.5500e-01, 2.2541e-02, 5.8680e-08, 5.0337e-07, 2.2407e-02,
        2.5739e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.935

[Epoch: 169, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8147e-03, 1.0662e-01, 4.1705e-05, 4.9836e-03, 7.6485e-01, 8.0731e-03,
        1.1062e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 169, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0041, 0.5306, 0.0732, 0.0027, 0.0032, 0.0531, 0.3330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 170, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9819, 0.0032, 0.0033, 0.0027, 0.0032, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 170, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9292e-01, 3.3351e-08, 4.0521e-06, 6.0887e-07, 1.1874e-06, 3.0341e-03,
        8.0404e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.675

[Epoch: 170, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.8125e-05, 9.7558e-01, 1.2901e-02, 7.2543e-08, 9.4446e-07, 1.1478e-02,
        2.0610e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.912

[Epoch: 170, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.8652e-03, 1.0279e-01, 3.9090e-05, 4.6546e-03, 7.5446e-01, 7.2705e-03,
        1.2692e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 170, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.4852, 0.0494, 0.0043, 0.0048, 0.0673, 0.3852],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 171, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9820, 0.0024, 0.0037, 0.0032, 0.0032, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 171, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1897e-01, 3.1395e-08, 3.8241e-06, 5.9509e-07, 2.1945e-06, 2.9152e-03,
        7.7811e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.667

[Epoch: 171, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.0684e-05, 9.5957e-01, 1.8441e-02, 1.0189e-07, 6.6515e-07, 2.1953e-02,
        3.5284e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 171, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.3514e-03, 1.0406e-01, 3.2353e-05, 2.6593e-03, 7.9650e-01, 6.1640e-03,
        8.7239e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 171, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0030, 0.5460, 0.0500, 0.0022, 0.0027, 0.0556, 0.3405],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 172, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.9780, 0.0039, 0.0034, 0.0034, 0.0036, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 172, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8577e-01, 2.8864e-08, 4.3083e-06, 5.6796e-07, 1.3607e-06, 2.6841e-03,
        8.1154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.693

[Epoch: 172, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.4204e-05, 9.6815e-01, 1.6115e-02, 3.8761e-08, 5.3637e-07, 1.5691e-02,
        2.3955e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.909

[Epoch: 172, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.6423e-03, 8.7336e-02, 2.3387e-05, 3.4135e-03, 7.5810e-01, 6.6803e-03,
        1.4180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 172, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5090, 0.0758, 0.0026, 0.0034, 0.0632, 0.3429],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 173, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.9858, 0.0024, 0.0031, 0.0017, 0.0025, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 173, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1987e-01, 5.4583e-08, 6.6467e-06, 1.5942e-06, 2.0785e-06, 3.6991e-03,
        7.7642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.677

[Epoch: 173, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.7927e-05, 9.6538e-01, 1.5664e-02, 7.6739e-08, 1.0659e-06, 1.8921e-02,
        4.1895e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 173, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4840e-03, 1.0788e-01, 3.7150e-05, 3.9539e-03, 7.8913e-01, 6.4809e-03,
        8.9039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 173, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0028, 0.5330, 0.0529, 0.0030, 0.0035, 0.0498, 0.3550],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 174, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.9780, 0.0033, 0.0043, 0.0036, 0.0038, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 174, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.0010e-01, 3.3756e-08, 5.6829e-06, 3.8071e-07, 2.1082e-06, 3.5773e-03,
        7.9632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.680

[Epoch: 174, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.0574e-05, 9.6331e-01, 2.1578e-02, 5.1578e-08, 5.8379e-07, 1.5066e-02,
        2.0489e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.912

[Epoch: 174, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.8062e-03, 1.2198e-01, 3.9376e-05, 3.2224e-03, 7.1872e-01, 7.8175e-03,
        1.4442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 174, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0048, 0.4984, 0.0618, 0.0036, 0.0054, 0.0782, 0.3477],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 175, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.9757, 0.0040, 0.0044, 0.0039, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 175, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9516e-01, 2.2452e-08, 2.7474e-06, 7.0477e-07, 1.6486e-06, 2.9192e-03,
        8.0191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.651

[Epoch: 175, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.3367e-05, 9.8007e-01, 1.0277e-02, 2.8680e-08, 3.7985e-07, 9.6361e-03,
        1.1773e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.926

[Epoch: 175, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.1916e-03, 9.2051e-02, 2.9163e-05, 3.6790e-03, 8.0546e-01, 7.0127e-03,
        8.7575e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 175, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.5353, 0.0660, 0.0029, 0.0034, 0.0449, 0.3441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 176, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.9769, 0.0029, 0.0043, 0.0042, 0.0040, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 176, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9924e-01, 3.8758e-08, 5.5796e-06, 8.2044e-07, 1.4988e-06, 3.1629e-03,
        7.9759e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.723

[Epoch: 176, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8666e-05, 9.5944e-01, 1.7860e-02, 6.7744e-08, 2.9121e-07, 2.2673e-02,
        2.1847e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.901

[Epoch: 176, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.5786e-03, 1.0736e-01, 3.8203e-05, 4.5754e-03, 7.5282e-01, 5.7829e-03,
        1.2585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 176, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.5081, 0.0513, 0.0030, 0.0031, 0.0594, 0.3718],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 177, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9822, 0.0030, 0.0025, 0.0034, 0.0023, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 177, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9704e-01, 1.7638e-08, 3.4810e-06, 3.4651e-07, 1.7338e-06, 2.9735e-03,
        7.9999e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.657

[Epoch: 177, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.0770e-05, 9.7081e-01, 1.3123e-02, 6.5593e-08, 5.4891e-07, 1.6034e-02,
        2.2717e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.942

[Epoch: 177, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.8358e-03, 1.0331e-01, 2.7449e-05, 4.0120e-03, 7.7851e-01, 5.9464e-03,
        1.0437e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 177, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0030, 0.5260, 0.0642, 0.0028, 0.0028, 0.0620, 0.3391],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 178, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9795, 0.0027, 0.0047, 0.0033, 0.0042, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 178, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3292e-01, 3.5615e-08, 1.0378e-05, 5.4242e-07, 1.9325e-06, 4.0774e-03,
        7.6299e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.697

[Epoch: 178, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.5036e-05, 9.6032e-01, 2.0837e-02, 5.2610e-08, 5.6245e-07, 1.8795e-02,
        3.3868e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.929

[Epoch: 178, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.4760e-03, 1.0669e-01, 4.0023e-05, 4.2708e-03, 7.5273e-01, 7.4545e-03,
        1.2434e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 178, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0036, 0.4819, 0.0649, 0.0029, 0.0031, 0.0590, 0.3846],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 179, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.9773, 0.0033, 0.0038, 0.0032, 0.0037, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 179, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5944e-01, 3.1888e-08, 3.1743e-06, 6.0382e-07, 1.5659e-06, 3.0847e-03,
        8.3747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.686

[Epoch: 179, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.4351e-05, 9.6372e-01, 2.0966e-02, 7.6427e-08, 1.0119e-06, 1.5273e-02,
        4.5054e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.853

[Epoch: 179, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.0619e-03, 1.0181e-01, 3.1528e-05, 3.1679e-03, 7.9749e-01, 7.2885e-03,
        8.7150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 179, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0030, 0.5455, 0.0583, 0.0031, 0.0032, 0.0602, 0.3267],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 180, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.9765, 0.0031, 0.0048, 0.0042, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 180, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.7842e-01, 4.1197e-08, 7.2066e-06, 4.5279e-07, 2.0344e-06, 4.2387e-03,
        7.1733e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.666

[Epoch: 180, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.5329e-05, 9.6656e-01, 1.4991e-02, 5.3848e-08, 4.2504e-07, 1.8415e-02,
        2.4477e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.967

[Epoch: 180, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4415e-03, 9.0917e-02, 3.1379e-05, 2.9635e-03, 7.4814e-01, 7.2385e-03,
        1.4727e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 180, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.5108, 0.0524, 0.0023, 0.0038, 0.0692, 0.3570],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 181, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9773, 0.0035, 0.0039, 0.0039, 0.0035, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 181, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.4145e-01, 9.8895e-09, 2.1230e-06, 4.6922e-07, 1.8689e-06, 3.2733e-03,
        8.5527e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.698

[Epoch: 181, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.8574e-05, 9.6166e-01, 2.0013e-02, 5.1811e-08, 4.9760e-07, 1.8293e-02,
        2.4814e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.949

[Epoch: 181, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4241e-03, 1.1995e-01, 2.6583e-05, 3.5431e-03, 7.7445e-01, 6.1196e-03,
        9.2483e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 181, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0041, 0.5278, 0.0666, 0.0027, 0.0035, 0.0489, 0.3464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 182, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9785, 0.0028, 0.0043, 0.0039, 0.0031, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 182, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4882e-01, 4.2805e-08, 6.2524e-06, 5.1325e-07, 2.0061e-06, 4.0404e-03,
        7.4713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.663

[Epoch: 182, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([7.9016e-05, 9.7064e-01, 1.5578e-02, 5.3193e-08, 6.0440e-07, 1.3702e-02,
        1.6250e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.943

[Epoch: 182, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.9811e-03, 8.9525e-02, 2.6948e-05, 4.7060e-03, 7.7893e-01, 6.4784e-03,
        1.1636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 182, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0032, 0.5198, 0.0535, 0.0031, 0.0034, 0.0667, 0.3502],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 183, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9797, 0.0039, 0.0032, 0.0032, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 183, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5293e-01, 1.2196e-08, 3.5976e-06, 4.1553e-07, 9.5772e-07, 2.1812e-03,
        8.4489e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.683

[Epoch: 183, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.9814e-05, 9.7009e-01, 1.6312e-02, 7.4187e-08, 6.1406e-07, 1.3565e-02,
        2.3715e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.932

[Epoch: 183, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([5.0897e-03, 1.2167e-01, 3.5136e-05, 3.3889e-03, 7.3750e-01, 7.9054e-03,
        1.2442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 183, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0048, 0.4992, 0.0649, 0.0042, 0.0045, 0.0642, 0.3583],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 184, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.9846, 0.0017, 0.0028, 0.0025, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 184, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.8834e-01, 2.9594e-08, 6.1831e-06, 3.5049e-07, 2.5846e-06, 4.1677e-03,
        7.0749e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.701

[Epoch: 184, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([1.9359e-05, 9.6459e-01, 1.5779e-02, 4.2021e-08, 3.7896e-07, 1.9614e-02,
        2.2739e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.934

[Epoch: 184, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.1476e-03, 9.3714e-02, 2.2356e-05, 3.3144e-03, 7.8648e-01, 6.5024e-03,
        1.0582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 184, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0029, 0.5442, 0.0521, 0.0015, 0.0023, 0.0563, 0.3407],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 185, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.9795, 0.0030, 0.0040, 0.0035, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 185, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.2804e-01, 2.2167e-08, 2.7693e-06, 5.0211e-07, 1.6890e-06, 1.8974e-03,
        8.7006e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.670

[Epoch: 185, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.2818e-05, 9.7251e-01, 1.2425e-02, 8.7760e-08, 6.0433e-07, 1.5007e-02,
        2.2591e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.927

[Epoch: 185, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.4588e-03, 1.1167e-01, 2.2911e-05, 3.4861e-03, 7.8266e-01, 6.2860e-03,
        9.2413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 185, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0042, 0.4901, 0.0567, 0.0028, 0.0039, 0.0674, 0.3750],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 186, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.9824, 0.0029, 0.0030, 0.0026, 0.0030, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 186, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.9377e-01, 8.9624e-08, 9.7708e-06, 1.0277e-06, 3.8582e-06, 5.5251e-03,
        7.0069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.672

[Epoch: 186, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.2116e-05, 9.6112e-01, 2.0947e-02, 5.2884e-08, 9.6621e-07, 1.7880e-02,
        2.8533e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.916

[Epoch: 186, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.6727e-03, 9.9270e-02, 2.7572e-05, 3.2621e-03, 7.3056e-01, 5.9185e-03,
        1.5629e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 186, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0038, 0.5113, 0.0690, 0.0028, 0.0034, 0.0662, 0.3434],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 187, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.9823, 0.0023, 0.0035, 0.0033, 0.0033, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 187, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5237e-01, 6.5224e-08, 6.4654e-06, 8.9411e-07, 4.4127e-06, 3.4277e-03,
        8.4420e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.684

[Epoch: 187, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.0675e-05, 9.6216e-01, 2.0656e-02, 8.0818e-08, 7.7651e-07, 1.7146e-02,
        3.5291e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.927

[Epoch: 187, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.5879e-03, 1.0558e-01, 3.6077e-05, 3.8528e-03, 7.9348e-01, 6.9254e-03,
        8.6535e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 187, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0043, 0.5691, 0.0599, 0.0031, 0.0043, 0.0296, 0.3298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 188, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.9777, 0.0029, 0.0036, 0.0041, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 188, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.1015e-01, 2.9548e-08, 4.5013e-06, 6.6068e-07, 4.6462e-06, 4.3984e-03,
        7.8544e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.686

[Epoch: 188, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3958e-05, 9.7169e-01, 1.4152e-02, 5.1312e-08, 5.1137e-07, 1.4111e-02,
        2.4689e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.874

[Epoch: 188, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.2627e-03, 9.1320e-02, 2.3396e-05, 3.2085e-03, 7.7320e-01, 6.3433e-03,
        1.2265e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 188, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0059, 0.4389, 0.0680, 0.0036, 0.0047, 0.0849, 0.3940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 189, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.9775, 0.0037, 0.0045, 0.0037, 0.0030, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 189, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9046e-01, 3.4363e-08, 3.4554e-06, 4.2079e-07, 2.5715e-06, 3.3491e-03,
        8.0618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.659

[Epoch: 189, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.9264e-05, 9.6797e-01, 1.7052e-02, 3.3481e-08, 2.5374e-07, 1.4934e-02,
        2.1862e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.948

[Epoch: 189, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([2.9157e-03, 1.1446e-01, 1.9149e-05, 2.9210e-03, 7.5596e-01, 7.1833e-03,
        1.1654e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 189, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0033, 0.5416, 0.0418, 0.0024, 0.0031, 0.0759, 0.3318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 190, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9797, 0.0032, 0.0034, 0.0036, 0.0028, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 190, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2279e-01, 2.9129e-08, 5.7747e-06, 6.8982e-07, 3.5237e-06, 3.9860e-03,
        7.7321e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.693

[Epoch: 190, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.0030e-05, 9.6905e-01, 1.4025e-02, 6.6224e-08, 2.6995e-07, 1.6897e-02,
        1.4702e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.943

[Epoch: 190, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.1889e-03, 9.9778e-02, 2.7096e-05, 3.4065e-03, 7.8236e-01, 5.9503e-03,
        1.0429e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 190, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0032, 0.5256, 0.0749, 0.0037, 0.0031, 0.0479, 0.3417],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 191, batch: 40/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.9797, 0.0030, 0.0036, 0.0039, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 191, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.9314e-01, 3.4307e-08, 5.4252e-06, 6.7023e-07, 2.8619e-06, 3.7721e-03,
        8.0307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.684

[Epoch: 191, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.1741e-05, 9.5874e-01, 2.0414e-02, 5.0271e-08, 4.2443e-07, 2.0807e-02,
        2.8053e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 191, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.6349e-03, 1.0961e-01, 2.5621e-05, 3.8582e-03, 7.5621e-01, 6.7580e-03,
        1.1990e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 191, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0045, 0.5079, 0.0523, 0.0038, 0.0036, 0.0576, 0.3703],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 192, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9774, 0.0036, 0.0045, 0.0038, 0.0033, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 192, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8202e-01, 2.2153e-08, 3.6191e-06, 4.2651e-07, 1.6388e-06, 3.1769e-03,
        8.1480e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.703

[Epoch: 192, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.5291e-05, 9.6824e-01, 1.6098e-02, 5.6212e-08, 4.5167e-07, 1.5622e-02,
        1.7877e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.930

[Epoch: 192, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.8106e-03, 9.7935e-02, 3.1771e-05, 3.0600e-03, 7.8038e-01, 6.7587e-03,
        1.0802e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 192, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0035, 0.4846, 0.0689, 0.0033, 0.0034, 0.0739, 0.3625],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 193, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9816, 0.0024, 0.0030, 0.0026, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 193, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.4529e-01, 3.1667e-08, 3.4036e-06, 6.1254e-07, 3.6575e-06, 4.7699e-03,
        7.4993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.629

[Epoch: 193, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8165e-05, 9.6452e-01, 1.6936e-02, 6.4815e-08, 4.8495e-07, 1.8513e-02,
        2.9687e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.906

[Epoch: 193, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.2852e-03, 1.1837e-01, 2.6473e-05, 4.0899e-03, 7.3286e-01, 8.3671e-03,
        1.3199e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 193, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0025, 0.5704, 0.0538, 0.0024, 0.0027, 0.0458, 0.3224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 194, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.9777, 0.0031, 0.0042, 0.0045, 0.0031, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 194, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.6885e-01, 2.1778e-08, 3.3119e-06, 5.0476e-07, 2.7210e-06, 2.6570e-03,
        8.2849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.709

[Epoch: 194, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.6027e-05, 9.6780e-01, 1.4253e-02, 5.5001e-08, 4.3122e-07, 1.7910e-02,
        2.7830e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.936

[Epoch: 194, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.2149e-03, 8.1104e-02, 2.0023e-05, 2.6922e-03, 8.1793e-01, 5.5817e-03,
        8.9453e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 194, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0044, 0.4747, 0.0544, 0.0035, 0.0037, 0.0777, 0.3817],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 195, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.9795, 0.0027, 0.0036, 0.0033, 0.0028, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 195, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.2205e-01, 3.1606e-08, 6.1803e-06, 4.9564e-07, 1.9155e-06, 3.2789e-03,
        7.7466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.710

[Epoch: 195, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([5.0172e-05, 9.7135e-01, 1.5288e-02, 6.4280e-08, 7.6094e-07, 1.3311e-02,
        3.9785e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.919

[Epoch: 195, batch: 160/203] total loss per batch: 0.742
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.1303e-03, 1.1244e-01, 2.4347e-05, 3.5209e-03, 7.3011e-01, 7.5177e-03,
        1.4325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 195, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5272, 0.0748, 0.0031, 0.0029, 0.0462, 0.3428],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 196, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9796, 0.0033, 0.0050, 0.0030, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 196, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.8512e-01, 3.1212e-08, 2.6650e-06, 9.0759e-07, 2.5500e-06, 2.8957e-03,
        8.1198e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.658

[Epoch: 196, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.1240e-05, 9.5480e-01, 2.3216e-02, 3.2133e-08, 7.0008e-07, 2.1955e-02,
        2.0294e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 196, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.1890e-03, 1.0774e-01, 3.2955e-05, 3.5538e-03, 7.8071e-01, 7.9426e-03,
        9.5824e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 196, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0027, 0.5380, 0.0440, 0.0028, 0.0035, 0.0535, 0.3556],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 197, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.9779, 0.0028, 0.0042, 0.0042, 0.0031, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 197, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.3642e-01, 2.8521e-08, 2.8525e-06, 4.8148e-07, 2.2703e-06, 3.9490e-03,
        7.5963e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.691

[Epoch: 197, batch: 120/203] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.8458e-05, 9.7452e-01, 1.1924e-02, 6.0331e-08, 4.2193e-07, 1.3525e-02,
        3.6332e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.917

[Epoch: 197, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.8977e-03, 9.4811e-02, 3.8040e-05, 3.9083e-03, 7.7932e-01, 6.8877e-03,
        1.1113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 197, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0056, 0.4886, 0.0834, 0.0044, 0.0050, 0.0688, 0.3441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 198, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.9757, 0.0033, 0.0048, 0.0045, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 198, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5692e-01, 1.0938e-08, 2.1106e-06, 7.5521e-07, 1.4220e-06, 2.5818e-03,
        8.4049e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.656

[Epoch: 198, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([3.6982e-05, 9.6964e-01, 1.6349e-02, 6.7136e-08, 6.5247e-07, 1.3971e-02,
        1.5822e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.912

[Epoch: 198, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.1488e-03, 1.0332e-01, 2.3811e-05, 2.9366e-03, 7.7251e-01, 5.9717e-03,
        1.1208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 198, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5114, 0.0494, 0.0025, 0.0029, 0.0700, 0.3608],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 199, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.9830, 0.0023, 0.0029, 0.0027, 0.0027, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 199, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([2.6930e-01, 4.0000e-08, 2.9473e-06, 5.6280e-07, 2.3755e-06, 3.7164e-03,
        7.2697e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.723

[Epoch: 199, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([2.2715e-05, 9.6421e-01, 1.8496e-02, 3.2899e-08, 2.3966e-07, 1.7276e-02,
        2.1979e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.931

[Epoch: 199, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([4.8735e-03, 1.1843e-01, 3.8742e-05, 6.5145e-03, 7.3700e-01, 8.8700e-03,
        1.2427e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 199, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0031, 0.5490, 0.0561, 0.0029, 0.0037, 0.0463, 0.3391],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 200, batch: 40/203] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.9830, 0.0028, 0.0026, 0.0024, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 200, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.2033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0033, 0.7933])
Policy pred: tensor([1.5113e-01, 1.7464e-08, 3.5721e-06, 6.7465e-07, 2.1188e-06, 3.0906e-03,
        8.4577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.680 -0.646

[Epoch: 200, batch: 120/203] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9667, 0.0167, 0.0000, 0.0000, 0.0167, 0.0000])
Policy pred: tensor([4.3787e-05, 9.6161e-01, 1.7319e-02, 1.0148e-07, 2.8611e-07, 2.1024e-02,
        2.2316e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.915 -0.920

[Epoch: 200, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.1033, 0.0000, 0.0033, 0.7700, 0.0067, 0.1133])
Policy pred: tensor([3.0274e-03, 8.7127e-02, 2.5509e-05, 3.4748e-03, 8.0358e-01, 5.9640e-03,
        9.6805e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 200, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5167, 0.0600, 0.0033, 0.0033, 0.0600, 0.3533])
Policy pred: tensor([0.0034, 0.4617, 0.0597, 0.0035, 0.0030, 0.0704, 0.3983],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

