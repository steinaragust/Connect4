Training set samples: 6319
Batch size: 32
[Epoch: 1, batch: 39/198] total loss per batch: 1.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3773e-01, 6.4363e-01, 4.5948e-02, 1.8652e-04, 3.1836e-02, 6.3523e-05,
        4.0604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.169

[Epoch: 1, batch: 78/198] total loss per batch: 1.520
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.2144, 0.4564, 0.0573, 0.0693, 0.0396, 0.0624, 0.1007],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.013

[Epoch: 1, batch: 117/198] total loss per batch: 1.529
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0196, 0.5709, 0.0974, 0.0876, 0.1474, 0.0341, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.027

[Epoch: 1, batch: 156/198] total loss per batch: 1.516
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([2.8459e-01, 4.0409e-02, 4.4682e-02, 3.0881e-04, 3.2063e-02, 4.3200e-02,
        5.5475e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 1, batch: 195/198] total loss per batch: 1.467
Policy (actual, predicted): 1 3
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0807, 0.1199, 0.0092, 0.6444, 0.0295, 0.0777, 0.0386],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.037

[Epoch: 2, batch: 39/198] total loss per batch: 1.200
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3695e-01, 7.1627e-01, 1.4065e-02, 6.3242e-05, 1.7613e-02, 1.5945e-05,
        1.5027e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.236

[Epoch: 2, batch: 78/198] total loss per batch: 1.134
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1404, 0.6863, 0.0333, 0.0380, 0.0155, 0.0298, 0.0568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.002

[Epoch: 2, batch: 117/198] total loss per batch: 1.146
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.8010, 0.0207, 0.0597, 0.0850, 0.0100, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 2, batch: 156/198] total loss per batch: 1.174
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.5644e-01, 1.3484e-02, 1.2599e-02, 4.5973e-05, 5.5862e-03, 8.2287e-03,
        8.0362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.056

[Epoch: 2, batch: 195/198] total loss per batch: 1.143
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1112, 0.3946, 0.0105, 0.2696, 0.0398, 0.1369, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 3, batch: 39/198] total loss per batch: 0.962
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6128e-02, 9.1968e-01, 3.8209e-03, 6.1761e-06, 3.9225e-03, 1.7296e-06,
        6.4422e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.221

[Epoch: 3, batch: 78/198] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0801, 0.7731, 0.0536, 0.0256, 0.0142, 0.0260, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.018

[Epoch: 3, batch: 117/198] total loss per batch: 0.913
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.8325, 0.0171, 0.0663, 0.0690, 0.0046, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.044

[Epoch: 3, batch: 156/198] total loss per batch: 0.940
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.9753e-02, 4.5217e-03, 7.7261e-03, 3.9457e-05, 1.2845e-03, 2.6415e-03,
        9.0403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.051

[Epoch: 3, batch: 195/198] total loss per batch: 0.915
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0399, 0.7914, 0.0043, 0.0530, 0.0325, 0.0693, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 4, batch: 39/198] total loss per batch: 0.853
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3262e-02, 9.7134e-01, 2.2648e-03, 2.3094e-06, 1.6350e-03, 4.1739e-07,
        1.4978e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.250

[Epoch: 4, batch: 78/198] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1028, 0.5784, 0.1108, 0.0514, 0.0423, 0.0479, 0.0665],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.020

[Epoch: 4, batch: 117/198] total loss per batch: 0.851
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.8048, 0.0331, 0.0475, 0.0936, 0.0041, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.030

[Epoch: 4, batch: 156/198] total loss per batch: 0.868
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.7197e-01, 5.0580e-03, 1.3067e-02, 2.2328e-05, 3.3218e-03, 3.2845e-03,
        8.0328e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.045

[Epoch: 4, batch: 195/198] total loss per batch: 0.852
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0385, 0.8429, 0.0039, 0.0413, 0.0308, 0.0275, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 5, batch: 39/198] total loss per batch: 0.826
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.5992e-02, 9.6899e-01, 1.7247e-03, 2.2141e-06, 1.6065e-03, 4.9418e-07,
        1.6805e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.303

[Epoch: 5, batch: 78/198] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1361, 0.5355, 0.0546, 0.0800, 0.0567, 0.0560, 0.0810],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.043

[Epoch: 5, batch: 117/198] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7839, 0.0463, 0.0837, 0.0689, 0.0027, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 5, batch: 156/198] total loss per batch: 0.836
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.0285e-02, 4.1879e-03, 5.6492e-03, 1.8146e-05, 1.6099e-03, 1.4633e-03,
        9.0679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 5, batch: 195/198] total loss per batch: 0.827
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0294, 0.8312, 0.0043, 0.0779, 0.0107, 0.0264, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.003

[Epoch: 6, batch: 39/198] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.7922e-02, 9.7564e-01, 2.3453e-03, 2.5039e-06, 2.8110e-03, 8.7794e-07,
        1.2808e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.332

[Epoch: 6, batch: 78/198] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0922, 0.6003, 0.0773, 0.0665, 0.0644, 0.0402, 0.0590],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.038

[Epoch: 6, batch: 117/198] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.7483, 0.1145, 0.0744, 0.0478, 0.0031, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.035

[Epoch: 6, batch: 156/198] total loss per batch: 0.821
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.6059e-01, 6.3722e-03, 6.7614e-03, 1.9115e-05, 2.6970e-03, 1.8965e-03,
        8.2166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 6, batch: 195/198] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0217, 0.8322, 0.0057, 0.0865, 0.0121, 0.0214, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 7, batch: 39/198] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.1972e-02, 9.8254e-01, 2.0330e-03, 1.7106e-06, 1.7084e-03, 2.3547e-07,
        1.7484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.345

[Epoch: 7, batch: 78/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1122, 0.5538, 0.0478, 0.0908, 0.0722, 0.0482, 0.0749],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.044

[Epoch: 7, batch: 117/198] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7626, 0.1070, 0.0746, 0.0429, 0.0019, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.048

[Epoch: 7, batch: 156/198] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1660e-01, 5.2153e-03, 7.6880e-03, 2.5062e-05, 3.2637e-03, 1.5773e-03,
        8.6564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 7, batch: 195/198] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0418, 0.8504, 0.0038, 0.0694, 0.0060, 0.0127, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.008

[Epoch: 8, batch: 39/198] total loss per batch: 0.778
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.7796e-02, 9.7458e-01, 2.4971e-03, 1.9970e-06, 2.7596e-03, 5.0671e-07,
        2.3683e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.344

[Epoch: 8, batch: 78/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0769, 0.5956, 0.0989, 0.0721, 0.0429, 0.0385, 0.0751],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.061

[Epoch: 8, batch: 117/198] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7565, 0.1018, 0.0651, 0.0599, 0.0021, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 8, batch: 156/198] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.2607e-02, 8.7442e-03, 6.3992e-03, 1.4473e-05, 1.8528e-03, 1.5770e-03,
        8.9881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 8, batch: 195/198] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0721, 0.8419, 0.0037, 0.0494, 0.0110, 0.0119, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 9, batch: 39/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.7860e-03, 9.8276e-01, 1.9720e-03, 1.9486e-06, 3.8196e-03, 4.5269e-07,
        1.6565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.341

[Epoch: 9, batch: 78/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0727, 0.6175, 0.0414, 0.0841, 0.0648, 0.0526, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.057

[Epoch: 9, batch: 117/198] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.7794, 0.0999, 0.0706, 0.0404, 0.0016, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.047

[Epoch: 9, batch: 156/198] total loss per batch: 0.799
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([6.0644e-02, 3.8445e-03, 1.0407e-02, 1.3373e-05, 2.7558e-03, 9.8861e-04,
        9.2135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 9, batch: 195/198] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0270, 0.8639, 0.0048, 0.0791, 0.0073, 0.0101, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 10, batch: 39/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.5519e-02, 9.7704e-01, 2.3724e-03, 2.6246e-06, 2.8499e-03, 3.4700e-07,
        2.2200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.348

[Epoch: 10, batch: 78/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0583, 0.6413, 0.0528, 0.0806, 0.0688, 0.0470, 0.0512],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.069

[Epoch: 10, batch: 117/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0011, 0.7924, 0.1134, 0.0437, 0.0415, 0.0018, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.052

[Epoch: 10, batch: 156/198] total loss per batch: 0.796
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.4838e-02, 5.1377e-03, 6.7028e-03, 1.6197e-05, 1.8253e-03, 2.6027e-03,
        9.2888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 10, batch: 195/198] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0449, 0.8618, 0.0036, 0.0610, 0.0052, 0.0128, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 11, batch: 39/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.5679e-03, 9.8207e-01, 2.2049e-03, 1.7024e-06, 4.1131e-03, 5.7777e-07,
        2.0369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.356

[Epoch: 11, batch: 78/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1012, 0.4877, 0.0620, 0.1115, 0.0780, 0.0759, 0.0838],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.077

[Epoch: 11, batch: 117/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7730, 0.1075, 0.0661, 0.0385, 0.0022, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.048

[Epoch: 11, batch: 156/198] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.9930e-02, 5.3504e-03, 9.2284e-03, 6.2537e-06, 2.6740e-03, 1.7984e-03,
        9.2101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 11, batch: 195/198] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0549, 0.8375, 0.0055, 0.0792, 0.0052, 0.0097, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 12, batch: 39/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.3684e-02, 9.7882e-01, 2.9367e-03, 3.5733e-06, 2.9585e-03, 4.4821e-07,
        1.5955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.386

[Epoch: 12, batch: 78/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0806, 0.5434, 0.1012, 0.0876, 0.0695, 0.0602, 0.0575],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.093

[Epoch: 12, batch: 117/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.7499, 0.1190, 0.0856, 0.0357, 0.0020, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.026

[Epoch: 12, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3275e-01, 9.0378e-03, 9.6985e-03, 1.9393e-05, 3.8701e-03, 3.2680e-03,
        8.4136e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 12, batch: 195/198] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0340, 0.9082, 0.0039, 0.0371, 0.0044, 0.0080, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 13, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.1411e-02, 9.8103e-01, 1.9769e-03, 2.1626e-06, 3.6305e-03, 4.8589e-07,
        1.9514e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 13, batch: 78/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0513, 0.7467, 0.0247, 0.0596, 0.0440, 0.0319, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.070

[Epoch: 13, batch: 117/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7667, 0.1381, 0.0505, 0.0320, 0.0024, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.042

[Epoch: 13, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3795e-01, 6.3738e-03, 1.0894e-02, 1.4146e-05, 3.1526e-03, 2.8102e-03,
        8.3881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 13, batch: 195/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0986, 0.7764, 0.0061, 0.0904, 0.0037, 0.0143, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.029

[Epoch: 14, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([8.7459e-03, 9.8454e-01, 2.4486e-03, 2.7573e-06, 2.6198e-03, 3.5281e-07,
        1.6462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.379

[Epoch: 14, batch: 78/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0817, 0.6672, 0.0295, 0.0598, 0.0661, 0.0286, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.079

[Epoch: 14, batch: 117/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.7791, 0.0850, 0.0904, 0.0337, 0.0022, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 14, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([2.6016e-01, 1.4904e-02, 1.7392e-02, 2.8484e-05, 8.0067e-03, 8.1555e-03,
        6.9135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 14, batch: 195/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0164, 0.9282, 0.0028, 0.0376, 0.0026, 0.0084, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.025

[Epoch: 15, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.2232e-02, 9.7893e-01, 2.1340e-03, 3.0537e-06, 5.0947e-03, 6.5811e-07,
        1.6009e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 15, batch: 78/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1095, 0.5269, 0.0844, 0.0890, 0.0672, 0.0742, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 15, batch: 117/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7664, 0.1418, 0.0389, 0.0430, 0.0020, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 15, batch: 156/198] total loss per batch: 0.790
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.1330e-02, 8.1167e-03, 9.5711e-03, 7.7670e-06, 9.6086e-04, 2.6181e-03,
        8.9740e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 15, batch: 195/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0814, 0.7895, 0.0078, 0.0884, 0.0080, 0.0163, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 16, batch: 39/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.7104e-03, 9.8168e-01, 2.5509e-03, 4.0693e-06, 4.0071e-03, 2.9190e-07,
        2.0493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.387

[Epoch: 16, batch: 78/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0926, 0.4292, 0.0558, 0.1585, 0.0949, 0.0985, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 16, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7514, 0.1012, 0.1023, 0.0344, 0.0024, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 16, batch: 156/198] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8055e-02, 9.6253e-03, 1.1284e-02, 9.0627e-06, 2.4551e-03, 1.7327e-03,
        8.8684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.002

[Epoch: 16, batch: 195/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0344, 0.8561, 0.0057, 0.0735, 0.0047, 0.0140, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 17, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([7.7847e-03, 9.8374e-01, 2.6921e-03, 2.1726e-06, 3.4564e-03, 5.7760e-07,
        2.3274e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.371

[Epoch: 17, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0894, 0.6272, 0.0431, 0.0579, 0.0654, 0.0650, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 17, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7870, 0.1030, 0.0567, 0.0430, 0.0022, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.043

[Epoch: 17, batch: 156/198] total loss per batch: 0.788
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0404e-01, 1.0149e-02, 1.1967e-02, 9.4405e-06, 2.0941e-03, 2.8541e-03,
        8.6888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 17, batch: 195/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0367, 0.8802, 0.0029, 0.0606, 0.0027, 0.0121, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 18, batch: 39/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6247e-03, 9.7998e-01, 2.2176e-03, 3.6655e-06, 8.9635e-03, 9.2685e-07,
        2.2075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.400

[Epoch: 18, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1138, 0.6400, 0.0461, 0.0631, 0.0473, 0.0227, 0.0669],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 18, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.7358, 0.1298, 0.0862, 0.0365, 0.0020, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 18, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5577e-02, 8.2883e-03, 1.0181e-02, 9.5699e-06, 2.4040e-03, 3.2498e-03,
        8.9029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.002

[Epoch: 18, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0494, 0.8658, 0.0042, 0.0597, 0.0038, 0.0109, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 19, batch: 39/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.4843e-03, 9.8432e-01, 2.5761e-03, 3.0431e-06, 4.6638e-03, 6.1894e-07,
        1.9483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.403

[Epoch: 19, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0761, 0.6318, 0.0685, 0.0693, 0.0662, 0.0342, 0.0540],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.097

[Epoch: 19, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7738, 0.1246, 0.0524, 0.0401, 0.0024, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 19, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.4285e-02, 9.6937e-03, 1.0570e-02, 1.0838e-05, 2.2628e-03, 2.9637e-03,
        9.0021e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 19, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0646, 0.8471, 0.0037, 0.0608, 0.0052, 0.0137, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 20, batch: 39/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([7.2261e-03, 9.8469e-01, 1.4564e-03, 3.6009e-06, 4.1227e-03, 6.6791e-07,
        2.4981e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.386

[Epoch: 20, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1331, 0.4761, 0.0460, 0.1008, 0.0810, 0.0931, 0.0699],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.119

[Epoch: 20, batch: 117/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7535, 0.0801, 0.1173, 0.0381, 0.0024, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 20, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3540e-02, 1.1647e-02, 1.0746e-02, 7.9793e-06, 1.8234e-03, 2.6253e-03,
        8.7961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.004

[Epoch: 20, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0356, 0.8188, 0.0048, 0.1112, 0.0044, 0.0202, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 21, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([8.7374e-03, 9.7846e-01, 3.4327e-03, 4.7168e-06, 6.9961e-03, 3.6884e-07,
        2.3683e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.398

[Epoch: 21, batch: 78/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0557, 0.6482, 0.0414, 0.0960, 0.0582, 0.0567, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.091

[Epoch: 21, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.7912, 0.1241, 0.0389, 0.0364, 0.0021, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.024

[Epoch: 21, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.2182e-02, 7.0709e-03, 1.2438e-02, 7.6893e-06, 2.4378e-03, 3.8996e-03,
        8.8196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 21, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0298, 0.9041, 0.0050, 0.0457, 0.0027, 0.0086, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 22, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.8207e-03, 9.7552e-01, 3.1178e-03, 5.8836e-06, 5.5229e-03, 1.7027e-06,
        6.0138e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 22, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0829, 0.6861, 0.0474, 0.0632, 0.0475, 0.0168, 0.0561],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 22, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7606, 0.1002, 0.0911, 0.0401, 0.0022, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.044

[Epoch: 22, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5998e-02, 4.2997e-03, 9.2415e-03, 8.0185e-06, 3.6254e-03, 1.9242e-03,
        8.9490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.001

[Epoch: 22, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0886, 0.7863, 0.0070, 0.0889, 0.0075, 0.0125, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.013

[Epoch: 23, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.1727e-03, 9.8215e-01, 2.5872e-03, 4.1493e-06, 7.6049e-03, 6.0297e-07,
        2.4831e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.444

[Epoch: 23, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0548, 0.5791, 0.0735, 0.0737, 0.0722, 0.0798, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 23, batch: 117/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7266, 0.1544, 0.0689, 0.0393, 0.0023, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.039

[Epoch: 23, batch: 156/198] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1352e-01, 6.8900e-03, 6.8754e-03, 8.1434e-06, 3.6949e-03, 3.5203e-03,
        8.6549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 23, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0178, 0.9321, 0.0033, 0.0339, 0.0018, 0.0094, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 24, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9675e-03, 9.8529e-01, 3.1608e-03, 3.8609e-06, 5.2816e-03, 3.9724e-07,
        2.2968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.398

[Epoch: 24, batch: 78/198] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0899, 0.5212, 0.0592, 0.0886, 0.0934, 0.0768, 0.0709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 24, batch: 117/198] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.7875, 0.0966, 0.0669, 0.0372, 0.0022, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.046

[Epoch: 24, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3045e-02, 6.4922e-03, 1.4113e-02, 9.6645e-06, 2.8883e-03, 2.0692e-03,
        8.8138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 24, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1053, 0.7657, 0.0056, 0.0981, 0.0060, 0.0132, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 25, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.0367e-02, 9.7540e-01, 3.5397e-03, 5.8658e-06, 7.2250e-03, 1.8569e-06,
        3.4628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 25, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0905, 0.6167, 0.0378, 0.0991, 0.0415, 0.0546, 0.0597],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 25, batch: 117/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7631, 0.1204, 0.0777, 0.0292, 0.0022, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.042

[Epoch: 25, batch: 156/198] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0893e-02, 7.3494e-03, 9.8779e-03, 1.1293e-05, 6.0664e-03, 3.3197e-03,
        8.8248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 25, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0166, 0.9086, 0.0044, 0.0517, 0.0051, 0.0087, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 26, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.7489e-03, 9.8642e-01, 1.8921e-03, 4.9641e-06, 4.2473e-03, 7.6002e-07,
        3.6870e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.383

[Epoch: 26, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0777, 0.6517, 0.0456, 0.0573, 0.0672, 0.0237, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.086

[Epoch: 26, batch: 117/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.7703, 0.1300, 0.0493, 0.0414, 0.0020, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.037

[Epoch: 26, batch: 156/198] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0893e-01, 8.4180e-03, 1.1462e-02, 7.8601e-06, 3.0216e-03, 3.5668e-03,
        8.6459e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 26, batch: 195/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0369, 0.8282, 0.0065, 0.1011, 0.0070, 0.0143, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 27, batch: 39/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.4234e-03, 9.8371e-01, 2.4991e-03, 3.9736e-06, 6.3802e-03, 1.6838e-06,
        2.9859e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.363

[Epoch: 27, batch: 78/198] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0636, 0.6051, 0.0603, 0.0835, 0.0760, 0.0632, 0.0482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.116

[Epoch: 27, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7612, 0.1019, 0.1012, 0.0247, 0.0021, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.021

[Epoch: 27, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2245e-01, 6.7406e-03, 1.4533e-02, 1.0023e-05, 1.7707e-03, 2.2020e-03,
        8.5230e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 27, batch: 195/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0476, 0.8770, 0.0055, 0.0515, 0.0052, 0.0092, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.001

[Epoch: 28, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.6423e-03, 9.8100e-01, 2.4648e-03, 5.4855e-06, 5.7055e-03, 1.7244e-06,
        5.1826e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.408

[Epoch: 28, batch: 78/198] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0956, 0.6143, 0.0534, 0.0613, 0.0661, 0.0615, 0.0479],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 28, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7690, 0.1036, 0.0828, 0.0365, 0.0018, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.018

[Epoch: 28, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3988e-01, 3.8382e-03, 9.2459e-03, 1.0495e-05, 3.9261e-03, 1.6909e-03,
        8.4141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 28, batch: 195/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0381, 0.8756, 0.0032, 0.0628, 0.0042, 0.0130, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 29, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.3787e-03, 9.7586e-01, 2.4038e-03, 6.5952e-06, 1.0288e-02, 1.2338e-06,
        6.0638e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.440

[Epoch: 29, batch: 78/198] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1210, 0.5088, 0.0514, 0.1431, 0.0570, 0.0454, 0.0731],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 29, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.8094, 0.1128, 0.0411, 0.0273, 0.0018, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.024

[Epoch: 29, batch: 156/198] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0518e-01, 6.6110e-03, 8.7911e-03, 1.0274e-05, 2.6592e-03, 2.8996e-03,
        8.7385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 29, batch: 195/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0675, 0.8162, 0.0048, 0.0910, 0.0032, 0.0106, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 30, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8349e-03, 9.8847e-01, 2.0873e-03, 3.1078e-06, 4.2498e-03, 9.2001e-07,
        2.3546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.388

[Epoch: 30, batch: 78/198] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0919, 0.5881, 0.0414, 0.1270, 0.0547, 0.0252, 0.0717],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 30, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.6905, 0.1351, 0.1242, 0.0420, 0.0016, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.016

[Epoch: 30, batch: 156/198] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.3926e-02, 1.0980e-02, 1.1584e-02, 1.0118e-05, 2.2062e-03, 4.6495e-03,
        8.8664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 30, batch: 195/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0454, 0.8708, 0.0043, 0.0555, 0.0043, 0.0166, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.033

[Epoch: 31, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.3665e-02, 9.7224e-01, 4.0442e-03, 4.8040e-06, 4.9320e-03, 1.8876e-07,
        5.1096e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.410

[Epoch: 31, batch: 78/198] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0981, 0.5753, 0.0373, 0.0957, 0.0722, 0.0715, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 31, batch: 117/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.8062, 0.0869, 0.0664, 0.0297, 0.0022, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.029

[Epoch: 31, batch: 156/198] total loss per batch: 0.784
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.2552e-02, 1.2066e-02, 1.0562e-02, 1.3740e-05, 2.3366e-03, 3.9293e-03,
        8.9854e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 31, batch: 195/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0491, 0.8719, 0.0055, 0.0578, 0.0035, 0.0100, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 32, batch: 39/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.8311e-03, 9.8114e-01, 2.8161e-03, 4.1291e-06, 4.4075e-03, 4.7926e-07,
        6.8041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.411

[Epoch: 32, batch: 78/198] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0786, 0.5751, 0.0566, 0.0365, 0.0749, 0.0812, 0.0972],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.090

[Epoch: 32, batch: 117/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7568, 0.1148, 0.0772, 0.0360, 0.0024, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.036

[Epoch: 32, batch: 156/198] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0629e-01, 9.0681e-03, 1.5954e-02, 6.9079e-06, 2.0115e-03, 2.7147e-03,
        8.6396e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.040

[Epoch: 32, batch: 195/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0532, 0.8461, 0.0050, 0.0739, 0.0041, 0.0142, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 33, batch: 39/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.9010e-03, 9.8436e-01, 2.1807e-03, 2.9977e-06, 4.4593e-03, 5.0979e-07,
        2.1003e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.430

[Epoch: 33, batch: 78/198] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0677, 0.6696, 0.0437, 0.0509, 0.0478, 0.0468, 0.0734],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 33, batch: 117/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7375, 0.1508, 0.0646, 0.0360, 0.0025, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.021

[Epoch: 33, batch: 156/198] total loss per batch: 0.781
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5905e-02, 8.3188e-03, 1.2091e-02, 8.2280e-06, 3.3324e-03, 4.4415e-03,
        8.8590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 33, batch: 195/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0300, 0.8712, 0.0049, 0.0732, 0.0043, 0.0130, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 34, batch: 39/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3948e-03, 9.7853e-01, 3.2613e-03, 4.9652e-06, 9.8263e-03, 1.7054e-06,
        4.9796e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.404

[Epoch: 34, batch: 78/198] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1054, 0.6236, 0.0382, 0.0644, 0.0638, 0.0464, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 34, batch: 117/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.8061, 0.0752, 0.0787, 0.0292, 0.0022, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.008

[Epoch: 34, batch: 156/198] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0841e-01, 5.8557e-03, 9.8562e-03, 1.2262e-05, 2.9734e-03, 3.7455e-03,
        8.6915e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.001

[Epoch: 34, batch: 195/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0748, 0.8227, 0.0048, 0.0803, 0.0060, 0.0092, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 35, batch: 39/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.7116e-03, 9.8634e-01, 2.9513e-03, 4.5118e-06, 5.7815e-03, 4.4626e-07,
        2.2094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.404

[Epoch: 35, batch: 78/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1093, 0.6153, 0.0432, 0.0863, 0.0495, 0.0383, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 35, batch: 117/198] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.7336, 0.1425, 0.0667, 0.0407, 0.0029, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.018

[Epoch: 35, batch: 156/198] total loss per batch: 0.784
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3228e-01, 8.8586e-03, 1.1714e-02, 6.9407e-06, 4.6942e-03, 2.3036e-03,
        8.4014e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.031

[Epoch: 35, batch: 195/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0163, 0.8885, 0.0053, 0.0750, 0.0042, 0.0072, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 36, batch: 39/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9231e-03, 9.8511e-01, 2.2358e-03, 1.9963e-06, 4.2319e-03, 1.1216e-06,
        4.4971e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.377

[Epoch: 36, batch: 78/198] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0624, 0.6921, 0.0283, 0.0693, 0.0600, 0.0554, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 36, batch: 117/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.7704, 0.1214, 0.0641, 0.0365, 0.0021, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 36, batch: 156/198] total loss per batch: 0.788
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0218e-01, 8.0621e-03, 1.4260e-02, 1.9412e-05, 1.9448e-03, 2.1155e-03,
        8.7141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 36, batch: 195/198] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1045, 0.8156, 0.0048, 0.0403, 0.0067, 0.0245, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 37, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.3941e-03, 9.7582e-01, 3.4641e-03, 2.4347e-06, 1.3669e-02, 8.1183e-07,
        2.6471e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.401

[Epoch: 37, batch: 78/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0650, 0.5152, 0.0434, 0.0784, 0.0984, 0.0672, 0.1325],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.120

[Epoch: 37, batch: 117/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7514, 0.1216, 0.0771, 0.0397, 0.0026, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.010

[Epoch: 37, batch: 156/198] total loss per batch: 0.793
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([2.0342e-01, 1.4966e-02, 9.3502e-03, 1.8360e-05, 2.6961e-03, 3.2332e-03,
        7.6632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 37, batch: 195/198] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0308, 0.8440, 0.0046, 0.1018, 0.0036, 0.0102, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 38, batch: 39/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2283e-03, 9.8489e-01, 3.5951e-03, 1.9508e-06, 2.1510e-03, 3.7583e-06,
        6.1294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.403

[Epoch: 38, batch: 78/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1097, 0.4620, 0.0477, 0.1184, 0.0902, 0.0864, 0.0857],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.125

[Epoch: 38, batch: 117/198] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7910, 0.0835, 0.0735, 0.0435, 0.0021, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.024

[Epoch: 38, batch: 156/198] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.4471e-02, 7.3852e-03, 1.1208e-02, 7.3906e-06, 1.5331e-03, 2.2555e-03,
        8.9314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 38, batch: 195/198] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0718, 0.8583, 0.0114, 0.0249, 0.0025, 0.0262, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 39, batch: 39/198] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.4880e-03, 9.8317e-01, 2.4176e-03, 1.9532e-06, 5.7196e-03, 5.7043e-07,
        2.2068e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.379

[Epoch: 39, batch: 78/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0981, 0.5726, 0.0551, 0.0746, 0.0631, 0.0539, 0.0826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 39, batch: 117/198] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.7307, 0.1299, 0.0993, 0.0327, 0.0010, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.014

[Epoch: 39, batch: 156/198] total loss per batch: 0.794
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([6.4609e-02, 3.9264e-03, 1.4118e-02, 1.5656e-05, 1.5313e-03, 1.6775e-03,
        9.1412e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 39, batch: 195/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0392, 0.8268, 0.0127, 0.0958, 0.0071, 0.0149, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 40, batch: 39/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.0277e-02, 9.6505e-01, 7.8762e-03, 5.6342e-06, 1.0272e-02, 6.7409e-06,
        6.5130e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.513

[Epoch: 40, batch: 78/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0527, 0.6120, 0.0686, 0.1123, 0.0448, 0.0525, 0.0571],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.097

[Epoch: 40, batch: 117/198] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.7923, 0.1092, 0.0579, 0.0324, 0.0016, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.048

[Epoch: 40, batch: 156/198] total loss per batch: 0.818
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0420e-01, 1.2175e-02, 4.3108e-03, 3.5005e-06, 1.9261e-03, 2.6046e-03,
        8.7478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 40, batch: 195/198] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0401, 0.8161, 0.0100, 0.0767, 0.0046, 0.0450, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 41, batch: 39/198] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([9.3318e-03, 9.7368e-01, 4.2674e-03, 3.8942e-06, 5.6553e-03, 7.0092e-07,
        7.0597e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.369

[Epoch: 41, batch: 78/198] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0989, 0.5441, 0.0617, 0.1442, 0.0557, 0.0524, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 41, batch: 117/198] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0011, 0.7115, 0.1656, 0.0617, 0.0544, 0.0024, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.018

[Epoch: 41, batch: 156/198] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.7028e-02, 1.6885e-03, 7.1659e-03, 4.6480e-06, 1.6889e-03, 6.0434e-04,
        9.3182e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.121

[Epoch: 41, batch: 195/198] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0660, 0.8364, 0.0084, 0.0659, 0.0141, 0.0079, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.014

[Epoch: 42, batch: 39/198] total loss per batch: 0.780
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.4782e-02, 8.6216e-01, 9.0852e-02, 3.5527e-06, 7.6414e-03, 5.0236e-06,
        1.4553e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.433

[Epoch: 42, batch: 78/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0872, 0.6687, 0.0347, 0.0835, 0.0528, 0.0346, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.075

[Epoch: 42, batch: 117/198] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0011, 0.7856, 0.1372, 0.0366, 0.0360, 0.0016, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.010

[Epoch: 42, batch: 156/198] total loss per batch: 0.796
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.1378e-02, 1.4841e-02, 9.6726e-03, 1.4694e-05, 1.8430e-03, 4.6949e-03,
        8.9756e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.031

[Epoch: 42, batch: 195/198] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0290, 0.9025, 0.0057, 0.0484, 0.0024, 0.0092, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.007

[Epoch: 43, batch: 39/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.0012e-02, 9.7730e-01, 5.8382e-03, 1.3944e-06, 4.9302e-03, 1.6980e-07,
        1.9169e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.395

[Epoch: 43, batch: 78/198] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0868, 0.5820, 0.0527, 0.0953, 0.0575, 0.0572, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.088

[Epoch: 43, batch: 117/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0012, 0.7386, 0.1126, 0.1185, 0.0253, 0.0015, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 43, batch: 156/198] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2267e-01, 8.0758e-03, 1.2337e-02, 1.8048e-05, 1.7176e-03, 3.4619e-03,
        8.5172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 43, batch: 195/198] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0551, 0.8330, 0.0070, 0.0872, 0.0037, 0.0111, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.004

[Epoch: 44, batch: 39/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.9051e-03, 9.8182e-01, 5.1144e-03, 1.6046e-06, 5.3892e-03, 1.0596e-07,
        2.7648e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 44, batch: 78/198] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0784, 0.6240, 0.0429, 0.0855, 0.0551, 0.0506, 0.0635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.088

[Epoch: 44, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0010, 0.7695, 0.1141, 0.0846, 0.0261, 0.0016, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.006

[Epoch: 44, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0145e-01, 9.1391e-03, 9.1828e-03, 1.5833e-05, 2.2532e-03, 4.1845e-03,
        8.7378e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 44, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0357, 0.8712, 0.0060, 0.0688, 0.0040, 0.0109, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.005

[Epoch: 45, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.9286e-03, 9.8099e-01, 4.6830e-03, 1.5260e-06, 6.4162e-03, 1.0146e-07,
        2.9773e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 45, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0879, 0.6026, 0.0440, 0.0873, 0.0621, 0.0530, 0.0631],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 45, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0013, 0.7790, 0.1188, 0.0679, 0.0282, 0.0019, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.010

[Epoch: 45, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.9505e-02, 8.3816e-03, 9.4516e-03, 1.1881e-05, 2.1135e-03, 3.3525e-03,
        8.8718e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 45, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0457, 0.8747, 0.0052, 0.0595, 0.0031, 0.0093, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.004

[Epoch: 46, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.1440e-03, 9.8075e-01, 4.7716e-03, 1.2808e-06, 6.5346e-03, 9.6552e-08,
        2.8026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.410

[Epoch: 46, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0881, 0.5828, 0.0479, 0.0927, 0.0659, 0.0610, 0.0617],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 46, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0015, 0.7734, 0.1160, 0.0739, 0.0301, 0.0020, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.009

[Epoch: 46, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.4552e-02, 8.0180e-03, 9.3102e-03, 1.1318e-05, 2.2734e-03, 3.7038e-03,
        8.8213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 46, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0481, 0.8556, 0.0053, 0.0753, 0.0035, 0.0096, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.004

[Epoch: 47, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.6718e-03, 9.8118e-01, 4.3772e-03, 1.2610e-06, 6.9250e-03, 9.0853e-08,
        2.8403e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 47, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0989, 0.5805, 0.0507, 0.0804, 0.0688, 0.0550, 0.0657],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.095

[Epoch: 47, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.7696, 0.1280, 0.0650, 0.0304, 0.0020, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 47, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.1905e-02, 8.5702e-03, 9.2426e-03, 9.7302e-06, 2.1459e-03, 3.4322e-03,
        8.8469e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 47, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0521, 0.8669, 0.0047, 0.0619, 0.0029, 0.0093, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.003

[Epoch: 48, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5800e-03, 9.8094e-01, 4.5266e-03, 1.2852e-06, 7.1388e-03, 9.3260e-08,
        2.8089e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 48, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0916, 0.5938, 0.0437, 0.0819, 0.0680, 0.0584, 0.0625],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 48, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.7687, 0.1188, 0.0753, 0.0300, 0.0023, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 48, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6187e-02, 7.8918e-03, 1.1240e-02, 8.9767e-06, 3.0729e-03, 3.5249e-03,
        8.7807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 48, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0452, 0.8599, 0.0050, 0.0741, 0.0037, 0.0100, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.000

[Epoch: 49, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5856e-03, 9.8076e-01, 4.5971e-03, 1.1429e-06, 6.9806e-03, 8.4660e-08,
        3.0770e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 49, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0866, 0.6133, 0.0444, 0.0829, 0.0613, 0.0447, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 49, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.7769, 0.1159, 0.0697, 0.0295, 0.0025, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 49, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3211e-02, 6.8035e-03, 1.1219e-02, 8.1489e-06, 2.5828e-03, 3.0205e-03,
        8.8316e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 49, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0554, 0.8674, 0.0043, 0.0591, 0.0028, 0.0091, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 50, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.4764e-03, 9.8104e-01, 4.1887e-03, 1.3152e-06, 7.2105e-03, 8.5044e-08,
        3.0855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 50, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0868, 0.5843, 0.0403, 0.0945, 0.0658, 0.0642, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 50, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7761, 0.1194, 0.0646, 0.0315, 0.0028, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.006

[Epoch: 50, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7684e-02, 8.3844e-03, 9.5482e-03, 8.7707e-06, 2.6647e-03, 3.5647e-03,
        8.7815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 50, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0398, 0.8601, 0.0044, 0.0768, 0.0038, 0.0130, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 51, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.3217e-03, 9.8137e-01, 4.4646e-03, 1.1086e-06, 6.3850e-03, 9.8600e-08,
        3.4532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 51, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0862, 0.5959, 0.0526, 0.0780, 0.0640, 0.0495, 0.0738],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 51, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.7640, 0.1230, 0.0737, 0.0307, 0.0028, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 51, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0377e-01, 7.2571e-03, 1.0497e-02, 7.2577e-06, 3.0568e-03, 3.5004e-03,
        8.7191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 51, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0655, 0.8532, 0.0049, 0.0630, 0.0040, 0.0075, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 52, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.4312e-03, 9.8154e-01, 4.1152e-03, 1.2032e-06, 7.1161e-03, 8.2799e-08,
        2.7965e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 52, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0885, 0.6009, 0.0497, 0.0860, 0.0610, 0.0450, 0.0689],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 52, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.7595, 0.1230, 0.0779, 0.0312, 0.0027, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 52, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.1664e-02, 6.6237e-03, 9.0989e-03, 6.6669e-06, 3.0322e-03, 3.8759e-03,
        8.8570e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 52, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0362, 0.8819, 0.0045, 0.0616, 0.0035, 0.0095, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 53, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5974e-03, 9.8135e-01, 3.9297e-03, 1.0055e-06, 6.9158e-03, 8.8718e-08,
        3.2107e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.415

[Epoch: 53, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0931, 0.5890, 0.0521, 0.0719, 0.0582, 0.0573, 0.0784],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 53, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.7875, 0.1130, 0.0616, 0.0295, 0.0029, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 53, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0361e-01, 7.2317e-03, 1.1874e-02, 6.8668e-06, 2.9634e-03, 3.1104e-03,
        8.7120e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 53, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0703, 0.8246, 0.0049, 0.0808, 0.0035, 0.0124, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 54, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2173e-03, 9.8203e-01, 4.2462e-03, 1.2363e-06, 6.8030e-03, 7.8219e-08,
        2.7031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.426

[Epoch: 54, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0827, 0.5980, 0.0368, 0.0787, 0.0640, 0.0685, 0.0712],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 54, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7556, 0.1280, 0.0777, 0.0297, 0.0029, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 54, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.8339e-02, 7.6851e-03, 1.0898e-02, 5.9738e-06, 3.2862e-03, 3.2844e-03,
        8.7650e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 54, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0301, 0.9038, 0.0040, 0.0483, 0.0033, 0.0083, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 55, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4259e-03, 9.8274e-01, 4.2497e-03, 6.5269e-07, 6.4868e-03, 5.2668e-08,
        3.0944e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 55, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0939, 0.5794, 0.0418, 0.0970, 0.0629, 0.0615, 0.0635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 55, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.7831, 0.1112, 0.0653, 0.0308, 0.0028, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 55, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7176e-02, 5.9550e-03, 1.0664e-02, 6.0170e-06, 3.4085e-03, 3.4301e-03,
        8.7936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 55, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0665, 0.8407, 0.0040, 0.0728, 0.0033, 0.0096, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 56, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.6460e-03, 9.8268e-01, 3.7857e-03, 1.1812e-06, 6.3526e-03, 7.8320e-08,
        3.5338e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.416

[Epoch: 56, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0758, 0.6296, 0.0646, 0.0767, 0.0592, 0.0244, 0.0696],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 56, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7573, 0.1316, 0.0707, 0.0314, 0.0029, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 56, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0836e-01, 7.2215e-03, 1.0851e-02, 5.7567e-06, 2.7861e-03, 3.3443e-03,
        8.6743e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 56, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0357, 0.8444, 0.0039, 0.0946, 0.0027, 0.0146, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 57, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.1116e-03, 9.7547e-01, 4.5958e-03, 1.0366e-06, 8.9702e-03, 1.4547e-07,
        4.8530e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.416

[Epoch: 57, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0764, 0.6130, 0.0543, 0.0715, 0.0707, 0.0496, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 57, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7970, 0.0972, 0.0666, 0.0294, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 57, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1922e-01, 6.3474e-03, 8.6236e-03, 5.8451e-06, 3.5270e-03, 1.9313e-03,
        8.6035e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 57, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0449, 0.9102, 0.0043, 0.0282, 0.0029, 0.0077, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 58, batch: 39/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2540e-03, 9.8536e-01, 2.6631e-03, 1.0320e-06, 5.7206e-03, 9.8092e-08,
        3.0041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 58, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0836, 0.5554, 0.0434, 0.1023, 0.0555, 0.0948, 0.0650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 58, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7136, 0.1493, 0.0948, 0.0316, 0.0035, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.012

[Epoch: 58, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0915e-01, 7.8714e-03, 1.0774e-02, 7.2778e-06, 2.6547e-03, 4.7365e-03,
        8.6481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 58, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0827, 0.7580, 0.0050, 0.1258, 0.0059, 0.0154, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.009

[Epoch: 59, batch: 39/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4032e-03, 9.8128e-01, 4.5220e-03, 1.5824e-06, 6.3900e-03, 1.5422e-07,
        4.4081e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 59, batch: 78/198] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1031, 0.5488, 0.0468, 0.1056, 0.0819, 0.0493, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 59, batch: 117/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.8024, 0.1128, 0.0382, 0.0359, 0.0024, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 59, batch: 156/198] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7788e-02, 9.1717e-03, 6.7880e-03, 8.9718e-06, 4.1937e-03, 4.2948e-03,
        8.7775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.000

[Epoch: 59, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0195, 0.9323, 0.0038, 0.0296, 0.0024, 0.0089, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.036

[Epoch: 60, batch: 39/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5091e-03, 9.8208e-01, 4.0526e-03, 1.6830e-06, 5.3489e-03, 2.4701e-07,
        4.0035e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.386

[Epoch: 60, batch: 78/198] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1137, 0.5846, 0.0413, 0.0743, 0.0663, 0.0399, 0.0799],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 60, batch: 117/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.7544, 0.1135, 0.0960, 0.0283, 0.0023, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 60, batch: 156/198] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.3485e-02, 5.7005e-03, 1.1424e-02, 8.3574e-06, 2.7254e-03, 2.6230e-03,
        9.0403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 60, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0458, 0.8421, 0.0074, 0.0800, 0.0067, 0.0115, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 61, batch: 39/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6083e-03, 9.7794e-01, 4.4282e-03, 1.9779e-06, 7.6132e-03, 1.4711e-07,
        3.4035e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 61, batch: 78/198] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0605, 0.7327, 0.0314, 0.0445, 0.0464, 0.0294, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 61, batch: 117/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.7719, 0.0997, 0.0906, 0.0284, 0.0027, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.011

[Epoch: 61, batch: 156/198] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.3978e-02, 5.0203e-03, 9.8339e-03, 7.6583e-06, 3.4432e-03, 2.5435e-03,
        8.9517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 61, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0660, 0.8483, 0.0039, 0.0616, 0.0041, 0.0140, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.028

[Epoch: 62, batch: 39/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8170e-03, 9.8066e-01, 3.8909e-03, 1.3156e-06, 6.8639e-03, 1.9709e-07,
        4.7714e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 62, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0697, 0.5791, 0.0375, 0.1051, 0.0805, 0.0783, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 62, batch: 117/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7732, 0.1175, 0.0689, 0.0303, 0.0024, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 62, batch: 156/198] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0417e-02, 5.6787e-03, 1.5122e-02, 5.2039e-06, 4.1757e-03, 5.3138e-03,
        8.7929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 62, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0261, 0.8889, 0.0048, 0.0650, 0.0022, 0.0084, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 63, batch: 39/198] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.8715e-03, 9.8006e-01, 3.6964e-03, 1.7727e-06, 7.9906e-03, 1.4260e-07,
        3.3776e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.364

[Epoch: 63, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0888, 0.5392, 0.0625, 0.1239, 0.0652, 0.0463, 0.0741],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.110

[Epoch: 63, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7688, 0.1212, 0.0756, 0.0265, 0.0024, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 63, batch: 156/198] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8797e-02, 8.9639e-03, 8.6608e-03, 7.4118e-06, 2.3209e-03, 3.0105e-03,
        8.8824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 63, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.1074, 0.8074, 0.0063, 0.0642, 0.0024, 0.0089, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 64, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0071e-03, 9.8304e-01, 3.6929e-03, 1.7928e-06, 7.3672e-03, 1.4746e-07,
        2.8930e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.462

[Epoch: 64, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1195, 0.5691, 0.0631, 0.0644, 0.0556, 0.0575, 0.0709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 64, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.7430, 0.1362, 0.0737, 0.0351, 0.0038, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 64, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1840e-01, 5.2756e-03, 1.4900e-02, 5.6922e-06, 4.0164e-03, 2.5309e-03,
        8.5488e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 64, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0168, 0.8866, 0.0063, 0.0644, 0.0062, 0.0150, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 65, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.2255e-03, 9.7926e-01, 4.3031e-03, 1.5945e-06, 7.8217e-03, 1.1937e-07,
        3.3839e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.400

[Epoch: 65, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0858, 0.5741, 0.0324, 0.0873, 0.0750, 0.0777, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 65, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.8119, 0.0894, 0.0627, 0.0271, 0.0026, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 65, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0384e-01, 9.3478e-03, 9.9706e-03, 5.6604e-06, 3.0543e-03, 4.2704e-03,
        8.6951e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 65, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0337, 0.8760, 0.0045, 0.0674, 0.0024, 0.0100, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 66, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.1321e-03, 9.8393e-01, 3.4253e-03, 1.3083e-06, 5.4673e-03, 9.4663e-08,
        3.0389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.430

[Epoch: 66, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0730, 0.6413, 0.0432, 0.0807, 0.0666, 0.0330, 0.0623],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 66, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7158, 0.1695, 0.0708, 0.0335, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.010

[Epoch: 66, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3201e-02, 4.8111e-03, 9.4977e-03, 7.0267e-06, 4.1381e-03, 3.9015e-03,
        8.8444e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 66, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0734, 0.8102, 0.0056, 0.0860, 0.0033, 0.0151, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 67, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8088e-03, 9.8497e-01, 2.9244e-03, 1.0950e-06, 6.2076e-03, 1.8442e-07,
        3.0875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.429

[Epoch: 67, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1353, 0.5752, 0.0589, 0.0632, 0.0613, 0.0338, 0.0722],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 67, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.8079, 0.0833, 0.0747, 0.0258, 0.0024, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 67, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1961e-01, 5.3194e-03, 1.0251e-02, 4.1602e-06, 3.4006e-03, 1.6821e-03,
        8.5974e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 67, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0311, 0.9096, 0.0048, 0.0399, 0.0031, 0.0087, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.025

[Epoch: 68, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3252e-03, 9.8159e-01, 3.9119e-03, 1.3014e-06, 7.6434e-03, 1.4688e-07,
        3.5295e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.432

[Epoch: 68, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0920, 0.6006, 0.0271, 0.0774, 0.0587, 0.0699, 0.0743],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.110

[Epoch: 68, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7331, 0.1609, 0.0624, 0.0339, 0.0024, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 68, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1296e-01, 1.0411e-02, 1.0594e-02, 1.0080e-05, 4.9160e-03, 3.1375e-03,
        8.5797e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 68, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0414, 0.8336, 0.0041, 0.0973, 0.0028, 0.0143, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 69, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([8.3490e-03, 9.7617e-01, 4.0200e-03, 1.4055e-06, 8.3301e-03, 9.6065e-08,
        3.1334e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 69, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0766, 0.5533, 0.0486, 0.1313, 0.0534, 0.0580, 0.0789],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.097

[Epoch: 69, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7580, 0.1218, 0.0820, 0.0288, 0.0023, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 69, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.1040e-02, 6.1047e-03, 9.0425e-03, 6.5535e-06, 3.6784e-03, 4.3371e-03,
        8.9579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 69, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0742, 0.8538, 0.0035, 0.0513, 0.0023, 0.0103, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 70, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.8929e-03, 9.8642e-01, 2.6154e-03, 1.1478e-06, 5.9630e-03, 1.6210e-07,
        3.1115e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.411

[Epoch: 70, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0910, 0.5881, 0.0514, 0.0775, 0.0699, 0.0483, 0.0738],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 70, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7845, 0.1091, 0.0661, 0.0313, 0.0027, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 70, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0138e-01, 7.9454e-03, 1.1141e-02, 8.8404e-06, 2.1583e-03, 2.3894e-03,
        8.7497e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 70, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0246, 0.9108, 0.0038, 0.0457, 0.0030, 0.0100, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 71, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.9441e-03, 9.8503e-01, 3.5342e-03, 1.1505e-06, 5.3052e-03, 1.9977e-07,
        3.1841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.416

[Epoch: 71, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0952, 0.6129, 0.0367, 0.0774, 0.0600, 0.0575, 0.0602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.091

[Epoch: 71, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.7703, 0.1214, 0.0688, 0.0298, 0.0028, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 71, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3408e-01, 7.6016e-03, 1.3456e-02, 9.4757e-06, 3.7713e-03, 2.6844e-03,
        8.3840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 71, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0451, 0.8302, 0.0057, 0.1005, 0.0036, 0.0109, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.008

[Epoch: 72, batch: 39/198] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.7010e-03, 9.7346e-01, 4.5474e-03, 2.0391e-06, 1.0661e-02, 2.7733e-07,
        4.6332e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 72, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0685, 0.6254, 0.0504, 0.0720, 0.0758, 0.0421, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 72, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0047, 0.7739, 0.1102, 0.0732, 0.0304, 0.0022, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.012

[Epoch: 72, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0399e-01, 5.1285e-03, 1.2568e-02, 8.6457e-06, 4.2797e-03, 5.0901e-03,
        8.6893e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 72, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0734, 0.8456, 0.0047, 0.0598, 0.0037, 0.0090, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 73, batch: 39/198] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.1020e-03, 9.8322e-01, 3.6692e-03, 1.8733e-06, 5.2514e-03, 2.0082e-07,
        5.7535e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 73, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0913, 0.5876, 0.0440, 0.0900, 0.0563, 0.0432, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 73, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.7836, 0.1235, 0.0559, 0.0278, 0.0025, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 73, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0272e-01, 7.8861e-03, 1.1115e-02, 9.1326e-06, 2.5347e-03, 2.7844e-03,
        8.7295e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 73, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0354, 0.8562, 0.0044, 0.0822, 0.0046, 0.0124, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.011

[Epoch: 74, batch: 39/198] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3016e-03, 9.8344e-01, 3.5881e-03, 1.7163e-06, 7.3392e-03, 1.1622e-07,
        3.3334e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.405

[Epoch: 74, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1221, 0.5331, 0.0445, 0.0874, 0.0640, 0.0694, 0.0796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 74, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7392, 0.1252, 0.0945, 0.0312, 0.0028, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 74, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.2303e-02, 6.0685e-03, 1.1719e-02, 6.8675e-06, 2.7322e-03, 3.1480e-03,
        8.8402e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 74, batch: 195/198] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0503, 0.8773, 0.0046, 0.0504, 0.0049, 0.0100, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 75, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.4238e-03, 9.7966e-01, 4.2803e-03, 1.6522e-06, 6.6019e-03, 2.5996e-07,
        4.0357e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.440

[Epoch: 75, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0833, 0.5968, 0.0461, 0.0888, 0.0624, 0.0587, 0.0639],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 75, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7927, 0.1132, 0.0611, 0.0224, 0.0026, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 75, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3193e-02, 6.2285e-03, 1.1418e-02, 1.0404e-05, 3.2981e-03, 3.4786e-03,
        8.8237e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 75, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0518, 0.8464, 0.0052, 0.0780, 0.0033, 0.0097, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 76, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.7487e-03, 9.8113e-01, 3.0184e-03, 1.3418e-06, 7.6041e-03, 1.5755e-07,
        5.4957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.398

[Epoch: 76, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0803, 0.5956, 0.0445, 0.1057, 0.0535, 0.0489, 0.0715],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 76, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.7579, 0.1282, 0.0742, 0.0312, 0.0022, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 76, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7424e-02, 6.0199e-03, 1.0923e-02, 7.2774e-06, 3.4435e-03, 2.5239e-03,
        8.7966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 76, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0509, 0.8647, 0.0034, 0.0627, 0.0025, 0.0121, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.008

[Epoch: 77, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8306e-03, 9.8285e-01, 3.7449e-03, 2.6428e-06, 7.2485e-03, 2.0541e-07,
        3.3206e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.437

[Epoch: 77, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0941, 0.6024, 0.0444, 0.0723, 0.0615, 0.0560, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 77, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7863, 0.1115, 0.0618, 0.0304, 0.0024, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 77, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.3015e-01, 8.2028e-03, 1.5252e-02, 6.5428e-06, 3.3987e-03, 4.9272e-03,
        8.3807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 77, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0486, 0.8618, 0.0049, 0.0705, 0.0041, 0.0078, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 78, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.6459e-03, 9.8478e-01, 2.6490e-03, 2.0742e-06, 5.2447e-03, 1.4061e-07,
        2.6771e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 78, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0743, 0.6347, 0.0429, 0.0713, 0.0771, 0.0507, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 78, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.7684, 0.1122, 0.0797, 0.0309, 0.0021, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 78, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0265e-01, 7.1759e-03, 9.9370e-03, 8.1100e-06, 3.2435e-03, 4.1085e-03,
        8.7287e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 78, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0495, 0.8713, 0.0044, 0.0621, 0.0033, 0.0071, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 79, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.5675e-03, 9.8634e-01, 2.9044e-03, 1.2991e-06, 5.3992e-03, 8.5458e-08,
        2.7907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 79, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0785, 0.6088, 0.0583, 0.1007, 0.0549, 0.0324, 0.0665],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.110

[Epoch: 79, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7588, 0.1277, 0.0705, 0.0328, 0.0031, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 79, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1571e-01, 5.0373e-03, 1.1382e-02, 7.8529e-06, 3.2643e-03, 3.9027e-03,
        8.6069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 79, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0436, 0.8815, 0.0033, 0.0558, 0.0025, 0.0085, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 80, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9937e-03, 9.7980e-01, 3.6215e-03, 1.7347e-06, 9.0081e-03, 1.7468e-07,
        3.5699e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.434

[Epoch: 80, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0909, 0.6139, 0.0367, 0.0862, 0.0469, 0.0651, 0.0604],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 80, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7930, 0.1076, 0.0620, 0.0279, 0.0027, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 80, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.3521e-02, 7.5948e-03, 8.9852e-03, 6.0074e-06, 3.2819e-03, 2.2462e-03,
        8.9436e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 80, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0533, 0.8409, 0.0059, 0.0701, 0.0042, 0.0212, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.025

[Epoch: 81, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.1692e-03, 9.7928e-01, 3.4456e-03, 2.8726e-06, 8.7935e-03, 4.1907e-07,
        4.3126e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.410

[Epoch: 81, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0953, 0.5531, 0.0523, 0.0634, 0.0762, 0.0696, 0.0901],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.092

[Epoch: 81, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7662, 0.1196, 0.0746, 0.0290, 0.0028, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 81, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.0612e-02, 4.2126e-03, 1.0776e-02, 1.0285e-05, 2.5492e-03, 3.7818e-03,
        8.9806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 81, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0554, 0.8493, 0.0031, 0.0774, 0.0035, 0.0075, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 82, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.5046e-03, 9.8624e-01, 2.6938e-03, 1.7629e-06, 5.2010e-03, 2.4218e-07,
        3.3628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 82, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1004, 0.5038, 0.0565, 0.0868, 0.1037, 0.0654, 0.0833],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.093

[Epoch: 82, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7571, 0.1161, 0.0826, 0.0337, 0.0033, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 82, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1309e-01, 5.4134e-03, 9.2373e-03, 7.2582e-06, 5.7255e-03, 4.1749e-03,
        8.6235e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 82, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0334, 0.9082, 0.0035, 0.0426, 0.0026, 0.0072, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 83, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4561e-03, 9.8674e-01, 2.4557e-03, 1.7990e-06, 4.5993e-03, 2.1690e-07,
        2.7510e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 83, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0962, 0.6164, 0.0502, 0.0773, 0.0501, 0.0484, 0.0615],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.110

[Epoch: 83, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.7586, 0.1328, 0.0717, 0.0267, 0.0026, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 83, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.5350e-02, 8.4367e-03, 9.0841e-03, 6.9382e-06, 3.5815e-03, 1.9687e-03,
        8.9157e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 83, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0604, 0.8067, 0.0044, 0.1063, 0.0043, 0.0144, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 84, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.5854e-03, 9.7838e-01, 3.9644e-03, 1.9349e-06, 9.1321e-03, 2.8117e-07,
        2.9327e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 84, batch: 78/198] total loss per batch: 0.735
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0621, 0.6997, 0.0342, 0.0713, 0.0415, 0.0392, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.085

[Epoch: 84, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.8082, 0.0968, 0.0587, 0.0278, 0.0024, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 84, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0533e-01, 8.4253e-03, 1.0054e-02, 1.2520e-05, 1.7852e-03, 3.9104e-03,
        8.7048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 84, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0631, 0.8737, 0.0049, 0.0394, 0.0021, 0.0133, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 85, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.8494e-03, 9.8407e-01, 2.8310e-03, 1.7784e-06, 5.7690e-03, 4.5727e-07,
        5.4799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 85, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1174, 0.5397, 0.0361, 0.0920, 0.0782, 0.0552, 0.0814],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 85, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.7112, 0.1643, 0.0765, 0.0364, 0.0025, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 85, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0406e-01, 7.7916e-03, 1.3234e-02, 4.8637e-06, 3.6875e-03, 3.1222e-03,
        8.6810e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 85, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0299, 0.8700, 0.0054, 0.0771, 0.0053, 0.0080, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 86, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.6607e-03, 9.8369e-01, 2.6591e-03, 2.2165e-06, 6.7010e-03, 1.7636e-07,
        2.2851e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 86, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1011, 0.5560, 0.0452, 0.1069, 0.0700, 0.0437, 0.0771],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 86, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7659, 0.1094, 0.0891, 0.0264, 0.0026, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 86, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.9045e-02, 7.0597e-03, 1.3848e-02, 1.0687e-05, 4.6590e-03, 3.2650e-03,
        8.8211e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 86, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0473, 0.8625, 0.0057, 0.0647, 0.0039, 0.0121, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 87, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.9089e-03, 9.7990e-01, 2.6171e-03, 2.6267e-06, 8.1449e-03, 3.4287e-07,
        3.4264e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 87, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0862, 0.5730, 0.0694, 0.0910, 0.0484, 0.0540, 0.0781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 87, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7786, 0.1173, 0.0625, 0.0313, 0.0030, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 87, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1702e-01, 5.6875e-03, 1.2884e-02, 7.0630e-06, 3.6836e-03, 2.7419e-03,
        8.5798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.041

[Epoch: 87, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0628, 0.8400, 0.0034, 0.0784, 0.0019, 0.0107, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 88, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.0396e-03, 9.8292e-01, 2.4833e-03, 2.7444e-06, 7.7624e-03, 3.7996e-07,
        4.7955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 88, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0805, 0.6100, 0.0368, 0.0930, 0.0681, 0.0617, 0.0500],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 88, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7760, 0.1087, 0.0747, 0.0325, 0.0025, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.016

[Epoch: 88, batch: 156/198] total loss per batch: 0.780
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5621e-02, 5.9105e-03, 8.9918e-03, 1.2714e-05, 2.3397e-03, 3.5688e-03,
        8.8356e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 88, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0315, 0.8886, 0.0050, 0.0548, 0.0033, 0.0133, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.037

[Epoch: 89, batch: 39/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([1.8268e-02, 9.5709e-01, 6.2399e-03, 3.4757e-06, 9.4941e-03, 2.2404e-07,
        8.9035e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.476

[Epoch: 89, batch: 78/198] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0757, 0.5422, 0.0566, 0.1105, 0.0681, 0.0690, 0.0779],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 89, batch: 117/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7477, 0.0757, 0.1012, 0.0643, 0.0041, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.032

[Epoch: 89, batch: 156/198] total loss per batch: 0.789
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.2606e-02, 6.6506e-03, 5.9353e-03, 3.0419e-06, 3.0112e-03, 1.8943e-03,
        9.0990e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 89, batch: 195/198] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0775, 0.8437, 0.0031, 0.0553, 0.0050, 0.0099, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.004

[Epoch: 90, batch: 39/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.4406e-03, 9.8207e-01, 1.7754e-03, 3.1355e-06, 5.1548e-03, 3.1816e-07,
        5.5513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.440

[Epoch: 90, batch: 78/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0856, 0.6443, 0.0325, 0.0779, 0.0453, 0.0495, 0.0650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 90, batch: 117/198] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.7297, 0.1386, 0.0738, 0.0440, 0.0061, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 90, batch: 156/198] total loss per batch: 0.787
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.0184e-02, 2.7225e-03, 1.0070e-02, 5.4381e-06, 3.1659e-03, 1.6120e-03,
        9.0224e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 90, batch: 195/198] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0590, 0.8594, 0.0032, 0.0633, 0.0027, 0.0080, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 91, batch: 39/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.6464e-03, 9.8295e-01, 1.7937e-03, 2.7341e-06, 7.6312e-03, 1.0948e-07,
        3.9792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.447

[Epoch: 91, batch: 78/198] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1072, 0.5893, 0.0463, 0.0788, 0.0614, 0.0490, 0.0679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 91, batch: 117/198] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.7627, 0.1437, 0.0582, 0.0253, 0.0031, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.011

[Epoch: 91, batch: 156/198] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0584e-01, 4.0440e-03, 8.6852e-03, 1.0096e-05, 2.8416e-03, 3.5710e-03,
        8.7501e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 91, batch: 195/198] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0329, 0.8961, 0.0031, 0.0516, 0.0025, 0.0104, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 92, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1774e-03, 9.8535e-01, 3.1027e-03, 3.6250e-06, 5.2220e-03, 1.2075e-07,
        3.1419e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.411

[Epoch: 92, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0880, 0.6219, 0.0442, 0.0732, 0.0613, 0.0463, 0.0651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 92, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7670, 0.1334, 0.0575, 0.0311, 0.0040, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.009

[Epoch: 92, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0930e-01, 5.5328e-03, 1.1827e-02, 9.0174e-06, 3.4791e-03, 4.0570e-03,
        8.6579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 92, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0559, 0.8356, 0.0036, 0.0877, 0.0034, 0.0083, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 93, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4131e-03, 9.8436e-01, 3.0555e-03, 3.3103e-06, 5.5380e-03, 1.5124e-07,
        3.6335e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.410

[Epoch: 93, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0878, 0.5961, 0.0477, 0.0874, 0.0647, 0.0521, 0.0643],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 93, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7707, 0.1241, 0.0635, 0.0322, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.006

[Epoch: 93, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0381e-01, 6.0223e-03, 1.1443e-02, 7.9211e-06, 3.6129e-03, 3.7789e-03,
        8.7133e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 93, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0505, 0.8649, 0.0038, 0.0623, 0.0035, 0.0103, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 94, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.6931e-03, 9.8370e-01, 2.9862e-03, 3.0932e-06, 6.0062e-03, 1.3878e-07,
        3.6153e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 94, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0910, 0.5859, 0.0475, 0.0871, 0.0631, 0.0561, 0.0694],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 94, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7769, 0.1157, 0.0667, 0.0311, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 94, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0990e-01, 6.6131e-03, 1.1054e-02, 7.6908e-06, 3.3326e-03, 3.8740e-03,
        8.6522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 94, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0471, 0.8685, 0.0037, 0.0646, 0.0033, 0.0091, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 95, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3654e-03, 9.8370e-01, 3.1226e-03, 2.9646e-06, 6.3777e-03, 1.2440e-07,
        3.4357e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 95, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0900, 0.6016, 0.0464, 0.0813, 0.0616, 0.0518, 0.0672],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 95, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7681, 0.1245, 0.0675, 0.0304, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 95, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1162e-01, 6.6521e-03, 1.0883e-02, 6.8501e-06, 3.3935e-03, 3.5380e-03,
        8.6390e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 95, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0491, 0.8604, 0.0036, 0.0690, 0.0035, 0.0100, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 96, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4705e-03, 9.8351e-01, 3.1329e-03, 2.7134e-06, 6.5038e-03, 1.1715e-07,
        3.3838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 96, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0931, 0.5827, 0.0478, 0.0852, 0.0654, 0.0555, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 96, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.7734, 0.1185, 0.0680, 0.0308, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 96, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0644e-01, 7.4223e-03, 1.1111e-02, 6.7848e-06, 3.5436e-03, 3.3891e-03,
        8.6809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 96, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0505, 0.8675, 0.0036, 0.0613, 0.0033, 0.0100, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 97, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5977e-03, 9.8352e-01, 3.2066e-03, 2.6584e-06, 6.2740e-03, 1.1047e-07,
        3.3955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.412

[Epoch: 97, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0922, 0.5893, 0.0471, 0.0815, 0.0638, 0.0558, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 97, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7686, 0.1195, 0.0712, 0.0312, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 97, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0437e-01, 7.2794e-03, 1.0814e-02, 5.9821e-06, 3.3418e-03, 3.4316e-03,
        8.7075e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 97, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0497, 0.8520, 0.0034, 0.0782, 0.0032, 0.0097, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 98, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5212e-03, 9.8292e-01, 3.2074e-03, 2.5073e-06, 6.8016e-03, 1.0386e-07,
        3.5463e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 98, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0838, 0.6121, 0.0456, 0.0825, 0.0619, 0.0502, 0.0641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 98, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7716, 0.1207, 0.0681, 0.0300, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 98, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7435e-02, 7.2974e-03, 1.0338e-02, 5.7498e-06, 3.4811e-03, 3.5051e-03,
        8.7794e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 98, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0494, 0.8680, 0.0034, 0.0628, 0.0034, 0.0097, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 99, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8849e-03, 9.8214e-01, 3.5628e-03, 2.0565e-06, 6.7262e-03, 9.1025e-08,
        3.6876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.417

[Epoch: 99, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0837, 0.6023, 0.0481, 0.0987, 0.0575, 0.0461, 0.0637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 99, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7671, 0.1207, 0.0727, 0.0301, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 99, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9092e-02, 7.3469e-03, 1.0222e-02, 6.5600e-06, 3.5776e-03, 4.0569e-03,
        8.7570e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 99, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0481, 0.8720, 0.0035, 0.0616, 0.0032, 0.0088, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 100, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3582e-03, 9.8407e-01, 2.8338e-03, 2.1741e-06, 6.4100e-03, 1.0150e-07,
        3.3293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.431

[Epoch: 100, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0912, 0.5659, 0.0542, 0.0824, 0.0757, 0.0583, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 100, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7725, 0.1162, 0.0707, 0.0307, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 100, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0012e-01, 5.8669e-03, 9.6550e-03, 5.5100e-06, 3.1910e-03, 3.3871e-03,
        8.7778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 100, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0582, 0.8355, 0.0035, 0.0868, 0.0031, 0.0088, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 101, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3883e-03, 9.8277e-01, 3.1070e-03, 2.0866e-06, 7.2780e-03, 1.0714e-07,
        3.4558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 101, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1066, 0.6110, 0.0440, 0.0671, 0.0587, 0.0444, 0.0681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 101, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7624, 0.1280, 0.0702, 0.0299, 0.0030, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 101, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0601e-02, 6.4646e-03, 1.1437e-02, 4.7756e-06, 2.9225e-03, 3.4110e-03,
        8.8516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 101, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0420, 0.8876, 0.0033, 0.0452, 0.0027, 0.0158, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 102, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2380e-03, 9.8135e-01, 3.1939e-03, 2.7429e-06, 7.4164e-03, 1.4953e-07,
        3.8010e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 102, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0867, 0.5646, 0.0459, 0.0900, 0.0695, 0.0666, 0.0766],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 102, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7709, 0.1171, 0.0737, 0.0282, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 102, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7687e-02, 5.9742e-03, 8.2865e-03, 5.9026e-06, 3.0608e-03, 3.5541e-03,
        8.8143e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 102, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0468, 0.8642, 0.0036, 0.0700, 0.0036, 0.0081, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 103, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3788e-03, 9.8375e-01, 3.3434e-03, 1.7940e-06, 6.5262e-03, 1.1776e-07,
        2.9956e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 103, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0903, 0.5848, 0.0443, 0.0705, 0.0730, 0.0597, 0.0773],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 103, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7755, 0.1208, 0.0637, 0.0302, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.006

[Epoch: 103, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6840e-02, 6.1652e-03, 9.0591e-03, 4.2528e-06, 2.9624e-03, 3.8027e-03,
        8.8117e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 103, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0515, 0.8532, 0.0033, 0.0757, 0.0031, 0.0093, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 104, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3317e-03, 9.8355e-01, 3.1544e-03, 2.2193e-06, 6.5879e-03, 1.2190e-07,
        3.3708e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.434

[Epoch: 104, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0899, 0.6186, 0.0493, 0.0803, 0.0543, 0.0415, 0.0662],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 104, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7735, 0.1144, 0.0739, 0.0289, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.006

[Epoch: 104, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9833e-02, 8.9223e-03, 1.3307e-02, 5.0447e-06, 3.2850e-03, 3.1220e-03,
        8.7153e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 104, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0516, 0.8558, 0.0031, 0.0704, 0.0034, 0.0115, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 105, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4700e-03, 9.8269e-01, 3.2060e-03, 1.9573e-06, 6.7077e-03, 1.3087e-07,
        3.9288e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.409

[Epoch: 105, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0837, 0.5948, 0.0378, 0.1055, 0.0589, 0.0565, 0.0628],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 105, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7707, 0.1274, 0.0642, 0.0285, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 105, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1008e-01, 9.5063e-03, 1.0966e-02, 5.8153e-06, 3.8016e-03, 3.3000e-03,
        8.6234e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 105, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0546, 0.8722, 0.0042, 0.0485, 0.0037, 0.0128, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 106, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0917e-03, 9.8299e-01, 3.3515e-03, 1.5779e-06, 6.4455e-03, 9.4971e-08,
        4.1183e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.433

[Epoch: 106, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0985, 0.5817, 0.0465, 0.0710, 0.0704, 0.0618, 0.0701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 106, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.7620, 0.1097, 0.0871, 0.0315, 0.0030, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 106, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1048e-01, 6.9444e-03, 9.4180e-03, 4.8000e-06, 2.7462e-03, 4.3555e-03,
        8.6605e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 106, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0442, 0.8489, 0.0032, 0.0884, 0.0028, 0.0095, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 107, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9313e-03, 9.7868e-01, 4.5585e-03, 2.2867e-06, 8.2018e-03, 1.3075e-07,
        4.6254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.430

[Epoch: 107, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0801, 0.5911, 0.0531, 0.0921, 0.0594, 0.0488, 0.0754],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 107, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7772, 0.1242, 0.0611, 0.0268, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 107, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0178e-02, 7.2391e-03, 1.1533e-02, 5.3132e-06, 3.6505e-03, 3.5646e-03,
        8.8383e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 107, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0370, 0.8927, 0.0027, 0.0566, 0.0032, 0.0057, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 108, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2448e-03, 9.8494e-01, 2.6440e-03, 1.9152e-06, 6.8317e-03, 1.1662e-07,
        2.3412e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.416

[Epoch: 108, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1060, 0.6327, 0.0476, 0.0718, 0.0489, 0.0393, 0.0537],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 108, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7659, 0.1204, 0.0708, 0.0334, 0.0035, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 108, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.6161e-02, 4.4880e-03, 9.0230e-03, 7.0299e-06, 3.4206e-03, 3.1361e-03,
        8.9376e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 108, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0895, 0.8268, 0.0035, 0.0625, 0.0034, 0.0108, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 109, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1678e-03, 9.8339e-01, 3.0185e-03, 2.4850e-06, 6.6677e-03, 1.7379e-07,
        3.7541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.420

[Epoch: 109, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0858, 0.6360, 0.0367, 0.0621, 0.0661, 0.0514, 0.0617],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 109, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7789, 0.1145, 0.0700, 0.0282, 0.0026, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 109, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1700e-01, 5.4439e-03, 9.2720e-03, 5.7912e-06, 2.9871e-03, 4.4232e-03,
        8.6086e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 109, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0267, 0.8640, 0.0034, 0.0853, 0.0029, 0.0137, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 110, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.8781e-03, 9.8188e-01, 3.2182e-03, 2.4887e-06, 6.3831e-03, 1.4007e-07,
        3.6416e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 110, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0835, 0.5214, 0.0488, 0.1020, 0.0814, 0.0816, 0.0813],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 110, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7854, 0.1097, 0.0681, 0.0271, 0.0029, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.006

[Epoch: 110, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1974e-01, 9.6320e-03, 1.0599e-02, 5.7382e-06, 3.2715e-03, 4.2570e-03,
        8.5250e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 110, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0457, 0.8638, 0.0035, 0.0694, 0.0036, 0.0099, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 111, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5103e-03, 9.7852e-01, 4.6227e-03, 2.4475e-06, 8.2998e-03, 1.8080e-07,
        4.0479e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.372

[Epoch: 111, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0798, 0.5995, 0.0505, 0.0859, 0.0603, 0.0495, 0.0743],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 111, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.7435, 0.1408, 0.0713, 0.0337, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 111, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1536e-01, 6.3496e-03, 1.1640e-02, 6.8276e-06, 2.9853e-03, 3.3272e-03,
        8.6033e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 111, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0662, 0.8643, 0.0040, 0.0463, 0.0035, 0.0120, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 112, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.4551e-03, 9.8515e-01, 2.7990e-03, 2.5891e-06, 6.0826e-03, 1.3711e-07,
        3.5080e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.457

[Epoch: 112, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1122, 0.5977, 0.0428, 0.0641, 0.0649, 0.0537, 0.0647],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 112, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7859, 0.1000, 0.0738, 0.0314, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 112, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7741e-02, 6.4189e-03, 1.0546e-02, 7.0341e-06, 3.0084e-03, 3.6348e-03,
        8.7864e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 112, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0359, 0.8514, 0.0034, 0.0941, 0.0018, 0.0104, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 113, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3277e-03, 9.8496e-01, 2.5705e-03, 2.6591e-06, 5.4389e-03, 1.4421e-07,
        3.6965e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.432

[Epoch: 113, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0694, 0.6142, 0.0437, 0.1003, 0.0589, 0.0524, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 113, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7733, 0.1251, 0.0676, 0.0242, 0.0030, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 113, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8448e-02, 7.2369e-03, 1.0186e-02, 5.9305e-06, 2.9061e-03, 3.2696e-03,
        8.8795e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 113, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0489, 0.8740, 0.0035, 0.0570, 0.0041, 0.0087, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.002

[Epoch: 114, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2007e-03, 9.8091e-01, 3.8076e-03, 2.4722e-06, 7.6488e-03, 1.6561e-07,
        3.4323e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 114, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0832, 0.5795, 0.0594, 0.0880, 0.0605, 0.0493, 0.0802],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 114, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.7645, 0.1234, 0.0682, 0.0337, 0.0033, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 114, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.6404e-02, 7.0406e-03, 1.0567e-02, 4.9879e-06, 3.6956e-03, 3.1720e-03,
        8.8912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 114, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0564, 0.8620, 0.0038, 0.0600, 0.0032, 0.0104, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 115, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.0309e-03, 9.8572e-01, 3.2101e-03, 2.4305e-06, 5.7235e-03, 1.5456e-07,
        3.3120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 115, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1114, 0.5381, 0.0531, 0.0835, 0.0791, 0.0595, 0.0754],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 115, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7676, 0.1166, 0.0769, 0.0290, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 115, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0183e-01, 6.8041e-03, 1.2732e-02, 6.7353e-06, 3.0229e-03, 3.7330e-03,
        8.7187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 115, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0407, 0.8581, 0.0033, 0.0815, 0.0028, 0.0106, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 116, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1339e-03, 9.8455e-01, 2.7965e-03, 1.8934e-06, 5.9992e-03, 1.1475e-07,
        3.5177e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 116, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0860, 0.6081, 0.0380, 0.0851, 0.0597, 0.0571, 0.0661],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 116, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.7794, 0.1181, 0.0651, 0.0278, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 116, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3254e-02, 8.5303e-03, 1.1410e-02, 6.9676e-06, 3.6429e-03, 3.9911e-03,
        8.7916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 116, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0524, 0.8615, 0.0038, 0.0637, 0.0047, 0.0107, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 117, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8383e-03, 9.8052e-01, 3.6734e-03, 1.8090e-06, 8.4993e-03, 1.0899e-07,
        3.4706e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.439

[Epoch: 117, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0668, 0.6517, 0.0390, 0.0793, 0.0560, 0.0433, 0.0639],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 117, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.7691, 0.1180, 0.0719, 0.0306, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 117, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.3603e-02, 6.6423e-03, 9.9640e-03, 6.1812e-06, 3.3995e-03, 2.4938e-03,
        8.9389e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 117, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0535, 0.8660, 0.0049, 0.0577, 0.0037, 0.0098, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.001

[Epoch: 118, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2952e-03, 9.8147e-01, 3.6166e-03, 2.2987e-06, 8.1011e-03, 2.5780e-07,
        3.5183e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.394

[Epoch: 118, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1129, 0.5402, 0.0626, 0.0792, 0.0696, 0.0583, 0.0772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 118, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7670, 0.1210, 0.0714, 0.0307, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 118, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0051e-01, 4.6870e-03, 1.0755e-02, 4.5652e-06, 2.8487e-03, 2.8475e-03,
        8.7835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 118, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0431, 0.8758, 0.0028, 0.0657, 0.0020, 0.0080, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.006

[Epoch: 119, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.0396e-03, 9.8242e-01, 3.0541e-03, 2.5899e-06, 7.2927e-03, 2.2052e-07,
        3.1925e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.437

[Epoch: 119, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1034, 0.6146, 0.0327, 0.0750, 0.0516, 0.0543, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 119, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7749, 0.1233, 0.0641, 0.0284, 0.0030, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 119, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0537e-01, 5.0880e-03, 1.0908e-02, 5.1192e-06, 3.5661e-03, 4.4859e-03,
        8.7058e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 119, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0664, 0.8209, 0.0038, 0.0911, 0.0056, 0.0091, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 120, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1651e-03, 9.8388e-01, 3.6375e-03, 1.8497e-06, 6.3520e-03, 9.1049e-08,
        2.9655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.411

[Epoch: 120, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0697, 0.6133, 0.0580, 0.0965, 0.0652, 0.0384, 0.0588],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 120, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7776, 0.1143, 0.0697, 0.0276, 0.0031, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 120, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0443e-01, 7.5544e-03, 1.1205e-02, 5.9557e-06, 2.8414e-03, 3.1564e-03,
        8.7081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 120, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0398, 0.8981, 0.0044, 0.0378, 0.0022, 0.0147, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.029

[Epoch: 121, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5868e-03, 9.8579e-01, 2.4147e-03, 1.6331e-06, 5.5378e-03, 1.1519e-07,
        2.6671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.433

[Epoch: 121, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0893, 0.5443, 0.0598, 0.0926, 0.0759, 0.0669, 0.0713],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 121, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7363, 0.1513, 0.0772, 0.0253, 0.0027, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 121, batch: 156/198] total loss per batch: 0.778
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([5.3783e-02, 5.6141e-03, 7.6189e-03, 3.7349e-06, 3.6741e-03, 2.9462e-03,
        9.2636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 121, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0403, 0.8406, 0.0034, 0.0911, 0.0021, 0.0186, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.000

[Epoch: 122, batch: 39/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4309e-03, 9.8108e-01, 2.3098e-03, 1.8970e-06, 9.1605e-03, 3.1092e-07,
        4.0132e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.415

[Epoch: 122, batch: 78/198] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0868, 0.5822, 0.0372, 0.0948, 0.0651, 0.0609, 0.0730],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 122, batch: 117/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7992, 0.0829, 0.0730, 0.0345, 0.0025, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 122, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5233e-02, 5.2908e-03, 1.5661e-02, 3.2525e-06, 3.6608e-03, 3.0672e-03,
        8.7708e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 122, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0454, 0.8747, 0.0049, 0.0609, 0.0021, 0.0092, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.027

[Epoch: 123, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.9615e-03, 9.8330e-01, 3.1481e-03, 1.6706e-06, 6.4432e-03, 2.7717e-07,
        3.1443e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 123, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0900, 0.6133, 0.0431, 0.0764, 0.0514, 0.0532, 0.0726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 123, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.8064, 0.1098, 0.0508, 0.0249, 0.0026, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.021

[Epoch: 123, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6495e-02, 5.9364e-03, 1.0526e-02, 3.2039e-06, 3.4522e-03, 3.3868e-03,
        8.8020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 123, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0542, 0.8654, 0.0059, 0.0611, 0.0030, 0.0075, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 124, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.6702e-03, 9.8390e-01, 2.7738e-03, 1.5966e-06, 6.7301e-03, 2.2218e-07,
        3.9289e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.426

[Epoch: 124, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0814, 0.6373, 0.0445, 0.0672, 0.0575, 0.0443, 0.0678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.095

[Epoch: 124, batch: 117/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.7526, 0.1157, 0.0800, 0.0421, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 124, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0039e-01, 7.6934e-03, 1.0104e-02, 3.0965e-06, 4.4541e-03, 3.5676e-03,
        8.7379e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 124, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0528, 0.8492, 0.0048, 0.0794, 0.0023, 0.0078, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 125, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3222e-03, 9.8264e-01, 2.6963e-03, 1.5738e-06, 7.2284e-03, 2.4677e-07,
        4.1102e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.428

[Epoch: 125, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0820, 0.6066, 0.0467, 0.0811, 0.0629, 0.0551, 0.0656],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 125, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7824, 0.1135, 0.0602, 0.0326, 0.0038, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.007

[Epoch: 125, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9671e-02, 6.8660e-03, 9.9413e-03, 3.0878e-06, 3.9819e-03, 4.1095e-03,
        8.7543e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 125, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0488, 0.8683, 0.0044, 0.0636, 0.0024, 0.0092, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 126, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.9445e-03, 9.8298e-01, 3.0406e-03, 1.3697e-06, 7.3575e-03, 2.0878e-07,
        3.6733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 126, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0947, 0.5701, 0.0505, 0.0879, 0.0698, 0.0512, 0.0758],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 126, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7685, 0.1216, 0.0702, 0.0308, 0.0030, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 126, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6224e-02, 5.2882e-03, 1.0901e-02, 2.6038e-06, 3.2466e-03, 3.4961e-03,
        8.8084e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 126, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0508, 0.8638, 0.0044, 0.0671, 0.0026, 0.0080, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 127, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0364e-03, 9.8357e-01, 2.9682e-03, 1.3480e-06, 7.0674e-03, 1.8490e-07,
        3.3605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.426

[Epoch: 127, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0935, 0.5943, 0.0456, 0.0804, 0.0642, 0.0515, 0.0706],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 127, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7669, 0.1200, 0.0721, 0.0318, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 127, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5580e-02, 5.2258e-03, 1.0716e-02, 2.4144e-06, 3.2133e-03, 3.1066e-03,
        8.8216e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 127, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0496, 0.8680, 0.0042, 0.0634, 0.0026, 0.0090, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 128, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4600e-03, 9.8318e-01, 3.1158e-03, 1.3480e-06, 6.9067e-03, 1.4187e-07,
        3.3325e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 128, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0877, 0.5982, 0.0467, 0.0852, 0.0649, 0.0495, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 128, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7662, 0.1245, 0.0702, 0.0302, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 128, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.8438e-02, 7.0990e-03, 9.5782e-03, 2.3412e-06, 3.1889e-03, 3.4575e-03,
        8.7824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 128, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0499, 0.8535, 0.0041, 0.0752, 0.0028, 0.0103, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 129, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3018e-03, 9.8343e-01, 2.9689e-03, 1.1881e-06, 6.6688e-03, 1.7469e-07,
        3.6308e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 129, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0889, 0.5830, 0.0484, 0.0917, 0.0644, 0.0542, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 129, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7764, 0.1131, 0.0691, 0.0314, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 129, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.4898e-02, 8.5265e-03, 1.0964e-02, 2.1204e-06, 2.9121e-03, 2.8526e-03,
        8.7984e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 129, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0542, 0.8606, 0.0042, 0.0624, 0.0034, 0.0114, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 130, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.6822e-03, 9.8235e-01, 3.0929e-03, 1.1329e-06, 7.3768e-03, 1.3865e-07,
        3.5007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.420

[Epoch: 130, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0931, 0.5684, 0.0460, 0.0921, 0.0720, 0.0554, 0.0730],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 130, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7615, 0.1267, 0.0707, 0.0312, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 130, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.4102e-02, 7.4281e-03, 1.1441e-02, 2.1028e-06, 3.1330e-03, 2.4779e-03,
        8.8142e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 130, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0497, 0.8573, 0.0039, 0.0725, 0.0034, 0.0104, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 131, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0604e-03, 9.8462e-01, 2.7958e-03, 1.0319e-06, 6.2898e-03, 1.2790e-07,
        3.2372e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.420

[Epoch: 131, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0882, 0.5958, 0.0486, 0.0773, 0.0662, 0.0573, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 131, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7771, 0.1154, 0.0688, 0.0295, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.006

[Epoch: 131, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0919e-01, 4.5070e-03, 9.2291e-03, 2.5699e-06, 3.6592e-03, 3.6447e-03,
        8.6977e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 131, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0392, 0.8870, 0.0033, 0.0568, 0.0026, 0.0087, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 132, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4424e-03, 9.8454e-01, 3.0003e-03, 9.4951e-07, 6.0915e-03, 1.0617e-07,
        2.9277e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 132, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0704, 0.6483, 0.0438, 0.0744, 0.0544, 0.0465, 0.0621],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 132, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7616, 0.1256, 0.0710, 0.0327, 0.0033, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 132, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5761e-02, 5.8504e-03, 7.3255e-03, 2.5747e-06, 3.3729e-03, 3.7721e-03,
        8.8392e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 132, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0555, 0.8431, 0.0039, 0.0815, 0.0037, 0.0081, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 133, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.7436e-03, 9.8157e-01, 2.7305e-03, 1.0199e-06, 8.2884e-03, 1.1887e-07,
        3.6685e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.402

[Epoch: 133, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0745, 0.6096, 0.0678, 0.0816, 0.0578, 0.0387, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 133, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7732, 0.1165, 0.0712, 0.0288, 0.0037, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 133, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.8728e-02, 4.6745e-03, 9.5328e-03, 2.2590e-06, 2.5896e-03, 3.3412e-03,
        9.0113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 133, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0581, 0.8670, 0.0049, 0.0461, 0.0032, 0.0165, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 134, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3371e-03, 9.8373e-01, 2.8531e-03, 1.6779e-06, 6.6060e-03, 1.9426e-07,
        3.4733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.431

[Epoch: 134, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1393, 0.4715, 0.0553, 0.1065, 0.0765, 0.0669, 0.0839],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 134, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7858, 0.1143, 0.0640, 0.0259, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 134, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1243e-01, 1.0374e-02, 1.2946e-02, 2.6211e-06, 2.5522e-03, 1.8126e-03,
        8.5988e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 134, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0247, 0.8676, 0.0035, 0.0899, 0.0027, 0.0092, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 135, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1995e-03, 9.8276e-01, 3.9090e-03, 2.5784e-06, 6.3686e-03, 1.6251e-07,
        3.7587e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 135, batch: 78/198] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0969, 0.5786, 0.0429, 0.0635, 0.0672, 0.0716, 0.0792],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.123

[Epoch: 135, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.7498, 0.1229, 0.0820, 0.0352, 0.0029, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.005

[Epoch: 135, batch: 156/198] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5453e-02, 6.1183e-03, 1.2651e-02, 2.0068e-06, 3.3347e-03, 4.8196e-03,
        8.7762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 135, batch: 195/198] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0774, 0.8288, 0.0031, 0.0705, 0.0042, 0.0104, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.034

[Epoch: 136, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([6.6940e-03, 9.7621e-01, 4.3416e-03, 2.2913e-06, 8.8505e-03, 1.0640e-07,
        3.8978e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.409

[Epoch: 136, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0627, 0.6419, 0.0327, 0.0793, 0.0645, 0.0557, 0.0633],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.111

[Epoch: 136, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7724, 0.1228, 0.0653, 0.0294, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 136, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0735e-01, 5.3504e-03, 9.3611e-03, 3.5412e-06, 2.9823e-03, 4.1770e-03,
        8.7078e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 136, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0463, 0.8810, 0.0031, 0.0551, 0.0028, 0.0089, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 137, batch: 39/198] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2144e-03, 9.8477e-01, 2.3696e-03, 1.7614e-06, 6.6480e-03, 2.1193e-07,
        2.9952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.438

[Epoch: 137, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0724, 0.6535, 0.0472, 0.0724, 0.0518, 0.0432, 0.0594],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 137, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7831, 0.1117, 0.0665, 0.0290, 0.0035, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 137, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7532e-02, 6.7536e-03, 9.2313e-03, 2.9109e-06, 3.0624e-03, 2.7319e-03,
        8.8069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 137, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0452, 0.8584, 0.0044, 0.0779, 0.0037, 0.0078, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 138, batch: 39/198] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.0537e-03, 9.8418e-01, 2.7305e-03, 1.5169e-06, 6.1708e-03, 1.5605e-07,
        2.8639e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.437

[Epoch: 138, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0937, 0.5678, 0.0601, 0.0868, 0.0692, 0.0489, 0.0735],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 138, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7750, 0.1150, 0.0683, 0.0312, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 138, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6519e-02, 6.5300e-03, 1.1378e-02, 2.5610e-06, 3.4739e-03, 3.3697e-03,
        8.7873e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 138, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0526, 0.8732, 0.0031, 0.0529, 0.0039, 0.0109, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 139, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.3164e-03, 9.8259e-01, 3.1724e-03, 2.2479e-06, 6.2938e-03, 1.9786e-07,
        3.6295e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.403

[Epoch: 139, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0860, 0.6051, 0.0454, 0.0780, 0.0610, 0.0547, 0.0698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 139, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7667, 0.1199, 0.0730, 0.0307, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.003

[Epoch: 139, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0202e-01, 6.2481e-03, 9.9476e-03, 2.3938e-06, 2.9092e-03, 3.3273e-03,
        8.7555e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 139, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0533, 0.8514, 0.0041, 0.0744, 0.0046, 0.0084, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 140, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1642e-03, 9.8196e-01, 3.4781e-03, 1.4619e-06, 7.9497e-03, 1.0507e-07,
        3.4493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.432

[Epoch: 140, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0909, 0.5993, 0.0361, 0.0873, 0.0582, 0.0537, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 140, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7697, 0.1183, 0.0703, 0.0315, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 140, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0660e-01, 7.5122e-03, 1.0384e-02, 2.4810e-06, 3.0938e-03, 3.1276e-03,
        8.6928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 140, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0421, 0.8746, 0.0038, 0.0653, 0.0032, 0.0085, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 141, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3006e-03, 9.8367e-01, 2.9470e-03, 1.4159e-06, 6.7338e-03, 1.1568e-07,
        3.3467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 141, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0961, 0.5724, 0.0504, 0.0906, 0.0659, 0.0562, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 141, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7687, 0.1185, 0.0730, 0.0300, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 141, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0580e-01, 8.1967e-03, 1.2396e-02, 2.0993e-06, 3.0183e-03, 2.7886e-03,
        8.6779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 141, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0576, 0.8530, 0.0046, 0.0661, 0.0041, 0.0110, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 142, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2524e-03, 9.8177e-01, 3.2299e-03, 1.7823e-06, 6.8647e-03, 1.4299e-07,
        3.8822e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 142, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0981, 0.5855, 0.0405, 0.0810, 0.0681, 0.0546, 0.0722],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 142, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7772, 0.1179, 0.0670, 0.0282, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 142, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0159e-01, 7.8752e-03, 1.1823e-02, 2.3728e-06, 3.8857e-03, 3.7494e-03,
        8.7107e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 142, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0455, 0.8701, 0.0035, 0.0639, 0.0034, 0.0098, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 143, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3442e-03, 9.8479e-01, 2.8419e-03, 1.5596e-06, 5.8485e-03, 1.1579e-07,
        3.1729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.393

[Epoch: 143, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0976, 0.5930, 0.0473, 0.0731, 0.0646, 0.0532, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 143, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7610, 0.1234, 0.0755, 0.0305, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 143, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0230e-01, 6.5724e-03, 1.0723e-02, 2.3789e-06, 3.5873e-03, 4.0346e-03,
        8.7278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 143, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0537, 0.8594, 0.0033, 0.0695, 0.0027, 0.0088, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 144, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.9787e-03, 9.8454e-01, 2.8901e-03, 1.2357e-06, 7.0773e-03, 5.9926e-08,
        2.5147e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.431

[Epoch: 144, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0751, 0.6474, 0.0412, 0.0775, 0.0521, 0.0474, 0.0592],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 144, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7779, 0.1161, 0.0669, 0.0296, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 144, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0938e-01, 8.2330e-03, 1.1895e-02, 2.2987e-06, 3.1299e-03, 3.3072e-03,
        8.6406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 144, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0448, 0.8716, 0.0033, 0.0657, 0.0032, 0.0090, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 145, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.1265e-03, 9.8122e-01, 3.4223e-03, 1.5567e-06, 7.9431e-03, 7.7372e-08,
        3.2881e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.444

[Epoch: 145, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0824, 0.6146, 0.0489, 0.0912, 0.0537, 0.0495, 0.0596],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 145, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7622, 0.1240, 0.0721, 0.0307, 0.0034, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 145, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1058e-01, 5.7309e-03, 9.1388e-03, 2.5835e-06, 3.6122e-03, 3.0411e-03,
        8.6790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 145, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0493, 0.8658, 0.0042, 0.0598, 0.0043, 0.0124, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 146, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2068e-03, 9.8205e-01, 3.3768e-03, 1.7075e-06, 6.0795e-03, 1.4650e-07,
        4.2862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.416

[Epoch: 146, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0994, 0.5544, 0.0436, 0.0771, 0.0809, 0.0668, 0.0778],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 146, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.7614, 0.1236, 0.0735, 0.0295, 0.0035, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 146, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0257e-01, 6.2410e-03, 8.7953e-03, 2.7883e-06, 3.2543e-03, 4.2625e-03,
        8.7487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 146, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0520, 0.8568, 0.0029, 0.0716, 0.0026, 0.0093, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 147, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3809e-03, 9.8494e-01, 3.2221e-03, 1.8436e-06, 5.4787e-03, 1.3504e-07,
        2.9760e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.385

[Epoch: 147, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1038, 0.5449, 0.0444, 0.0883, 0.0753, 0.0557, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 147, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.7755, 0.1029, 0.0780, 0.0336, 0.0039, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.001

[Epoch: 147, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5901e-02, 6.8459e-03, 9.7071e-03, 2.7264e-06, 2.9322e-03, 1.6410e-03,
        8.8297e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 147, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0557, 0.8488, 0.0027, 0.0748, 0.0041, 0.0112, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 148, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0183e-03, 9.8385e-01, 2.9941e-03, 1.4523e-06, 7.5845e-03, 6.7769e-08,
        2.5518e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.440

[Epoch: 148, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0907, 0.5948, 0.0437, 0.0865, 0.0654, 0.0533, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 148, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.7771, 0.1138, 0.0662, 0.0314, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 148, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7976e-02, 5.1690e-03, 1.1886e-02, 3.5681e-06, 4.4427e-03, 2.6390e-03,
        8.7788e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 148, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0470, 0.8735, 0.0037, 0.0611, 0.0020, 0.0097, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 149, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8458e-03, 9.8194e-01, 4.1466e-03, 1.7419e-06, 6.8717e-03, 9.5162e-08,
        3.1920e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.411

[Epoch: 149, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0837, 0.5893, 0.0468, 0.0953, 0.0620, 0.0544, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.093

[Epoch: 149, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.7627, 0.1390, 0.0671, 0.0218, 0.0023, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 149, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8046e-02, 7.1908e-03, 8.3055e-03, 3.3830e-06, 3.1850e-03, 5.3656e-03,
        8.8790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 149, batch: 195/198] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0451, 0.8511, 0.0033, 0.0768, 0.0057, 0.0125, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.009

[Epoch: 150, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.6284e-03, 9.8358e-01, 2.8244e-03, 1.7334e-06, 4.8115e-03, 2.0448e-07,
        4.1488e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.428

[Epoch: 150, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0782, 0.6288, 0.0575, 0.0659, 0.0556, 0.0489, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.093

[Epoch: 150, batch: 117/198] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7650, 0.1126, 0.0717, 0.0414, 0.0026, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 150, batch: 156/198] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.7444e-02, 6.3812e-03, 1.0418e-02, 2.1397e-06, 2.6714e-03, 3.7203e-03,
        8.8936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 150, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0585, 0.8691, 0.0035, 0.0543, 0.0020, 0.0101, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 151, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.6837e-03, 9.8370e-01, 2.9340e-03, 1.3748e-06, 7.5683e-03, 8.7240e-08,
        3.1174e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.420

[Epoch: 151, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0962, 0.5743, 0.0414, 0.0877, 0.0679, 0.0560, 0.0765],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 151, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.7754, 0.1204, 0.0686, 0.0250, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 151, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2143e-01, 8.4014e-03, 1.3484e-02, 2.9567e-06, 3.7221e-03, 3.1794e-03,
        8.4978e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 151, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0422, 0.8665, 0.0040, 0.0698, 0.0042, 0.0107, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 152, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2533e-03, 9.8454e-01, 3.1910e-03, 1.6387e-06, 5.4090e-03, 8.5413e-08,
        3.6051e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.426

[Epoch: 152, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0853, 0.5999, 0.0486, 0.0823, 0.0617, 0.0524, 0.0699],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.097

[Epoch: 152, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.7689, 0.1262, 0.0690, 0.0258, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 152, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9886e-02, 7.0214e-03, 1.0049e-02, 3.4562e-06, 3.8608e-03, 4.2441e-03,
        8.7493e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 152, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0504, 0.8712, 0.0032, 0.0607, 0.0025, 0.0095, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 153, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4245e-03, 9.8373e-01, 3.2663e-03, 1.3211e-06, 6.1251e-03, 9.8842e-08,
        3.4570e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 153, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0858, 0.6064, 0.0439, 0.0828, 0.0622, 0.0505, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 153, batch: 117/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.7735, 0.1190, 0.0691, 0.0283, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 153, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0116e-01, 5.8874e-03, 9.6001e-03, 2.6920e-06, 3.3109e-03, 3.4035e-03,
        8.7663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 153, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0528, 0.8649, 0.0030, 0.0657, 0.0025, 0.0087, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 154, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5763e-03, 9.8120e-01, 3.5857e-03, 1.3516e-06, 7.7773e-03, 9.9838e-08,
        3.8551e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 154, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0928, 0.5952, 0.0456, 0.0818, 0.0649, 0.0508, 0.0690],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 154, batch: 117/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7735, 0.1174, 0.0702, 0.0289, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 154, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0626e-01, 6.8565e-03, 1.0466e-02, 2.5586e-06, 3.0732e-03, 3.3000e-03,
        8.7004e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 154, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0482, 0.8642, 0.0033, 0.0688, 0.0033, 0.0097, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 155, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8246e-03, 9.8237e-01, 3.5574e-03, 1.2449e-06, 6.4711e-03, 1.0814e-07,
        3.7766e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 155, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0885, 0.5760, 0.0509, 0.0861, 0.0718, 0.0540, 0.0727],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 155, batch: 117/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7729, 0.1186, 0.0692, 0.0297, 0.0030, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 155, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0271e-01, 6.5294e-03, 9.6503e-03, 2.3044e-06, 3.3014e-03, 3.8359e-03,
        8.7397e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 155, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0534, 0.8513, 0.0035, 0.0742, 0.0037, 0.0107, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 156, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5443e-03, 9.8246e-01, 3.4411e-03, 1.1464e-06, 6.6658e-03, 9.7531e-08,
        3.8909e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 156, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0871, 0.5868, 0.0482, 0.0871, 0.0646, 0.0564, 0.0697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 156, batch: 117/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7662, 0.1222, 0.0705, 0.0312, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.011

[Epoch: 156, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0371e-01, 6.7437e-03, 1.0850e-02, 2.1569e-06, 3.4788e-03, 3.5118e-03,
        8.7171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 156, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0491, 0.8650, 0.0037, 0.0637, 0.0036, 0.0113, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 157, batch: 39/198] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5323e-03, 9.8157e-01, 3.3690e-03, 1.0695e-06, 7.7525e-03, 9.8335e-08,
        3.7729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 157, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0949, 0.5885, 0.0436, 0.0892, 0.0566, 0.0587, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 157, batch: 117/198] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7678, 0.1238, 0.0687, 0.0296, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 157, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9305e-02, 6.9236e-03, 1.0118e-02, 2.3942e-06, 3.3368e-03, 3.1074e-03,
        8.7721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 157, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0433, 0.8736, 0.0036, 0.0630, 0.0031, 0.0102, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 158, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.1149e-03, 9.8255e-01, 3.0091e-03, 1.0538e-06, 6.7847e-03, 8.0754e-08,
        3.5353e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.421

[Epoch: 158, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0929, 0.5976, 0.0475, 0.0722, 0.0596, 0.0587, 0.0715],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 158, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7730, 0.1161, 0.0700, 0.0310, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 158, batch: 156/198] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.4479e-02, 6.6748e-03, 1.0450e-02, 2.1836e-06, 3.2325e-03, 3.2221e-03,
        8.8194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 158, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0564, 0.8542, 0.0032, 0.0707, 0.0029, 0.0094, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 159, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.6729e-03, 9.8698e-01, 2.7353e-03, 9.8774e-07, 4.9089e-03, 7.7496e-08,
        2.6982e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 159, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0847, 0.5982, 0.0490, 0.0815, 0.0697, 0.0483, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 159, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7729, 0.1243, 0.0660, 0.0273, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 159, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.9599e-02, 5.6915e-03, 9.0857e-03, 2.0385e-06, 2.8409e-03, 2.7125e-03,
        9.0007e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 159, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0511, 0.8632, 0.0038, 0.0651, 0.0043, 0.0097, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 160, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1993e-03, 9.8399e-01, 2.9003e-03, 9.4122e-07, 6.6176e-03, 5.8810e-08,
        3.2946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.428

[Epoch: 160, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0953, 0.5757, 0.0505, 0.0907, 0.0687, 0.0505, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 160, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7721, 0.1150, 0.0749, 0.0284, 0.0034, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 160, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6657e-02, 7.6115e-03, 1.2095e-02, 1.5591e-06, 3.1169e-03, 3.1038e-03,
        8.7741e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 160, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0413, 0.8872, 0.0032, 0.0552, 0.0024, 0.0081, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 161, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.1429e-03, 9.8387e-01, 3.1784e-03, 1.0592e-06, 7.0027e-03, 6.8738e-08,
        3.8074e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.408

[Epoch: 161, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0838, 0.6431, 0.0387, 0.0770, 0.0467, 0.0429, 0.0679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 161, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7641, 0.1326, 0.0665, 0.0265, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 161, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1893e-01, 8.3031e-03, 1.3156e-02, 2.2317e-06, 3.6977e-03, 4.2034e-03,
        8.5171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 161, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0513, 0.8295, 0.0038, 0.0945, 0.0040, 0.0125, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 162, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.0883e-03, 9.7713e-01, 4.1450e-03, 1.6295e-06, 9.0842e-03, 1.2025e-07,
        4.5464e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 162, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0776, 0.6696, 0.0391, 0.0555, 0.0487, 0.0481, 0.0614],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.096

[Epoch: 162, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7752, 0.0923, 0.0797, 0.0392, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.005

[Epoch: 162, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7749e-02, 7.1425e-03, 1.1321e-02, 2.8164e-06, 4.1038e-03, 3.9409e-03,
        8.7574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 162, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0561, 0.8737, 0.0026, 0.0500, 0.0035, 0.0108, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.025

[Epoch: 163, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3887e-03, 9.8279e-01, 3.9780e-03, 1.6239e-06, 5.7891e-03, 1.1753e-07,
        4.0515e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.432

[Epoch: 163, batch: 78/198] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0914, 0.5740, 0.0390, 0.0940, 0.0688, 0.0605, 0.0722],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.089

[Epoch: 163, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.7560, 0.1324, 0.0676, 0.0323, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.006

[Epoch: 163, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.2311e-02, 4.3253e-03, 6.5606e-03, 3.6535e-06, 3.1072e-03, 2.2998e-03,
        8.9139e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 163, batch: 195/198] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0440, 0.8798, 0.0028, 0.0598, 0.0024, 0.0094, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 164, batch: 39/198] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3221e-03, 9.8495e-01, 3.3288e-03, 1.2498e-06, 4.9877e-03, 1.0964e-07,
        3.4055e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.409

[Epoch: 164, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0997, 0.5874, 0.0457, 0.0764, 0.0650, 0.0548, 0.0710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 164, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7672, 0.1176, 0.0707, 0.0351, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 164, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0614e-01, 6.4765e-03, 8.0249e-03, 2.5430e-06, 3.1031e-03, 1.9674e-03,
        8.7428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 164, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0578, 0.8416, 0.0032, 0.0811, 0.0036, 0.0100, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 165, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8715e-03, 9.8525e-01, 3.0308e-03, 1.2752e-06, 5.7972e-03, 1.1069e-07,
        3.0512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 165, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0927, 0.5764, 0.0639, 0.0825, 0.0654, 0.0555, 0.0636],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 165, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7766, 0.1185, 0.0646, 0.0290, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.004

[Epoch: 165, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0483e-01, 8.2918e-03, 1.1665e-02, 3.2111e-06, 3.2178e-03, 3.4871e-03,
        8.6851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 165, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0517, 0.8512, 0.0046, 0.0679, 0.0047, 0.0150, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 166, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2390e-03, 9.8361e-01, 3.0619e-03, 1.1551e-06, 6.5750e-03, 1.1992e-07,
        3.5099e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 166, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0954, 0.5580, 0.0434, 0.1000, 0.0705, 0.0544, 0.0781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.106

[Epoch: 166, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7756, 0.1186, 0.0676, 0.0285, 0.0032, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 166, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1171e-01, 7.9318e-03, 1.3527e-02, 2.9215e-06, 3.5689e-03, 3.6850e-03,
        8.5958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 166, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0496, 0.8693, 0.0033, 0.0613, 0.0027, 0.0108, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 167, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2933e-03, 9.8385e-01, 2.8934e-03, 1.1052e-06, 6.7633e-03, 8.7478e-08,
        3.1964e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 167, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0860, 0.5943, 0.0426, 0.0859, 0.0634, 0.0575, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 167, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7703, 0.1192, 0.0727, 0.0288, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.011

[Epoch: 167, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0800e-01, 7.2733e-03, 9.6300e-03, 2.4212e-06, 3.4028e-03, 4.2696e-03,
        8.6742e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 167, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0422, 0.8787, 0.0030, 0.0634, 0.0022, 0.0080, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 168, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2631e-03, 9.8430e-01, 3.0062e-03, 1.0589e-06, 6.2467e-03, 7.0603e-08,
        3.1841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 168, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0799, 0.6105, 0.0536, 0.0788, 0.0602, 0.0507, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 168, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7645, 0.1217, 0.0701, 0.0331, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 168, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8862e-02, 7.0717e-03, 1.0252e-02, 2.2595e-06, 3.1627e-03, 3.8900e-03,
        8.8676e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 168, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0549, 0.8553, 0.0033, 0.0718, 0.0030, 0.0085, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 169, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5794e-03, 9.8225e-01, 3.4604e-03, 1.2476e-06, 6.9589e-03, 9.5529e-08,
        3.7527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.415

[Epoch: 169, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0871, 0.6188, 0.0461, 0.0800, 0.0603, 0.0431, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 169, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7638, 0.1262, 0.0688, 0.0305, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 169, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0062e-01, 7.2636e-03, 1.0848e-02, 2.4273e-06, 3.1313e-03, 2.7797e-03,
        8.7535e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 169, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0560, 0.8583, 0.0036, 0.0605, 0.0044, 0.0128, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 170, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.8720e-03, 9.8046e-01, 3.8374e-03, 1.3528e-06, 7.7038e-03, 1.1804e-07,
        4.1293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 170, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0980, 0.5724, 0.0416, 0.0832, 0.0738, 0.0570, 0.0739],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 170, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7663, 0.1209, 0.0739, 0.0281, 0.0036, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.013

[Epoch: 170, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.5171e-02, 5.6109e-03, 9.1343e-03, 2.6078e-06, 4.0239e-03, 2.4476e-03,
        8.8361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 170, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0430, 0.8598, 0.0033, 0.0766, 0.0032, 0.0107, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 171, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.0569e-03, 9.8181e-01, 3.6699e-03, 1.0544e-06, 6.8036e-03, 7.5230e-08,
        3.6628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 171, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0893, 0.5672, 0.0500, 0.0868, 0.0632, 0.0741, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 171, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7786, 0.1126, 0.0662, 0.0324, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.012

[Epoch: 171, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([7.5508e-02, 4.7050e-03, 8.0928e-03, 2.4153e-06, 3.5139e-03, 3.6129e-03,
        9.0457e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 171, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0523, 0.8660, 0.0035, 0.0643, 0.0027, 0.0086, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 172, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0592e-03, 9.8498e-01, 2.8130e-03, 1.0630e-06, 6.2871e-03, 8.3665e-08,
        2.8644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 172, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0916, 0.5805, 0.0515, 0.0981, 0.0590, 0.0478, 0.0713],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 172, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7655, 0.1201, 0.0742, 0.0310, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.010

[Epoch: 172, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.9696e-02, 5.6185e-03, 8.0753e-03, 2.7529e-06, 2.6248e-03, 2.6933e-03,
        8.9129e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 172, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0452, 0.8653, 0.0037, 0.0676, 0.0043, 0.0101, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.006

[Epoch: 173, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2468e-03, 9.8578e-01, 2.3203e-03, 1.0706e-06, 5.7694e-03, 1.2476e-07,
        2.8867e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.409

[Epoch: 173, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0972, 0.5985, 0.0478, 0.0808, 0.0619, 0.0488, 0.0649],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 173, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7706, 0.1258, 0.0650, 0.0279, 0.0040, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.012

[Epoch: 173, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1402e-01, 8.2904e-03, 1.0301e-02, 2.8841e-06, 3.7353e-03, 2.8834e-03,
        8.6077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 173, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0541, 0.8557, 0.0040, 0.0644, 0.0039, 0.0138, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 174, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1108e-03, 9.8608e-01, 2.5301e-03, 1.2937e-06, 4.8241e-03, 5.8483e-08,
        3.4574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.425

[Epoch: 174, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0856, 0.5953, 0.0378, 0.0748, 0.0677, 0.0663, 0.0725],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.102

[Epoch: 174, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7712, 0.1115, 0.0792, 0.0282, 0.0028, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 174, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1326e-01, 8.0427e-03, 1.2578e-02, 2.4612e-06, 4.0037e-03, 5.0060e-03,
        8.5711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 174, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0503, 0.8659, 0.0034, 0.0676, 0.0022, 0.0073, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.001

[Epoch: 175, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0678e-03, 9.8326e-01, 3.3618e-03, 1.0776e-06, 7.0518e-03, 5.2544e-08,
        3.2540e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 175, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0917, 0.6099, 0.0412, 0.0774, 0.0538, 0.0443, 0.0817],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 175, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7644, 0.1248, 0.0700, 0.0303, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.001

[Epoch: 175, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.1120e-01, 4.7796e-03, 1.1390e-02, 2.8465e-06, 3.2909e-03, 4.5265e-03,
        8.6481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 175, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0447, 0.8811, 0.0031, 0.0565, 0.0029, 0.0092, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 176, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2164e-03, 9.8298e-01, 3.2943e-03, 1.4842e-06, 7.1260e-03, 1.1390e-07,
        3.3768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.418

[Epoch: 176, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0805, 0.6208, 0.0455, 0.0853, 0.0637, 0.0421, 0.0622],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 176, batch: 117/198] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7780, 0.1147, 0.0646, 0.0331, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 176, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0433e-01, 6.0830e-03, 9.8286e-03, 2.9105e-06, 2.8891e-03, 2.6069e-03,
        8.7426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 176, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0473, 0.8655, 0.0037, 0.0681, 0.0038, 0.0086, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 177, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.5263e-03, 9.8202e-01, 3.5681e-03, 1.8873e-06, 5.4274e-03, 1.3915e-07,
        4.4518e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 177, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0771, 0.6219, 0.0453, 0.0714, 0.0646, 0.0543, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 177, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.7515, 0.1279, 0.0774, 0.0323, 0.0034, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 177, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9868e-02, 6.4127e-03, 8.5524e-03, 2.6555e-06, 3.2836e-03, 3.4490e-03,
        8.7843e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 177, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0633, 0.8394, 0.0034, 0.0781, 0.0031, 0.0094, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 178, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4679e-03, 9.8081e-01, 3.6560e-03, 1.3975e-06, 7.7487e-03, 1.0975e-07,
        4.3121e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.415

[Epoch: 178, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0979, 0.5839, 0.0437, 0.0909, 0.0603, 0.0541, 0.0692],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 178, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7777, 0.1137, 0.0682, 0.0309, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 178, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0074e-01, 6.6230e-03, 1.0109e-02, 2.0542e-06, 3.0599e-03, 3.2508e-03,
        8.7621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 178, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0406, 0.8712, 0.0033, 0.0673, 0.0034, 0.0102, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 179, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3395e-03, 9.8260e-01, 3.3068e-03, 1.3997e-06, 7.3515e-03, 1.0600e-07,
        3.4039e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 179, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0944, 0.5636, 0.0468, 0.0939, 0.0672, 0.0539, 0.0803],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 179, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7644, 0.1235, 0.0715, 0.0311, 0.0031, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 179, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8622e-02, 5.3730e-03, 9.3560e-03, 2.5943e-06, 3.0799e-03, 2.7968e-03,
        8.9077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 179, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0570, 0.8549, 0.0040, 0.0655, 0.0043, 0.0108, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 180, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.2772e-03, 9.8353e-01, 3.2313e-03, 1.2486e-06, 6.4004e-03, 8.5671e-08,
        3.5645e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.423

[Epoch: 180, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0929, 0.5731, 0.0540, 0.0780, 0.0723, 0.0631, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.110

[Epoch: 180, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7757, 0.1149, 0.0707, 0.0294, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 180, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7401e-02, 7.9138e-03, 9.6826e-03, 3.0227e-06, 2.9506e-03, 3.2216e-03,
        8.7883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 180, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0452, 0.8714, 0.0043, 0.0617, 0.0032, 0.0107, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 181, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3489e-03, 9.8440e-01, 2.9052e-03, 1.1935e-06, 5.9462e-03, 9.0413e-08,
        3.3943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 181, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0949, 0.5593, 0.0538, 0.0909, 0.0708, 0.0567, 0.0736],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.112

[Epoch: 181, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.7838, 0.1161, 0.0633, 0.0278, 0.0030, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 181, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0274e-01, 8.7137e-03, 1.0307e-02, 2.4983e-06, 3.6834e-03, 3.7703e-03,
        8.7078e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 181, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0454, 0.8653, 0.0033, 0.0708, 0.0026, 0.0090, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 182, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1278e-03, 9.8450e-01, 2.8870e-03, 1.0984e-06, 6.5905e-03, 7.8338e-08,
        2.8975e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 182, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0882, 0.6024, 0.0462, 0.0827, 0.0592, 0.0482, 0.0732],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.107

[Epoch: 182, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.7619, 0.1243, 0.0758, 0.0294, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 182, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0819e-01, 8.8833e-03, 1.2554e-02, 2.0589e-06, 3.9767e-03, 3.7308e-03,
        8.6267e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 182, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0548, 0.8627, 0.0030, 0.0642, 0.0027, 0.0100, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 183, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0298e-03, 9.8390e-01, 3.1790e-03, 8.6825e-07, 7.0768e-03, 6.0912e-08,
        2.8097e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.407

[Epoch: 183, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0823, 0.6408, 0.0385, 0.0738, 0.0526, 0.0466, 0.0654],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.100

[Epoch: 183, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.7707, 0.1210, 0.0695, 0.0294, 0.0030, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.000

[Epoch: 183, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0827e-01, 6.4973e-03, 1.1841e-02, 2.1908e-06, 3.5615e-03, 4.0655e-03,
        8.6576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 183, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0523, 0.8677, 0.0033, 0.0594, 0.0044, 0.0103, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 184, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4452e-03, 9.8163e-01, 3.8984e-03, 1.2368e-06, 7.1950e-03, 6.7233e-08,
        3.8305e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.428

[Epoch: 184, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0803, 0.6159, 0.0464, 0.0880, 0.0637, 0.0495, 0.0562],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 184, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.7663, 0.1192, 0.0718, 0.0319, 0.0036, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 184, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0935e-01, 6.4056e-03, 8.9201e-03, 2.7596e-06, 3.8423e-03, 3.8389e-03,
        8.6764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 184, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0383, 0.8689, 0.0031, 0.0762, 0.0026, 0.0075, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 185, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([5.2879e-03, 9.7958e-01, 3.8061e-03, 1.3979e-06, 7.2504e-03, 1.2522e-07,
        4.0732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 185, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0902, 0.5913, 0.0437, 0.0883, 0.0613, 0.0483, 0.0770],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 185, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.7571, 0.1338, 0.0693, 0.0280, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 185, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.3410e-02, 4.7239e-03, 8.7296e-03, 3.1515e-06, 2.8467e-03, 2.0129e-03,
        8.8827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 185, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0642, 0.8432, 0.0029, 0.0697, 0.0032, 0.0129, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.005

[Epoch: 186, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.7040e-03, 9.8136e-01, 3.7126e-03, 1.2712e-06, 7.9185e-03, 1.2202e-07,
        3.3042e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 186, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1028, 0.5693, 0.0575, 0.0777, 0.0593, 0.0574, 0.0760],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 186, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7771, 0.1036, 0.0755, 0.0344, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 186, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.3599e-02, 3.8222e-03, 7.0350e-03, 2.9668e-06, 2.5242e-03, 1.9321e-03,
        9.0108e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 186, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0531, 0.8687, 0.0037, 0.0568, 0.0038, 0.0108, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 187, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.3912e-03, 9.8548e-01, 2.8567e-03, 1.1960e-06, 6.0789e-03, 7.1074e-08,
        3.1876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 187, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0946, 0.5589, 0.0468, 0.0834, 0.0812, 0.0666, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 187, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.7873, 0.1174, 0.0567, 0.0301, 0.0027, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.002

[Epoch: 187, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.0141e-02, 7.8666e-03, 9.9018e-03, 2.2432e-06, 2.4266e-03, 4.1287e-03,
        8.8553e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 187, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0388, 0.8534, 0.0047, 0.0854, 0.0036, 0.0100, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 188, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.7368e-03, 9.8526e-01, 2.9567e-03, 1.2198e-06, 5.9179e-03, 1.0056e-07,
        3.1275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.420

[Epoch: 188, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0823, 0.5917, 0.0415, 0.1031, 0.0645, 0.0462, 0.0707],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 188, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.7557, 0.1235, 0.0811, 0.0304, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.004

[Epoch: 188, batch: 156/198] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2243e-01, 7.9627e-03, 1.1494e-02, 2.7847e-06, 4.0451e-03, 3.5071e-03,
        8.5056e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 188, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0504, 0.8747, 0.0038, 0.0523, 0.0029, 0.0128, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 189, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.5351e-03, 9.8315e-01, 3.0228e-03, 1.2287e-06, 7.5401e-03, 9.2263e-08,
        3.7523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.424

[Epoch: 189, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0905, 0.5819, 0.0554, 0.0844, 0.0581, 0.0552, 0.0744],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.098

[Epoch: 189, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7665, 0.1231, 0.0704, 0.0299, 0.0037, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 189, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0124e-01, 8.9631e-03, 1.0243e-02, 2.9351e-06, 3.8540e-03, 3.4242e-03,
        8.7227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 189, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0489, 0.8730, 0.0036, 0.0577, 0.0027, 0.0111, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 190, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8460e-03, 9.8496e-01, 2.7448e-03, 1.2441e-06, 6.2171e-03, 6.1434e-08,
        3.2332e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.419

[Epoch: 190, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0910, 0.6109, 0.0415, 0.0738, 0.0615, 0.0509, 0.0704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.103

[Epoch: 190, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7758, 0.1156, 0.0695, 0.0298, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 190, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.7519e-02, 7.5901e-03, 9.7121e-03, 2.4335e-06, 3.3886e-03, 3.9219e-03,
        8.7787e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 190, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0470, 0.8669, 0.0023, 0.0704, 0.0027, 0.0080, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 191, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5641e-03, 9.8212e-01, 3.7181e-03, 1.4045e-06, 7.1504e-03, 7.3605e-08,
        3.4428e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 191, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0776, 0.6190, 0.0454, 0.0822, 0.0638, 0.0436, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 191, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7565, 0.1268, 0.0744, 0.0318, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.009

[Epoch: 191, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0094e-01, 5.7075e-03, 9.0827e-03, 2.1845e-06, 3.6829e-03, 3.2326e-03,
        8.7735e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 191, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0547, 0.8570, 0.0029, 0.0696, 0.0036, 0.0091, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 192, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.4531e-03, 9.8243e-01, 3.6425e-03, 1.2368e-06, 6.8892e-03, 1.0770e-07,
        3.5822e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 192, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0879, 0.6065, 0.0482, 0.0819, 0.0636, 0.0464, 0.0656],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.109

[Epoch: 192, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.7701, 0.1193, 0.0683, 0.0312, 0.0039, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.008

[Epoch: 192, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.6233e-02, 6.5169e-03, 9.3920e-03, 2.4465e-06, 3.4242e-03, 2.6113e-03,
        8.8182e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 192, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0560, 0.8498, 0.0033, 0.0715, 0.0037, 0.0115, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 193, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.3871e-03, 9.8175e-01, 3.5306e-03, 1.2394e-06, 7.2694e-03, 1.1825e-07,
        4.0604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.422

[Epoch: 193, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.1029, 0.5812, 0.0423, 0.0806, 0.0603, 0.0645, 0.0682],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.108

[Epoch: 193, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7734, 0.1170, 0.0697, 0.0300, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.013

[Epoch: 193, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.8402e-02, 6.5119e-03, 9.4582e-03, 2.1712e-06, 2.9935e-03, 2.9590e-03,
        8.7967e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 193, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0475, 0.8599, 0.0033, 0.0702, 0.0035, 0.0115, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 194, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.9466e-03, 9.8366e-01, 3.1886e-03, 1.0975e-06, 6.6745e-03, 7.7989e-08,
        3.5312e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 194, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0896, 0.5499, 0.0462, 0.1006, 0.0726, 0.0643, 0.0767],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 194, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.7743, 0.1183, 0.0708, 0.0280, 0.0027, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.012

[Epoch: 194, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.4841e-02, 6.3807e-03, 9.6178e-03, 2.2502e-06, 3.4111e-03, 3.9151e-03,
        8.8183e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 194, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0486, 0.8719, 0.0037, 0.0600, 0.0035, 0.0091, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 195, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([2.8714e-03, 9.8648e-01, 2.6041e-03, 8.4362e-07, 5.4734e-03, 6.7470e-08,
        2.5705e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.414

[Epoch: 195, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0900, 0.5791, 0.0543, 0.0831, 0.0710, 0.0480, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 195, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.7718, 0.1202, 0.0690, 0.0300, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.012

[Epoch: 195, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0457e-01, 8.6476e-03, 1.1219e-02, 2.1133e-06, 4.1770e-03, 3.8371e-03,
        8.6755e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 195, batch: 195/198] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0480, 0.8710, 0.0034, 0.0632, 0.0026, 0.0091, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 196, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.0270e-03, 9.8622e-01, 2.6957e-03, 9.0290e-07, 5.5194e-03, 5.0978e-08,
        2.5381e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.428

[Epoch: 196, batch: 78/198] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0895, 0.6185, 0.0498, 0.0676, 0.0578, 0.0475, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.104

[Epoch: 196, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7680, 0.1212, 0.0706, 0.0306, 0.0029, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.007

[Epoch: 196, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.2112e-01, 7.3812e-03, 1.3644e-02, 2.0070e-06, 3.7041e-03, 3.7602e-03,
        8.5039e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 196, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0499, 0.8560, 0.0033, 0.0710, 0.0033, 0.0126, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 197, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.5307e-03, 9.7995e-01, 3.5933e-03, 1.3988e-06, 8.8667e-03, 6.9569e-08,
        4.0567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.427

[Epoch: 197, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0820, 0.6327, 0.0403, 0.0780, 0.0532, 0.0514, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.099

[Epoch: 197, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.7684, 0.1197, 0.0711, 0.0307, 0.0034, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.011

[Epoch: 197, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([1.0761e-01, 6.2514e-03, 9.8920e-03, 2.4714e-06, 3.0298e-03, 3.7809e-03,
        8.6943e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 197, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0512, 0.8660, 0.0030, 0.0627, 0.0036, 0.0096, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 198, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([4.2439e-03, 9.7966e-01, 4.5499e-03, 1.4909e-06, 7.7355e-03, 1.0495e-07,
        3.8094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.409

[Epoch: 198, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0801, 0.5743, 0.0512, 0.0927, 0.0724, 0.0522, 0.0771],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.105

[Epoch: 198, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.7666, 0.1215, 0.0710, 0.0309, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.002

[Epoch: 198, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([8.8564e-02, 5.7714e-03, 8.6994e-03, 2.0646e-06, 3.0500e-03, 2.7364e-03,
        8.9118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 198, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0525, 0.8684, 0.0031, 0.0622, 0.0028, 0.0084, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 199, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.7971e-03, 9.8365e-01, 3.2161e-03, 1.4484e-06, 5.7513e-03, 1.1451e-07,
        3.5838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.413

[Epoch: 199, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0970, 0.5976, 0.0476, 0.0713, 0.0684, 0.0510, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.101

[Epoch: 199, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.7745, 0.1139, 0.0698, 0.0304, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 -0.000

[Epoch: 199, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9314e-02, 5.2782e-03, 9.7349e-03, 2.6023e-06, 2.7702e-03, 2.6705e-03,
        8.8023e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 199, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0472, 0.8467, 0.0032, 0.0869, 0.0031, 0.0097, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 200, batch: 39/198] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9833, 0.0033, 0.0000, 0.0067, 0.0000, 0.0033])
Policy pred: tensor([3.1727e-03, 9.8274e-01, 3.3158e-03, 1.0794e-06, 7.0422e-03, 9.1585e-08,
        3.7251e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.421 -0.437

[Epoch: 200, batch: 78/198] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0900, 0.5933, 0.0467, 0.0833, 0.0633, 0.0533, 0.0700])
Policy pred: tensor([0.0928, 0.6012, 0.0422, 0.0918, 0.0538, 0.0565, 0.0616],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.102 0.094

[Epoch: 200, batch: 117/198] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.7700, 0.1200, 0.0700, 0.0300, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.7675, 0.1263, 0.0690, 0.0279, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.006 0.003

[Epoch: 200, batch: 156/198] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.1000, 0.0067, 0.0100, 0.0000, 0.0033, 0.0033, 0.8767])
Policy pred: tensor([9.9658e-02, 7.1771e-03, 8.5118e-03, 2.6884e-06, 3.5392e-03, 2.9678e-03,
        8.7814e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 200, batch: 195/198] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0500, 0.8633, 0.0033, 0.0667, 0.0033, 0.0100, 0.0033])
Policy pred: tensor([0.0502, 0.8617, 0.0049, 0.0601, 0.0055, 0.0129, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

