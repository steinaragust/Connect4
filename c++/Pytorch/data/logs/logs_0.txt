Training set samples: 6991
Batch size: 32
[Epoch: 1, batch: 43/219] total loss per batch: 2.106
Policy (actual, predicted): 1 5
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1711, 0.1143, 0.1110, 0.1012, 0.1838, 0.1898, 0.1288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.005

[Epoch: 1, batch: 86/219] total loss per batch: 2.062
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.1371, 0.1068, 0.1451, 0.1095, 0.2571, 0.1152, 0.1292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.032

[Epoch: 1, batch: 129/219] total loss per batch: 2.018
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([0.1436, 0.1312, 0.1393, 0.1256, 0.1448, 0.1551, 0.1603],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.014

[Epoch: 1, batch: 172/219] total loss per batch: 2.038
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1540, 0.1339, 0.0882, 0.1377, 0.1897, 0.2031, 0.0935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 1, batch: 215/219] total loss per batch: 2.009
Policy (actual, predicted): 2 5
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1518, 0.1642, 0.1356, 0.1117, 0.1283, 0.1732, 0.1352],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 2, batch: 43/219] total loss per batch: 1.973
Policy (actual, predicted): 1 0
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1910, 0.1372, 0.1346, 0.0896, 0.1558, 0.1721, 0.1198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.019

[Epoch: 2, batch: 86/219] total loss per batch: 1.928
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.1125, 0.1160, 0.1160, 0.1323, 0.3224, 0.0729, 0.1280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.066

[Epoch: 2, batch: 129/219] total loss per batch: 1.845
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([0.0144, 0.2607, 0.2060, 0.1591, 0.0172, 0.0250, 0.3176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.034

[Epoch: 2, batch: 172/219] total loss per batch: 1.826
Policy (actual, predicted): 3 4
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1728, 0.1226, 0.1015, 0.1541, 0.1790, 0.1757, 0.0942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 2, batch: 215/219] total loss per batch: 1.802
Policy (actual, predicted): 2 0
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2101, 0.1591, 0.1625, 0.1449, 0.0677, 0.1435, 0.1122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 3, batch: 43/219] total loss per batch: 1.798
Policy (actual, predicted): 1 4
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1873, 0.0996, 0.1089, 0.1171, 0.2140, 0.1782, 0.0949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 3, batch: 86/219] total loss per batch: 1.780
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0255, 0.1368, 0.1353, 0.1691, 0.2939, 0.0440, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 3, batch: 129/219] total loss per batch: 1.742
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([0.0073, 0.2300, 0.2217, 0.2224, 0.0131, 0.0120, 0.2936],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.013

[Epoch: 3, batch: 172/219] total loss per batch: 1.725
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1407, 0.1424, 0.0816, 0.1528, 0.1885, 0.1922, 0.1019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 3, batch: 215/219] total loss per batch: 1.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1803, 0.1043, 0.2214, 0.1832, 0.1105, 0.1345, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.043

[Epoch: 4, batch: 43/219] total loss per batch: 1.726
Policy (actual, predicted): 1 4
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1646, 0.0869, 0.1134, 0.1092, 0.2361, 0.2148, 0.0751],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.003

[Epoch: 4, batch: 86/219] total loss per batch: 1.719
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0101, 0.1484, 0.0519, 0.0935, 0.4133, 0.0180, 0.2648],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.033

[Epoch: 4, batch: 129/219] total loss per batch: 1.694
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([0.0015, 0.2545, 0.2216, 0.2342, 0.0096, 0.0129, 0.2657],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.005

[Epoch: 4, batch: 172/219] total loss per batch: 1.671
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1041, 0.1377, 0.0721, 0.1829, 0.1994, 0.2212, 0.0827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.013

[Epoch: 4, batch: 215/219] total loss per batch: 1.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2353, 0.0883, 0.2725, 0.1313, 0.0888, 0.1264, 0.0574],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 5, batch: 43/219] total loss per batch: 1.673
Policy (actual, predicted): 1 4
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1461, 0.1030, 0.1690, 0.0782, 0.2221, 0.2159, 0.0657],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.055

[Epoch: 5, batch: 86/219] total loss per batch: 1.654
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0062, 0.2152, 0.0666, 0.1094, 0.2970, 0.0211, 0.2845],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.021

[Epoch: 5, batch: 129/219] total loss per batch: 1.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([0.0008, 0.1840, 0.1806, 0.3295, 0.0109, 0.0043, 0.2899],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.051

[Epoch: 5, batch: 172/219] total loss per batch: 1.626
Policy (actual, predicted): 3 4
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1033, 0.1453, 0.1001, 0.1700, 0.2068, 0.1828, 0.0918],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 5, batch: 215/219] total loss per batch: 1.638
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2483, 0.0673, 0.3085, 0.0911, 0.1074, 0.1360, 0.0415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.017

[Epoch: 6, batch: 43/219] total loss per batch: 1.620
Policy (actual, predicted): 1 4
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1302, 0.1280, 0.1950, 0.0408, 0.2713, 0.1910, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.058

[Epoch: 6, batch: 86/219] total loss per batch: 1.588
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0096, 0.2389, 0.0942, 0.1268, 0.2442, 0.0528, 0.2337],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 6, batch: 129/219] total loss per batch: 1.575
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2365e-04, 1.4827e-01, 1.7244e-01, 2.3726e-01, 1.4343e-02, 6.7706e-03,
        4.2070e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.052

[Epoch: 6, batch: 172/219] total loss per batch: 1.567
Policy (actual, predicted): 3 1
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0876, 0.2024, 0.1087, 0.1601, 0.1872, 0.1603, 0.0938],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.032

[Epoch: 6, batch: 215/219] total loss per batch: 1.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2118, 0.0466, 0.3574, 0.0829, 0.1198, 0.1311, 0.0504],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.018

[Epoch: 7, batch: 43/219] total loss per batch: 1.569
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1182, 0.1546, 0.2810, 0.0290, 0.2258, 0.1460, 0.0454],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 7, batch: 86/219] total loss per batch: 1.524
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0150, 0.2331, 0.1207, 0.0881, 0.2110, 0.0260, 0.3060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.038

[Epoch: 7, batch: 129/219] total loss per batch: 1.522
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6801e-04, 1.3931e-01, 2.0668e-01, 1.7333e-01, 1.3726e-02, 1.0623e-02,
        4.5617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.050

[Epoch: 7, batch: 172/219] total loss per batch: 1.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1100, 0.1701, 0.0752, 0.2002, 0.1915, 0.1661, 0.0869],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.026

[Epoch: 7, batch: 215/219] total loss per batch: 1.538
Policy (actual, predicted): 2 0
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2871, 0.0613, 0.2655, 0.0707, 0.1533, 0.1381, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 8, batch: 43/219] total loss per batch: 1.507
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1071, 0.1033, 0.4710, 0.0246, 0.1401, 0.1220, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 8, batch: 86/219] total loss per batch: 1.447
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0249, 0.1622, 0.1884, 0.0539, 0.2265, 0.0170, 0.3270],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.030

[Epoch: 8, batch: 129/219] total loss per batch: 1.455
Policy (actual, predicted): 3 6
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6034e-04, 1.7847e-01, 1.7540e-01, 3.0022e-01, 2.6800e-03, 5.2362e-03,
        3.3784e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.046

[Epoch: 8, batch: 172/219] total loss per batch: 1.449
Policy (actual, predicted): 3 4
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1284, 0.1752, 0.0771, 0.2082, 0.2704, 0.0696, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.051

[Epoch: 8, batch: 215/219] total loss per batch: 1.491
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2530, 0.0717, 0.3162, 0.0788, 0.1454, 0.0998, 0.0352],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.036

[Epoch: 9, batch: 43/219] total loss per batch: 1.448
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0414, 0.1237, 0.6258, 0.0102, 0.1001, 0.0789, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.028

[Epoch: 9, batch: 86/219] total loss per batch: 1.376
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0280, 0.0914, 0.3105, 0.0482, 0.1935, 0.0035, 0.3249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 9, batch: 129/219] total loss per batch: 1.391
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1728e-05, 2.4808e-01, 1.5874e-01, 4.1588e-01, 1.0097e-03, 4.7295e-04,
        1.7581e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.158

[Epoch: 9, batch: 172/219] total loss per batch: 1.370
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1560, 0.1412, 0.0376, 0.2869, 0.1485, 0.1363, 0.0935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.032

[Epoch: 9, batch: 215/219] total loss per batch: 1.416
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2110, 0.0476, 0.4117, 0.0625, 0.1732, 0.0617, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 10, batch: 43/219] total loss per batch: 1.372
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0558, 0.1020, 0.6632, 0.0096, 0.0523, 0.1024, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.019

[Epoch: 10, batch: 86/219] total loss per batch: 1.313
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0177, 0.0566, 0.3640, 0.0254, 0.0579, 0.0055, 0.4729],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.015

[Epoch: 10, batch: 129/219] total loss per batch: 1.302
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3911e-05, 1.4187e-01, 1.3237e-01, 4.7717e-01, 1.3966e-03, 4.0084e-04,
        2.4678e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.017

[Epoch: 10, batch: 172/219] total loss per batch: 1.305
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1015, 0.1882, 0.0324, 0.2727, 0.2100, 0.1492, 0.0460],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 10, batch: 215/219] total loss per batch: 1.312
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1848, 0.0219, 0.6333, 0.0333, 0.0815, 0.0188, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.035

[Epoch: 11, batch: 43/219] total loss per batch: 1.296
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0248, 0.1791, 0.6171, 0.0063, 0.0656, 0.1039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.051

[Epoch: 11, batch: 86/219] total loss per batch: 1.229
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0541, 0.0309, 0.5684, 0.0290, 0.0270, 0.0019, 0.2887],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.010

[Epoch: 11, batch: 129/219] total loss per batch: 1.210
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.9570e-05, 1.6679e-01, 9.9726e-02, 6.1776e-01, 5.7597e-04, 2.6617e-04,
        1.1487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.146

[Epoch: 11, batch: 172/219] total loss per batch: 1.231
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1108, 0.2282, 0.0371, 0.2926, 0.1027, 0.1637, 0.0649],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.048

[Epoch: 11, batch: 215/219] total loss per batch: 1.226
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2539, 0.0151, 0.5365, 0.0327, 0.1467, 0.0064, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.082

[Epoch: 12, batch: 43/219] total loss per batch: 1.215
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0485, 0.0800, 0.6540, 0.0216, 0.1312, 0.0528, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.063

[Epoch: 12, batch: 86/219] total loss per batch: 1.183
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0183, 0.0373, 0.2235, 0.0288, 0.0239, 0.0034, 0.6650],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.001

[Epoch: 12, batch: 129/219] total loss per batch: 1.157
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.4508e-05, 1.8762e-01, 1.5914e-01, 3.7698e-01, 2.4686e-04, 1.3165e-04,
        2.7585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.156

[Epoch: 12, batch: 172/219] total loss per batch: 1.159
Policy (actual, predicted): 3 1
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1464, 0.4640, 0.0174, 0.1187, 0.1312, 0.0713, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.010

[Epoch: 12, batch: 215/219] total loss per batch: 1.164
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1938, 0.0149, 0.5493, 0.0598, 0.1397, 0.0273, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.014

[Epoch: 13, batch: 43/219] total loss per batch: 1.158
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0165, 0.2522, 0.6032, 0.0122, 0.0673, 0.0450, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.018

[Epoch: 13, batch: 86/219] total loss per batch: 1.121
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0213, 0.0439, 0.3265, 0.0514, 0.0167, 0.0015, 0.5386],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 13, batch: 129/219] total loss per batch: 1.101
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.0966e-05, 6.3286e-02, 4.2352e-02, 7.1416e-01, 1.2499e-04, 1.9414e-04,
        1.7982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.102

[Epoch: 13, batch: 172/219] total loss per batch: 1.097
Policy (actual, predicted): 3 1
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.1392, 0.3313, 0.0120, 0.1992, 0.0961, 0.1891, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 13, batch: 215/219] total loss per batch: 1.089
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2700, 0.0441, 0.4414, 0.0448, 0.1322, 0.0251, 0.0423],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.062

[Epoch: 14, batch: 43/219] total loss per batch: 1.110
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0161, 0.5965, 0.2297, 0.0088, 0.1224, 0.0182, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.019

[Epoch: 14, batch: 86/219] total loss per batch: 1.049
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6687e-02, 4.6475e-02, 8.0890e-01, 1.7099e-02, 1.2359e-02, 4.9483e-04,
        9.7982e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.034

[Epoch: 14, batch: 129/219] total loss per batch: 1.036
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.5494e-05, 1.7403e-02, 3.3295e-02, 9.1747e-01, 6.0927e-05, 6.2665e-05,
        3.1688e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.057

[Epoch: 14, batch: 172/219] total loss per batch: 1.032
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.2057, 0.1571, 0.0067, 0.2631, 0.0870, 0.2184, 0.0621],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.038

[Epoch: 14, batch: 215/219] total loss per batch: 1.027
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1342, 0.0077, 0.7194, 0.0300, 0.0826, 0.0231, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.029

[Epoch: 15, batch: 43/219] total loss per batch: 1.033
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0837, 0.4986, 0.2801, 0.0112, 0.0594, 0.0549, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.007

[Epoch: 15, batch: 86/219] total loss per batch: 1.025
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.7634e-03, 7.9188e-03, 8.1538e-01, 1.1616e-02, 7.8834e-03, 7.1971e-04,
        1.5172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 15, batch: 129/219] total loss per batch: 1.015
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.4962e-05, 3.7736e-02, 5.4641e-02, 6.6772e-01, 6.0957e-04, 3.6717e-05,
        2.3917e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.051

[Epoch: 15, batch: 172/219] total loss per batch: 0.992
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0235, 0.3993, 0.0050, 0.4189, 0.0416, 0.0313, 0.0804],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 15, batch: 215/219] total loss per batch: 0.991
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0415, 0.0074, 0.8161, 0.0386, 0.0596, 0.0292, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 16, batch: 43/219] total loss per batch: 1.002
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0537, 0.6045, 0.2581, 0.0154, 0.0206, 0.0452, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.050

[Epoch: 16, batch: 86/219] total loss per batch: 0.983
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([0.0099, 0.0233, 0.6127, 0.0405, 0.0270, 0.0015, 0.2850],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.060

[Epoch: 16, batch: 129/219] total loss per batch: 0.965
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0946e-05, 1.1657e-02, 2.2873e-02, 8.8444e-01, 7.9980e-04, 9.0381e-06,
        8.0209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.120

[Epoch: 16, batch: 172/219] total loss per batch: 0.964
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0867, 0.1630, 0.0132, 0.5102, 0.0601, 0.0883, 0.0784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 16, batch: 215/219] total loss per batch: 0.934
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.2407, 0.0120, 0.4872, 0.0330, 0.1467, 0.0753, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 17, batch: 43/219] total loss per batch: 0.959
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0649, 0.7052, 0.1168, 0.0098, 0.0733, 0.0262, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.029

[Epoch: 17, batch: 86/219] total loss per batch: 0.941
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.5097e-03, 1.8197e-02, 7.5467e-01, 3.5109e-02, 1.1296e-02, 6.0133e-04,
        1.7862e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.014

[Epoch: 17, batch: 129/219] total loss per batch: 0.926
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1626e-05, 3.0392e-02, 2.2075e-02, 9.2019e-01, 5.1941e-05, 1.8899e-05,
        2.7264e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.034

[Epoch: 17, batch: 172/219] total loss per batch: 0.908
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0502, 0.0465, 0.0133, 0.6476, 0.1392, 0.0732, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.022

[Epoch: 17, batch: 215/219] total loss per batch: 0.914
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0441, 0.0210, 0.7305, 0.0479, 0.1199, 0.0317, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 18, batch: 43/219] total loss per batch: 0.936
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1740, 0.4655, 0.2095, 0.0320, 0.0364, 0.0560, 0.0267],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.018

[Epoch: 18, batch: 86/219] total loss per batch: 0.922
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.2838e-04, 4.6810e-02, 6.8674e-01, 6.4765e-03, 9.7394e-03, 9.1921e-04,
        2.4869e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.025

[Epoch: 18, batch: 129/219] total loss per batch: 0.896
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7277e-05, 2.0356e-02, 1.8672e-02, 9.3785e-01, 8.6904e-05, 5.0558e-05,
        2.2964e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.174

[Epoch: 18, batch: 172/219] total loss per batch: 0.891
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0173, 0.0735, 0.0037, 0.7385, 0.0273, 0.1045, 0.0352],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 18, batch: 215/219] total loss per batch: 0.884
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1130, 0.1236, 0.4981, 0.0448, 0.1161, 0.0954, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.010

[Epoch: 19, batch: 43/219] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0808, 0.4310, 0.2863, 0.0036, 0.1817, 0.0092, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.017

[Epoch: 19, batch: 86/219] total loss per batch: 0.900
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.7802e-04, 9.9872e-03, 8.3841e-01, 1.2428e-02, 4.5050e-03, 2.4653e-04,
        1.3384e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.042

[Epoch: 19, batch: 129/219] total loss per batch: 0.880
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1204e-05, 1.2076e-02, 9.4913e-03, 9.5524e-01, 8.4845e-05, 2.0687e-04,
        2.2893e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.100

[Epoch: 19, batch: 172/219] total loss per batch: 0.841
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0301, 0.0206, 0.0055, 0.7038, 0.1460, 0.0760, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 19, batch: 215/219] total loss per batch: 0.863
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0936, 0.0760, 0.6718, 0.0468, 0.0661, 0.0240, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 20, batch: 43/219] total loss per batch: 0.875
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2233, 0.2181, 0.3685, 0.0129, 0.1530, 0.0152, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.006

[Epoch: 20, batch: 86/219] total loss per batch: 0.878
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.7756e-04, 4.1160e-02, 7.9332e-01, 3.2790e-02, 6.4029e-03, 3.7444e-04,
        1.2528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.049

[Epoch: 20, batch: 129/219] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.3652e-05, 3.9221e-01, 3.0642e-02, 4.8631e-01, 1.0123e-04, 2.2976e-04,
        9.0472e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.005

[Epoch: 20, batch: 172/219] total loss per batch: 0.845
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0170, 0.0423, 0.0061, 0.6235, 0.1358, 0.0938, 0.0815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 20, batch: 215/219] total loss per batch: 0.849
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.1880, 0.0236, 0.6343, 0.0402, 0.0699, 0.0394, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 21, batch: 43/219] total loss per batch: 0.855
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1893, 0.2416, 0.2672, 0.0291, 0.1484, 0.1001, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.016

[Epoch: 21, batch: 86/219] total loss per batch: 0.857
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.8164e-04, 2.9554e-02, 8.8156e-01, 2.1208e-02, 4.8640e-03, 3.0027e-04,
        6.2136e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.058

[Epoch: 21, batch: 129/219] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5739e-05, 1.8985e-01, 1.1688e-02, 7.8329e-01, 1.2828e-04, 4.0672e-04,
        1.4618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.007

[Epoch: 21, batch: 172/219] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0371, 0.0216, 0.0076, 0.7349, 0.1037, 0.0373, 0.0579],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.004

[Epoch: 21, batch: 215/219] total loss per batch: 0.822
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0281, 0.0134, 0.8401, 0.0242, 0.0801, 0.0085, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 22, batch: 43/219] total loss per batch: 0.845
Policy (actual, predicted): 1 4
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0582, 0.1343, 0.3543, 0.0242, 0.3836, 0.0205, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 22, batch: 86/219] total loss per batch: 0.843
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.4239e-04, 2.5030e-02, 3.8328e-01, 1.2094e-02, 5.7052e-03, 5.0383e-04,
        5.7304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.063

[Epoch: 22, batch: 129/219] total loss per batch: 0.817
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.7781e-05, 2.4779e-01, 8.2404e-03, 7.2572e-01, 5.5226e-05, 9.1364e-05,
        1.8083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.004

[Epoch: 22, batch: 172/219] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0821, 0.0458, 0.0060, 0.5541, 0.0810, 0.1859, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 22, batch: 215/219] total loss per batch: 0.803
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0628, 0.0300, 0.6849, 0.0192, 0.1475, 0.0446, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.024

[Epoch: 23, batch: 43/219] total loss per batch: 0.837
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1019, 0.6870, 0.1427, 0.0144, 0.0380, 0.0132, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.014

[Epoch: 23, batch: 86/219] total loss per batch: 0.822
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.5937e-04, 1.7198e-02, 8.5923e-01, 1.7382e-02, 1.7776e-03, 2.6208e-04,
        1.0390e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.019

[Epoch: 23, batch: 129/219] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.9758e-05, 1.6652e-01, 2.3906e-02, 7.6748e-01, 2.3117e-05, 2.6118e-04,
        4.1797e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.009

[Epoch: 23, batch: 172/219] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0046, 0.0107, 0.0016, 0.9248, 0.0177, 0.0087, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.026

[Epoch: 23, batch: 215/219] total loss per batch: 0.803
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0443, 0.0249, 0.7812, 0.0451, 0.0389, 0.0532, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 24, batch: 43/219] total loss per batch: 0.817
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1526, 0.5434, 0.1003, 0.0168, 0.1485, 0.0306, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.004

[Epoch: 24, batch: 86/219] total loss per batch: 0.807
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.9540e-05, 1.2612e-02, 9.1227e-01, 1.8169e-02, 2.1361e-03, 1.1316e-04,
        5.4607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.032

[Epoch: 24, batch: 129/219] total loss per batch: 0.787
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.4806e-05, 7.5514e-02, 1.5396e-02, 8.7332e-01, 2.8892e-04, 7.9931e-05,
        3.5379e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.011

[Epoch: 24, batch: 172/219] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0606, 0.0733, 0.0052, 0.7041, 0.0776, 0.0491, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 24, batch: 215/219] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0361, 0.0164, 0.8248, 0.0239, 0.0757, 0.0165, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.040

[Epoch: 25, batch: 43/219] total loss per batch: 0.805
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2247, 0.2589, 0.3532, 0.0480, 0.0711, 0.0310, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.006

[Epoch: 25, batch: 86/219] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.1720e-05, 1.8553e-02, 9.2625e-01, 1.8042e-02, 2.1331e-03, 8.4065e-05,
        3.4856e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.028

[Epoch: 25, batch: 129/219] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.1758e-05, 1.4043e-01, 3.6509e-02, 7.9524e-01, 4.9935e-05, 1.2362e-03,
        2.6487e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.002

[Epoch: 25, batch: 172/219] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0195, 0.0308, 0.0054, 0.7130, 0.1032, 0.0400, 0.0879],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 25, batch: 215/219] total loss per batch: 0.767
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0348, 0.0553, 0.7869, 0.0254, 0.0329, 0.0577, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.021

[Epoch: 26, batch: 43/219] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1403, 0.6276, 0.0656, 0.0560, 0.0644, 0.0343, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 26, batch: 86/219] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2774e-04, 8.2937e-03, 8.6779e-01, 4.2001e-03, 6.4929e-03, 8.1814e-05,
        1.1302e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 26, batch: 129/219] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.7516e-05, 8.1084e-02, 2.0814e-02, 8.2222e-01, 6.7831e-05, 6.7370e-04,
        7.5118e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.004

[Epoch: 26, batch: 172/219] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0392, 0.0435, 0.0031, 0.8080, 0.0172, 0.0754, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 26, batch: 215/219] total loss per batch: 0.753
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0124, 0.0068, 0.8853, 0.0083, 0.0816, 0.0041, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.065

[Epoch: 27, batch: 43/219] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1147, 0.6249, 0.1142, 0.0257, 0.0910, 0.0159, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 27, batch: 86/219] total loss per batch: 0.781
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.2380e-04, 2.8011e-02, 7.7147e-01, 7.4743e-02, 1.0608e-02, 4.2788e-04,
        1.1442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.029

[Epoch: 27, batch: 129/219] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.2494e-05, 4.6912e-01, 1.2745e-02, 4.8577e-01, 4.5168e-05, 2.6375e-04,
        3.1971e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.008

[Epoch: 27, batch: 172/219] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0239, 0.0201, 0.0020, 0.8694, 0.0411, 0.0360, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 27, batch: 215/219] total loss per batch: 0.752
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0237, 0.0704, 0.7959, 0.0304, 0.0149, 0.0553, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.034

[Epoch: 28, batch: 43/219] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2988, 0.3502, 0.0601, 0.0347, 0.1925, 0.0231, 0.0405],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.007

[Epoch: 28, batch: 86/219] total loss per batch: 0.778
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.2265e-05, 2.1752e-02, 8.5409e-01, 1.0316e-02, 4.1218e-03, 2.1867e-04,
        1.0945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.025

[Epoch: 28, batch: 129/219] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.3928e-05, 3.9942e-01, 2.2707e-02, 5.3074e-01, 2.2529e-05, 5.4581e-04,
        4.6515e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.001

[Epoch: 28, batch: 172/219] total loss per batch: 0.742
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0445, 0.0593, 0.0049, 0.8054, 0.0115, 0.0443, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.037

[Epoch: 28, batch: 215/219] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0085, 0.0253, 0.8706, 0.0127, 0.0672, 0.0105, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 29, batch: 43/219] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1401, 0.3218, 0.2232, 0.0289, 0.2083, 0.0548, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.005

[Epoch: 29, batch: 86/219] total loss per batch: 0.770
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.5622e-04, 1.4397e-02, 7.7843e-01, 2.8997e-02, 7.7755e-03, 4.3501e-04,
        1.6931e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.032

[Epoch: 29, batch: 129/219] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.6260e-05, 2.6423e-01, 2.1507e-02, 6.6489e-01, 6.4735e-05, 1.9648e-04,
        4.9066e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 0.009

[Epoch: 29, batch: 172/219] total loss per batch: 0.738
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0214, 0.0294, 0.0097, 0.6616, 0.0450, 0.2010, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 29, batch: 215/219] total loss per batch: 0.736
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0349, 0.0174, 0.7631, 0.0308, 0.0665, 0.0796, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 30, batch: 43/219] total loss per batch: 0.763
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1753, 0.2991, 0.3809, 0.0147, 0.1116, 0.0120, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 30, batch: 86/219] total loss per batch: 0.758
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.0037e-05, 2.3223e-02, 9.2101e-01, 7.4676e-03, 2.4601e-03, 8.8272e-05,
        4.5664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.031

[Epoch: 30, batch: 129/219] total loss per batch: 0.743
Policy (actual, predicted): 3 1
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.3389e-05, 4.7021e-01, 2.4449e-02, 4.3396e-01, 1.2528e-04, 6.4054e-04,
        7.0545e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.008

[Epoch: 30, batch: 172/219] total loss per batch: 0.727
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0169, 0.0203, 0.0029, 0.8920, 0.0221, 0.0279, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 30, batch: 215/219] total loss per batch: 0.726
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0117, 0.0170, 0.9128, 0.0135, 0.0344, 0.0072, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.023

[Epoch: 31, batch: 43/219] total loss per batch: 0.762
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1829, 0.1833, 0.4129, 0.0497, 0.1162, 0.0375, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.005

[Epoch: 31, batch: 86/219] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.6697e-04, 2.4651e-02, 7.5672e-01, 3.1122e-02, 7.4321e-03, 2.2032e-04,
        1.7958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 31, batch: 129/219] total loss per batch: 0.730
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.0147e-05, 4.3329e-01, 2.8111e-02, 4.9891e-01, 3.1813e-05, 1.5662e-04,
        3.9425e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.100

[Epoch: 31, batch: 172/219] total loss per batch: 0.719
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0206, 0.0167, 0.0030, 0.8571, 0.0473, 0.0376, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 31, batch: 215/219] total loss per batch: 0.728
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0385, 0.0280, 0.8145, 0.0274, 0.0443, 0.0282, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 32, batch: 43/219] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0861, 0.7337, 0.0758, 0.0180, 0.0660, 0.0077, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.000

[Epoch: 32, batch: 86/219] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.4817e-05, 1.3018e-02, 9.2948e-01, 1.4868e-02, 2.5158e-03, 1.3245e-04,
        3.9919e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.025

[Epoch: 32, batch: 129/219] total loss per batch: 0.737
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.8954e-06, 3.1207e-01, 1.0091e-02, 6.3441e-01, 9.5343e-06, 1.3385e-04,
        4.3280e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.020

[Epoch: 32, batch: 172/219] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0450, 0.0877, 0.0091, 0.5358, 0.0542, 0.2053, 0.0628],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.030

[Epoch: 32, batch: 215/219] total loss per batch: 0.722
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0238, 0.0231, 0.6929, 0.0418, 0.1909, 0.0200, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.047

[Epoch: 33, batch: 43/219] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1800, 0.4314, 0.1837, 0.0188, 0.1476, 0.0215, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.002

[Epoch: 33, batch: 86/219] total loss per batch: 0.758
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6690e-04, 2.4352e-02, 9.3486e-01, 1.5592e-02, 3.1420e-03, 1.7888e-04,
        2.1710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.035

[Epoch: 33, batch: 129/219] total loss per batch: 0.726
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.2310e-05, 1.1362e-01, 1.9070e-02, 8.1663e-01, 3.3238e-05, 9.7458e-04,
        4.9639e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.021

[Epoch: 33, batch: 172/219] total loss per batch: 0.717
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0254, 0.0247, 0.0026, 0.9060, 0.0184, 0.0156, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.041

[Epoch: 33, batch: 215/219] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0135, 0.0078, 0.9497, 0.0126, 0.0068, 0.0074, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 34, batch: 43/219] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0985, 0.6214, 0.0950, 0.0181, 0.1226, 0.0063, 0.0379],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.000

[Epoch: 34, batch: 86/219] total loss per batch: 0.750
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2822e-04, 5.0331e-02, 7.3646e-01, 1.8413e-02, 1.1276e-02, 3.1856e-04,
        1.8307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 34, batch: 129/219] total loss per batch: 0.724
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.6301e-05, 1.7076e-01, 1.6092e-02, 7.6261e-01, 1.6047e-05, 1.8919e-04,
        5.0303e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.031

[Epoch: 34, batch: 172/219] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0164, 0.0136, 0.0047, 0.7793, 0.0267, 0.1225, 0.0367],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 34, batch: 215/219] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0211, 0.0140, 0.8651, 0.0284, 0.0424, 0.0216, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 35, batch: 43/219] total loss per batch: 0.741
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2367, 0.2187, 0.2389, 0.0265, 0.2046, 0.0418, 0.0328],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 35, batch: 86/219] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.3353e-05, 6.7557e-03, 9.6994e-01, 3.1227e-03, 1.2382e-03, 2.4753e-05,
        1.8865e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 35, batch: 129/219] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.9215e-05, 4.2647e-01, 1.6614e-02, 4.9684e-01, 3.3433e-05, 3.1392e-04,
        5.9703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.039

[Epoch: 35, batch: 172/219] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0472, 0.0114, 0.0052, 0.8898, 0.0120, 0.0192, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.036

[Epoch: 35, batch: 215/219] total loss per batch: 0.712
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0146, 0.0067, 0.9142, 0.0096, 0.0315, 0.0150, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 36, batch: 43/219] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1012, 0.6908, 0.1271, 0.0197, 0.0455, 0.0036, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.001

[Epoch: 36, batch: 86/219] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.4186e-04, 5.6130e-02, 7.8741e-01, 2.9912e-02, 8.4346e-03, 1.1766e-04,
        1.1766e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 36, batch: 129/219] total loss per batch: 0.718
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.6178e-04, 3.1838e-01, 3.1233e-02, 5.3735e-01, 2.2561e-04, 6.3926e-04,
        1.1191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.032

[Epoch: 36, batch: 172/219] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0314, 0.0264, 0.0055, 0.8095, 0.0900, 0.0255, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.026

[Epoch: 36, batch: 215/219] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0222, 0.0201, 0.8668, 0.0172, 0.0487, 0.0182, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 37, batch: 43/219] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2237, 0.3057, 0.2716, 0.0281, 0.1375, 0.0195, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.002

[Epoch: 37, batch: 86/219] total loss per batch: 0.737
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.1728e-05, 9.7258e-03, 9.3586e-01, 1.4697e-02, 3.4204e-03, 5.1103e-05,
        3.6190e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.035

[Epoch: 37, batch: 129/219] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1118e-05, 2.5548e-01, 1.1635e-02, 6.9788e-01, 1.2782e-05, 9.5475e-05,
        3.4889e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.032

[Epoch: 37, batch: 172/219] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0251, 0.0322, 0.0043, 0.8177, 0.0220, 0.0578, 0.0409],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.026

[Epoch: 37, batch: 215/219] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0279, 0.0297, 0.8309, 0.0182, 0.0675, 0.0203, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 38, batch: 43/219] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1516, 0.5961, 0.1316, 0.0229, 0.0522, 0.0264, 0.0192],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.003

[Epoch: 38, batch: 86/219] total loss per batch: 0.735
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6098e-04, 2.4148e-02, 9.0783e-01, 1.7094e-02, 8.7237e-03, 8.4355e-05,
        4.1958e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 38, batch: 129/219] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.6073e-05, 1.7762e-01, 1.9918e-02, 7.5414e-01, 1.7927e-05, 3.7640e-05,
        4.8214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.037

[Epoch: 38, batch: 172/219] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0179, 0.0222, 0.0054, 0.8838, 0.0208, 0.0298, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 38, batch: 215/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0069, 0.0068, 0.9453, 0.0076, 0.0180, 0.0127, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 39, batch: 43/219] total loss per batch: 0.732
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1298, 0.3148, 0.3369, 0.0154, 0.1628, 0.0162, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 39, batch: 86/219] total loss per batch: 0.731
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.8162e-04, 3.7677e-02, 8.9575e-01, 1.5082e-02, 7.2785e-03, 8.3918e-05,
        4.3951e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.045

[Epoch: 39, batch: 129/219] total loss per batch: 0.710
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.4379e-05, 2.5834e-01, 8.5292e-03, 7.0572e-01, 1.1657e-05, 4.9523e-04,
        2.6881e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.078

[Epoch: 39, batch: 172/219] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0554, 0.0191, 0.0062, 0.8426, 0.0365, 0.0187, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 39, batch: 215/219] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0260, 0.0075, 0.8926, 0.0145, 0.0387, 0.0167, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 40, batch: 43/219] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2685, 0.4015, 0.1749, 0.0204, 0.0648, 0.0547, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.003

[Epoch: 40, batch: 86/219] total loss per batch: 0.736
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.4866e-04, 2.8630e-02, 8.5695e-01, 2.5462e-02, 9.5130e-03, 1.2515e-04,
        7.9173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 40, batch: 129/219] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.1390e-05, 2.0526e-01, 2.9767e-02, 6.9787e-01, 6.6714e-05, 1.3217e-04,
        6.6858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.018

[Epoch: 40, batch: 172/219] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0139, 0.0098, 0.0054, 0.9267, 0.0140, 0.0112, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 40, batch: 215/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0074, 0.0132, 0.9177, 0.0154, 0.0338, 0.0078, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 41, batch: 43/219] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0659, 0.5821, 0.2163, 0.0124, 0.1087, 0.0051, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.001

[Epoch: 41, batch: 86/219] total loss per batch: 0.732
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.9520e-04, 4.3692e-02, 8.7584e-01, 1.1513e-02, 1.0561e-02, 3.2143e-05,
        5.8164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.033

[Epoch: 41, batch: 129/219] total loss per batch: 0.710
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.7164e-05, 2.5314e-01, 1.9213e-02, 7.0291e-01, 1.2485e-05, 1.2165e-04,
        2.4554e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.087

[Epoch: 41, batch: 172/219] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0313, 0.0160, 0.0057, 0.8587, 0.0142, 0.0497, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 41, batch: 215/219] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0107, 0.0087, 0.8627, 0.0156, 0.0752, 0.0218, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 42, batch: 43/219] total loss per batch: 0.730
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1995, 0.5025, 0.1919, 0.0172, 0.0394, 0.0400, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.003

[Epoch: 42, batch: 86/219] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.9659e-05, 2.6937e-02, 8.8221e-01, 3.0064e-02, 9.9157e-03, 1.0553e-04,
        5.0666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 42, batch: 129/219] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.9973e-06, 4.2473e-01, 1.1130e-02, 5.2623e-01, 2.2978e-05, 4.3656e-04,
        3.7445e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.071

[Epoch: 42, batch: 172/219] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0179, 0.0145, 0.0066, 0.8362, 0.0762, 0.0319, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.027

[Epoch: 42, batch: 215/219] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0131, 0.0067, 0.9342, 0.0152, 0.0177, 0.0095, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 43, batch: 43/219] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0978, 0.4604, 0.2559, 0.0226, 0.1268, 0.0230, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.000

[Epoch: 43, batch: 86/219] total loss per batch: 0.729
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.2061e-04, 1.5149e-02, 9.3450e-01, 1.4409e-02, 9.1995e-03, 7.1790e-05,
        2.6349e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 43, batch: 129/219] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3268e-04, 1.6846e-01, 1.1336e-02, 7.7117e-01, 4.0150e-05, 1.8648e-04,
        4.8668e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.079

[Epoch: 43, batch: 172/219] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0462, 0.0240, 0.0062, 0.8263, 0.0226, 0.0415, 0.0333],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 43, batch: 215/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0062, 0.0102, 0.8681, 0.0167, 0.0671, 0.0202, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.014

[Epoch: 44, batch: 43/219] total loss per batch: 0.728
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2461, 0.5068, 0.1543, 0.0394, 0.0303, 0.0175, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.000

[Epoch: 44, batch: 86/219] total loss per batch: 0.725
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.1062e-05, 4.3729e-02, 7.4718e-01, 8.3167e-02, 1.3378e-02, 2.9905e-04,
        1.1215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 44, batch: 129/219] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1339e-05, 2.7531e-01, 2.4806e-02, 6.5280e-01, 9.1161e-05, 6.4668e-04,
        4.6334e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.139

[Epoch: 44, batch: 172/219] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0228, 0.0074, 0.0060, 0.8961, 0.0185, 0.0317, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.032

[Epoch: 44, batch: 215/219] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0279, 0.0125, 0.8762, 0.0123, 0.0317, 0.0348, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 45, batch: 43/219] total loss per batch: 0.725
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1332, 0.4121, 0.2086, 0.0178, 0.1528, 0.0238, 0.0516],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.004

[Epoch: 45, batch: 86/219] total loss per batch: 0.722
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.1484e-05, 1.8438e-02, 9.4511e-01, 1.2295e-02, 6.4018e-03, 2.2312e-05,
        1.7675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.029

[Epoch: 45, batch: 129/219] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0846e-05, 3.2301e-01, 1.1357e-02, 6.1211e-01, 1.1421e-05, 6.2477e-05,
        5.3440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.111

[Epoch: 45, batch: 172/219] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0187, 0.0156, 0.0028, 0.9128, 0.0183, 0.0236, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 45, batch: 215/219] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0130, 0.0128, 0.9145, 0.0138, 0.0256, 0.0106, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 46, batch: 43/219] total loss per batch: 0.721
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1303, 0.6746, 0.0679, 0.0189, 0.0923, 0.0059, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.005

[Epoch: 46, batch: 86/219] total loss per batch: 0.717
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.8994e-05, 2.0530e-02, 8.9656e-01, 9.3912e-03, 4.9510e-03, 7.2198e-05,
        6.8452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.030

[Epoch: 46, batch: 129/219] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.0917e-05, 2.5630e-01, 1.6604e-02, 6.9579e-01, 2.7559e-05, 1.4757e-04,
        3.1069e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.124

[Epoch: 46, batch: 172/219] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0377, 0.0289, 0.0086, 0.8076, 0.0169, 0.0510, 0.0492],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.026

[Epoch: 46, batch: 215/219] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0101, 0.0073, 0.9219, 0.0099, 0.0351, 0.0121, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 47, batch: 43/219] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1739, 0.5236, 0.1455, 0.0128, 0.1154, 0.0164, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.008

[Epoch: 47, batch: 86/219] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.8223e-05, 2.1528e-02, 9.5104e-01, 8.9676e-03, 6.8284e-03, 4.2357e-05,
        1.1513e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 47, batch: 129/219] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.5697e-05, 4.1369e-01, 1.3992e-02, 5.1453e-01, 3.4703e-05, 3.1884e-04,
        5.7415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.042

[Epoch: 47, batch: 172/219] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0458, 0.0290, 0.0158, 0.7693, 0.0686, 0.0209, 0.0506],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 47, batch: 215/219] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0258, 0.0180, 0.8639, 0.0149, 0.0522, 0.0148, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 48, batch: 43/219] total loss per batch: 0.717
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1637, 0.3372, 0.2532, 0.0180, 0.2071, 0.0066, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.015

[Epoch: 48, batch: 86/219] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.4143e-05, 2.8875e-02, 9.0120e-01, 1.9035e-02, 8.6564e-03, 4.6250e-05,
        4.2095e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 48, batch: 129/219] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.8054e-05, 1.7640e-01, 2.7422e-02, 7.7489e-01, 6.4435e-05, 5.8115e-04,
        2.0581e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.050

[Epoch: 48, batch: 172/219] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0378, 0.0207, 0.0053, 0.8283, 0.0103, 0.0798, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.029

[Epoch: 48, batch: 215/219] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0223, 0.0205, 0.8523, 0.0197, 0.0522, 0.0239, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.015

[Epoch: 49, batch: 43/219] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.3068, 0.4139, 0.1739, 0.0153, 0.0635, 0.0066, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.024

[Epoch: 49, batch: 86/219] total loss per batch: 0.718
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6112e-04, 1.4918e-02, 9.1276e-01, 1.6117e-02, 8.9935e-03, 3.9010e-05,
        4.7006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.039

[Epoch: 49, batch: 129/219] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.8159e-05, 3.5395e-01, 1.9037e-02, 5.6459e-01, 3.6983e-05, 1.8079e-04,
        6.2114e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.056

[Epoch: 49, batch: 172/219] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0210, 0.0128, 0.0076, 0.8751, 0.0278, 0.0261, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 49, batch: 215/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0150, 0.0061, 0.9267, 0.0116, 0.0239, 0.0105, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.021

[Epoch: 50, batch: 43/219] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1078, 0.5960, 0.1370, 0.0171, 0.1181, 0.0065, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.004

[Epoch: 50, batch: 86/219] total loss per batch: 0.717
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.5367e-05, 4.6261e-02, 8.9702e-01, 1.4319e-02, 9.1617e-03, 7.1387e-05,
        3.3101e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 50, batch: 129/219] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.4699e-06, 2.2784e-01, 1.2749e-02, 7.3640e-01, 3.0429e-05, 1.2433e-04,
        2.2852e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.166

[Epoch: 50, batch: 172/219] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0613, 0.0169, 0.0041, 0.8315, 0.0213, 0.0462, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 50, batch: 215/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0269, 0.0258, 0.8342, 0.0190, 0.0350, 0.0495, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.014

[Epoch: 51, batch: 43/219] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1044, 0.5920, 0.1531, 0.0128, 0.1004, 0.0190, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.002

[Epoch: 51, batch: 86/219] total loss per batch: 0.711
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0716e-04, 6.0072e-03, 9.4148e-01, 1.3853e-02, 8.4723e-03, 4.7907e-05,
        3.0033e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.025

[Epoch: 51, batch: 129/219] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2708e-05, 3.1123e-01, 1.7160e-02, 6.4095e-01, 2.7008e-05, 6.7484e-04,
        2.9941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.060

[Epoch: 51, batch: 172/219] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0200, 0.0142, 0.0060, 0.8351, 0.0275, 0.0582, 0.0390],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 51, batch: 215/219] total loss per batch: 0.674
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0120, 0.0108, 0.9066, 0.0143, 0.0429, 0.0073, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 52, batch: 43/219] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1091, 0.6395, 0.1267, 0.0124, 0.0952, 0.0076, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.009

[Epoch: 52, batch: 86/219] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2076e-04, 2.4187e-02, 9.0431e-01, 1.4564e-02, 1.3097e-02, 8.5674e-05,
        4.3637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.046

[Epoch: 52, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0259e-05, 1.9155e-01, 1.1667e-02, 7.7763e-01, 1.6760e-05, 1.2424e-04,
        1.8995e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.072

[Epoch: 52, batch: 172/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0168, 0.0082, 0.0059, 0.8787, 0.0139, 0.0325, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 52, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0068, 0.0082, 0.9295, 0.0144, 0.0221, 0.0122, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 53, batch: 43/219] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1112, 0.5809, 0.1607, 0.0137, 0.0917, 0.0165, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 53, batch: 86/219] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.3533e-05, 1.9438e-02, 8.9551e-01, 1.8474e-02, 1.5382e-02, 4.3216e-05,
        5.1065e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 53, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7271e-05, 2.2896e-01, 1.5988e-02, 7.1954e-01, 2.7568e-05, 3.4838e-04,
        3.5121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.116

[Epoch: 53, batch: 172/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0309, 0.0114, 0.0066, 0.8704, 0.0203, 0.0317, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 53, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0140, 0.0099, 0.8941, 0.0174, 0.0417, 0.0130, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 54, batch: 43/219] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1189, 0.5245, 0.2135, 0.0140, 0.1078, 0.0083, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 54, batch: 86/219] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.9347e-05, 1.9726e-02, 9.3331e-01, 1.5591e-02, 8.3318e-03, 3.0654e-05,
        2.2927e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 54, batch: 129/219] total loss per batch: 0.673
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7808e-05, 3.5334e-01, 1.1260e-02, 6.0749e-01, 1.7840e-05, 4.4290e-05,
        2.7825e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.078

[Epoch: 54, batch: 172/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0250, 0.0140, 0.0077, 0.8623, 0.0215, 0.0315, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 54, batch: 215/219] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0082, 0.0095, 0.9347, 0.0099, 0.0206, 0.0134, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.032

[Epoch: 55, batch: 43/219] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1604, 0.4498, 0.2403, 0.0202, 0.1073, 0.0128, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 55, batch: 86/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.7318e-05, 1.2380e-02, 9.3336e-01, 1.6453e-02, 1.2383e-02, 5.6908e-05,
        2.5322e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.049

[Epoch: 55, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.4618e-05, 1.9947e-01, 1.6781e-02, 7.5067e-01, 2.1639e-05, 1.5300e-04,
        3.2886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.102

[Epoch: 55, batch: 172/219] total loss per batch: 0.664
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0344, 0.0142, 0.0085, 0.8255, 0.0265, 0.0579, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 55, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0105, 0.0082, 0.9227, 0.0129, 0.0269, 0.0120, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 56, batch: 43/219] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0927, 0.5217, 0.2257, 0.0142, 0.1232, 0.0103, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 56, batch: 86/219] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.7989e-05, 2.4781e-02, 9.0669e-01, 2.1412e-02, 9.1308e-03, 3.4290e-05,
        3.7878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 56, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3639e-05, 2.3751e-01, 1.2751e-02, 7.2392e-01, 8.1050e-06, 2.9301e-05,
        2.5772e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.077

[Epoch: 56, batch: 172/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0195, 0.0168, 0.0062, 0.8581, 0.0145, 0.0387, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 56, batch: 215/219] total loss per batch: 0.665
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0131, 0.0115, 0.9122, 0.0167, 0.0249, 0.0148, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.010

[Epoch: 57, batch: 43/219] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2316, 0.3273, 0.2339, 0.0209, 0.1597, 0.0128, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.014

[Epoch: 57, batch: 86/219] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.3103e-05, 2.7877e-02, 8.9623e-01, 2.5369e-02, 2.1441e-02, 5.8664e-05,
        2.8945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.046

[Epoch: 57, batch: 129/219] total loss per batch: 0.678
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2223e-05, 2.4884e-01, 1.3095e-02, 7.1229e-01, 1.8078e-05, 1.2355e-04,
        2.5619e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.053

[Epoch: 57, batch: 172/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0310, 0.0061, 0.0080, 0.8727, 0.0262, 0.0323, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 57, batch: 215/219] total loss per batch: 0.666
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0133, 0.0141, 0.8988, 0.0100, 0.0323, 0.0206, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 58, batch: 43/219] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0813, 0.5865, 0.1835, 0.0162, 0.1136, 0.0058, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.051

[Epoch: 58, batch: 86/219] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.1787e-05, 1.0131e-02, 9.4439e-01, 1.1515e-02, 6.8267e-03, 3.4786e-05,
        2.7075e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 58, batch: 129/219] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.6703e-06, 2.9913e-01, 1.7264e-02, 6.5483e-01, 1.3974e-05, 6.7822e-05,
        2.8683e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.083

[Epoch: 58, batch: 172/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0169, 0.0128, 0.0043, 0.8869, 0.0126, 0.0277, 0.0388],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 58, batch: 215/219] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0117, 0.0122, 0.9178, 0.0127, 0.0245, 0.0151, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.017

[Epoch: 59, batch: 43/219] total loss per batch: 0.699
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1353, 0.4368, 0.2644, 0.0096, 0.1274, 0.0073, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 59, batch: 86/219] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1057e-04, 4.3533e-02, 8.8981e-01, 2.0574e-02, 1.0130e-02, 5.5447e-05,
        3.5786e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 59, batch: 129/219] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7896e-05, 2.4162e-01, 1.9834e-02, 7.1100e-01, 1.0086e-05, 3.4291e-05,
        2.7483e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 59, batch: 172/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0385, 0.0160, 0.0061, 0.8468, 0.0185, 0.0424, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 59, batch: 215/219] total loss per batch: 0.665
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0214, 0.0181, 0.8525, 0.0109, 0.0545, 0.0317, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 60, batch: 43/219] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1484, 0.4892, 0.1996, 0.0236, 0.1184, 0.0073, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.001

[Epoch: 60, batch: 86/219] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.5301e-05, 1.6643e-02, 8.8112e-01, 2.2552e-02, 2.2276e-02, 1.7670e-04,
        5.7185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 60, batch: 129/219] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.2978e-06, 1.8794e-01, 1.8187e-02, 7.7082e-01, 1.4881e-05, 2.5374e-05,
        2.3005e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.123

[Epoch: 60, batch: 172/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0197, 0.0102, 0.0071, 0.8807, 0.0188, 0.0269, 0.0366],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 60, batch: 215/219] total loss per batch: 0.668
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0120, 0.0095, 0.9361, 0.0106, 0.0120, 0.0157, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.038

[Epoch: 61, batch: 43/219] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1631, 0.4665, 0.1930, 0.0190, 0.1388, 0.0085, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 61, batch: 86/219] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.2950e-05, 8.9132e-03, 9.5672e-01, 1.7745e-02, 5.8952e-03, 4.9678e-05,
        1.0605e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 61, batch: 129/219] total loss per batch: 0.681
Policy (actual, predicted): 3 1
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.3503e-05, 4.7805e-01, 2.3058e-02, 4.5427e-01, 4.1181e-06, 6.8224e-05,
        4.4516e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.112

[Epoch: 61, batch: 172/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0247, 0.0089, 0.0067, 0.8823, 0.0245, 0.0199, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 61, batch: 215/219] total loss per batch: 0.666
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0143, 0.0125, 0.9004, 0.0128, 0.0358, 0.0179, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 62, batch: 43/219] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1453, 0.6391, 0.1025, 0.0183, 0.0736, 0.0117, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 62, batch: 86/219] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.8409e-05, 3.1399e-02, 8.8191e-01, 1.7924e-02, 1.3567e-02, 6.6754e-05,
        5.5075e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.040

[Epoch: 62, batch: 129/219] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.1777e-06, 2.9830e-01, 1.7424e-02, 6.6172e-01, 1.7740e-05, 3.8686e-05,
        2.2490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.135

[Epoch: 62, batch: 172/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0256, 0.0225, 0.0105, 0.8452, 0.0190, 0.0386, 0.0384],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 62, batch: 215/219] total loss per batch: 0.665
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0234, 0.0125, 0.8863, 0.0133, 0.0367, 0.0204, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 63, batch: 43/219] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1284, 0.6005, 0.1093, 0.0148, 0.1112, 0.0118, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 63, batch: 86/219] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.6756e-05, 1.1961e-02, 9.1183e-01, 1.9815e-02, 1.0803e-02, 4.7116e-05,
        4.5500e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.042

[Epoch: 63, batch: 129/219] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.6750e-06, 2.2306e-01, 2.5148e-02, 7.0974e-01, 7.6956e-06, 8.2388e-05,
        4.1953e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.118

[Epoch: 63, batch: 172/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0384, 0.0153, 0.0077, 0.8202, 0.0413, 0.0524, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 63, batch: 215/219] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0185, 0.0165, 0.8916, 0.0155, 0.0269, 0.0218, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 64, batch: 43/219] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1503, 0.5152, 0.1829, 0.0144, 0.1186, 0.0090, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.005

[Epoch: 64, batch: 86/219] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.8260e-05, 1.4614e-02, 9.2419e-01, 1.7467e-02, 1.1089e-02, 1.2945e-04,
        3.2438e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.039

[Epoch: 64, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6586e-05, 1.8046e-01, 1.8066e-02, 7.7863e-01, 2.2422e-05, 6.7682e-05,
        2.2734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.110

[Epoch: 64, batch: 172/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0325, 0.0223, 0.0092, 0.8573, 0.0153, 0.0266, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 64, batch: 215/219] total loss per batch: 0.663
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0112, 0.0100, 0.9362, 0.0072, 0.0214, 0.0093, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 65, batch: 43/219] total loss per batch: 0.699
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1996, 0.3863, 0.1924, 0.0200, 0.1573, 0.0168, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.042

[Epoch: 65, batch: 86/219] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.7424e-05, 1.9019e-02, 9.1672e-01, 2.1235e-02, 1.0651e-02, 2.3933e-05,
        3.2329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.050

[Epoch: 65, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.9316e-05, 4.2874e-01, 2.3049e-02, 5.0713e-01, 3.2417e-05, 1.7620e-04,
        4.0836e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.075

[Epoch: 65, batch: 172/219] total loss per batch: 0.664
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0218, 0.0090, 0.0060, 0.9054, 0.0208, 0.0194, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 65, batch: 215/219] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0139, 0.0112, 0.9246, 0.0096, 0.0225, 0.0145, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 66, batch: 43/219] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1622, 0.3627, 0.2124, 0.0188, 0.2037, 0.0184, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 66, batch: 86/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.9709e-05, 1.2166e-02, 9.2851e-01, 1.6256e-02, 1.0811e-02, 7.0384e-05,
        3.2151e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 66, batch: 129/219] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.1973e-05, 2.4682e-01, 1.8606e-02, 7.0871e-01, 2.3540e-05, 4.9429e-05,
        2.5774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.111

[Epoch: 66, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0201, 0.0240, 0.0047, 0.8633, 0.0150, 0.0330, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 66, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0124, 0.0107, 0.9109, 0.0140, 0.0289, 0.0168, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 67, batch: 43/219] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.2548, 0.3264, 0.2577, 0.0119, 0.1259, 0.0049, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 67, batch: 86/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.8385e-05, 2.6003e-02, 9.2582e-01, 1.5668e-02, 1.4047e-02, 1.8712e-05,
        1.8399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 67, batch: 129/219] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.2322e-05, 3.4060e-01, 2.5097e-02, 5.9345e-01, 5.0933e-05, 1.5077e-04,
        4.0599e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.057

[Epoch: 67, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0147, 0.0070, 0.0075, 0.9187, 0.0211, 0.0151, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 67, batch: 215/219] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0134, 0.0121, 0.9265, 0.0119, 0.0214, 0.0106, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.014

[Epoch: 68, batch: 43/219] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1605, 0.5446, 0.1217, 0.0127, 0.1359, 0.0123, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.056

[Epoch: 68, batch: 86/219] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6326e-05, 1.5997e-02, 8.8680e-01, 1.7523e-02, 2.1559e-02, 1.5663e-04,
        5.7950e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 68, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3701e-05, 2.7420e-01, 1.5707e-02, 6.9004e-01, 1.2638e-05, 3.4033e-05,
        1.9996e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.082

[Epoch: 68, batch: 172/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0368, 0.0219, 0.0089, 0.8440, 0.0144, 0.0409, 0.0331],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 68, batch: 215/219] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0263, 0.0156, 0.8689, 0.0134, 0.0383, 0.0260, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 69, batch: 43/219] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1061, 0.5596, 0.2138, 0.0148, 0.0852, 0.0048, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 69, batch: 86/219] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.3271e-05, 1.5287e-02, 9.3838e-01, 1.2217e-02, 7.4931e-03, 3.8870e-05,
        2.6523e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.034

[Epoch: 69, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.7390e-05, 2.5028e-01, 1.7135e-02, 6.9459e-01, 1.9700e-05, 4.8293e-05,
        3.7842e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.137

[Epoch: 69, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0197, 0.0142, 0.0065, 0.8990, 0.0108, 0.0242, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 69, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0062, 0.0079, 0.9438, 0.0142, 0.0164, 0.0076, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.010

[Epoch: 70, batch: 43/219] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1062, 0.6518, 0.1282, 0.0112, 0.0851, 0.0060, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.010

[Epoch: 70, batch: 86/219] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.1827e-05, 1.7318e-02, 9.2228e-01, 2.9462e-02, 7.9566e-03, 1.5971e-05,
        2.2945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.047

[Epoch: 70, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1249e-05, 2.0454e-01, 1.6679e-02, 7.4222e-01, 1.0264e-05, 3.0420e-05,
        3.6509e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.087

[Epoch: 70, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0297, 0.0157, 0.0096, 0.8553, 0.0169, 0.0322, 0.0407],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 70, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0152, 0.0169, 0.8950, 0.0113, 0.0383, 0.0162, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.022

[Epoch: 71, batch: 43/219] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1623, 0.4037, 0.2381, 0.0172, 0.1430, 0.0092, 0.0264],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 71, batch: 86/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.3827e-05, 1.5956e-02, 9.3370e-01, 1.5824e-02, 9.9727e-03, 4.2216e-05,
        2.4485e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.054

[Epoch: 71, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.7268e-06, 2.7404e-01, 1.4756e-02, 6.8039e-01, 9.0920e-06, 4.6314e-05,
        3.0744e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.116

[Epoch: 71, batch: 172/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0338, 0.0144, 0.0104, 0.8713, 0.0254, 0.0234, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 71, batch: 215/219] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0131, 0.0123, 0.9162, 0.0113, 0.0271, 0.0140, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 72, batch: 43/219] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1348, 0.5405, 0.1734, 0.0129, 0.1074, 0.0095, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 72, batch: 86/219] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.9995e-05, 1.8187e-02, 8.9370e-01, 1.8451e-02, 2.2190e-02, 5.0977e-05,
        4.7398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.044

[Epoch: 72, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.8608e-05, 2.4716e-01, 2.7253e-02, 6.9468e-01, 1.7326e-05, 3.4640e-05,
        3.0832e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.148

[Epoch: 72, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0142, 0.0206, 0.0084, 0.8573, 0.0209, 0.0419, 0.0367],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 72, batch: 215/219] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0136, 0.9235, 0.0133, 0.0237, 0.0129, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 73, batch: 43/219] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1395, 0.3945, 0.2549, 0.0227, 0.1741, 0.0061, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 73, batch: 86/219] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.5894e-05, 2.6186e-02, 9.1357e-01, 1.7616e-02, 1.2982e-02, 3.1757e-05,
        2.9585e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.051

[Epoch: 73, batch: 129/219] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.2159e-06, 4.4048e-01, 1.2096e-02, 5.1078e-01, 2.7410e-06, 6.3669e-06,
        3.6628e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.063

[Epoch: 73, batch: 172/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0284, 0.0187, 0.0108, 0.8450, 0.0235, 0.0219, 0.0517],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 73, batch: 215/219] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0186, 0.0184, 0.8788, 0.0188, 0.0335, 0.0222, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.022

[Epoch: 74, batch: 43/219] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0973, 0.5553, 0.2241, 0.0103, 0.0851, 0.0092, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 74, batch: 86/219] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.1547e-05, 1.9189e-02, 9.2682e-01, 1.5371e-02, 1.5973e-02, 4.3996e-05,
        2.2536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.044

[Epoch: 74, batch: 129/219] total loss per batch: 0.677
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3587e-05, 3.2969e-01, 3.0033e-02, 5.9512e-01, 1.2084e-05, 7.6553e-05,
        4.5064e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.087

[Epoch: 74, batch: 172/219] total loss per batch: 0.664
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0259, 0.0180, 0.0109, 0.8664, 0.0267, 0.0218, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 74, batch: 215/219] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0174, 0.0127, 0.8948, 0.0164, 0.0360, 0.0145, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 75, batch: 43/219] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1906, 0.4040, 0.1668, 0.0227, 0.1786, 0.0069, 0.0304],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 75, batch: 86/219] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.0912e-05, 3.3192e-02, 8.7628e-01, 4.0585e-02, 7.3337e-03, 1.0550e-05,
        4.2575e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.051

[Epoch: 75, batch: 129/219] total loss per batch: 0.676
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.1493e-06, 1.8105e-01, 1.4147e-02, 7.8848e-01, 5.9269e-06, 1.0359e-05,
        1.6297e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.062

[Epoch: 75, batch: 172/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0302, 0.0126, 0.0055, 0.8944, 0.0132, 0.0185, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 75, batch: 215/219] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0071, 0.0212, 0.9125, 0.0093, 0.0312, 0.0139, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.017

[Epoch: 76, batch: 43/219] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1055, 0.4883, 0.2687, 0.0188, 0.0963, 0.0104, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.019

[Epoch: 76, batch: 86/219] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.9298e-05, 1.8814e-02, 8.8764e-01, 2.1584e-02, 2.7498e-02, 5.9856e-05,
        4.4351e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.039

[Epoch: 76, batch: 129/219] total loss per batch: 0.675
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.3382e-06, 2.6673e-01, 1.5665e-02, 6.8728e-01, 1.5095e-05, 3.8079e-05,
        3.0270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.035

[Epoch: 76, batch: 172/219] total loss per batch: 0.665
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0494, 0.0257, 0.0172, 0.7945, 0.0411, 0.0356, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 76, batch: 215/219] total loss per batch: 0.662
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0135, 0.0076, 0.9167, 0.0097, 0.0308, 0.0172, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 77, batch: 43/219] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1280, 0.5672, 0.1251, 0.0131, 0.1509, 0.0042, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.062

[Epoch: 77, batch: 86/219] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.8450e-05, 8.7391e-03, 9.6097e-01, 1.1790e-02, 5.9970e-03, 2.5757e-06,
        1.2475e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.056

[Epoch: 77, batch: 129/219] total loss per batch: 0.674
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.7306e-06, 1.3080e-01, 1.2676e-02, 8.3123e-01, 6.4926e-06, 1.1222e-05,
        2.5272e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.143

[Epoch: 77, batch: 172/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0161, 0.0239, 0.0083, 0.8619, 0.0201, 0.0419, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 77, batch: 215/219] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0128, 0.0146, 0.9032, 0.0128, 0.0325, 0.0178, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.026

[Epoch: 78, batch: 43/219] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1348, 0.6132, 0.1053, 0.0112, 0.1073, 0.0074, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.017

[Epoch: 78, batch: 86/219] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.1493e-05, 3.0073e-02, 8.7954e-01, 4.6028e-02, 1.9117e-02, 3.8156e-05,
        2.5152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.039

[Epoch: 78, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.2778e-06, 2.9538e-01, 1.7584e-02, 6.5953e-01, 8.6202e-06, 2.6697e-05,
        2.7464e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.090

[Epoch: 78, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0250, 0.0136, 0.0086, 0.8774, 0.0274, 0.0139, 0.0342],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 78, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0106, 0.0188, 0.9061, 0.0090, 0.0368, 0.0135, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.025

[Epoch: 79, batch: 43/219] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0912, 0.7038, 0.0871, 0.0078, 0.0962, 0.0051, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 79, batch: 86/219] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.4793e-05, 6.9520e-03, 9.5053e-01, 1.2272e-02, 8.6265e-03, 6.2901e-06,
        2.1591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.053

[Epoch: 79, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.6516e-06, 2.1507e-01, 2.3373e-02, 7.3153e-01, 4.1627e-06, 1.4345e-05,
        3.0000e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 79, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0210, 0.0086, 0.0097, 0.8755, 0.0282, 0.0302, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 79, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0102, 0.0110, 0.9076, 0.0127, 0.0297, 0.0217, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 80, batch: 43/219] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1100, 0.4851, 0.2283, 0.0154, 0.1318, 0.0076, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.006

[Epoch: 80, batch: 86/219] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.2327e-05, 4.4352e-02, 7.6481e-01, 6.4709e-02, 3.2313e-02, 4.8737e-05,
        9.3709e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.036

[Epoch: 80, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.7350e-06, 3.5248e-01, 2.0144e-02, 5.9779e-01, 3.8981e-06, 2.7092e-05,
        2.9549e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.112

[Epoch: 80, batch: 172/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0200, 0.0166, 0.0079, 0.8944, 0.0138, 0.0270, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 80, batch: 215/219] total loss per batch: 0.655
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0104, 0.0216, 0.8975, 0.0116, 0.0395, 0.0128, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 81, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1461, 0.5711, 0.1488, 0.0105, 0.0957, 0.0078, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 81, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.0851e-05, 5.4884e-03, 9.6392e-01, 1.4046e-02, 5.3290e-03, 1.0083e-05,
        1.1189e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.046

[Epoch: 81, batch: 129/219] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.2585e-05, 3.2264e-01, 2.4625e-02, 6.1350e-01, 3.3907e-06, 3.4094e-05,
        3.9162e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.116

[Epoch: 81, batch: 172/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0289, 0.0184, 0.0087, 0.8621, 0.0152, 0.0345, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 81, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0098, 0.0142, 0.9227, 0.0120, 0.0183, 0.0175, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 82, batch: 43/219] total loss per batch: 0.693
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1970, 0.2456, 0.3153, 0.0218, 0.1688, 0.0084, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 0.002

[Epoch: 82, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.5996e-05, 2.4653e-02, 9.2832e-01, 1.6300e-02, 7.4018e-03, 4.1793e-05,
        2.3266e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 82, batch: 129/219] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.7048e-06, 3.6021e-01, 1.8372e-02, 5.9325e-01, 1.0101e-05, 1.8232e-05,
        2.8127e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.115

[Epoch: 82, batch: 172/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0206, 0.0168, 0.0119, 0.8624, 0.0225, 0.0237, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 82, batch: 215/219] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0116, 0.0168, 0.8963, 0.0105, 0.0392, 0.0186, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.021

[Epoch: 83, batch: 43/219] total loss per batch: 0.692
Policy (actual, predicted): 1 2
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1708, 0.2970, 0.3738, 0.0126, 0.1190, 0.0069, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.019

[Epoch: 83, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1065e-05, 4.3579e-03, 9.6798e-01, 1.4108e-02, 4.4313e-03, 8.8640e-06,
        9.0981e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.041

[Epoch: 83, batch: 129/219] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.9893e-06, 1.7703e-01, 1.8114e-02, 7.8220e-01, 3.5510e-06, 2.7538e-05,
        2.2625e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.146

[Epoch: 83, batch: 172/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0297, 0.0091, 0.0129, 0.8624, 0.0231, 0.0264, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 83, batch: 215/219] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0120, 0.0252, 0.8745, 0.0171, 0.0314, 0.0311, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 84, batch: 43/219] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1089, 0.5449, 0.1776, 0.0147, 0.1110, 0.0106, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 84, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.8874e-05, 2.4242e-02, 8.5526e-01, 3.5669e-02, 2.0517e-02, 8.2897e-05,
        6.4174e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.042

[Epoch: 84, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7208e-05, 2.3540e-01, 1.7822e-02, 7.0794e-01, 8.1520e-06, 2.3795e-05,
        3.8788e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.051

[Epoch: 84, batch: 172/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0243, 0.0365, 0.0087, 0.8220, 0.0233, 0.0285, 0.0567],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 84, batch: 215/219] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0105, 0.9237, 0.0077, 0.0361, 0.0098, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.015

[Epoch: 85, batch: 43/219] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1588, 0.4181, 0.2190, 0.0127, 0.1546, 0.0080, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 85, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.7306e-05, 4.7381e-03, 9.6340e-01, 1.6401e-02, 4.7764e-03, 1.8751e-05,
        1.0644e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 85, batch: 129/219] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.4175e-06, 3.1126e-01, 2.0414e-02, 6.3221e-01, 5.0169e-06, 2.2880e-05,
        3.6083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.115

[Epoch: 85, batch: 172/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0207, 0.0068, 0.0091, 0.9091, 0.0140, 0.0211, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 85, batch: 215/219] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0071, 0.0130, 0.9275, 0.0098, 0.0201, 0.0168, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 86, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1426, 0.4475, 0.2024, 0.0162, 0.1578, 0.0106, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.016

[Epoch: 86, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.3142e-05, 2.7455e-02, 8.8067e-01, 3.5553e-02, 2.3167e-02, 1.5518e-04,
        3.2940e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 86, batch: 129/219] total loss per batch: 0.671
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.3795e-05, 2.2345e-01, 2.1729e-02, 7.0821e-01, 6.3473e-06, 8.2539e-05,
        4.6474e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.053

[Epoch: 86, batch: 172/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0196, 0.0108, 0.0094, 0.8831, 0.0299, 0.0233, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 86, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0067, 0.0092, 0.9246, 0.0085, 0.0334, 0.0130, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 87, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0994, 0.5748, 0.1614, 0.0133, 0.1303, 0.0067, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.017

[Epoch: 87, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3290e-05, 9.2904e-03, 9.4689e-01, 1.3521e-02, 1.1009e-02, 2.9224e-05,
        1.9244e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.043

[Epoch: 87, batch: 129/219] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.1655e-06, 2.8000e-01, 2.2619e-02, 6.6815e-01, 3.4109e-06, 2.1054e-05,
        2.9196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.058

[Epoch: 87, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0180, 0.0077, 0.0079, 0.8999, 0.0199, 0.0267, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 87, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0138, 0.0225, 0.8922, 0.0073, 0.0338, 0.0226, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.010

[Epoch: 88, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1106, 0.5475, 0.1693, 0.0133, 0.1331, 0.0070, 0.0192],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 88, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.9736e-05, 1.8243e-02, 9.2555e-01, 2.3351e-02, 9.0320e-03, 6.9981e-05,
        2.3698e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.034

[Epoch: 88, batch: 129/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.4424e-06, 2.8642e-01, 1.6332e-02, 6.5958e-01, 2.0511e-06, 4.9065e-05,
        3.7620e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.113

[Epoch: 88, batch: 172/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0291, 0.0100, 0.0117, 0.8826, 0.0167, 0.0213, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 88, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0099, 0.0168, 0.9185, 0.0099, 0.0238, 0.0149, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 89, batch: 43/219] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0984, 0.6409, 0.1180, 0.0113, 0.1062, 0.0084, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.009

[Epoch: 89, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.8152e-05, 1.1268e-02, 9.3088e-01, 2.6714e-02, 9.7264e-03, 2.4009e-05,
        2.1371e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.033

[Epoch: 89, batch: 129/219] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.4910e-06, 2.4285e-01, 1.7257e-02, 7.1924e-01, 4.0109e-06, 1.3911e-05,
        2.0630e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.094

[Epoch: 89, batch: 172/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0344, 0.0197, 0.0127, 0.8075, 0.0280, 0.0627, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 89, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0178, 0.0199, 0.8914, 0.0102, 0.0296, 0.0224, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 90, batch: 43/219] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1177, 0.5266, 0.1893, 0.0149, 0.1232, 0.0082, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.027

[Epoch: 90, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.2979e-05, 1.6224e-02, 9.2826e-01, 2.3917e-02, 1.1834e-02, 8.1073e-05,
        1.9654e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.039

[Epoch: 90, batch: 129/219] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.0113e-06, 3.1430e-01, 1.3724e-02, 6.4670e-01, 1.2862e-06, 8.0167e-05,
        2.5191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.088

[Epoch: 90, batch: 172/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0169, 0.0068, 0.0061, 0.9243, 0.0164, 0.0099, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 90, batch: 215/219] total loss per batch: 0.655
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0070, 0.0117, 0.9392, 0.0075, 0.0195, 0.0114, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 91, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1238, 0.4836, 0.1988, 0.0172, 0.1443, 0.0143, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.022

[Epoch: 91, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.7570e-05, 1.6894e-02, 9.0813e-01, 2.6677e-02, 1.8757e-02, 4.0346e-05,
        2.9482e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.031

[Epoch: 91, batch: 129/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2270e-05, 2.7243e-01, 2.1505e-02, 6.7962e-01, 9.6968e-06, 3.2163e-05,
        2.6389e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.076

[Epoch: 91, batch: 172/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0295, 0.0222, 0.0113, 0.8610, 0.0282, 0.0254, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 91, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0122, 0.0141, 0.9150, 0.0093, 0.0269, 0.0138, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 92, batch: 43/219] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1443, 0.4958, 0.1836, 0.0128, 0.1340, 0.0055, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 92, batch: 86/219] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.0219e-05, 1.4715e-02, 9.1339e-01, 1.4059e-02, 1.4055e-02, 3.6851e-05,
        4.3712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.019

[Epoch: 92, batch: 129/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.5482e-06, 2.7904e-01, 2.0579e-02, 6.7267e-01, 6.5150e-06, 3.0019e-05,
        2.7665e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.109

[Epoch: 92, batch: 172/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0188, 0.0081, 0.0070, 0.8963, 0.0198, 0.0152, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 92, batch: 215/219] total loss per batch: 0.653
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0161, 0.9356, 0.0080, 0.0200, 0.0102, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 93, batch: 43/219] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1669, 0.4057, 0.2277, 0.0198, 0.1387, 0.0119, 0.0293],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.016

[Epoch: 93, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0964e-05, 1.8213e-02, 9.0799e-01, 3.6613e-02, 1.5655e-02, 1.2822e-04,
        2.1389e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.030

[Epoch: 93, batch: 129/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.6567e-06, 2.7758e-01, 1.6057e-02, 6.8262e-01, 2.9323e-06, 1.1652e-05,
        2.3724e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 93, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0511, 0.0219, 0.0155, 0.7726, 0.0405, 0.0479, 0.0505],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 93, batch: 215/219] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0112, 0.0159, 0.9223, 0.0089, 0.0190, 0.0147, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 94, batch: 43/219] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1400, 0.4589, 0.2379, 0.0152, 0.1132, 0.0119, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.012

[Epoch: 94, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.3774e-05, 1.9189e-02, 9.0238e-01, 2.5800e-02, 1.2828e-02, 3.4957e-05,
        3.9736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.030

[Epoch: 94, batch: 129/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.1282e-06, 2.2270e-01, 1.9190e-02, 7.3420e-01, 2.7073e-06, 2.0132e-05,
        2.3878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.123

[Epoch: 94, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0225, 0.0117, 0.0086, 0.9003, 0.0215, 0.0172, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 94, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0084, 0.0165, 0.9152, 0.0109, 0.0249, 0.0179, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 95, batch: 43/219] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1710, 0.4776, 0.1831, 0.0161, 0.1290, 0.0085, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 95, batch: 86/219] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.9985e-05, 1.1146e-02, 9.4389e-01, 1.4062e-02, 9.3865e-03, 5.2772e-05,
        2.1439e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.021

[Epoch: 95, batch: 129/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.2449e-06, 3.7365e-01, 1.9884e-02, 5.7712e-01, 3.0689e-06, 5.1676e-06,
        2.9337e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.130

[Epoch: 95, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0270, 0.0111, 0.0089, 0.8773, 0.0217, 0.0220, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 95, batch: 215/219] total loss per batch: 0.654
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0116, 0.0180, 0.9214, 0.0065, 0.0186, 0.0176, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 96, batch: 43/219] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1567, 0.4602, 0.1815, 0.0211, 0.1189, 0.0114, 0.0501],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 96, batch: 86/219] total loss per batch: 0.687
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.7107e-05, 1.5821e-02, 9.1787e-01, 2.3841e-02, 1.7143e-02, 4.6913e-05,
        2.5243e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.037

[Epoch: 96, batch: 129/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.0924e-06, 2.1092e-01, 2.1453e-02, 7.4583e-01, 6.6503e-06, 3.0782e-05,
        2.1763e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.080

[Epoch: 96, batch: 172/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0306, 0.0127, 0.0113, 0.8690, 0.0275, 0.0199, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 96, batch: 215/219] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0131, 0.0183, 0.8869, 0.0134, 0.0324, 0.0278, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 97, batch: 43/219] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1266, 0.5621, 0.1498, 0.0153, 0.1207, 0.0092, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.016

[Epoch: 97, batch: 86/219] total loss per batch: 0.687
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.6234e-05, 1.7848e-02, 9.3733e-01, 1.7302e-02, 8.3356e-03, 2.0637e-05,
        1.9139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.038

[Epoch: 97, batch: 129/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.1873e-06, 2.7631e-01, 1.4370e-02, 6.9225e-01, 2.5325e-06, 1.2931e-05,
        1.7052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.163

[Epoch: 97, batch: 172/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0237, 0.0186, 0.0104, 0.8653, 0.0147, 0.0352, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 97, batch: 215/219] total loss per batch: 0.651
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0103, 0.0117, 0.9278, 0.0066, 0.0224, 0.0143, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 98, batch: 43/219] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1223, 0.5964, 0.1474, 0.0104, 0.0990, 0.0058, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 98, batch: 86/219] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.4536e-05, 1.3311e-02, 9.0862e-01, 3.2208e-02, 1.6868e-02, 2.2077e-05,
        2.8949e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.024

[Epoch: 98, batch: 129/219] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.3388e-06, 2.9913e-01, 1.7307e-02, 6.5488e-01, 1.8630e-06, 1.2132e-05,
        2.8673e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.075

[Epoch: 98, batch: 172/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0360, 0.0116, 0.0086, 0.8580, 0.0357, 0.0272, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 98, batch: 215/219] total loss per batch: 0.651
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0108, 0.0165, 0.9239, 0.0089, 0.0198, 0.0160, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 99, batch: 43/219] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0953, 0.5250, 0.2516, 0.0154, 0.0861, 0.0099, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.013

[Epoch: 99, batch: 86/219] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6387e-05, 1.6796e-02, 9.3672e-01, 1.6125e-02, 9.7532e-03, 2.0528e-05,
        2.0565e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.013

[Epoch: 99, batch: 129/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.3961e-06, 2.4646e-01, 1.6518e-02, 7.0629e-01, 2.0568e-06, 1.2960e-05,
        3.0710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.089

[Epoch: 99, batch: 172/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0435, 0.0361, 0.0131, 0.8117, 0.0178, 0.0384, 0.0395],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 99, batch: 215/219] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0060, 0.0120, 0.9385, 0.0057, 0.0188, 0.0143, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 100, batch: 43/219] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1133, 0.5909, 0.1108, 0.0128, 0.1340, 0.0069, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 100, batch: 86/219] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.8337e-05, 1.9262e-02, 9.2734e-01, 2.6969e-02, 1.0347e-02, 1.7978e-05,
        1.6006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.022

[Epoch: 100, batch: 129/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.6408e-06, 3.2599e-01, 1.3437e-02, 6.3435e-01, 6.8986e-06, 2.5824e-05,
        2.6188e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.056

[Epoch: 100, batch: 172/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0385, 0.0200, 0.0127, 0.8182, 0.0209, 0.0393, 0.0505],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 100, batch: 215/219] total loss per batch: 0.653
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0080, 0.0114, 0.9298, 0.0074, 0.0245, 0.0141, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 101, batch: 43/219] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1409, 0.3675, 0.2867, 0.0140, 0.1666, 0.0083, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 101, batch: 86/219] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0336e-05, 2.0729e-02, 8.8637e-01, 2.6831e-02, 2.1975e-02, 4.6273e-05,
        4.4039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.034

[Epoch: 101, batch: 129/219] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.2403e-06, 1.8213e-01, 1.7880e-02, 7.7666e-01, 2.2202e-06, 5.3463e-06,
        2.3317e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.117

[Epoch: 101, batch: 172/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0231, 0.0173, 0.0104, 0.8853, 0.0225, 0.0214, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 101, batch: 215/219] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0141, 0.0275, 0.8902, 0.0108, 0.0261, 0.0260, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 102, batch: 43/219] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1555, 0.4653, 0.2049, 0.0209, 0.1206, 0.0092, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.012

[Epoch: 102, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.4348e-05, 1.5916e-02, 9.3155e-01, 2.5277e-02, 1.0687e-02, 1.6843e-05,
        1.6531e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.018

[Epoch: 102, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.3137e-06, 1.9375e-01, 1.2541e-02, 7.7587e-01, 4.1485e-06, 2.6790e-05,
        1.7802e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.068

[Epoch: 102, batch: 172/219] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0188, 0.0096, 0.0098, 0.8980, 0.0187, 0.0204, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 102, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0089, 0.0143, 0.9236, 0.0097, 0.0243, 0.0148, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 103, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1468, 0.4391, 0.2159, 0.0152, 0.1555, 0.0100, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 103, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.4160e-06, 1.5775e-02, 9.3360e-01, 2.0737e-02, 9.1218e-03, 1.8035e-05,
        2.0740e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.018

[Epoch: 103, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.7017e-06, 2.9905e-01, 1.9487e-02, 6.5474e-01, 1.7391e-06, 1.1442e-05,
        2.6708e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.091

[Epoch: 103, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0202, 0.0146, 0.0076, 0.8865, 0.0213, 0.0220, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 103, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0160, 0.9270, 0.0076, 0.0185, 0.0168, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 104, batch: 43/219] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1197, 0.4974, 0.2081, 0.0196, 0.1235, 0.0124, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 104, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.2509e-05, 1.7884e-02, 9.2152e-01, 2.1069e-02, 1.5291e-02, 2.1538e-05,
        2.4193e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.027

[Epoch: 104, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.4896e-06, 2.8816e-01, 1.7332e-02, 6.6392e-01, 2.2254e-06, 1.4133e-05,
        3.0569e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 104, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0264, 0.0160, 0.0108, 0.8869, 0.0198, 0.0224, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.000

[Epoch: 104, batch: 215/219] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0068, 0.0140, 0.9363, 0.0056, 0.0185, 0.0149, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 105, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1253, 0.4828, 0.2046, 0.0146, 0.1417, 0.0074, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 105, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.4188e-06, 1.3678e-02, 9.3358e-01, 1.8845e-02, 1.1073e-02, 1.4876e-05,
        2.2797e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.018

[Epoch: 105, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2197e-06, 3.0169e-01, 1.7988e-02, 6.5577e-01, 1.4396e-06, 1.0798e-05,
        2.4538e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.122

[Epoch: 105, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0241, 0.0131, 0.0094, 0.8702, 0.0250, 0.0249, 0.0334],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 105, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0113, 0.0175, 0.9017, 0.0101, 0.0309, 0.0231, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 106, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1419, 0.4892, 0.1967, 0.0156, 0.1284, 0.0070, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.027

[Epoch: 106, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.7150e-06, 1.6839e-02, 9.3222e-01, 2.0324e-02, 1.2132e-02, 1.3548e-05,
        1.8460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.023

[Epoch: 106, batch: 129/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5068e-06, 2.4921e-01, 1.5455e-02, 7.1055e-01, 1.8833e-06, 6.8549e-06,
        2.4776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.067

[Epoch: 106, batch: 172/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0218, 0.0171, 0.0101, 0.8850, 0.0225, 0.0190, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 106, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0071, 0.0154, 0.9298, 0.0076, 0.0194, 0.0151, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 107, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1443, 0.4786, 0.1690, 0.0178, 0.1503, 0.0076, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 107, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3173e-05, 1.7308e-02, 9.0174e-01, 3.2073e-02, 1.6391e-02, 1.8791e-05,
        3.2460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.017

[Epoch: 107, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.6233e-06, 3.0856e-01, 1.6464e-02, 6.5067e-01, 2.6776e-06, 7.1067e-06,
        2.4298e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.105

[Epoch: 107, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0309, 0.0126, 0.0126, 0.8659, 0.0272, 0.0241, 0.0267],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 107, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0121, 0.0182, 0.9107, 0.0085, 0.0269, 0.0191, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 108, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1414, 0.4485, 0.2342, 0.0138, 0.1374, 0.0065, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 108, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.5779e-06, 1.0624e-02, 9.4777e-01, 1.9749e-02, 7.1430e-03, 1.9843e-05,
        1.4681e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.013

[Epoch: 108, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1865e-06, 2.0775e-01, 1.5495e-02, 7.5641e-01, 1.0749e-06, 1.1475e-05,
        2.0333e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 108, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0219, 0.0255, 0.0105, 0.8693, 0.0217, 0.0243, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 108, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0079, 0.0238, 0.9111, 0.0070, 0.0240, 0.0196, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 109, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1443, 0.4714, 0.2145, 0.0172, 0.1215, 0.0075, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 109, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6889e-05, 3.6652e-02, 8.7088e-01, 3.2002e-02, 1.9687e-02, 2.3617e-05,
        4.0738e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.006

[Epoch: 109, batch: 129/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.8408e-06, 3.4548e-01, 1.6783e-02, 6.1078e-01, 1.8601e-06, 1.0135e-05,
        2.6941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.122

[Epoch: 109, batch: 172/219] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0270, 0.0128, 0.0086, 0.8730, 0.0290, 0.0253, 0.0244],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 109, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0124, 0.0177, 0.9059, 0.0084, 0.0313, 0.0193, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.021

[Epoch: 110, batch: 43/219] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1082, 0.5441, 0.1805, 0.0139, 0.1318, 0.0078, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 110, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.5053e-06, 8.9982e-03, 9.4703e-01, 1.6371e-02, 9.5298e-03, 2.6431e-05,
        1.8038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.021

[Epoch: 110, batch: 129/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.8791e-06, 2.4872e-01, 1.7848e-02, 7.1043e-01, 2.0799e-06, 1.3017e-05,
        2.2984e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.099

[Epoch: 110, batch: 172/219] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0379, 0.0238, 0.0143, 0.8306, 0.0190, 0.0365, 0.0379],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 110, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0079, 0.0205, 0.9186, 0.0065, 0.0223, 0.0199, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.015

[Epoch: 111, batch: 43/219] total loss per batch: 0.684
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1357, 0.4711, 0.2224, 0.0116, 0.1234, 0.0077, 0.0283],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.039

[Epoch: 111, batch: 86/219] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.1613e-05, 2.2371e-02, 8.8805e-01, 3.3729e-02, 2.1791e-02, 2.6967e-05,
        3.4006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.010

[Epoch: 111, batch: 129/219] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.2496e-06, 2.2697e-01, 1.2280e-02, 7.4231e-01, 2.0595e-06, 7.7149e-06,
        1.8427e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.088

[Epoch: 111, batch: 172/219] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0220, 0.0174, 0.0089, 0.8817, 0.0267, 0.0194, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 111, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0074, 0.0141, 0.9195, 0.0110, 0.0244, 0.0172, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.015

[Epoch: 112, batch: 43/219] total loss per batch: 0.684
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1610, 0.4253, 0.2366, 0.0142, 0.1385, 0.0063, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 112, batch: 86/219] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0088e-05, 9.1565e-03, 9.4284e-01, 1.6919e-02, 1.2132e-02, 3.0129e-05,
        1.8910e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.022

[Epoch: 112, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0928e-05, 3.0921e-01, 1.9493e-02, 6.3985e-01, 3.6441e-06, 2.8042e-05,
        3.1403e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.078

[Epoch: 112, batch: 172/219] total loss per batch: 0.652
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0171, 0.0151, 0.0113, 0.8764, 0.0213, 0.0296, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 112, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0080, 0.0158, 0.9278, 0.0077, 0.0215, 0.0143, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 113, batch: 43/219] total loss per batch: 0.684
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1126, 0.5690, 0.1579, 0.0129, 0.1140, 0.0077, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 113, batch: 86/219] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.5231e-05, 1.2266e-02, 9.3618e-01, 2.4258e-02, 1.0552e-02, 1.1449e-05,
        1.6712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.007

[Epoch: 113, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2985e-06, 3.0903e-01, 1.5956e-02, 6.4844e-01, 1.1402e-06, 7.6745e-06,
        2.6561e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.118

[Epoch: 113, batch: 172/219] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0343, 0.0157, 0.0130, 0.8325, 0.0385, 0.0270, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 113, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0091, 0.0185, 0.9102, 0.0090, 0.0270, 0.0213, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 114, batch: 43/219] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1530, 0.4245, 0.2432, 0.0143, 0.1457, 0.0069, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 114, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.6699e-06, 1.9084e-02, 8.9762e-01, 2.3512e-02, 2.2246e-02, 3.7782e-05,
        3.7493e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.014

[Epoch: 114, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.4575e-06, 2.3352e-01, 1.4258e-02, 7.2869e-01, 5.2097e-06, 1.0551e-05,
        2.3521e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.076

[Epoch: 114, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0176, 0.0132, 0.0099, 0.9072, 0.0176, 0.0174, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.000

[Epoch: 114, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0157, 0.9391, 0.0054, 0.0156, 0.0136, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 115, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1233, 0.6051, 0.1301, 0.0128, 0.1004, 0.0099, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 115, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1662e-05, 9.6129e-03, 9.4886e-01, 2.2351e-02, 8.4786e-03, 1.1333e-05,
        1.0677e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.003

[Epoch: 115, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.7221e-06, 2.8729e-01, 1.5989e-02, 6.7367e-01, 1.5841e-06, 9.5623e-06,
        2.3039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.099

[Epoch: 115, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0158, 0.0119, 0.0085, 0.9000, 0.0173, 0.0195, 0.0270],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.024

[Epoch: 115, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0105, 0.0175, 0.8987, 0.0100, 0.0326, 0.0255, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 116, batch: 43/219] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1560, 0.4969, 0.1699, 0.0164, 0.1296, 0.0073, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.050

[Epoch: 116, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6147e-05, 2.0680e-02, 9.0784e-01, 2.5949e-02, 1.4617e-02, 2.3426e-05,
        3.0876e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.012

[Epoch: 116, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.3465e-06, 2.6791e-01, 1.2602e-02, 6.9332e-01, 1.9242e-06, 9.8415e-06,
        2.6149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 116, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0266, 0.0131, 0.0104, 0.8832, 0.0234, 0.0173, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 116, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0069, 0.0190, 0.9284, 0.0056, 0.0219, 0.0127, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.012

[Epoch: 117, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1384, 0.4675, 0.2200, 0.0149, 0.1324, 0.0068, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.020

[Epoch: 117, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.8409e-06, 1.2386e-02, 9.2922e-01, 2.3241e-02, 1.5388e-02, 1.4766e-05,
        1.9739e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.007

[Epoch: 117, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.9410e-06, 2.5657e-01, 1.7030e-02, 7.0548e-01, 1.9795e-06, 6.8605e-06,
        2.0912e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.083

[Epoch: 117, batch: 172/219] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0291, 0.0278, 0.0120, 0.7923, 0.0326, 0.0613, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 117, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0084, 0.0191, 0.9112, 0.0083, 0.0240, 0.0235, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 118, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1630, 0.4690, 0.1742, 0.0178, 0.1421, 0.0058, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.048

[Epoch: 118, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2124e-05, 1.0010e-02, 9.4006e-01, 2.3498e-02, 9.4275e-03, 2.5927e-05,
        1.6971e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.016

[Epoch: 118, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.6819e-06, 3.2906e-01, 1.4740e-02, 6.3071e-01, 1.1390e-06, 4.1211e-06,
        2.5479e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.134

[Epoch: 118, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0276, 0.0157, 0.0107, 0.8583, 0.0362, 0.0243, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 118, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0086, 0.0198, 0.9101, 0.0078, 0.0285, 0.0179, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 119, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1223, 0.4661, 0.2060, 0.0176, 0.1503, 0.0081, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.009

[Epoch: 119, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.5656e-05, 1.4049e-02, 9.2509e-01, 2.6369e-02, 1.3574e-02, 2.7747e-05,
        2.0880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.016

[Epoch: 119, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.8662e-06, 3.1725e-01, 2.0547e-02, 6.3747e-01, 3.1751e-06, 1.0406e-05,
        2.4710e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.068

[Epoch: 119, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0204, 0.0183, 0.0095, 0.8855, 0.0228, 0.0184, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 119, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0069, 0.0209, 0.9243, 0.0070, 0.0200, 0.0158, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.015

[Epoch: 120, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1214, 0.4949, 0.2168, 0.0151, 0.1239, 0.0069, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.023

[Epoch: 120, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0607e-05, 1.6328e-02, 9.1842e-01, 2.8143e-02, 1.6892e-02, 2.2132e-05,
        2.0181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.006

[Epoch: 120, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.6208e-06, 2.2319e-01, 1.4237e-02, 7.4248e-01, 1.2383e-06, 5.4299e-06,
        2.0086e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.139

[Epoch: 120, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0234, 0.0156, 0.0103, 0.8771, 0.0273, 0.0231, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.023

[Epoch: 120, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0084, 0.0205, 0.9112, 0.0090, 0.0226, 0.0224, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 121, batch: 43/219] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1080, 0.5696, 0.1513, 0.0158, 0.1253, 0.0064, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 121, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.9357e-06, 1.2076e-02, 9.3317e-01, 2.0585e-02, 1.2737e-02, 3.0933e-05,
        2.1390e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.019

[Epoch: 121, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.0634e-06, 2.8735e-01, 2.5070e-02, 6.5041e-01, 2.9575e-06, 1.1259e-05,
        3.7147e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.028

[Epoch: 121, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0258, 0.0168, 0.0095, 0.8608, 0.0265, 0.0241, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 121, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0124, 0.9465, 0.0046, 0.0146, 0.0118, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 122, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1436, 0.4492, 0.2010, 0.0180, 0.1490, 0.0102, 0.0289],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 122, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.4341e-06, 2.6907e-02, 8.9637e-01, 2.8997e-02, 1.7536e-02, 3.1776e-05,
        3.0145e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.008

[Epoch: 122, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6270e-06, 3.1736e-01, 1.4041e-02, 6.3716e-01, 2.0719e-06, 6.7543e-06,
        3.1431e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.144

[Epoch: 122, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0219, 0.0216, 0.0144, 0.8527, 0.0276, 0.0324, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 122, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0097, 0.0270, 0.8873, 0.0096, 0.0306, 0.0274, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.018

[Epoch: 123, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1078, 0.5328, 0.1944, 0.0164, 0.1147, 0.0083, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.056

[Epoch: 123, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2085e-05, 1.0830e-02, 9.4704e-01, 1.8139e-02, 9.0746e-03, 1.7638e-05,
        1.4885e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.017

[Epoch: 123, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.7313e-06, 2.2696e-01, 2.2278e-02, 7.2813e-01, 1.2117e-06, 7.2428e-06,
        2.2621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.095

[Epoch: 123, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0248, 0.0120, 0.0107, 0.8814, 0.0188, 0.0242, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 123, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0076, 0.0150, 0.9380, 0.0064, 0.0134, 0.0157, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 124, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1713, 0.4496, 0.1890, 0.0142, 0.1453, 0.0060, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.017

[Epoch: 124, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.2170e-06, 1.1720e-02, 9.3548e-01, 2.5885e-02, 8.7012e-03, 1.7575e-05,
        1.8191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.012

[Epoch: 124, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.0354e-06, 1.6884e-01, 1.1155e-02, 8.0045e-01, 2.7295e-06, 5.3114e-06,
        1.9545e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.097

[Epoch: 124, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0270, 0.0212, 0.0124, 0.8494, 0.0244, 0.0280, 0.0375],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 124, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0061, 0.0223, 0.9120, 0.0072, 0.0305, 0.0175, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.006

[Epoch: 125, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1177, 0.5635, 0.1714, 0.0186, 0.1036, 0.0084, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 125, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.2308e-05, 1.9528e-02, 8.9025e-01, 4.0239e-02, 2.1098e-02, 4.1309e-05,
        2.8819e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.016

[Epoch: 125, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2600e-06, 2.6254e-01, 2.1217e-02, 6.9195e-01, 1.0161e-06, 8.5156e-06,
        2.4282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.093

[Epoch: 125, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0237, 0.0133, 0.0098, 0.8652, 0.0300, 0.0316, 0.0264],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 125, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0058, 0.0117, 0.9412, 0.0052, 0.0170, 0.0152, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 126, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1275, 0.5648, 0.1458, 0.0121, 0.1207, 0.0072, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 126, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.2840e-06, 1.2173e-02, 9.4975e-01, 1.8280e-02, 5.2309e-03, 1.4959e-05,
        1.4547e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.000

[Epoch: 126, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5466e-06, 2.8337e-01, 1.4411e-02, 6.7831e-01, 1.2358e-06, 5.6397e-06,
        2.3905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 126, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0362, 0.0252, 0.0128, 0.8350, 0.0318, 0.0288, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 126, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0058, 0.0224, 0.9137, 0.0075, 0.0241, 0.0217, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 127, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1393, 0.5583, 0.1572, 0.0152, 0.1093, 0.0064, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 127, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6516e-05, 1.4346e-02, 8.8936e-01, 3.4068e-02, 2.1294e-02, 2.6950e-05,
        4.0886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.012

[Epoch: 127, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.3797e-06, 3.1048e-01, 1.9335e-02, 6.4737e-01, 1.4858e-06, 8.9937e-06,
        2.2808e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.071

[Epoch: 127, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0245, 0.0172, 0.0092, 0.8626, 0.0280, 0.0332, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 127, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0081, 0.0167, 0.9283, 0.0063, 0.0195, 0.0153, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 128, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1523, 0.3861, 0.2424, 0.0187, 0.1573, 0.0069, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 128, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.7130e-05, 1.6618e-02, 9.1695e-01, 3.4014e-02, 1.1562e-02, 1.5223e-05,
        2.0820e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.003

[Epoch: 128, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.4413e-07, 2.5405e-01, 1.6634e-02, 7.0350e-01, 1.9553e-06, 2.0113e-06,
        2.5806e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.113

[Epoch: 128, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0228, 0.0178, 0.0108, 0.8585, 0.0308, 0.0299, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 128, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0064, 0.0155, 0.9262, 0.0069, 0.0241, 0.0170, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.015

[Epoch: 129, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1491, 0.4282, 0.2370, 0.0178, 0.1313, 0.0092, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 129, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.5292e-06, 7.5121e-03, 9.5636e-01, 1.2181e-02, 1.0403e-02, 2.1456e-05,
        1.3514e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.017

[Epoch: 129, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6826e-06, 2.8040e-01, 1.8279e-02, 6.8314e-01, 1.4509e-06, 6.9053e-06,
        1.8174e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.106

[Epoch: 129, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0244, 0.0138, 0.0085, 0.8950, 0.0199, 0.0214, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 129, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0078, 0.0168, 0.9250, 0.0069, 0.0194, 0.0184, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 130, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1494, 0.4684, 0.2073, 0.0127, 0.1380, 0.0068, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 130, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.7065e-06, 2.0388e-02, 9.0480e-01, 3.8575e-02, 1.4032e-02, 2.2161e-05,
        2.2179e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.010

[Epoch: 130, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6135e-06, 2.9869e-01, 1.8997e-02, 6.5315e-01, 1.0359e-06, 7.5411e-06,
        2.9144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.064

[Epoch: 130, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0211, 0.0174, 0.0102, 0.8733, 0.0181, 0.0289, 0.0311],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 130, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0175, 0.9093, 0.0091, 0.0312, 0.0194, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 131, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1209, 0.4806, 0.2028, 0.0204, 0.1446, 0.0089, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 131, batch: 86/219] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2116e-05, 1.1125e-02, 9.3092e-01, 2.0154e-02, 1.5485e-02, 4.5370e-05,
        2.2257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.013

[Epoch: 131, batch: 129/219] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.2160e-06, 3.2891e-01, 2.7455e-02, 6.1080e-01, 1.2732e-06, 4.1334e-06,
        3.2823e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.083

[Epoch: 131, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0211, 0.0137, 0.0089, 0.8891, 0.0254, 0.0231, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 131, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0072, 0.0122, 0.9311, 0.0063, 0.0192, 0.0189, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 132, batch: 43/219] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1427, 0.4677, 0.2171, 0.0116, 0.1299, 0.0046, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 132, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1890e-05, 2.2949e-02, 9.1083e-01, 2.8896e-02, 1.3055e-02, 7.1443e-05,
        2.4183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.001

[Epoch: 132, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.9185e-06, 2.6353e-01, 1.8096e-02, 6.9391e-01, 2.8612e-06, 1.1372e-05,
        2.4447e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.075

[Epoch: 132, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0231, 0.0200, 0.0138, 0.8367, 0.0298, 0.0344, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 132, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0068, 0.0173, 0.9301, 0.0061, 0.0203, 0.0152, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 133, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1497, 0.4981, 0.1500, 0.0153, 0.1556, 0.0071, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 133, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2502e-05, 1.1773e-02, 9.3355e-01, 2.0078e-02, 1.2522e-02, 1.8890e-05,
        2.2041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.008

[Epoch: 133, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.3472e-06, 2.5207e-01, 1.7506e-02, 7.0485e-01, 1.1553e-06, 5.4722e-06,
        2.5562e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 133, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0312, 0.0175, 0.0097, 0.8631, 0.0213, 0.0290, 0.0282],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 133, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0093, 0.0158, 0.9161, 0.0074, 0.0241, 0.0212, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 134, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1372, 0.5065, 0.1653, 0.0162, 0.1442, 0.0052, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.047

[Epoch: 134, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2001e-05, 2.6766e-02, 8.9715e-01, 2.5301e-02, 2.1625e-02, 6.0434e-05,
        2.9082e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.011

[Epoch: 134, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.5170e-06, 2.4210e-01, 1.4062e-02, 7.2389e-01, 8.7997e-07, 5.2684e-06,
        1.9937e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.106

[Epoch: 134, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0271, 0.0207, 0.0113, 0.8561, 0.0237, 0.0313, 0.0298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 134, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0086, 0.0243, 0.9058, 0.0080, 0.0250, 0.0230, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 135, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1443, 0.5411, 0.1588, 0.0137, 0.1166, 0.0059, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 135, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3771e-05, 1.2008e-02, 9.4373e-01, 1.8023e-02, 1.0101e-02, 1.7950e-05,
        1.6103e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 135, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2830e-06, 2.4728e-01, 1.8796e-02, 7.0131e-01, 1.3094e-06, 4.9742e-06,
        3.2603e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.112

[Epoch: 135, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0288, 0.0155, 0.0096, 0.8790, 0.0211, 0.0192, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 135, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0178, 0.9209, 0.0069, 0.0252, 0.0175, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 136, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1308, 0.4374, 0.2376, 0.0187, 0.1449, 0.0085, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 136, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0220e-05, 1.6422e-02, 9.1836e-01, 2.5834e-02, 1.5407e-02, 2.3957e-05,
        2.3942e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.012

[Epoch: 136, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5128e-06, 2.3508e-01, 1.2256e-02, 7.3309e-01, 1.1860e-06, 3.4869e-06,
        1.9568e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.125

[Epoch: 136, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0190, 0.0177, 0.0092, 0.8707, 0.0261, 0.0290, 0.0284],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 136, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0226, 0.9298, 0.0064, 0.0186, 0.0122, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 137, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1195, 0.5594, 0.1652, 0.0139, 0.1101, 0.0057, 0.0263],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 137, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.6032e-05, 1.4588e-02, 9.3938e-01, 1.8972e-02, 1.1052e-02, 1.6869e-05,
        1.5979e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.005

[Epoch: 137, batch: 129/219] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.0696e-06, 3.5707e-01, 1.7470e-02, 5.9392e-01, 7.2635e-07, 3.6231e-06,
        3.1533e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.105

[Epoch: 137, batch: 172/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0341, 0.0199, 0.0109, 0.8372, 0.0318, 0.0325, 0.0336],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 137, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0113, 0.9354, 0.0050, 0.0195, 0.0184, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 138, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1184, 0.4275, 0.2452, 0.0189, 0.1610, 0.0077, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 138, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.5830e-05, 1.7719e-02, 9.1524e-01, 2.8225e-02, 1.8045e-02, 3.0330e-05,
        2.0718e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.012

[Epoch: 138, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.1299e-06, 2.5724e-01, 1.4083e-02, 7.0849e-01, 1.0542e-06, 4.1455e-06,
        2.0185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.066

[Epoch: 138, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0200, 0.0109, 0.0088, 0.8888, 0.0207, 0.0222, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 138, batch: 215/219] total loss per batch: 0.646
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0062, 0.0254, 0.9159, 0.0054, 0.0261, 0.0174, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 139, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1268, 0.5463, 0.1571, 0.0128, 0.1260, 0.0069, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 139, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.2537e-05, 1.6705e-02, 9.0918e-01, 2.0729e-02, 2.2549e-02, 2.2149e-05,
        3.0806e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.007

[Epoch: 139, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1242e-06, 3.4816e-01, 1.6969e-02, 6.0303e-01, 8.6226e-07, 2.9784e-06,
        3.1836e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.048

[Epoch: 139, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0248, 0.0174, 0.0088, 0.8774, 0.0217, 0.0276, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.021

[Epoch: 139, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0087, 0.0149, 0.9075, 0.0084, 0.0280, 0.0264, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 140, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1438, 0.4418, 0.2375, 0.0139, 0.1363, 0.0062, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.027

[Epoch: 140, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([3.6356e-05, 1.6420e-02, 9.1512e-01, 3.0607e-02, 1.4003e-02, 3.8613e-05,
        2.3776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.007

[Epoch: 140, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.7662e-06, 2.4144e-01, 1.6772e-02, 7.2007e-01, 2.0389e-06, 6.4430e-06,
        2.1703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.106

[Epoch: 140, batch: 172/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0186, 0.0146, 0.0109, 0.8798, 0.0234, 0.0235, 0.0291],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 140, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0045, 0.0167, 0.9415, 0.0050, 0.0171, 0.0119, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 141, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1448, 0.4535, 0.2227, 0.0168, 0.1384, 0.0058, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 141, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([4.4509e-06, 1.0946e-02, 9.4003e-01, 2.1629e-02, 1.2211e-02, 2.1540e-05,
        1.5152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.014

[Epoch: 141, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.1661e-07, 2.3280e-01, 1.8643e-02, 7.2012e-01, 1.5762e-06, 7.5031e-06,
        2.8424e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.062

[Epoch: 141, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0349, 0.0260, 0.0147, 0.8165, 0.0407, 0.0340, 0.0331],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 141, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0090, 0.0185, 0.9075, 0.0073, 0.0249, 0.0282, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 142, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1168, 0.5074, 0.1946, 0.0147, 0.1354, 0.0063, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.052

[Epoch: 142, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3101e-05, 1.8908e-02, 9.0822e-01, 3.1612e-02, 1.8683e-02, 4.0041e-05,
        2.2527e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.004

[Epoch: 142, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2503e-06, 2.3051e-01, 1.7205e-02, 7.2687e-01, 1.3541e-06, 2.6271e-06,
        2.5407e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.130

[Epoch: 142, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0221, 0.0144, 0.0095, 0.8832, 0.0231, 0.0204, 0.0273],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 142, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0243, 0.9056, 0.0066, 0.0291, 0.0220, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 143, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.0959, 0.5300, 0.2093, 0.0187, 0.1197, 0.0065, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.018

[Epoch: 143, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.3581e-06, 1.2025e-02, 9.5096e-01, 1.9452e-02, 7.1873e-03, 1.4750e-05,
        1.0350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.005

[Epoch: 143, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.3618e-06, 2.7961e-01, 1.7119e-02, 6.7954e-01, 1.8460e-06, 4.1890e-06,
        2.3726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.126

[Epoch: 143, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0295, 0.0190, 0.0129, 0.8418, 0.0353, 0.0330, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 143, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0056, 0.0176, 0.9298, 0.0078, 0.0173, 0.0186, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 144, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1567, 0.5023, 0.1534, 0.0170, 0.1338, 0.0078, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 144, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.8544e-06, 1.5019e-02, 9.2531e-01, 1.9967e-02, 1.4153e-02, 2.7425e-05,
        2.5512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.001

[Epoch: 144, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7783e-06, 3.1416e-01, 1.7614e-02, 6.3587e-01, 1.5499e-06, 3.5187e-06,
        3.2354e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.072

[Epoch: 144, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0232, 0.0201, 0.0101, 0.8661, 0.0213, 0.0273, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 144, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0111, 0.0254, 0.9048, 0.0064, 0.0268, 0.0213, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 145, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1523, 0.4364, 0.1920, 0.0211, 0.1529, 0.0076, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 145, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.9863e-06, 2.1137e-02, 9.1429e-01, 3.2292e-02, 1.3821e-02, 4.1153e-05,
        1.8413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.006

[Epoch: 145, batch: 129/219] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6416e-06, 2.6851e-01, 1.8591e-02, 6.8834e-01, 1.3507e-06, 3.4773e-06,
        2.4552e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.116

[Epoch: 145, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0294, 0.0234, 0.0116, 0.8559, 0.0247, 0.0301, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 145, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0054, 0.0166, 0.9257, 0.0065, 0.0225, 0.0181, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 146, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1536, 0.4757, 0.2014, 0.0132, 0.1311, 0.0065, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.026

[Epoch: 146, batch: 86/219] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1567e-05, 1.5180e-02, 9.1625e-01, 2.6204e-02, 1.7101e-02, 2.8285e-05,
        2.5222e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.004

[Epoch: 146, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.1261e-06, 2.2804e-01, 1.4164e-02, 7.3670e-01, 1.0150e-06, 3.5549e-06,
        2.1087e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.086

[Epoch: 146, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0211, 0.0146, 0.0101, 0.8794, 0.0207, 0.0214, 0.0326],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 146, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0074, 0.0235, 0.9159, 0.0070, 0.0223, 0.0187, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 147, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1563, 0.4436, 0.1879, 0.0213, 0.1531, 0.0090, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 147, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3289e-05, 1.6206e-02, 9.1438e-01, 3.0017e-02, 1.5940e-02, 3.2099e-05,
        2.3409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.010

[Epoch: 147, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5311e-06, 2.6902e-01, 2.2703e-02, 6.8397e-01, 1.6222e-06, 7.7875e-06,
        2.4293e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.071

[Epoch: 147, batch: 172/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0199, 0.0151, 0.0092, 0.8977, 0.0173, 0.0201, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.002

[Epoch: 147, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0153, 0.9288, 0.0059, 0.0193, 0.0207, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.013

[Epoch: 148, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1183, 0.4878, 0.2163, 0.0139, 0.1427, 0.0045, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 148, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3166e-05, 1.7836e-02, 9.1736e-01, 2.6951e-02, 1.6293e-02, 3.8733e-05,
        2.1508e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.001

[Epoch: 148, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.2827e-06, 3.0689e-01, 1.9819e-02, 6.4948e-01, 2.4732e-06, 4.2878e-06,
        2.3800e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.100

[Epoch: 148, batch: 172/219] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0256, 0.0145, 0.0089, 0.8893, 0.0226, 0.0195, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 148, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0063, 0.0167, 0.9315, 0.0072, 0.0200, 0.0140, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 149, batch: 43/219] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1521, 0.4737, 0.2301, 0.0120, 0.1100, 0.0040, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 149, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([2.1924e-05, 1.4881e-02, 9.1869e-01, 2.5372e-02, 1.5438e-02, 1.5288e-05,
        2.5579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 149, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.7945e-06, 2.8794e-01, 2.0168e-02, 6.6420e-01, 1.6684e-06, 3.0157e-06,
        2.7688e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.055

[Epoch: 149, batch: 172/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0256, 0.0191, 0.0115, 0.8595, 0.0238, 0.0288, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 149, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0123, 0.0201, 0.8998, 0.0075, 0.0259, 0.0278, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 150, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1020, 0.5959, 0.1407, 0.0160, 0.1232, 0.0048, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.021

[Epoch: 150, batch: 86/219] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1128e-05, 1.5390e-02, 9.2312e-01, 2.2989e-02, 1.8579e-02, 2.5944e-05,
        1.9890e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.006

[Epoch: 150, batch: 129/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0871e-06, 2.1283e-01, 1.2636e-02, 7.5035e-01, 1.0725e-06, 4.7920e-06,
        2.4185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.072

[Epoch: 150, batch: 172/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0312, 0.0226, 0.0141, 0.8415, 0.0352, 0.0273, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 150, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0083, 0.0224, 0.9065, 0.0079, 0.0238, 0.0249, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 151, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1051, 0.5495, 0.1710, 0.0157, 0.1329, 0.0063, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 151, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.3093e-05, 1.3413e-02, 9.2949e-01, 2.1198e-02, 1.4289e-02, 1.4366e-05,
        2.1581e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.000

[Epoch: 151, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1982e-06, 2.5918e-01, 1.5981e-02, 7.0325e-01, 2.1501e-06, 1.9947e-06,
        2.1582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.086

[Epoch: 151, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0175, 0.0118, 0.0092, 0.9055, 0.0189, 0.0169, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 151, batch: 215/219] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0059, 0.0164, 0.9334, 0.0060, 0.0183, 0.0163, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 152, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1388, 0.4943, 0.1972, 0.0135, 0.1314, 0.0054, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 152, batch: 86/219] total loss per batch: 0.676
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.5808e-05, 2.0193e-02, 9.1042e-01, 3.5014e-02, 1.4543e-02, 2.7634e-05,
        1.9786e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 152, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1988e-06, 2.6205e-01, 1.7538e-02, 6.9689e-01, 1.1977e-06, 3.4540e-06,
        2.3512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 152, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0291, 0.0157, 0.0114, 0.8708, 0.0241, 0.0227, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 152, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0056, 0.0153, 0.9349, 0.0061, 0.0166, 0.0169, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 153, batch: 43/219] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1179, 0.5103, 0.1897, 0.0173, 0.1387, 0.0060, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 153, batch: 86/219] total loss per batch: 0.675
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.5643e-06, 1.5334e-02, 9.2440e-01, 2.3898e-02, 1.3003e-02, 1.3819e-05,
        2.3342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.001

[Epoch: 153, batch: 129/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3483e-06, 2.9005e-01, 2.0223e-02, 6.6214e-01, 1.6423e-06, 2.9747e-06,
        2.7583e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.088

[Epoch: 153, batch: 172/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0262, 0.0179, 0.0120, 0.8702, 0.0237, 0.0230, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 153, batch: 215/219] total loss per batch: 0.640
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0052, 0.0156, 0.9347, 0.0054, 0.0184, 0.0169, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 154, batch: 43/219] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1419, 0.4700, 0.2080, 0.0156, 0.1372, 0.0053, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 154, batch: 86/219] total loss per batch: 0.675
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.2284e-06, 1.9281e-02, 9.2060e-01, 2.4757e-02, 1.5991e-02, 1.6720e-05,
        1.9347e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 154, batch: 129/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1378e-06, 2.7049e-01, 1.7507e-02, 6.8600e-01, 1.3403e-06, 2.3737e-06,
        2.5998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.083

[Epoch: 154, batch: 172/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0310, 0.0230, 0.0148, 0.8244, 0.0294, 0.0374, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 154, batch: 215/219] total loss per batch: 0.640
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0085, 0.0214, 0.9054, 0.0077, 0.0240, 0.0260, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 155, batch: 43/219] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1284, 0.4915, 0.1996, 0.0176, 0.1334, 0.0061, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 155, batch: 86/219] total loss per batch: 0.675
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.0266e-06, 1.4845e-02, 9.3116e-01, 2.4069e-02, 1.1511e-02, 1.1528e-05,
        1.8393e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.001

[Epoch: 155, batch: 129/219] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.8619e-07, 2.5079e-01, 1.5871e-02, 7.0737e-01, 8.4663e-07, 2.2399e-06,
        2.5965e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.089

[Epoch: 155, batch: 172/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0255, 0.0197, 0.0128, 0.8624, 0.0275, 0.0250, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 155, batch: 215/219] total loss per batch: 0.640
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0069, 0.0170, 0.9270, 0.0059, 0.0202, 0.0185, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.013

[Epoch: 156, batch: 43/219] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1421, 0.4937, 0.1828, 0.0161, 0.1365, 0.0057, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 156, batch: 86/219] total loss per batch: 0.676
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1076e-05, 1.7452e-02, 9.1751e-01, 2.8201e-02, 1.6493e-02, 1.6773e-05,
        2.0313e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 156, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.2074e-07, 2.8137e-01, 1.6277e-02, 6.7587e-01, 1.2665e-06, 2.3592e-06,
        2.6483e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.089

[Epoch: 156, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0249, 0.0188, 0.0121, 0.8597, 0.0242, 0.0289, 0.0315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 156, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0061, 0.0229, 0.9142, 0.0067, 0.0224, 0.0227, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 157, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1244, 0.5268, 0.1777, 0.0130, 0.1380, 0.0043, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.046

[Epoch: 157, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.8143e-06, 1.1322e-02, 9.5030e-01, 1.8604e-02, 7.9161e-03, 8.2316e-06,
        1.1839e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.000

[Epoch: 157, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.9388e-07, 2.5096e-01, 1.3202e-02, 7.1484e-01, 7.8837e-07, 1.5183e-06,
        2.1003e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.099

[Epoch: 157, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0274, 0.0174, 0.0101, 0.8774, 0.0241, 0.0219, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 157, batch: 215/219] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0047, 0.0122, 0.9484, 0.0049, 0.0134, 0.0128, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 158, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1214, 0.5495, 0.1856, 0.0194, 0.0939, 0.0093, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.044

[Epoch: 158, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1257e-05, 2.4931e-02, 8.7545e-01, 3.9956e-02, 2.1670e-02, 2.0929e-05,
        3.7963e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 158, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.8490e-06, 2.8683e-01, 2.0549e-02, 6.6444e-01, 1.2767e-06, 4.7638e-06,
        2.8175e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.097

[Epoch: 158, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0183, 0.0135, 0.0087, 0.9000, 0.0189, 0.0184, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 158, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0037, 0.0238, 0.9252, 0.0052, 0.0215, 0.0162, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 159, batch: 43/219] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1195, 0.5749, 0.1619, 0.0125, 0.1139, 0.0039, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 159, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.2503e-06, 1.0218e-02, 9.5020e-01, 1.5817e-02, 1.0074e-02, 1.1247e-05,
        1.3678e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 159, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2527e-06, 2.8677e-01, 1.4186e-02, 6.7613e-01, 1.2200e-06, 2.8968e-06,
        2.2916e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.077

[Epoch: 159, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0386, 0.0211, 0.0124, 0.8481, 0.0271, 0.0258, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 159, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0054, 0.0150, 0.9326, 0.0067, 0.0196, 0.0162, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.006

[Epoch: 160, batch: 43/219] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1666, 0.3986, 0.2370, 0.0166, 0.1448, 0.0075, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 160, batch: 86/219] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1543e-05, 2.0038e-02, 9.1106e-01, 2.6983e-02, 1.8362e-02, 8.0344e-06,
        2.3534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 160, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6272e-06, 2.5568e-01, 1.6182e-02, 6.9852e-01, 8.1496e-07, 3.9995e-06,
        2.9610e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.073

[Epoch: 160, batch: 172/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0150, 0.0156, 0.0068, 0.8971, 0.0236, 0.0219, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.000

[Epoch: 160, batch: 215/219] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0063, 0.0169, 0.9122, 0.0076, 0.0268, 0.0235, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 161, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1690, 0.3700, 0.2523, 0.0209, 0.1564, 0.0062, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.018

[Epoch: 161, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.1222e-06, 1.3829e-02, 9.2099e-01, 2.8158e-02, 1.4303e-02, 3.1046e-05,
        2.2680e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.004

[Epoch: 161, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2342e-06, 2.3682e-01, 1.4664e-02, 7.2328e-01, 7.9221e-07, 2.1144e-06,
        2.5228e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.099

[Epoch: 161, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0353, 0.0235, 0.0131, 0.8253, 0.0313, 0.0321, 0.0395],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 161, batch: 215/219] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0228, 0.8989, 0.0085, 0.0292, 0.0253, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 162, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1230, 0.4778, 0.2040, 0.0163, 0.1554, 0.0078, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 162, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0556e-05, 1.2708e-02, 9.3496e-01, 2.4009e-02, 1.1980e-02, 1.1790e-05,
        1.6323e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.004

[Epoch: 162, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.9949e-06, 2.8482e-01, 1.6791e-02, 6.7202e-01, 1.5158e-06, 4.7564e-06,
        2.6364e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.058

[Epoch: 162, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0233, 0.0183, 0.0089, 0.8789, 0.0223, 0.0235, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 162, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0073, 0.0183, 0.9181, 0.0068, 0.0244, 0.0196, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 163, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1170, 0.5179, 0.1940, 0.0157, 0.1275, 0.0055, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 163, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.6762e-06, 1.4905e-02, 9.1604e-01, 2.6327e-02, 1.7088e-02, 1.4973e-05,
        2.5618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 163, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.3419e-07, 2.6675e-01, 1.3880e-02, 6.9695e-01, 7.0223e-07, 3.8985e-06,
        2.2408e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.107

[Epoch: 163, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0221, 0.0169, 0.0083, 0.8885, 0.0225, 0.0196, 0.0222],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 163, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0033, 0.0157, 0.9460, 0.0056, 0.0157, 0.0109, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 164, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1124, 0.5281, 0.1773, 0.0192, 0.1319, 0.0066, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.030

[Epoch: 164, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.7992e-06, 1.2020e-02, 9.3703e-01, 2.1771e-02, 1.3577e-02, 1.0881e-05,
        1.5584e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 164, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2282e-06, 2.7111e-01, 1.9820e-02, 6.8409e-01, 7.6224e-07, 3.2443e-06,
        2.4973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.088

[Epoch: 164, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0341, 0.0170, 0.0110, 0.8558, 0.0234, 0.0301, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 164, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0064, 0.0169, 0.9239, 0.0060, 0.0212, 0.0207, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.012

[Epoch: 165, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1378, 0.5316, 0.1666, 0.0136, 0.1231, 0.0049, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.047

[Epoch: 165, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.1777e-05, 1.5645e-02, 8.8575e-01, 2.9118e-02, 2.7921e-02, 2.1269e-05,
        4.1538e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.001

[Epoch: 165, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1659e-06, 2.8972e-01, 1.5296e-02, 6.6988e-01, 1.1650e-06, 3.4190e-06,
        2.5097e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.079

[Epoch: 165, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0225, 0.0184, 0.0122, 0.8598, 0.0317, 0.0264, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 165, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0043, 0.0177, 0.9305, 0.0069, 0.0201, 0.0162, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 166, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1512, 0.4978, 0.1799, 0.0142, 0.1322, 0.0053, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 166, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.3064e-06, 1.2502e-02, 9.4597e-01, 2.2681e-02, 8.2534e-03, 1.0659e-05,
        1.0578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 166, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.4645e-07, 2.5068e-01, 1.4746e-02, 7.0897e-01, 1.2515e-06, 2.6936e-06,
        2.5599e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.118

[Epoch: 166, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0331, 0.0217, 0.0116, 0.8471, 0.0287, 0.0291, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 166, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0085, 0.0250, 0.8931, 0.0079, 0.0301, 0.0299, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 167, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1175, 0.4626, 0.2456, 0.0152, 0.1359, 0.0061, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 167, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.6895e-06, 1.0957e-02, 9.3229e-01, 2.0110e-02, 1.7469e-02, 1.9308e-05,
        1.9150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 167, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.1727e-06, 2.5735e-01, 1.4455e-02, 7.0519e-01, 8.1699e-07, 3.3911e-06,
        2.3003e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.070

[Epoch: 167, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0228, 0.0169, 0.0118, 0.8595, 0.0290, 0.0289, 0.0311],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 167, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0059, 0.0197, 0.9174, 0.0084, 0.0255, 0.0166, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.010

[Epoch: 168, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1461, 0.4549, 0.2173, 0.0167, 0.1361, 0.0061, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.042

[Epoch: 168, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.6521e-06, 1.6279e-02, 9.1817e-01, 3.9529e-02, 1.0251e-02, 1.6723e-05,
        1.5740e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 168, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.7905e-07, 2.4367e-01, 1.5200e-02, 7.1639e-01, 1.1616e-06, 4.9477e-06,
        2.4730e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.104

[Epoch: 168, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0295, 0.0192, 0.0101, 0.8643, 0.0208, 0.0283, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 168, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0051, 0.0159, 0.9382, 0.0058, 0.0173, 0.0149, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 169, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1259, 0.5159, 0.1756, 0.0195, 0.1366, 0.0067, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 169, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0732e-05, 1.2578e-02, 9.2798e-01, 2.0600e-02, 1.5700e-02, 1.0991e-05,
        2.3125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.000

[Epoch: 169, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5971e-06, 2.9410e-01, 2.0160e-02, 6.5829e-01, 9.3869e-07, 4.1508e-06,
        2.7444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.094

[Epoch: 169, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0189, 0.0152, 0.0093, 0.8941, 0.0219, 0.0199, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 169, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0070, 0.0197, 0.9155, 0.0071, 0.0227, 0.0230, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 170, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1314, 0.5338, 0.1644, 0.0153, 0.1243, 0.0055, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.031

[Epoch: 170, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.3692e-06, 1.5821e-02, 9.1875e-01, 2.6745e-02, 1.4411e-02, 1.2160e-05,
        2.4257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 170, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.4702e-07, 3.1156e-01, 1.6913e-02, 6.4697e-01, 9.4992e-07, 4.8185e-06,
        2.4557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 170, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0220, 0.0196, 0.0105, 0.8743, 0.0194, 0.0296, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 170, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0056, 0.0176, 0.9279, 0.0066, 0.0173, 0.0203, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.010

[Epoch: 171, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1242, 0.5215, 0.2027, 0.0168, 0.1103, 0.0056, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.052

[Epoch: 171, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.6728e-06, 1.3726e-02, 9.2573e-01, 2.6140e-02, 1.4490e-02, 1.3686e-05,
        1.9895e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 171, batch: 129/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.6165e-06, 2.7842e-01, 1.7157e-02, 6.7977e-01, 9.1263e-07, 4.7964e-06,
        2.4643e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.105

[Epoch: 171, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0265, 0.0167, 0.0103, 0.8690, 0.0287, 0.0211, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 171, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0065, 0.0192, 0.9194, 0.0066, 0.0235, 0.0196, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 172, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1642, 0.4276, 0.2197, 0.0152, 0.1488, 0.0055, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 172, batch: 86/219] total loss per batch: 0.678
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0689e-05, 1.4908e-02, 9.1300e-01, 2.9358e-02, 1.6560e-02, 1.5995e-05,
        2.6149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 172, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.0989e-07, 2.4438e-01, 1.4665e-02, 7.1235e-01, 1.4611e-06, 3.0756e-06,
        2.8594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.081

[Epoch: 172, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0203, 0.0170, 0.0094, 0.8856, 0.0199, 0.0241, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 172, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0061, 0.0160, 0.9274, 0.0066, 0.0191, 0.0207, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 173, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1243, 0.5363, 0.1907, 0.0147, 0.1123, 0.0043, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.046

[Epoch: 173, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.9508e-06, 1.5855e-02, 9.3538e-01, 1.7643e-02, 1.3451e-02, 9.6700e-06,
        1.7652e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 173, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.8191e-06, 2.7422e-01, 1.9590e-02, 6.7959e-01, 1.1008e-06, 4.6121e-06,
        2.6597e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.108

[Epoch: 173, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0210, 0.0163, 0.0099, 0.8741, 0.0238, 0.0231, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 173, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0054, 0.0177, 0.9269, 0.0063, 0.0210, 0.0185, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 174, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1340, 0.4893, 0.1963, 0.0144, 0.1369, 0.0056, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 174, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.6193e-06, 1.5757e-02, 9.1615e-01, 3.5038e-02, 1.4018e-02, 2.0064e-05,
        1.9007e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.004

[Epoch: 174, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0190e-06, 2.5862e-01, 1.2868e-02, 7.0478e-01, 1.8841e-06, 3.5819e-06,
        2.3724e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 174, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0279, 0.0178, 0.0132, 0.8637, 0.0249, 0.0253, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 174, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0077, 0.0167, 0.9248, 0.0061, 0.0199, 0.0199, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 175, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1201, 0.5378, 0.1474, 0.0156, 0.1505, 0.0062, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 175, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.7316e-06, 1.7494e-02, 9.1786e-01, 2.6790e-02, 1.7576e-02, 7.4604e-06,
        2.0263e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 175, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.0095e-07, 2.4505e-01, 1.6365e-02, 7.1832e-01, 8.8148e-07, 2.6897e-06,
        2.0258e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.129

[Epoch: 175, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0186, 0.0163, 0.0083, 0.8950, 0.0212, 0.0208, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 175, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0062, 0.0242, 0.9032, 0.0084, 0.0259, 0.0270, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 176, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1312, 0.4789, 0.1979, 0.0188, 0.1409, 0.0067, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 176, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.2138e-06, 1.2467e-02, 9.4088e-01, 2.0862e-02, 9.2469e-03, 1.1418e-05,
        1.6529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.003

[Epoch: 176, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([2.5162e-06, 3.0567e-01, 1.7366e-02, 6.4848e-01, 1.3288e-06, 4.3736e-06,
        2.8474e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.065

[Epoch: 176, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0266, 0.0191, 0.0109, 0.8680, 0.0229, 0.0242, 0.0282],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 176, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0043, 0.0132, 0.9474, 0.0050, 0.0156, 0.0116, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 177, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1502, 0.4492, 0.2090, 0.0172, 0.1463, 0.0054, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 177, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.9271e-06, 2.0288e-02, 9.1151e-01, 2.8349e-02, 1.6678e-02, 9.2137e-06,
        2.3156e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 177, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.5548e-07, 2.7584e-01, 1.7673e-02, 6.8434e-01, 9.7594e-07, 2.6932e-06,
        2.2144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.112

[Epoch: 177, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0289, 0.0228, 0.0109, 0.8675, 0.0236, 0.0255, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 177, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0061, 0.0189, 0.9153, 0.0070, 0.0227, 0.0250, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 178, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1169, 0.5325, 0.1905, 0.0153, 0.1190, 0.0057, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.046

[Epoch: 178, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.1783e-06, 1.0989e-02, 9.3006e-01, 2.4970e-02, 1.3744e-02, 1.1141e-05,
        2.0216e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.006

[Epoch: 178, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.5113e-06, 2.6562e-01, 1.3897e-02, 6.9734e-01, 6.0171e-07, 3.5928e-06,
        2.3137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.072

[Epoch: 178, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0291, 0.0182, 0.0135, 0.8239, 0.0328, 0.0401, 0.0424],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 178, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0059, 0.0234, 0.9156, 0.0070, 0.0229, 0.0199, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 179, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1306, 0.4863, 0.2161, 0.0143, 0.1308, 0.0053, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 179, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.5569e-06, 1.4766e-02, 9.3398e-01, 1.8980e-02, 1.2967e-02, 7.2413e-06,
        1.9297e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 179, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.1497e-07, 2.8208e-01, 1.5328e-02, 6.7801e-01, 5.8895e-07, 2.0969e-06,
        2.4582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.088

[Epoch: 179, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0251, 0.0233, 0.0141, 0.8540, 0.0257, 0.0329, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 179, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0067, 0.0221, 0.9107, 0.0074, 0.0243, 0.0233, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 180, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1213, 0.5098, 0.1818, 0.0165, 0.1447, 0.0058, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.036

[Epoch: 180, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.0442e-06, 1.0547e-02, 9.3770e-01, 2.6178e-02, 1.2241e-02, 1.7488e-05,
        1.3306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 180, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.5904e-07, 2.4569e-01, 1.3422e-02, 7.1766e-01, 5.0814e-07, 3.2694e-06,
        2.3233e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.124

[Epoch: 180, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0201, 0.0129, 0.0087, 0.8910, 0.0233, 0.0167, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 180, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0052, 0.0221, 0.9249, 0.0066, 0.0192, 0.0179, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 181, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1405, 0.4769, 0.2158, 0.0166, 0.1193, 0.0051, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.054

[Epoch: 181, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.9622e-06, 2.6195e-02, 9.0284e-01, 3.1162e-02, 1.5764e-02, 7.4576e-06,
        2.4023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 181, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.0759e-07, 2.1884e-01, 1.5171e-02, 7.4316e-01, 3.9714e-07, 3.4949e-06,
        2.2821e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.107

[Epoch: 181, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0268, 0.0203, 0.0113, 0.8644, 0.0236, 0.0311, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 181, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0043, 0.0180, 0.9377, 0.0054, 0.0163, 0.0146, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 182, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1516, 0.4746, 0.1899, 0.0167, 0.1420, 0.0051, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 182, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.6637e-06, 9.7272e-03, 9.3573e-01, 2.5757e-02, 1.3747e-02, 1.3233e-05,
        1.5019e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 182, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.7213e-07, 3.3679e-01, 1.5871e-02, 6.2132e-01, 5.7441e-07, 3.4744e-06,
        2.6020e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.097

[Epoch: 182, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0234, 0.0128, 0.0071, 0.9044, 0.0173, 0.0154, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 182, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0045, 0.0178, 0.9305, 0.0058, 0.0190, 0.0182, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 183, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1037, 0.5899, 0.1587, 0.0132, 0.1144, 0.0037, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 183, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.8587e-06, 1.8110e-02, 9.1860e-01, 2.1256e-02, 1.5414e-02, 7.4897e-06,
        2.6601e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 183, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3358e-06, 3.0992e-01, 1.8044e-02, 6.4353e-01, 6.7907e-07, 2.7332e-06,
        2.8505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.059

[Epoch: 183, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0252, 0.0226, 0.0095, 0.8706, 0.0212, 0.0305, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 183, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0055, 0.0178, 0.9302, 0.0059, 0.0179, 0.0176, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.011

[Epoch: 184, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1598, 0.4559, 0.2028, 0.0146, 0.1426, 0.0042, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.047

[Epoch: 184, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.6103e-06, 1.4024e-02, 9.2176e-01, 2.8185e-02, 1.6652e-02, 1.6604e-05,
        1.9356e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.011

[Epoch: 184, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.2741e-07, 2.3029e-01, 1.3199e-02, 7.3004e-01, 5.8498e-07, 2.3785e-06,
        2.6466e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.090

[Epoch: 184, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0252, 0.0139, 0.0097, 0.8720, 0.0280, 0.0219, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 184, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0060, 0.0226, 0.9115, 0.0068, 0.0229, 0.0242, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 185, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1234, 0.5122, 0.1843, 0.0145, 0.1409, 0.0048, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.028

[Epoch: 185, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.5154e-06, 1.7416e-02, 9.2508e-01, 2.5378e-02, 1.1813e-02, 1.2496e-05,
        2.0292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.011

[Epoch: 185, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.0768e-06, 2.6744e-01, 1.6032e-02, 6.8576e-01, 7.0906e-07, 3.1165e-06,
        3.0766e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.091

[Epoch: 185, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0260, 0.0231, 0.0096, 0.8699, 0.0239, 0.0250, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 185, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0082, 0.0238, 0.9061, 0.0083, 0.0247, 0.0230, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 186, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1377, 0.4710, 0.2161, 0.0145, 0.1358, 0.0050, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.034

[Epoch: 186, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([6.7428e-06, 1.2859e-02, 9.2056e-01, 2.1223e-02, 1.6985e-02, 1.1679e-05,
        2.8357e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.010

[Epoch: 186, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.3481e-07, 2.3830e-01, 1.2637e-02, 7.2509e-01, 4.4923e-07, 2.2740e-06,
        2.3967e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.124

[Epoch: 186, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0280, 0.0183, 0.0114, 0.8575, 0.0298, 0.0254, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 186, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0049, 0.0182, 0.9354, 0.0056, 0.0180, 0.0144, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 187, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1471, 0.4726, 0.1797, 0.0225, 0.1454, 0.0071, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 187, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.3664e-06, 1.8306e-02, 9.3007e-01, 2.4991e-02, 1.2001e-02, 7.6835e-06,
        1.4614e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 187, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.2598e-06, 3.1134e-01, 1.7259e-02, 6.4430e-01, 6.6234e-07, 4.9700e-06,
        2.7101e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 187, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0260, 0.0190, 0.0106, 0.8754, 0.0248, 0.0210, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 187, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0051, 0.0181, 0.9277, 0.0059, 0.0189, 0.0194, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 188, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1278, 0.5031, 0.1853, 0.0149, 0.1383, 0.0049, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 188, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.1413e-06, 1.3107e-02, 9.0090e-01, 2.9876e-02, 2.0907e-02, 1.2388e-05,
        3.5188e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 188, batch: 129/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([6.8208e-07, 2.8717e-01, 1.6874e-02, 6.6915e-01, 5.8313e-07, 2.7425e-06,
        2.6803e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 188, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0242, 0.0165, 0.0107, 0.8740, 0.0233, 0.0254, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 188, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0056, 0.0176, 0.9250, 0.0068, 0.0224, 0.0177, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 189, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1423, 0.5163, 0.1634, 0.0177, 0.1327, 0.0060, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 189, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.0695e-06, 1.4717e-02, 9.3942e-01, 2.2269e-02, 9.3683e-03, 5.1251e-06,
        1.4218e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 189, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.5697e-07, 2.5696e-01, 1.4124e-02, 7.0313e-01, 3.1816e-07, 1.9290e-06,
        2.5787e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.075

[Epoch: 189, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0282, 0.0202, 0.0108, 0.8711, 0.0224, 0.0243, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.020

[Epoch: 189, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0196, 0.9195, 0.0060, 0.0206, 0.0232, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 190, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1359, 0.5002, 0.1844, 0.0158, 0.1369, 0.0041, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.041

[Epoch: 190, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.2038e-06, 1.3679e-02, 9.3019e-01, 1.9925e-02, 1.5615e-02, 1.1301e-05,
        2.0575e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 190, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([7.2545e-07, 2.4509e-01, 1.5774e-02, 7.1743e-01, 9.8158e-07, 2.8379e-06,
        2.1696e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.101

[Epoch: 190, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0267, 0.0188, 0.0120, 0.8565, 0.0253, 0.0279, 0.0329],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 190, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0061, 0.0198, 0.9185, 0.0075, 0.0221, 0.0207, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 191, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1519, 0.4540, 0.2216, 0.0155, 0.1270, 0.0067, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.025

[Epoch: 191, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.5250e-06, 1.6389e-02, 9.1990e-01, 3.4756e-02, 1.1326e-02, 7.5802e-06,
        1.7617e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.004

[Epoch: 191, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.4316e-07, 2.4251e-01, 1.4613e-02, 7.1345e-01, 5.9094e-07, 2.1652e-06,
        2.9425e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.074

[Epoch: 191, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0350, 0.0228, 0.0139, 0.8310, 0.0360, 0.0353, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 191, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0078, 0.0209, 0.9125, 0.0069, 0.0241, 0.0227, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 192, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1264, 0.4814, 0.2008, 0.0182, 0.1431, 0.0066, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.033

[Epoch: 192, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0481e-05, 1.4375e-02, 9.3015e-01, 1.9403e-02, 1.5045e-02, 7.2100e-06,
        2.1014e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 192, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.3165e-06, 2.7341e-01, 1.5477e-02, 6.8585e-01, 5.0227e-07, 3.7314e-06,
        2.5253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.091

[Epoch: 192, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0232, 0.0251, 0.0140, 0.8304, 0.0252, 0.0422, 0.0397],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 192, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0055, 0.0255, 0.9080, 0.0073, 0.0252, 0.0211, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 193, batch: 43/219] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1330, 0.4571, 0.2482, 0.0172, 0.1161, 0.0060, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.029

[Epoch: 193, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.5971e-06, 1.6017e-02, 9.2391e-01, 2.5069e-02, 1.4031e-02, 9.9804e-06,
        2.0956e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.020

[Epoch: 193, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([3.6161e-07, 2.6939e-01, 1.3181e-02, 6.9364e-01, 5.6515e-07, 3.0898e-06,
        2.3788e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.094

[Epoch: 193, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0229, 0.0128, 0.0106, 0.8924, 0.0243, 0.0189, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 193, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0070, 0.0157, 0.9330, 0.0057, 0.0176, 0.0172, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 194, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1214, 0.5144, 0.1790, 0.0172, 0.1386, 0.0082, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.040

[Epoch: 194, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0331e-05, 1.6670e-02, 9.2580e-01, 2.5655e-02, 1.5334e-02, 1.6283e-05,
        1.6515e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 194, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([1.4090e-06, 2.8347e-01, 1.8850e-02, 6.7201e-01, 4.1034e-07, 3.7744e-06,
        2.5658e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 194, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0291, 0.0207, 0.0097, 0.8596, 0.0244, 0.0304, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 194, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0039, 0.0218, 0.9227, 0.0063, 0.0229, 0.0157, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.012

[Epoch: 195, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1254, 0.5535, 0.1623, 0.0135, 0.1235, 0.0044, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.035

[Epoch: 195, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([5.9611e-06, 1.8624e-02, 9.1378e-01, 2.6917e-02, 1.4455e-02, 8.1060e-06,
        2.6214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.012

[Epoch: 195, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.2408e-07, 2.9100e-01, 1.8611e-02, 6.5623e-01, 4.4231e-07, 2.9760e-06,
        3.4160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.075

[Epoch: 195, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0239, 0.0143, 0.0100, 0.8787, 0.0232, 0.0251, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 195, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0037, 0.0149, 0.9445, 0.0054, 0.0143, 0.0144, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 196, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1341, 0.5014, 0.1912, 0.0141, 0.1337, 0.0056, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.043

[Epoch: 196, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.0175e-06, 1.5292e-02, 9.1779e-01, 2.4716e-02, 2.1153e-02, 2.0162e-05,
        2.1019e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.012

[Epoch: 196, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([9.3083e-07, 2.9756e-01, 1.6143e-02, 6.6129e-01, 7.0900e-07, 3.4254e-06,
        2.5000e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.100

[Epoch: 196, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0274, 0.0163, 0.0110, 0.8711, 0.0251, 0.0251, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 0.001

[Epoch: 196, batch: 215/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0060, 0.0220, 0.9110, 0.0063, 0.0245, 0.0240, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 197, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1226, 0.5151, 0.1979, 0.0145, 0.1270, 0.0046, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.037

[Epoch: 197, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([9.9272e-06, 1.6042e-02, 9.3906e-01, 1.9799e-02, 9.1185e-03, 6.4507e-06,
        1.5960e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.017

[Epoch: 197, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.9229e-07, 2.4962e-01, 1.7213e-02, 7.0247e-01, 5.0427e-07, 1.9760e-06,
        3.0693e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 197, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0238, 0.0163, 0.0110, 0.8586, 0.0287, 0.0296, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 197, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0051, 0.0186, 0.9273, 0.0063, 0.0191, 0.0189, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 198, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1622, 0.4289, 0.2141, 0.0159, 0.1431, 0.0072, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.045

[Epoch: 198, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([7.9273e-06, 1.7316e-02, 9.1587e-01, 2.3969e-02, 2.1090e-02, 2.5169e-05,
        2.1720e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 198, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([5.8862e-07, 2.3812e-01, 1.3091e-02, 7.2515e-01, 9.3490e-07, 4.9986e-06,
        2.3631e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.085

[Epoch: 198, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0205, 0.0171, 0.0087, 0.8869, 0.0208, 0.0213, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 198, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0059, 0.0217, 0.9200, 0.0066, 0.0194, 0.0212, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.016

[Epoch: 199, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1335, 0.4861, 0.1803, 0.0172, 0.1570, 0.0055, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.032

[Epoch: 199, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([8.3083e-06, 1.7499e-02, 9.2512e-01, 3.0502e-02, 1.0718e-02, 8.6933e-06,
        1.6145e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.019

[Epoch: 199, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([8.3867e-07, 2.6082e-01, 1.5204e-02, 7.0249e-01, 3.7961e-07, 2.5482e-06,
        2.1481e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.123

[Epoch: 199, batch: 172/219] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0222, 0.0162, 0.0094, 0.8888, 0.0218, 0.0221, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 199, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0044, 0.0172, 0.9316, 0.0059, 0.0195, 0.0172, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 200, batch: 43/219] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.1350, 0.4950, 0.1950, 0.0150, 0.1350, 0.0050, 0.0200])
Policy pred: tensor([0.1340, 0.5099, 0.1852, 0.0150, 0.1237, 0.0068, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.035 -0.038

[Epoch: 200, batch: 86/219] total loss per batch: 0.677
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0150, 0.9250, 0.0250, 0.0150, 0.0000, 0.0200])
Policy pred: tensor([1.0533e-05, 1.5496e-02, 9.1591e-01, 2.2246e-02, 1.9734e-02, 1.2243e-05,
        2.6591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 200, batch: 129/219] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0000, 0.2700, 0.0150, 0.6900, 0.0000, 0.0000, 0.0250])
Policy pred: tensor([4.8798e-07, 2.7654e-01, 1.4433e-02, 6.8552e-01, 9.5749e-07, 1.9421e-06,
        2.3499e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.095 -0.080

[Epoch: 200, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.0200, 0.0100, 0.8700, 0.0250, 0.0250, 0.0250])
Policy pred: tensor([0.0310, 0.0209, 0.0109, 0.8648, 0.0255, 0.0231, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 200, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0200, 0.9250, 0.0050, 0.0200, 0.0200, 0.0050])
Policy pred: tensor([0.0048, 0.0207, 0.9180, 0.0064, 0.0211, 0.0242, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.016

