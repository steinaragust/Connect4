Training set samples: 6016
Batch size: 32
[Epoch: 1, batch: 37/188] total loss per batch: 0.970
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.2117e-02, 3.8244e-02, 8.4725e-01, 7.2066e-02, 2.3527e-02, 3.7343e-05,
        6.7599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.017

[Epoch: 1, batch: 74/188] total loss per batch: 0.958
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0352, 0.0067, 0.0093, 0.0043, 0.9101, 0.0313, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.009

[Epoch: 1, batch: 111/188] total loss per batch: 0.993
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([7.0717e-01, 1.3528e-02, 1.3139e-05, 2.8057e-02, 3.1904e-04, 5.7131e-06,
        2.5090e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.027

[Epoch: 1, batch: 148/188] total loss per batch: 0.882
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([9.2951e-02, 1.3613e-01, 2.7699e-04, 1.7081e-01, 5.6613e-01, 1.3208e-06,
        3.3706e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.281

[Epoch: 1, batch: 185/188] total loss per batch: 0.964
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0588, 0.0172, 0.1523, 0.7195, 0.0063, 0.0345, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.009

[Epoch: 2, batch: 37/188] total loss per batch: 0.783
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.0627e-01, 1.3887e-01, 6.4871e-01, 6.1719e-02, 1.8919e-02, 7.7303e-05,
        2.5438e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.009

[Epoch: 2, batch: 74/188] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0139, 0.0030, 0.0060, 0.0038, 0.9582, 0.0112, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.009

[Epoch: 2, batch: 111/188] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4055e-01, 5.7629e-03, 1.1926e-05, 3.1805e-02, 1.6077e-04, 1.5961e-05,
        2.1692e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.009

[Epoch: 2, batch: 148/188] total loss per batch: 0.713
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.5288e-02, 6.5575e-02, 4.5933e-04, 6.0511e-02, 7.7279e-01, 4.7740e-06,
        3.5373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.043

[Epoch: 2, batch: 185/188] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0514, 0.0091, 0.0225, 0.8801, 0.0035, 0.0267, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.004

[Epoch: 3, batch: 37/188] total loss per batch: 0.703
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9337e-01, 1.3173e-01, 4.5678e-01, 1.7023e-01, 1.9213e-02, 1.3127e-04,
        2.8555e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.009

[Epoch: 3, batch: 74/188] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0209, 0.0100, 0.0071, 0.0047, 0.9011, 0.0521, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 3, batch: 111/188] total loss per batch: 0.713
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.7397e-01, 2.0016e-03, 9.1393e-06, 9.5604e-03, 1.3124e-04, 3.1920e-05,
        1.4291e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.006

[Epoch: 3, batch: 148/188] total loss per batch: 0.660
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.5160e-02, 1.0271e-01, 1.5275e-04, 3.4407e-02, 7.9766e-01, 3.1718e-06,
        1.9905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.012

[Epoch: 3, batch: 185/188] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0739, 0.0241, 0.0404, 0.7732, 0.0115, 0.0429, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.026

[Epoch: 4, batch: 37/188] total loss per batch: 0.669
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8886e-01, 2.7582e-01, 3.6281e-01, 1.2421e-01, 1.6010e-02, 1.2602e-04,
        3.2163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 4, batch: 74/188] total loss per batch: 0.673
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0131, 0.0046, 0.0092, 0.0027, 0.9411, 0.0228, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.007

[Epoch: 4, batch: 111/188] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4597e-01, 4.4627e-03, 2.1431e-05, 1.1999e-02, 2.1984e-04, 5.5228e-05,
        3.7270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.013

[Epoch: 4, batch: 148/188] total loss per batch: 0.646
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0780e-02, 5.8741e-02, 1.2887e-04, 8.7225e-02, 7.1938e-01, 8.9701e-07,
        8.3748e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.270

[Epoch: 4, batch: 185/188] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0884, 0.0155, 0.0514, 0.7869, 0.0146, 0.0325, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 5, batch: 37/188] total loss per batch: 0.670
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.4806e-01, 3.8199e-01, 3.9019e-01, 4.1336e-02, 1.5483e-02, 8.5591e-05,
        2.2850e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 5, batch: 74/188] total loss per batch: 0.669
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0079, 0.0081, 0.0081, 0.0027, 0.9491, 0.0215, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 5, batch: 111/188] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.7106e-01, 9.2979e-03, 7.5825e-06, 8.7190e-03, 4.5683e-05, 7.0253e-05,
        1.0795e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.004

[Epoch: 5, batch: 148/188] total loss per batch: 0.636
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.5114e-02, 4.8101e-02, 1.7002e-04, 1.8025e-02, 8.8222e-01, 4.0184e-06,
        3.6369e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.021

[Epoch: 5, batch: 185/188] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0301, 0.0149, 0.0527, 0.8602, 0.0105, 0.0215, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 6, batch: 37/188] total loss per batch: 0.660
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.3948e-01, 4.1470e-01, 3.2962e-01, 4.9125e-02, 1.0163e-02, 2.0418e-04,
        5.6703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 6, batch: 74/188] total loss per batch: 0.662
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0217, 0.0066, 0.0070, 0.0049, 0.9150, 0.0334, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 6, batch: 111/188] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5259e-01, 6.9061e-03, 8.8082e-06, 1.2359e-02, 4.2383e-05, 5.2057e-05,
        2.8041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.005

[Epoch: 6, batch: 148/188] total loss per batch: 0.631
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([2.5843e-02, 9.2625e-02, 5.7418e-05, 1.8058e-02, 8.2557e-01, 1.1001e-06,
        3.7845e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 0.030

[Epoch: 6, batch: 185/188] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0324, 0.0102, 0.0238, 0.9030, 0.0078, 0.0156, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 7, batch: 37/188] total loss per batch: 0.652
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.0780e-01, 2.1420e-01, 4.0721e-01, 6.5908e-02, 1.5961e-02, 1.2414e-04,
        8.8798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 7, batch: 74/188] total loss per batch: 0.651
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0153, 0.0039, 0.0047, 0.0027, 0.9462, 0.0248, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 7, batch: 111/188] total loss per batch: 0.672
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5186e-01, 4.7116e-03, 9.8305e-06, 1.2851e-02, 1.7531e-04, 6.9504e-05,
        3.0322e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.020

[Epoch: 7, batch: 148/188] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.1904e-03, 3.5159e-02, 1.6910e-04, 1.3246e-02, 9.3230e-01, 2.4786e-06,
        1.2930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.013

[Epoch: 7, batch: 185/188] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0232, 0.0117, 0.0261, 0.9108, 0.0058, 0.0117, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.008

[Epoch: 8, batch: 37/188] total loss per batch: 0.647
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6512e-01, 3.6872e-01, 3.7741e-01, 2.3695e-02, 8.8369e-03, 1.5712e-04,
        5.6066e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.005

[Epoch: 8, batch: 74/188] total loss per batch: 0.644
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0137, 0.0072, 0.0119, 0.0055, 0.9234, 0.0325, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 8, batch: 111/188] total loss per batch: 0.667
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3338e-01, 5.3611e-03, 1.3582e-05, 1.1362e-02, 8.9250e-05, 1.3276e-04,
        4.9657e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.040

[Epoch: 8, batch: 148/188] total loss per batch: 0.617
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.8156e-02, 3.5991e-02, 8.3839e-05, 2.4171e-02, 8.9231e-01, 6.3134e-06,
        2.9283e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.092

[Epoch: 8, batch: 185/188] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0321, 0.0119, 0.0093, 0.9143, 0.0093, 0.0139, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 9, batch: 37/188] total loss per batch: 0.643
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.2853e-01, 4.4675e-01, 2.0274e-01, 6.0965e-02, 2.1754e-02, 1.0084e-04,
        3.9153e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.006

[Epoch: 9, batch: 74/188] total loss per batch: 0.642
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0097, 0.0029, 0.0052, 0.0024, 0.9587, 0.0172, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 9, batch: 111/188] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5188e-01, 4.7110e-03, 2.0282e-05, 4.6646e-03, 1.4187e-04, 6.8560e-05,
        3.8518e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.005

[Epoch: 9, batch: 148/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.3232e-03, 3.3855e-02, 9.7021e-05, 1.5434e-02, 9.1432e-01, 2.2837e-06,
        2.9974e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.004

[Epoch: 9, batch: 185/188] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0333, 0.0135, 0.0128, 0.9103, 0.0082, 0.0160, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 10, batch: 37/188] total loss per batch: 0.638
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.1382e-01, 3.1969e-01, 3.9051e-01, 2.2908e-02, 1.1539e-02, 1.7771e-04,
        1.4135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 10, batch: 74/188] total loss per batch: 0.640
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0060, 0.0084, 0.0048, 0.9505, 0.0170, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.002

[Epoch: 10, batch: 111/188] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.0151e-01, 4.2406e-03, 1.2147e-05, 1.2913e-02, 8.5430e-05, 5.1898e-05,
        8.1191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 10, batch: 148/188] total loss per batch: 0.609
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([2.6452e-02, 4.2439e-02, 4.4701e-05, 2.0025e-02, 8.8983e-01, 3.6073e-06,
        2.1209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.084

[Epoch: 10, batch: 185/188] total loss per batch: 0.652
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0659, 0.0081, 0.0169, 0.8823, 0.0098, 0.0089, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 11, batch: 37/188] total loss per batch: 0.638
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8436e-01, 3.9968e-01, 2.8846e-01, 6.3177e-02, 9.9025e-03, 6.6672e-05,
        5.4355e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 11, batch: 74/188] total loss per batch: 0.635
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0153, 0.0046, 0.0093, 0.0037, 0.9189, 0.0420, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 11, batch: 111/188] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6828e-01, 3.5457e-03, 1.0056e-05, 7.4269e-03, 1.5861e-04, 7.9989e-05,
        2.0497e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.056

[Epoch: 11, batch: 148/188] total loss per batch: 0.608
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.4250e-02, 2.9626e-02, 1.2325e-04, 1.8692e-02, 9.1109e-01, 8.6758e-07,
        2.6217e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.178

[Epoch: 11, batch: 185/188] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0233, 0.0048, 0.0117, 0.9416, 0.0035, 0.0082, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 12, batch: 37/188] total loss per batch: 0.628
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.8287e-01, 2.5352e-01, 3.2281e-01, 3.0370e-02, 9.4059e-03, 1.0770e-04,
        1.0092e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.005

[Epoch: 12, batch: 74/188] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0057, 0.0031, 0.0070, 0.0026, 0.9630, 0.0143, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 12, batch: 111/188] total loss per batch: 0.650
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4829e-01, 3.7119e-03, 1.5631e-05, 7.9847e-03, 7.6409e-05, 5.8968e-05,
        3.9862e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.053

[Epoch: 12, batch: 148/188] total loss per batch: 0.599
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.4665e-02, 3.1516e-02, 7.0989e-05, 1.3664e-02, 9.1820e-01, 3.6093e-06,
        2.1878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.080

[Epoch: 12, batch: 185/188] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0242, 0.0066, 0.0106, 0.9384, 0.0071, 0.0074, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 13, batch: 37/188] total loss per batch: 0.623
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([7.5610e-02, 3.1941e-01, 5.1820e-01, 2.0160e-02, 9.2154e-03, 1.1355e-04,
        5.7282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 13, batch: 74/188] total loss per batch: 0.619
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0094, 0.0048, 0.0056, 0.0026, 0.9525, 0.0204, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 13, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6255e-01, 3.7293e-03, 6.9035e-06, 3.9144e-03, 8.0018e-05, 5.1633e-05,
        2.9664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.071

[Epoch: 13, batch: 148/188] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.1926e-02, 3.1437e-02, 4.3003e-05, 1.4646e-02, 9.1301e-01, 1.1374e-06,
        2.8938e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.042

[Epoch: 13, batch: 185/188] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0479, 0.0101, 0.0114, 0.9043, 0.0073, 0.0101, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 14, batch: 37/188] total loss per batch: 0.622
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.5538e-01, 4.0198e-01, 2.1724e-01, 2.0684e-02, 5.9267e-03, 6.1242e-05,
        9.8722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 14, batch: 74/188] total loss per batch: 0.619
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0073, 0.0042, 0.0059, 0.0035, 0.9561, 0.0181, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 14, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6299e-01, 3.4133e-03, 8.7147e-06, 6.2941e-03, 6.4344e-05, 4.3018e-05,
        2.7190e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.047

[Epoch: 14, batch: 148/188] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.0010e-02, 3.0942e-02, 7.1344e-05, 1.3569e-02, 9.2584e-01, 2.4814e-06,
        1.9565e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.110

[Epoch: 14, batch: 185/188] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0339, 0.0073, 0.0135, 0.9187, 0.0114, 0.0091, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 15, batch: 37/188] total loss per batch: 0.621
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9124e-01, 3.5639e-01, 3.5315e-01, 1.5213e-02, 8.6971e-03, 9.8916e-05,
        7.5208e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 15, batch: 74/188] total loss per batch: 0.620
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0087, 0.0038, 0.0069, 0.0036, 0.9517, 0.0202, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 15, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3882e-01, 6.0653e-03, 1.2654e-05, 5.1843e-03, 6.9055e-05, 6.9451e-05,
        4.9776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.084

[Epoch: 15, batch: 148/188] total loss per batch: 0.595
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([9.8120e-03, 2.7451e-02, 5.0475e-05, 1.3105e-02, 9.2470e-01, 9.5739e-07,
        2.4882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.079

[Epoch: 15, batch: 185/188] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0293, 0.0065, 0.0104, 0.9335, 0.0059, 0.0085, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 16, batch: 37/188] total loss per batch: 0.621
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.2399e-01, 3.9247e-01, 3.6550e-01, 1.8225e-02, 6.7000e-03, 9.4938e-05,
        9.3019e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 16, batch: 74/188] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0068, 0.0041, 0.0045, 0.0039, 0.9591, 0.0162, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 16, batch: 111/188] total loss per batch: 0.649
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5444e-01, 2.5324e-03, 1.1197e-05, 6.3317e-03, 1.0509e-04, 4.2049e-05,
        3.6534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.064

[Epoch: 16, batch: 148/188] total loss per batch: 0.597
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([9.8582e-03, 3.4280e-02, 6.8450e-05, 1.4497e-02, 9.1677e-01, 2.0221e-06,
        2.4528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.181

[Epoch: 16, batch: 185/188] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0519, 0.0076, 0.0125, 0.8996, 0.0096, 0.0111, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 17, batch: 37/188] total loss per batch: 0.623
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.1417e-01, 4.3584e-01, 2.5864e-01, 2.4327e-02, 4.8635e-03, 4.8341e-05,
        6.2108e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.005

[Epoch: 17, batch: 74/188] total loss per batch: 0.623
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0089, 0.0066, 0.0084, 0.0045, 0.9472, 0.0190, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 17, batch: 111/188] total loss per batch: 0.651
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([8.8724e-01, 9.6815e-03, 2.0781e-05, 1.1708e-02, 7.1869e-05, 6.9664e-05,
        9.1203e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.073

[Epoch: 17, batch: 148/188] total loss per batch: 0.599
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([9.1537e-03, 4.0272e-02, 6.8619e-05, 1.1550e-02, 9.1782e-01, 8.2601e-07,
        2.1138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.001

[Epoch: 17, batch: 185/188] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0428, 0.0065, 0.0154, 0.9075, 0.0104, 0.0115, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 18, batch: 37/188] total loss per batch: 0.625
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.5116e-01, 3.2341e-01, 3.8046e-01, 2.1878e-02, 4.3620e-03, 4.9344e-05,
        1.1869e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.012

[Epoch: 18, batch: 74/188] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0078, 0.0028, 0.0050, 0.0035, 0.9512, 0.0252, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 18, batch: 111/188] total loss per batch: 0.651
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.7691e-01, 2.2799e-03, 5.9785e-06, 2.1233e-03, 1.4776e-04, 4.4228e-05,
        1.8487e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.071

[Epoch: 18, batch: 148/188] total loss per batch: 0.601
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([8.3193e-03, 3.8544e-02, 6.3103e-05, 1.4781e-02, 8.9992e-01, 1.5797e-06,
        3.8369e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.079

[Epoch: 18, batch: 185/188] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0314, 0.0041, 0.0069, 0.9431, 0.0039, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 19, batch: 37/188] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.0885e-01, 3.5856e-01, 3.4482e-01, 2.0688e-02, 8.2679e-03, 5.6388e-05,
        5.8760e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 19, batch: 74/188] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0110, 0.0049, 0.0085, 0.0045, 0.9372, 0.0275, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 19, batch: 111/188] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4484e-01, 9.9900e-03, 7.2143e-06, 8.7455e-03, 4.5963e-05, 8.5867e-05,
        3.6284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.051

[Epoch: 19, batch: 148/188] total loss per batch: 0.603
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.2846e-03, 4.7214e-02, 4.0333e-05, 1.1837e-02, 9.1596e-01, 9.3008e-07,
        2.0663e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.036

[Epoch: 19, batch: 185/188] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0434, 0.0082, 0.0091, 0.9042, 0.0069, 0.0214, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 20, batch: 37/188] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.1180e-01, 3.4764e-01, 2.7930e-01, 1.4929e-02, 6.9842e-03, 6.2117e-05,
        1.3929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 20, batch: 74/188] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0028, 0.0060, 0.0034, 0.9587, 0.0168, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 20, batch: 111/188] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.8530e-01, 3.6730e-03, 7.6590e-06, 2.6446e-03, 4.3712e-05, 3.1959e-05,
        8.2951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 20, batch: 148/188] total loss per batch: 0.602
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([7.7365e-03, 3.5335e-02, 5.6562e-05, 8.5466e-03, 9.2294e-01, 3.2215e-06,
        2.5378e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.048

[Epoch: 20, batch: 185/188] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0438, 0.0070, 0.0101, 0.9193, 0.0068, 0.0075, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 21, batch: 37/188] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.1496e-01, 4.4396e-01, 3.7239e-01, 1.9074e-02, 9.8865e-03, 4.2397e-05,
        3.9683e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 21, batch: 74/188] total loss per batch: 0.624
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0080, 0.0050, 0.0051, 0.0028, 0.9583, 0.0176, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 21, batch: 111/188] total loss per batch: 0.650
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6453e-01, 4.4766e-03, 3.3931e-06, 4.8353e-03, 1.4118e-05, 5.3912e-05,
        2.6089e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.066

[Epoch: 21, batch: 148/188] total loss per batch: 0.600
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.2548e-03, 2.9725e-02, 6.0642e-05, 1.1477e-02, 9.2587e-01, 1.9774e-06,
        2.6608e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.175

[Epoch: 21, batch: 185/188] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0247, 0.0065, 0.0092, 0.9447, 0.0050, 0.0053, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 22, batch: 37/188] total loss per batch: 0.621
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.1810e-01, 3.0325e-01, 3.5904e-01, 1.9769e-02, 7.2689e-03, 5.5373e-05,
        9.2520e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 22, batch: 74/188] total loss per batch: 0.617
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0090, 0.0056, 0.0067, 0.0047, 0.9511, 0.0172, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 22, batch: 111/188] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4371e-01, 4.0686e-03, 1.1266e-05, 6.6104e-03, 6.7645e-05, 4.0300e-05,
        4.5489e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.062

[Epoch: 22, batch: 148/188] total loss per batch: 0.595
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.1346e-02, 4.8293e-02, 6.5451e-05, 1.0441e-02, 8.9273e-01, 2.4513e-06,
        3.7121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.073

[Epoch: 22, batch: 185/188] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0519, 0.0091, 0.0106, 0.9060, 0.0058, 0.0106, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 23, batch: 37/188] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.4468e-01, 4.1879e-01, 3.4046e-01, 1.4156e-02, 7.2667e-03, 4.1675e-05,
        7.4609e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 23, batch: 74/188] total loss per batch: 0.616
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0087, 0.0045, 0.0081, 0.0043, 0.9424, 0.0274, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 23, batch: 111/188] total loss per batch: 0.644
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4461e-01, 5.5835e-03, 7.0822e-06, 6.3962e-03, 4.4740e-05, 4.8852e-05,
        4.3313e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.080

[Epoch: 23, batch: 148/188] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0786e-03, 3.0468e-02, 5.3481e-05, 8.4941e-03, 9.3328e-01, 1.2703e-06,
        2.2621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.062

[Epoch: 23, batch: 185/188] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0289, 0.0053, 0.0082, 0.9404, 0.0055, 0.0066, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 24, batch: 37/188] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8853e-01, 3.6427e-01, 3.3585e-01, 1.2330e-02, 6.9182e-03, 3.4706e-05,
        9.2069e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 24, batch: 74/188] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0107, 0.0055, 0.0072, 0.0053, 0.9425, 0.0223, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 24, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4664e-01, 5.6563e-03, 6.2137e-06, 5.1593e-03, 5.9799e-05, 5.8011e-05,
        4.2420e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.083

[Epoch: 24, batch: 148/188] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.8575e-03, 3.1039e-02, 5.0221e-05, 6.1893e-03, 9.2913e-01, 1.4769e-06,
        2.7736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.083

[Epoch: 24, batch: 185/188] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0362, 0.0050, 0.0065, 0.9382, 0.0038, 0.0063, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 25, batch: 37/188] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9825e-01, 3.9328e-01, 3.0072e-01, 1.3318e-02, 7.9338e-03, 3.8863e-05,
        8.6458e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 25, batch: 74/188] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0073, 0.0040, 0.0054, 0.0039, 0.9590, 0.0161, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 25, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5746e-01, 5.5517e-03, 5.2870e-06, 5.1698e-03, 3.7504e-05, 4.0198e-05,
        3.1734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.089

[Epoch: 25, batch: 148/188] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([7.3995e-03, 3.8486e-02, 4.0963e-05, 1.0954e-02, 9.1668e-01, 1.7613e-06,
        2.6441e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.084

[Epoch: 25, batch: 185/188] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0523, 0.0089, 0.0088, 0.9047, 0.0077, 0.0092, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 26, batch: 37/188] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.5575e-01, 3.8181e-01, 3.6224e-01, 1.0981e-02, 6.4829e-03, 2.6791e-05,
        8.2713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 26, batch: 74/188] total loss per batch: 0.615
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0091, 0.0049, 0.0079, 0.0038, 0.9460, 0.0230, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 26, batch: 111/188] total loss per batch: 0.644
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3524e-01, 4.4678e-03, 5.5950e-06, 6.9349e-03, 6.3385e-05, 6.0597e-05,
        5.3225e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.097

[Epoch: 26, batch: 148/188] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.6178e-03, 3.8919e-02, 5.8841e-05, 7.0862e-03, 9.1722e-01, 1.4015e-06,
        3.1093e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 26, batch: 185/188] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0405, 0.0038, 0.0069, 0.9346, 0.0044, 0.0063, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 27, batch: 37/188] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.0397e-01, 3.9104e-01, 2.8996e-01, 1.2500e-02, 7.0555e-03, 3.5294e-05,
        9.5437e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 27, batch: 74/188] total loss per batch: 0.616
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0080, 0.0041, 0.0062, 0.0050, 0.9534, 0.0172, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 27, batch: 111/188] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.2054e-01, 7.0538e-03, 8.9039e-06, 5.5548e-03, 4.1589e-05, 3.5523e-05,
        6.6764e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.066

[Epoch: 27, batch: 148/188] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([3.6514e-03, 3.2753e-02, 7.8857e-05, 7.8503e-03, 9.2753e-01, 3.2105e-06,
        2.8132e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.117

[Epoch: 27, batch: 185/188] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0429, 0.0070, 0.0074, 0.9221, 0.0064, 0.0073, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 28, batch: 37/188] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.2423e-01, 4.0655e-01, 3.7101e-01, 1.2033e-02, 3.7501e-03, 3.2092e-05,
        8.2396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 28, batch: 74/188] total loss per batch: 0.618
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0068, 0.0033, 0.0064, 0.0040, 0.9570, 0.0166, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 28, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.7030e-01, 3.7313e-03, 4.1809e-06, 6.3793e-03, 4.4753e-05, 3.0460e-05,
        1.9511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.113

[Epoch: 28, batch: 148/188] total loss per batch: 0.595
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([8.8915e-03, 4.2994e-02, 7.6225e-05, 1.2076e-02, 8.9896e-01, 1.4625e-06,
        3.7000e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.046

[Epoch: 28, batch: 185/188] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0619, 0.0056, 0.0119, 0.9002, 0.0057, 0.0089, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 29, batch: 37/188] total loss per batch: 0.621
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.4721e-01, 2.6683e-01, 3.5513e-01, 1.0978e-02, 9.9586e-03, 3.1859e-05,
        1.0986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 29, batch: 74/188] total loss per batch: 0.619
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0139, 0.0054, 0.0076, 0.0034, 0.9293, 0.0339, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 29, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4185e-01, 5.2482e-03, 9.1727e-06, 5.3712e-03, 5.7805e-05, 3.3383e-05,
        4.7431e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.070

[Epoch: 29, batch: 148/188] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([1.8266e-03, 3.0852e-02, 6.6156e-05, 7.4609e-03, 9.4291e-01, 1.4961e-06,
        1.6885e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.155

[Epoch: 29, batch: 185/188] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0232, 0.0062, 0.0052, 0.9487, 0.0053, 0.0081, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 30, batch: 37/188] total loss per batch: 0.622
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.3571e-01, 5.0298e-01, 2.7593e-01, 1.5215e-02, 6.6486e-03, 2.5628e-05,
        6.3490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 30, batch: 74/188] total loss per batch: 0.621
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0086, 0.0045, 0.0055, 0.0045, 0.9593, 0.0141, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 30, batch: 111/188] total loss per batch: 0.650
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.2489e-01, 7.3559e-03, 9.0204e-06, 7.0180e-03, 5.8175e-05, 6.3305e-05,
        6.0602e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.052

[Epoch: 30, batch: 148/188] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([7.2186e-03, 3.3558e-02, 9.8954e-05, 6.7226e-03, 9.1934e-01, 1.4982e-06,
        3.3056e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.072

[Epoch: 30, batch: 185/188] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0504, 0.0051, 0.0081, 0.9210, 0.0036, 0.0071, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 31, batch: 37/188] total loss per batch: 0.622
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7291e-01, 3.3656e-01, 3.9153e-01, 1.0160e-02, 1.0446e-02, 5.9114e-05,
        7.8346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 31, batch: 74/188] total loss per batch: 0.621
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0109, 0.0052, 0.0071, 0.0030, 0.9469, 0.0211, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 31, batch: 111/188] total loss per batch: 0.648
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6605e-01, 4.5999e-03, 5.1843e-06, 7.5223e-03, 3.0406e-05, 2.8872e-05,
        2.1761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.120

[Epoch: 31, batch: 148/188] total loss per batch: 0.595
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.3377e-03, 6.1233e-02, 7.3595e-05, 1.1635e-02, 8.8520e-01, 2.4462e-06,
        3.7523e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.112

[Epoch: 31, batch: 185/188] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0332, 0.0086, 0.0086, 0.9303, 0.0082, 0.0067, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 32, batch: 37/188] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8733e-01, 3.8822e-01, 3.0266e-01, 1.4989e-02, 5.6317e-03, 2.3327e-05,
        1.0113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 32, batch: 74/188] total loss per batch: 0.615
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0129, 0.0045, 0.0055, 0.0051, 0.9459, 0.0211, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 32, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6057e-01, 6.2926e-03, 7.0214e-06, 4.8156e-03, 6.2004e-05, 5.3420e-05,
        2.8199e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.075

[Epoch: 32, batch: 148/188] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.1080e-03, 2.8448e-02, 7.5262e-05, 6.2810e-03, 9.2925e-01, 1.2878e-06,
        2.9841e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.048

[Epoch: 32, batch: 185/188] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0437, 0.0054, 0.0086, 0.9262, 0.0051, 0.0063, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 33, batch: 37/188] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8844e-01, 4.1246e-01, 3.1190e-01, 8.8926e-03, 5.2378e-03, 2.3993e-05,
        7.3042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 33, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0078, 0.0043, 0.0060, 0.0050, 0.9533, 0.0190, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 33, batch: 111/188] total loss per batch: 0.642
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6484e-01, 6.5295e-03, 5.5073e-06, 5.4951e-03, 3.4350e-05, 3.6717e-05,
        2.3056e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.090

[Epoch: 33, batch: 148/188] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.3782e-03, 3.7439e-02, 6.6273e-05, 7.9225e-03, 9.2544e-01, 1.6396e-06,
        2.4750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.100

[Epoch: 33, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0378, 0.0054, 0.0068, 0.9353, 0.0049, 0.0055, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 34, batch: 37/188] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6261e-01, 3.8583e-01, 3.3135e-01, 1.0562e-02, 5.0234e-03, 2.3889e-05,
        1.0460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 34, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0113, 0.0048, 0.0056, 0.0043, 0.9474, 0.0209, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 34, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3739e-01, 5.8188e-03, 8.0208e-06, 6.0352e-03, 4.8252e-05, 5.3011e-05,
        5.0645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.105

[Epoch: 34, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.2821e-03, 3.1082e-02, 7.6203e-05, 7.1143e-03, 9.2628e-01, 9.8690e-07,
        3.1161e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.099

[Epoch: 34, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0390, 0.0048, 0.0055, 0.9367, 0.0044, 0.0056, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 35, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8993e-01, 3.3062e-01, 3.7839e-01, 7.5627e-03, 6.0747e-03, 2.0148e-05,
        8.7398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 35, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0087, 0.0045, 0.0055, 0.0050, 0.9495, 0.0216, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 35, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4769e-01, 4.8157e-03, 4.8334e-06, 5.0691e-03, 2.8343e-05, 2.8773e-05,
        4.2366e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.104

[Epoch: 35, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.4720e-03, 3.9152e-02, 5.4232e-05, 7.7315e-03, 9.1908e-01, 1.6053e-06,
        2.7506e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.095

[Epoch: 35, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0438, 0.0060, 0.0061, 0.9287, 0.0055, 0.0058, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 36, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7455e-01, 4.7708e-01, 2.5014e-01, 1.1029e-02, 4.5617e-03, 2.1330e-05,
        8.2624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 36, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0043, 0.0053, 0.0039, 0.9574, 0.0141, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 36, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4308e-01, 6.1090e-03, 6.2559e-06, 5.5596e-03, 4.6342e-05, 5.2250e-05,
        4.5146e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.101

[Epoch: 36, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([3.6191e-03, 3.3176e-02, 5.6142e-05, 6.9710e-03, 9.2464e-01, 7.6673e-07,
        3.1534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.121

[Epoch: 36, batch: 185/188] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0515, 0.0057, 0.0070, 0.9202, 0.0049, 0.0064, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 37, batch: 37/188] total loss per batch: 0.614
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6696e-01, 2.8250e-01, 4.5495e-01, 7.3920e-03, 7.1307e-03, 1.7676e-05,
        8.1055e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 37, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0097, 0.0050, 0.0056, 0.0055, 0.9443, 0.0243, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 37, batch: 111/188] total loss per batch: 0.642
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4758e-01, 5.2617e-03, 5.6543e-06, 4.9897e-03, 3.3301e-05, 1.9303e-05,
        4.2108e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 37, batch: 148/188] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.1454e-03, 3.5677e-02, 6.1098e-05, 7.8871e-03, 9.2128e-01, 1.6540e-06,
        2.9947e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.084

[Epoch: 37, batch: 185/188] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0423, 0.0062, 0.0056, 0.9298, 0.0046, 0.0063, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 38, batch: 37/188] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9443e-01, 4.5097e-01, 2.4406e-01, 1.5691e-02, 5.8868e-03, 1.8765e-05,
        8.8954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 38, batch: 74/188] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0080, 0.0042, 0.0050, 0.0034, 0.9544, 0.0201, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 38, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4803e-01, 4.9514e-03, 5.7571e-06, 4.8201e-03, 5.7500e-05, 5.2625e-05,
        4.2081e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.078

[Epoch: 38, batch: 148/188] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.6154e-03, 4.0068e-02, 4.9580e-05, 7.5633e-03, 9.1837e-01, 1.3805e-06,
        2.9329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.115

[Epoch: 38, batch: 185/188] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0459, 0.0067, 0.0092, 0.9186, 0.0061, 0.0083, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 39, batch: 37/188] total loss per batch: 0.616
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7087e-01, 3.2665e-01, 3.7465e-01, 1.2294e-02, 8.7825e-03, 2.8311e-05,
        1.0673e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 39, batch: 74/188] total loss per batch: 0.615
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0090, 0.0053, 0.0049, 0.0049, 0.9557, 0.0154, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 39, batch: 111/188] total loss per batch: 0.644
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.7914e-01, 4.1944e-03, 2.9591e-06, 4.5020e-03, 3.9892e-05, 3.7222e-05,
        1.2085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.102

[Epoch: 39, batch: 148/188] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([8.5448e-03, 4.3076e-02, 4.8708e-05, 7.9278e-03, 9.1599e-01, 2.1298e-06,
        2.4412e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.094

[Epoch: 39, batch: 185/188] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0319, 0.0055, 0.0060, 0.9422, 0.0046, 0.0065, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.004

[Epoch: 40, batch: 37/188] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7173e-01, 4.6417e-01, 2.8456e-01, 1.1247e-02, 3.7258e-03, 1.9485e-05,
        6.4557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 40, batch: 74/188] total loss per batch: 0.616
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0099, 0.0046, 0.0058, 0.0037, 0.9444, 0.0272, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 40, batch: 111/188] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6476e-01, 6.1897e-03, 1.0726e-05, 3.9972e-03, 4.8481e-05, 5.3910e-05,
        2.4943e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 40, batch: 148/188] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.3976e-03, 3.1128e-02, 9.9366e-05, 5.9019e-03, 9.2474e-01, 2.0335e-06,
        3.2734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.099

[Epoch: 40, batch: 185/188] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0682, 0.0105, 0.0109, 0.8848, 0.0062, 0.0120, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 41, batch: 37/188] total loss per batch: 0.618
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9353e-01, 3.2084e-01, 3.4066e-01, 1.1921e-02, 7.3705e-03, 3.4145e-05,
        1.2565e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 41, batch: 74/188] total loss per batch: 0.616
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0067, 0.0044, 0.0039, 0.0036, 0.9597, 0.0166, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 41, batch: 111/188] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3874e-01, 5.2263e-03, 5.6265e-06, 6.0073e-03, 4.6319e-05, 6.7366e-05,
        4.9904e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.077

[Epoch: 41, batch: 148/188] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.7770e-03, 3.0902e-02, 2.6057e-05, 6.7517e-03, 9.2843e-01, 1.1840e-06,
        2.8117e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.087

[Epoch: 41, batch: 185/188] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0218, 0.0035, 0.0051, 0.9576, 0.0039, 0.0054, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 42, batch: 37/188] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6069e-01, 4.0728e-01, 3.4033e-01, 8.5825e-03, 3.4869e-03, 2.2768e-05,
        7.9604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 42, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0109, 0.0051, 0.0060, 0.0047, 0.9511, 0.0173, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 42, batch: 111/188] total loss per batch: 0.642
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3815e-01, 5.7494e-03, 6.1466e-06, 5.0854e-03, 6.1422e-05, 4.7500e-05,
        5.0905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.096

[Epoch: 42, batch: 148/188] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.9193e-03, 3.4090e-02, 6.5041e-05, 8.7691e-03, 9.1324e-01, 1.7110e-06,
        3.6917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.076

[Epoch: 42, batch: 185/188] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0475, 0.0056, 0.0066, 0.9229, 0.0044, 0.0075, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 43, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8604e-01, 3.8845e-01, 3.2134e-01, 9.4348e-03, 3.9783e-03, 2.4328e-05,
        9.0732e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 43, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0092, 0.0053, 0.0056, 0.0048, 0.9480, 0.0221, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 43, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3998e-01, 6.3946e-03, 7.0782e-06, 6.4100e-03, 5.4318e-05, 3.9128e-05,
        4.7113e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.082

[Epoch: 43, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0186e-03, 3.5966e-02, 2.9146e-05, 6.7069e-03, 9.2542e-01, 9.3064e-07,
        2.6860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.102

[Epoch: 43, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0476, 0.0065, 0.0072, 0.9205, 0.0062, 0.0069, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 44, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7138e-01, 3.9037e-01, 3.3734e-01, 7.1420e-03, 3.9695e-03, 1.8004e-05,
        8.9770e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 44, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0099, 0.0052, 0.0054, 0.0045, 0.9515, 0.0178, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 44, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4308e-01, 5.1594e-03, 4.9303e-06, 6.3439e-03, 6.2996e-05, 4.4228e-05,
        4.5305e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.081

[Epoch: 44, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.9112e-03, 2.9642e-02, 3.0649e-05, 6.5431e-03, 9.3005e-01, 1.0504e-06,
        2.7820e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.096

[Epoch: 44, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0397, 0.0043, 0.0055, 0.9360, 0.0040, 0.0058, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 45, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9901e-01, 3.8078e-01, 3.2267e-01, 7.4168e-03, 5.0406e-03, 1.3454e-05,
        8.5073e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 45, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0097, 0.0048, 0.0059, 0.0051, 0.9462, 0.0228, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 45, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5445e-01, 5.7207e-03, 5.3430e-06, 4.7808e-03, 4.3370e-05, 3.3880e-05,
        3.4965e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.073

[Epoch: 45, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0073e-03, 4.2989e-02, 2.5205e-05, 6.5933e-03, 9.1491e-01, 8.4718e-07,
        3.0478e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.094

[Epoch: 45, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0445, 0.0045, 0.0059, 0.9303, 0.0048, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 46, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6175e-01, 3.9851e-01, 3.3390e-01, 6.9312e-03, 4.0092e-03, 1.3059e-05,
        9.4894e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 46, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0117, 0.0060, 0.0063, 0.0055, 0.9430, 0.0212, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 46, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5786e-01, 5.1376e-03, 4.8584e-06, 4.6761e-03, 4.3400e-05, 3.3351e-05,
        3.2244e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.092

[Epoch: 46, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.6953e-03, 2.9503e-02, 2.0888e-05, 5.6413e-03, 9.3146e-01, 6.8125e-07,
        2.8677e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.088

[Epoch: 46, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0521, 0.0055, 0.0059, 0.9200, 0.0049, 0.0068, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 47, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.0187e-01, 3.7680e-01, 3.2151e-01, 6.7439e-03, 5.0586e-03, 1.7538e-05,
        8.7995e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 47, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0051, 0.0056, 0.0048, 0.9466, 0.0223, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 47, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5683e-01, 5.3452e-03, 2.8176e-06, 4.0431e-03, 3.3472e-05, 2.2842e-05,
        3.3719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 47, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.9402e-03, 4.3402e-02, 1.6580e-05, 6.8303e-03, 9.1412e-01, 7.0995e-07,
        2.8688e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.094

[Epoch: 47, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0331, 0.0036, 0.0047, 0.9476, 0.0032, 0.0040, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 48, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6839e-01, 4.0285e-01, 3.3090e-01, 6.2597e-03, 3.9731e-03, 1.0278e-05,
        8.7624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 48, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0111, 0.0048, 0.0069, 0.0047, 0.9496, 0.0182, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 48, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4131e-01, 4.8343e-03, 6.6356e-06, 5.9940e-03, 5.5607e-05, 4.5218e-05,
        4.7750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 48, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([3.2404e-03, 2.3340e-02, 1.9326e-05, 3.7588e-03, 9.3677e-01, 7.6393e-07,
        3.2871e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.091

[Epoch: 48, batch: 185/188] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0598, 0.0059, 0.0069, 0.9094, 0.0050, 0.0076, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 49, batch: 37/188] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7300e-01, 3.6072e-01, 3.5650e-01, 8.2483e-03, 5.4509e-03, 1.7343e-05,
        9.6063e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 49, batch: 74/188] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0090, 0.0053, 0.0050, 0.0047, 0.9503, 0.0208, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 49, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.2260e-01, 8.5161e-03, 3.8874e-06, 4.5596e-03, 4.9457e-05, 2.8071e-05,
        6.4240e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 49, batch: 148/188] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.3807e-03, 5.4288e-02, 1.8693e-05, 6.5828e-03, 9.0809e-01, 6.0989e-07,
        2.4643e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.116

[Epoch: 49, batch: 185/188] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0465, 0.0072, 0.0068, 0.9233, 0.0064, 0.0057, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 50, batch: 37/188] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7600e-01, 3.9975e-01, 3.3120e-01, 7.6969e-03, 5.6275e-03, 1.6806e-05,
        7.9717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 50, batch: 74/188] total loss per batch: 0.615
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0084, 0.0055, 0.0068, 0.0052, 0.9522, 0.0168, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 50, batch: 111/188] total loss per batch: 0.644
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4114e-01, 4.8663e-03, 4.5528e-06, 8.0318e-03, 7.0298e-05, 4.5664e-05,
        4.5837e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.077

[Epoch: 50, batch: 148/188] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.6158e-03, 2.8465e-02, 2.8336e-05, 6.5830e-03, 9.2322e-01, 6.1908e-07,
        3.6087e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.075

[Epoch: 50, batch: 185/188] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0407, 0.0064, 0.0064, 0.9287, 0.0054, 0.0070, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 51, batch: 37/188] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9291e-01, 3.5295e-01, 3.3778e-01, 1.1851e-02, 5.1959e-03, 1.9440e-05,
        9.9293e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 51, batch: 74/188] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0089, 0.0038, 0.0054, 0.0039, 0.9542, 0.0189, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 51, batch: 111/188] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5071e-01, 6.2262e-03, 3.2422e-06, 4.3694e-03, 4.8734e-05, 3.4428e-05,
        3.8607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.077

[Epoch: 51, batch: 148/188] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([7.0350e-03, 3.0296e-02, 2.8389e-05, 4.8207e-03, 9.3184e-01, 6.7192e-07,
        2.5980e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.102

[Epoch: 51, batch: 185/188] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0386, 0.0045, 0.0075, 0.9336, 0.0057, 0.0059, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 52, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.5437e-01, 4.0834e-01, 3.3586e-01, 7.9326e-03, 5.6645e-03, 1.5820e-05,
        8.7822e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 52, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0078, 0.0040, 0.0054, 0.0040, 0.9570, 0.0175, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 52, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5900e-01, 5.7378e-03, 4.0464e-06, 4.3484e-03, 4.4983e-05, 2.9942e-05,
        3.0839e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 52, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.8311e-03, 3.9460e-02, 2.8921e-05, 6.1008e-03, 9.1948e-01, 7.6951e-07,
        2.8103e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.105

[Epoch: 52, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0445, 0.0057, 0.0063, 0.9264, 0.0056, 0.0065, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 53, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8110e-01, 4.0672e-01, 3.0380e-01, 8.2939e-03, 4.6049e-03, 1.6785e-05,
        9.5463e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 53, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0096, 0.0043, 0.0052, 0.0045, 0.9514, 0.0201, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 53, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6795e-01, 5.4477e-03, 3.7710e-06, 3.7430e-03, 5.3512e-05, 3.7758e-05,
        2.2768e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.093

[Epoch: 53, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.7485e-03, 3.4588e-02, 2.3444e-05, 5.3097e-03, 9.2902e-01, 4.9915e-07,
        2.5306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.088

[Epoch: 53, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0384, 0.0042, 0.0052, 0.9381, 0.0042, 0.0055, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 54, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 2
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6990e-01, 3.6654e-01, 3.6804e-01, 7.1518e-03, 4.2608e-03, 1.2198e-05,
        8.4094e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 54, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0103, 0.0053, 0.0062, 0.0051, 0.9448, 0.0224, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 54, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4759e-01, 5.5954e-03, 4.4332e-06, 4.5913e-03, 4.8772e-05, 3.2335e-05,
        4.2140e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.091

[Epoch: 54, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.2992e-03, 3.4464e-02, 2.3423e-05, 5.6135e-03, 9.2144e-01, 6.1704e-07,
        3.3156e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.092

[Epoch: 54, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0452, 0.0047, 0.0054, 0.9291, 0.0047, 0.0059, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 55, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8920e-01, 4.0000e-01, 3.0762e-01, 6.6307e-03, 4.4437e-03, 1.2603e-05,
        9.2085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 55, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0112, 0.0054, 0.0060, 0.0051, 0.9436, 0.0228, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 55, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4386e-01, 5.3615e-03, 3.6040e-06, 5.1515e-03, 5.2601e-05, 3.2693e-05,
        4.5534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 55, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.3553e-03, 3.6867e-02, 1.8623e-05, 5.7600e-03, 9.2586e-01, 4.5024e-07,
        2.6138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.100

[Epoch: 55, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0439, 0.0044, 0.0052, 0.9325, 0.0044, 0.0054, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 56, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9376e-01, 3.7198e-01, 3.3073e-01, 7.1268e-03, 5.3248e-03, 1.1232e-05,
        9.1058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 56, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0108, 0.0053, 0.0061, 0.0053, 0.9435, 0.0229, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 56, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5033e-01, 5.3752e-03, 2.4208e-06, 5.0073e-03, 3.4978e-05, 2.3208e-05,
        3.9223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 56, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8135e-03, 3.4078e-02, 1.5205e-05, 5.2682e-03, 9.2563e-01, 4.6465e-07,
        3.0196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.091

[Epoch: 56, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0431, 0.0044, 0.0047, 0.9341, 0.0039, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 57, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6443e-01, 4.0576e-01, 3.1868e-01, 7.9155e-03, 5.9389e-03, 1.2890e-05,
        9.7261e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 57, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0095, 0.0053, 0.0055, 0.0046, 0.9478, 0.0219, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 57, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4970e-01, 4.7481e-03, 2.6928e-06, 4.3755e-03, 3.9860e-05, 1.9954e-05,
        4.1111e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.098

[Epoch: 57, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.1511e-03, 3.4574e-02, 1.7169e-05, 5.6048e-03, 9.2049e-01, 3.9137e-07,
        3.4167e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.104

[Epoch: 57, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0476, 0.0041, 0.0050, 0.9286, 0.0049, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 58, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.0101e-01, 3.6815e-01, 3.2753e-01, 9.0708e-03, 5.6597e-03, 1.2567e-05,
        8.8564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 58, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0040, 0.0054, 0.0039, 0.9583, 0.0155, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 58, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.3851e-01, 6.6244e-03, 2.4349e-06, 5.4722e-03, 3.2224e-05, 2.5318e-05,
        4.9330e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.068

[Epoch: 58, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8401e-03, 3.5585e-02, 1.7850e-05, 5.2180e-03, 9.2703e-01, 4.7114e-07,
        2.7305e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.096

[Epoch: 58, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0503, 0.0063, 0.0062, 0.9212, 0.0048, 0.0065, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 59, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6151e-01, 4.0185e-01, 3.2754e-01, 8.1203e-03, 6.0436e-03, 1.6899e-05,
        9.4924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 59, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0072, 0.0044, 0.0044, 0.0034, 0.9558, 0.0206, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 59, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.6127e-01, 4.8591e-03, 2.0725e-06, 4.5587e-03, 3.5184e-05, 2.0447e-05,
        2.9257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.097

[Epoch: 59, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.7941e-03, 3.3777e-02, 1.7009e-05, 6.6939e-03, 9.1346e-01, 4.4008e-07,
        3.9260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.103

[Epoch: 59, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0395, 0.0050, 0.0052, 0.9352, 0.0049, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 60, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9645e-01, 3.8895e-01, 3.1886e-01, 8.5329e-03, 4.6163e-03, 1.2883e-05,
        8.2575e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 60, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0072, 0.0032, 0.0050, 0.0040, 0.9638, 0.0136, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 60, batch: 111/188] total loss per batch: 0.642
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.2141e-01, 6.8501e-03, 3.8601e-06, 8.3112e-03, 3.7037e-05, 2.9486e-05,
        6.3354e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.076

[Epoch: 60, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([6.3095e-03, 3.7202e-02, 2.1195e-05, 5.7528e-03, 9.3145e-01, 9.8731e-07,
        1.9268e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.081

[Epoch: 60, batch: 185/188] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0486, 0.0059, 0.0070, 0.9226, 0.0053, 0.0058, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 61, batch: 37/188] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.2957e-01, 3.9295e-01, 3.7178e-01, 5.4533e-03, 5.4015e-03, 1.6966e-05,
        9.4828e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 61, batch: 74/188] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0083, 0.0056, 0.0056, 0.0039, 0.9545, 0.0176, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 61, batch: 111/188] total loss per batch: 0.642
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5447e-01, 6.2965e-03, 3.8615e-06, 5.3572e-03, 4.5105e-05, 2.7045e-05,
        3.3803e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 61, batch: 148/188] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9417e-03, 3.4688e-02, 1.8462e-05, 6.4708e-03, 9.2107e-01, 5.5225e-07,
        3.2809e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.115

[Epoch: 61, batch: 185/188] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0414, 0.0048, 0.0068, 0.9290, 0.0061, 0.0065, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 62, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([2.1669e-01, 3.8671e-01, 2.9903e-01, 6.5717e-03, 3.7152e-03, 1.2336e-05,
        8.7274e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 62, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0123, 0.0064, 0.0071, 0.0050, 0.9380, 0.0251, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 62, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5218e-01, 6.8939e-03, 3.9140e-06, 6.0305e-03, 5.4197e-05, 4.3244e-05,
        3.4791e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 62, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0576e-03, 3.7623e-02, 1.5868e-05, 5.2584e-03, 9.2437e-01, 5.1432e-07,
        2.7678e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.101

[Epoch: 62, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0431, 0.0048, 0.0060, 0.9305, 0.0050, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 63, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7230e-01, 3.9263e-01, 3.3614e-01, 5.5345e-03, 4.5694e-03, 1.2253e-05,
        8.8816e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 63, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0124, 0.0062, 0.0069, 0.0059, 0.9357, 0.0268, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 63, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5803e-01, 5.3088e-03, 3.0985e-06, 4.2819e-03, 4.2794e-05, 2.6142e-05,
        3.2306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.090

[Epoch: 63, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.3766e-03, 3.3758e-02, 1.4465e-05, 5.1247e-03, 9.3035e-01, 4.2541e-07,
        2.6374e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.100

[Epoch: 63, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0371, 0.0041, 0.0052, 0.9401, 0.0044, 0.0048, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 64, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8792e-01, 3.8581e-01, 3.2529e-01, 6.1416e-03, 4.8660e-03, 1.1176e-05,
        8.9961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 64, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0127, 0.0064, 0.0069, 0.0056, 0.9384, 0.0240, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 64, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5226e-01, 5.3819e-03, 2.5825e-06, 5.4085e-03, 3.4426e-05, 2.8513e-05,
        3.6888e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 64, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.5506e-03, 3.9095e-02, 1.2475e-05, 5.7044e-03, 9.1611e-01, 4.5660e-07,
        3.4532e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.098

[Epoch: 64, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0542, 0.0051, 0.0059, 0.9194, 0.0050, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 65, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8709e-01, 3.8510e-01, 3.2289e-01, 6.2395e-03, 5.4161e-03, 1.1707e-05,
        9.3248e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 65, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0101, 0.0054, 0.0059, 0.0049, 0.9478, 0.0210, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 65, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5052e-01, 5.2660e-03, 2.5679e-06, 4.7939e-03, 2.7239e-05, 1.8714e-05,
        3.9371e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.091

[Epoch: 65, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.7257e-03, 3.3700e-02, 1.0463e-05, 5.1845e-03, 9.2712e-01, 3.9327e-07,
        2.9262e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.105

[Epoch: 65, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0477, 0.0056, 0.0060, 0.9241, 0.0061, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 66, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7139e-01, 3.9257e-01, 3.3376e-01, 7.3477e-03, 5.3562e-03, 1.0275e-05,
        8.9564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 66, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0050, 0.0056, 0.0044, 0.9538, 0.0179, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 66, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4335e-01, 5.2840e-03, 2.2636e-06, 5.6367e-03, 3.0941e-05, 2.1627e-05,
        4.5674e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 66, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.3294e-03, 3.5659e-02, 1.0696e-05, 5.5625e-03, 9.2092e-01, 4.1991e-07,
        3.2522e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.093

[Epoch: 66, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0443, 0.0051, 0.0057, 0.9298, 0.0053, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 67, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7900e-01, 3.9092e-01, 3.2690e-01, 6.7782e-03, 5.4229e-03, 1.0322e-05,
        9.0961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 67, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0084, 0.0044, 0.0049, 0.0044, 0.9544, 0.0190, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 67, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5724e-01, 5.0571e-03, 2.2322e-06, 4.7815e-03, 2.2339e-05, 1.5282e-05,
        3.2880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 67, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.4052e-03, 3.3775e-02, 1.0840e-05, 5.3510e-03, 9.2412e-01, 3.9380e-07,
        3.1333e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.101

[Epoch: 67, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0427, 0.0055, 0.0059, 0.9293, 0.0060, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 68, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7069e-01, 3.9806e-01, 3.3657e-01, 5.8879e-03, 4.3115e-03, 8.3034e-06,
        8.4471e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 68, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0096, 0.0050, 0.0060, 0.0048, 0.9514, 0.0184, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 68, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4749e-01, 5.0565e-03, 2.3980e-06, 4.9281e-03, 3.0426e-05, 2.0439e-05,
        4.2476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 68, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.5596e-03, 3.2387e-02, 9.4311e-06, 5.0077e-03, 9.3479e-01, 3.7325e-07,
        2.3249e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.085

[Epoch: 68, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0462, 0.0057, 0.0059, 0.9263, 0.0053, 0.0054, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 69, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8708e-01, 3.6732e-01, 3.4079e-01, 5.5284e-03, 5.0894e-03, 9.8147e-06,
        9.4184e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 69, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0102, 0.0052, 0.0055, 0.0051, 0.9464, 0.0219, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 69, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5730e-01, 5.1031e-03, 2.2255e-06, 5.2401e-03, 2.5087e-05, 1.5394e-05,
        3.2311e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.082

[Epoch: 69, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0544e-03, 3.8927e-02, 1.1809e-05, 6.2377e-03, 9.1393e-01, 3.4371e-07,
        3.5841e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.108

[Epoch: 69, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0400, 0.0047, 0.0059, 0.9331, 0.0060, 0.0055, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 70, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7938e-01, 4.0963e-01, 3.2191e-01, 5.5043e-03, 3.4897e-03, 8.7311e-06,
        8.0078e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 70, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0117, 0.0053, 0.0072, 0.0050, 0.9442, 0.0211, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 70, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4401e-01, 5.4358e-03, 2.2979e-06, 4.6945e-03, 3.8558e-05, 2.9648e-05,
        4.5793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.083

[Epoch: 70, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0099e-03, 3.7573e-02, 1.1078e-05, 5.6748e-03, 9.2747e-01, 4.3632e-07,
        2.4265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.072

[Epoch: 70, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0471, 0.0053, 0.0052, 0.9294, 0.0036, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 71, batch: 37/188] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.9549e-01, 3.6759e-01, 3.2425e-01, 6.2717e-03, 6.8345e-03, 9.9531e-06,
        9.9564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 71, batch: 74/188] total loss per batch: 0.612
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0111, 0.0056, 0.0056, 0.0052, 0.9460, 0.0207, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 71, batch: 111/188] total loss per batch: 0.641
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5970e-01, 5.1429e-03, 2.4645e-06, 5.1386e-03, 2.7438e-05, 1.9234e-05,
        2.9970e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.084

[Epoch: 71, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.4915e-03, 3.1561e-02, 1.3950e-05, 5.5291e-03, 9.2705e-01, 4.2540e-07,
        3.1356e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.130

[Epoch: 71, batch: 185/188] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0419, 0.0050, 0.0055, 0.9315, 0.0052, 0.0060, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 72, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7004e-01, 3.9776e-01, 3.3021e-01, 7.1470e-03, 5.6604e-03, 1.1162e-05,
        8.9180e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 72, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0092, 0.0046, 0.0057, 0.0040, 0.9544, 0.0178, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 72, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4264e-01, 5.7703e-03, 2.4143e-06, 5.5082e-03, 3.4590e-05, 2.1772e-05,
        4.6022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.081

[Epoch: 72, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.5396e-03, 3.7178e-02, 1.0223e-05, 5.1930e-03, 9.2113e-01, 4.4225e-07,
        3.1948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.080

[Epoch: 72, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0422, 0.0045, 0.0047, 0.9362, 0.0042, 0.0044, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 73, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8597e-01, 3.7596e-01, 3.3017e-01, 7.1169e-03, 5.6792e-03, 1.0220e-05,
        9.5098e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 73, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0086, 0.0043, 0.0050, 0.0041, 0.9563, 0.0173, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 73, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4779e-01, 4.7915e-03, 2.0988e-06, 5.2612e-03, 2.4821e-05, 1.4864e-05,
        4.2111e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.084

[Epoch: 73, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9970e-03, 3.1374e-02, 9.6334e-06, 5.1651e-03, 9.2966e-01, 4.1695e-07,
        2.8789e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.099

[Epoch: 73, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0479, 0.0051, 0.0054, 0.9264, 0.0048, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 74, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7007e-01, 4.0208e-01, 3.2723e-01, 6.2393e-03, 5.7364e-03, 9.9290e-06,
        8.8637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 74, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0046, 0.0054, 0.0043, 0.9549, 0.0177, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 74, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4743e-01, 4.7887e-03, 2.1428e-06, 5.0611e-03, 2.3625e-05, 1.4729e-05,
        4.2676e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.080

[Epoch: 74, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9330e-03, 3.8017e-02, 9.4216e-06, 5.2363e-03, 9.1969e-01, 3.8812e-07,
        3.2119e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 74, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0442, 0.0049, 0.0053, 0.9315, 0.0049, 0.0050, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 75, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7833e-01, 3.8647e-01, 3.3653e-01, 6.2977e-03, 4.7502e-03, 8.7912e-06,
        8.7617e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 75, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0094, 0.0048, 0.0054, 0.0046, 0.9528, 0.0183, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 75, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5331e-01, 4.7699e-03, 1.7326e-06, 4.9727e-03, 2.2077e-05, 1.4475e-05,
        3.6913e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.083

[Epoch: 75, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.7465e-03, 3.4542e-02, 8.8042e-06, 5.0537e-03, 9.2752e-01, 3.3536e-07,
        2.8132e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.091

[Epoch: 75, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0437, 0.0045, 0.0050, 0.9331, 0.0043, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 76, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7727e-01, 3.9349e-01, 3.3065e-01, 5.5386e-03, 5.0622e-03, 7.6995e-06,
        8.7983e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 76, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0098, 0.0050, 0.0060, 0.0047, 0.9488, 0.0208, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 76, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4986e-01, 4.9573e-03, 1.6550e-06, 4.8193e-03, 2.1104e-05, 1.3153e-05,
        4.0327e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 76, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.7005e-03, 3.6031e-02, 8.3217e-06, 4.9882e-03, 9.2569e-01, 2.9301e-07,
        2.8586e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.100

[Epoch: 76, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0451, 0.0051, 0.0051, 0.9298, 0.0049, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 77, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8409e-01, 3.8451e-01, 3.3292e-01, 5.6027e-03, 4.8088e-03, 7.7332e-06,
        8.8063e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 77, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0109, 0.0054, 0.0056, 0.0052, 0.9468, 0.0207, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 77, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5176e-01, 5.1830e-03, 1.4678e-06, 4.7508e-03, 1.9575e-05, 1.3586e-05,
        3.8269e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 77, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9221e-03, 3.4509e-02, 7.8046e-06, 5.2609e-03, 9.2345e-01, 3.1206e-07,
        3.1846e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.087

[Epoch: 77, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0426, 0.0044, 0.0047, 0.9349, 0.0041, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 78, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7927e-01, 3.9237e-01, 3.2482e-01, 5.6214e-03, 5.1798e-03, 7.7634e-06,
        9.2734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 78, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0105, 0.0051, 0.0061, 0.0048, 0.9469, 0.0214, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 78, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5057e-01, 5.1308e-03, 1.3333e-06, 5.1408e-03, 1.8857e-05, 1.1442e-05,
        3.9124e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 78, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.2253e-03, 3.5149e-02, 8.0069e-06, 4.9489e-03, 9.2965e-01, 2.6209e-07,
        2.6023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.108

[Epoch: 78, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0491, 0.0057, 0.0051, 0.9251, 0.0050, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 79, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8544e-01, 3.8257e-01, 3.3322e-01, 5.9380e-03, 5.1578e-03, 7.2377e-06,
        8.7669e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 79, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0104, 0.0055, 0.0057, 0.0052, 0.9465, 0.0212, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 79, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4664e-01, 5.1102e-03, 1.7262e-06, 4.9204e-03, 1.9246e-05, 1.3833e-05,
        4.3297e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 79, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.4245e-03, 3.3865e-02, 8.0960e-06, 5.4210e-03, 9.1688e-01, 3.2700e-07,
        3.8400e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.086

[Epoch: 79, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0443, 0.0044, 0.0048, 0.9318, 0.0045, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 80, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7162e-01, 3.9796e-01, 3.2816e-01, 6.5663e-03, 5.3363e-03, 7.8661e-06,
        9.0357e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 80, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0098, 0.0048, 0.0060, 0.0047, 0.9497, 0.0201, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 80, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5236e-01, 4.9313e-03, 1.2610e-06, 5.1031e-03, 1.7062e-05, 1.0371e-05,
        3.7580e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 80, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.4599e-03, 3.7908e-02, 9.2441e-06, 5.1385e-03, 9.3008e-01, 2.9875e-07,
        2.2401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.114

[Epoch: 80, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0461, 0.0061, 0.0055, 0.9282, 0.0046, 0.0050, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 81, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8514e-01, 3.7025e-01, 3.4014e-01, 5.5774e-03, 5.1034e-03, 7.7495e-06,
        9.3776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 81, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0046, 0.0047, 0.0039, 0.9560, 0.0178, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 81, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4719e-01, 5.6206e-03, 1.6884e-06, 5.1454e-03, 1.8902e-05, 1.4665e-05,
        4.2012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.083

[Epoch: 81, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.8051e-03, 3.2675e-02, 7.8709e-06, 5.2570e-03, 9.2154e-01, 3.5538e-07,
        3.4715e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 81, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0477, 0.0051, 0.0055, 0.9248, 0.0058, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 82, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7639e-01, 4.0667e-01, 3.2235e-01, 6.0113e-03, 4.8849e-03, 7.3468e-06,
        8.3691e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 82, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0092, 0.0044, 0.0056, 0.0046, 0.9535, 0.0181, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 82, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4864e-01, 5.3790e-03, 1.3786e-06, 5.0854e-03, 1.7523e-05, 1.1242e-05,
        4.0868e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.084

[Epoch: 82, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.6307e-03, 3.4863e-02, 1.0126e-05, 5.1873e-03, 9.2782e-01, 3.5431e-07,
        2.6492e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.103

[Epoch: 82, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0432, 0.0050, 0.0054, 0.9306, 0.0055, 0.0053, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 83, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7418e-01, 3.8832e-01, 3.3407e-01, 5.3518e-03, 4.9301e-03, 7.3826e-06,
        9.3139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 83, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0096, 0.0049, 0.0050, 0.0044, 0.9516, 0.0194, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 83, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5186e-01, 5.3171e-03, 1.5262e-06, 4.9518e-03, 1.7758e-05, 1.1778e-05,
        3.7844e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 83, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.1334e-03, 3.5014e-02, 8.2502e-06, 5.2885e-03, 9.2352e-01, 3.2705e-07,
        3.1036e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.095

[Epoch: 83, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0473, 0.0054, 0.0055, 0.9253, 0.0055, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 84, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7798e-01, 3.9518e-01, 3.3064e-01, 5.3078e-03, 4.5041e-03, 6.6843e-06,
        8.6381e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 84, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0050, 0.0054, 0.0046, 0.9491, 0.0210, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 84, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4677e-01, 5.5282e-03, 1.5072e-06, 5.1372e-03, 1.8419e-05, 1.1585e-05,
        4.2529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 84, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9881e-03, 3.3903e-02, 8.2206e-06, 5.0448e-03, 9.2878e-01, 2.9402e-07,
        2.7276e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.095

[Epoch: 84, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0433, 0.0052, 0.0051, 0.9314, 0.0050, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 85, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8778e-01, 3.7945e-01, 3.2862e-01, 5.2989e-03, 4.9648e-03, 6.8266e-06,
        9.3876e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 85, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0104, 0.0050, 0.0056, 0.0048, 0.9479, 0.0210, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 85, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5126e-01, 5.0230e-03, 1.3231e-06, 5.0769e-03, 1.6610e-05, 1.0845e-05,
        3.8612e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 85, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.2409e-03, 3.5258e-02, 7.4524e-06, 5.2667e-03, 9.2302e-01, 2.9538e-07,
        3.1207e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.094

[Epoch: 85, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0442, 0.0051, 0.0051, 0.9303, 0.0050, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 86, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7449e-01, 3.9964e-01, 3.2803e-01, 5.4404e-03, 4.9447e-03, 6.5179e-06,
        8.7460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 86, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0098, 0.0049, 0.0052, 0.0045, 0.9505, 0.0202, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 86, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5109e-01, 4.9429e-03, 1.2605e-06, 5.0014e-03, 1.5363e-05, 9.4460e-06,
        3.8936e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.090

[Epoch: 86, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9103e-03, 3.4293e-02, 7.5561e-06, 5.0103e-03, 9.2745e-01, 2.7806e-07,
        2.8333e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.099

[Epoch: 86, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0459, 0.0049, 0.0052, 0.9291, 0.0050, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 87, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8660e-01, 3.7925e-01, 3.3105e-01, 5.4611e-03, 5.1210e-03, 6.2219e-06,
        9.2506e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 87, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0098, 0.0048, 0.0052, 0.0045, 0.9515, 0.0195, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 87, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5098e-01, 4.9600e-03, 1.2134e-06, 4.8929e-03, 1.4178e-05, 9.4537e-06,
        3.9143e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 87, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0563e-03, 3.3846e-02, 6.7152e-06, 5.0665e-03, 9.2435e-01, 2.5006e-07,
        3.1672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 87, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0455, 0.0054, 0.0051, 0.9285, 0.0051, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 88, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7792e-01, 3.9578e-01, 3.2486e-01, 5.8681e-03, 5.1702e-03, 7.0002e-06,
        9.0398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 88, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0093, 0.0046, 0.0050, 0.0043, 0.9527, 0.0196, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 88, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4624e-01, 4.8545e-03, 1.1949e-06, 5.0053e-03, 1.3709e-05, 7.9728e-06,
        4.3874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 88, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.4656e-03, 3.7627e-02, 7.4295e-06, 5.0160e-03, 9.2164e-01, 2.9326e-07,
        3.0243e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 88, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0437, 0.0047, 0.0051, 0.9316, 0.0052, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 89, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7499e-01, 3.8746e-01, 3.3446e-01, 5.6975e-03, 5.3881e-03, 6.2141e-06,
        9.1997e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 89, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0090, 0.0045, 0.0050, 0.0045, 0.9535, 0.0190, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 89, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5191e-01, 5.0116e-03, 1.0902e-06, 4.9153e-03, 1.4679e-05, 1.0180e-05,
        3.8136e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 89, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8899e-03, 3.2614e-02, 5.9954e-06, 5.2859e-03, 9.2948e-01, 2.2796e-07,
        2.7728e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.098

[Epoch: 89, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0460, 0.0059, 0.0056, 0.9261, 0.0054, 0.0053, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 90, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8945e-01, 3.8969e-01, 3.1999e-01, 5.9584e-03, 5.1231e-03, 7.6038e-06,
        8.9783e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 90, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0043, 0.0052, 0.0043, 0.9532, 0.0187, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 90, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4974e-01, 4.7893e-03, 1.3588e-06, 4.8125e-03, 1.2541e-05, 7.9168e-06,
        4.0640e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 90, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.0974e-03, 3.9169e-02, 8.7470e-06, 5.3892e-03, 9.1601e-01, 3.0767e-07,
        3.4329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.096

[Epoch: 90, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0417, 0.0045, 0.0048, 0.9346, 0.0050, 0.0051, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 91, batch: 37/188] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.6009e-01, 3.9177e-01, 3.5061e-01, 5.2103e-03, 4.5300e-03, 6.1695e-06,
        8.7777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 91, batch: 74/188] total loss per batch: 0.611
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0088, 0.0049, 0.0049, 0.0048, 0.9536, 0.0181, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 91, batch: 111/188] total loss per batch: 0.640
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5642e-01, 5.1699e-03, 1.2250e-06, 4.7453e-03, 1.9100e-05, 1.1363e-05,
        3.3637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 91, batch: 148/188] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.1141e-03, 3.1851e-02, 7.9245e-06, 5.4766e-03, 9.3074e-01, 2.3760e-07,
        2.6807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.099

[Epoch: 91, batch: 185/188] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0441, 0.0054, 0.0056, 0.9295, 0.0049, 0.0054, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 92, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8877e-01, 3.9067e-01, 3.1897e-01, 5.4340e-03, 5.1812e-03, 8.3076e-06,
        9.0962e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 92, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0114, 0.0053, 0.0060, 0.0052, 0.9443, 0.0225, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 92, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4730e-01, 4.8073e-03, 1.1261e-06, 4.8943e-03, 1.7228e-05, 9.9711e-06,
        4.2969e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.087

[Epoch: 92, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8301e-03, 3.7707e-02, 7.7839e-06, 5.1189e-03, 9.2191e-01, 2.7805e-07,
        3.0424e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.093

[Epoch: 92, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0447, 0.0050, 0.0049, 0.9309, 0.0047, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 93, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8016e-01, 3.8583e-01, 3.3671e-01, 5.4102e-03, 4.6246e-03, 6.7636e-06,
        8.7256e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 93, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0106, 0.0055, 0.0059, 0.0053, 0.9457, 0.0215, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 93, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5108e-01, 5.0759e-03, 1.0705e-06, 4.9996e-03, 1.6164e-05, 9.3961e-06,
        3.8818e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 93, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8629e-03, 3.4169e-02, 6.9551e-06, 5.1096e-03, 9.2627e-01, 2.3494e-07,
        2.9582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.101

[Epoch: 93, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0468, 0.0050, 0.0053, 0.9279, 0.0051, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 94, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8098e-01, 3.9183e-01, 3.2559e-01, 5.5017e-03, 5.2062e-03, 7.3765e-06,
        9.0887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 94, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0103, 0.0052, 0.0055, 0.0049, 0.9481, 0.0207, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 94, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4801e-01, 4.8829e-03, 9.8800e-07, 5.0446e-03, 1.4506e-05, 8.5929e-06,
        4.2035e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.085

[Epoch: 94, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9109e-03, 3.5198e-02, 6.3546e-06, 5.0733e-03, 9.2539e-01, 2.3697e-07,
        2.9422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.096

[Epoch: 94, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0457, 0.0053, 0.0052, 0.9284, 0.0053, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 95, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8305e-01, 3.8663e-01, 3.2850e-01, 5.6500e-03, 5.1136e-03, 6.4287e-06,
        9.1048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 95, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0097, 0.0052, 0.0052, 0.0049, 0.9499, 0.0200, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 95, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5123e-01, 4.9560e-03, 9.4321e-07, 4.9969e-03, 1.3209e-05, 8.1802e-06,
        3.8793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 95, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9789e-03, 3.5079e-02, 6.1119e-06, 5.1166e-03, 9.2440e-01, 2.2929e-07,
        3.0421e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.098

[Epoch: 95, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0468, 0.0054, 0.0054, 0.9270, 0.0054, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 96, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7273e-01, 3.9616e-01, 3.3079e-01, 5.5616e-03, 5.1647e-03, 6.7569e-06,
        8.9592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 96, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0096, 0.0049, 0.0051, 0.0047, 0.9507, 0.0201, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 96, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4839e-01, 4.9601e-03, 9.0084e-07, 5.0797e-03, 1.2475e-05, 7.7547e-06,
        4.1551e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.086

[Epoch: 96, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.8710e-03, 3.4657e-02, 5.7584e-06, 5.1218e-03, 9.2536e-01, 2.2450e-07,
        2.9988e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 96, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0446, 0.0054, 0.0053, 0.9290, 0.0055, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 97, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8269e-01, 3.8564e-01, 3.3083e-01, 5.4654e-03, 4.8557e-03, 5.8643e-06,
        9.0512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 97, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0096, 0.0049, 0.0052, 0.0047, 0.9514, 0.0194, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 97, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5255e-01, 5.0168e-03, 8.9164e-07, 4.9121e-03, 1.2658e-05, 7.8144e-06,
        3.7502e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 97, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([5.2008e-03, 3.5620e-02, 5.8909e-06, 5.0718e-03, 9.2484e-01, 2.1476e-07,
        2.9266e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 97, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0458, 0.0054, 0.0054, 0.9277, 0.0054, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 98, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7573e-01, 3.9201e-01, 3.3401e-01, 5.2152e-03, 4.7943e-03, 5.7485e-06,
        8.8231e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 98, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0100, 0.0051, 0.0051, 0.0048, 0.9488, 0.0213, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 98, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5095e-01, 5.0238e-03, 8.2751e-07, 4.9233e-03, 1.1485e-05, 7.3329e-06,
        3.9079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 98, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9049e-03, 3.3587e-02, 5.3184e-06, 5.1387e-03, 9.2668e-01, 2.0887e-07,
        2.9685e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 98, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0426, 0.0053, 0.0053, 0.9315, 0.0054, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 99, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.8192e-01, 3.8898e-01, 3.2949e-01, 5.0644e-03, 4.6559e-03, 5.3377e-06,
        8.9888e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 99, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0104, 0.0052, 0.0055, 0.0050, 0.9483, 0.0204, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 99, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.5292e-01, 5.1244e-03, 9.0558e-07, 4.9834e-03, 1.3609e-05, 8.1512e-06,
        3.6954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.088

[Epoch: 99, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9840e-03, 3.5485e-02, 5.7079e-06, 4.8950e-03, 9.2603e-01, 2.0290e-07,
        2.8605e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.097

[Epoch: 99, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0456, 0.0048, 0.0051, 0.9297, 0.0046, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 100, batch: 37/188] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.1800, 0.3900, 0.3300, 0.0050, 0.0050, 0.0000, 0.0900])
Policy pred: tensor([1.7992e-01, 3.8889e-01, 3.3218e-01, 5.0256e-03, 4.9151e-03, 5.8575e-06,
        8.9058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 100, batch: 74/188] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0050, 0.9500, 0.0200, 0.0050])
Policy pred: tensor([0.0107, 0.0053, 0.0054, 0.0051, 0.9457, 0.0222, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 100, batch: 111/188] total loss per batch: 0.639
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0050, 0.0000, 0.0050, 0.0000, 0.0000, 0.0400])
Policy pred: tensor([9.4852e-01, 5.2660e-03, 8.0443e-07, 5.0201e-03, 1.0748e-05, 7.3325e-06,
        4.1177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.087 -0.091

[Epoch: 100, batch: 148/188] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0350, 0.0000, 0.0050, 0.9250, 0.0000, 0.0300])
Policy pred: tensor([4.9766e-03, 3.3353e-02, 5.5288e-06, 5.2515e-03, 9.2499e-01, 1.9229e-07,
        3.1423e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.098 -0.101

[Epoch: 100, batch: 185/188] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0450, 0.0050, 0.0050, 0.9300, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0427, 0.0052, 0.0051, 0.9320, 0.0051, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

