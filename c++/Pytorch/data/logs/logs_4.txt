Training set samples: 6407
Batch size: 32
[Epoch: 1, batch: 40/201] total loss per batch: 1.594
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0259, 0.0180, 0.0494, 0.0785, 0.0282, 0.6486, 0.1514],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.032

[Epoch: 1, batch: 80/201] total loss per batch: 1.521
Policy (actual, predicted): 5 4
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.1493, 0.2040, 0.0431, 0.1016, 0.3027, 0.0416, 0.1578],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.015

[Epoch: 1, batch: 120/201] total loss per batch: 1.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([0.1546, 0.0007, 0.1985, 0.2633, 0.0454, 0.0829, 0.2546],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.017

[Epoch: 1, batch: 160/201] total loss per batch: 1.474
Policy (actual, predicted): 6 2
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.2852, 0.0991, 0.3797, 0.0984, 0.0195, 0.0787, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.018

[Epoch: 1, batch: 200/201] total loss per batch: 1.462
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.6849e-04, 2.1466e-04, 3.0682e-01, 4.4985e-04, 2.7204e-01, 8.7629e-05,
        4.2012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.319

[Epoch: 2, batch: 40/201] total loss per batch: 1.121
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0187, 0.0617, 0.0489, 0.0137, 0.7396, 0.1118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.025

[Epoch: 2, batch: 80/201] total loss per batch: 1.104
Policy (actual, predicted): 5 4
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0557, 0.1356, 0.0147, 0.0432, 0.5350, 0.1469, 0.0690],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.008

[Epoch: 2, batch: 120/201] total loss per batch: 1.049
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0487e-01, 1.1448e-04, 5.3465e-02, 6.0356e-01, 1.5675e-02, 5.9894e-02,
        1.6243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.005

[Epoch: 2, batch: 160/201] total loss per batch: 1.071
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.1740, 0.0499, 0.0734, 0.1768, 0.0113, 0.2402, 0.2744],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.026

[Epoch: 2, batch: 200/201] total loss per batch: 1.054
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.3382e-05, 3.0674e-05, 3.0068e-01, 1.2316e-04, 3.1159e-01, 2.4713e-05,
        3.8752e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.647

[Epoch: 3, batch: 40/201] total loss per batch: 0.842
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([8.0998e-04, 1.4301e-02, 2.3919e-02, 1.7676e-02, 6.6319e-03, 8.4907e-01,
        8.7592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.045

[Epoch: 3, batch: 80/201] total loss per batch: 0.831
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0473, 0.1272, 0.0092, 0.0174, 0.3715, 0.4027, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 3, batch: 120/201] total loss per batch: 0.802
Policy (actual, predicted): 3 6
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([2.0813e-02, 2.2551e-05, 4.8976e-02, 3.8915e-01, 9.7612e-03, 3.4200e-02,
        4.9707e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 -0.008

[Epoch: 3, batch: 160/201] total loss per batch: 0.817
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0137, 0.0061, 0.0263, 0.0409, 0.0023, 0.0906, 0.8202],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.008

[Epoch: 3, batch: 200/201] total loss per batch: 0.807
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1590e-05, 4.7460e-05, 3.0905e-01, 1.0927e-04, 3.1068e-01, 8.3424e-06,
        3.8010e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.782

[Epoch: 4, batch: 40/201] total loss per batch: 0.717
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0009, 0.0125, 0.0171, 0.0152, 0.0059, 0.8776, 0.0706],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.033

[Epoch: 4, batch: 80/201] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0144, 0.0796, 0.0060, 0.0285, 0.2281, 0.6184, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 4, batch: 120/201] total loss per batch: 0.740
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([2.0614e-02, 6.4496e-06, 7.2765e-02, 6.1351e-01, 8.1945e-03, 2.2190e-02,
        2.6272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 -0.010

[Epoch: 4, batch: 160/201] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0113, 0.0074, 0.0818, 0.2908, 0.0032, 0.1590, 0.4464],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.009

[Epoch: 4, batch: 200/201] total loss per batch: 0.763
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.8426e-06, 2.5669e-05, 2.8290e-01, 9.5816e-05, 3.3257e-01, 1.5283e-05,
        3.8439e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.857

[Epoch: 5, batch: 40/201] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0009, 0.0123, 0.0203, 0.0196, 0.0107, 0.8722, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.041

[Epoch: 5, batch: 80/201] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0224, 0.0614, 0.0125, 0.0318, 0.0821, 0.7672, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.007

[Epoch: 5, batch: 120/201] total loss per batch: 0.721
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([5.1336e-03, 7.8155e-06, 3.5639e-02, 7.1781e-01, 6.7907e-03, 1.8240e-02,
        2.1637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 -0.010

[Epoch: 5, batch: 160/201] total loss per batch: 0.742
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0203, 0.0116, 0.0674, 0.0645, 0.0034, 0.3186, 0.5143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.039

[Epoch: 5, batch: 200/201] total loss per batch: 0.748
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.4437e-06, 3.2508e-05, 3.0922e-01, 1.0072e-04, 3.2449e-01, 9.2969e-06,
        3.6614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.869

[Epoch: 6, batch: 40/201] total loss per batch: 0.681
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0033, 0.0173, 0.0191, 0.0142, 0.0112, 0.8938, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.037

[Epoch: 6, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0339, 0.0805, 0.0087, 0.0186, 0.1075, 0.7258, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 6, batch: 120/201] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([4.1904e-03, 4.9527e-06, 2.1529e-02, 9.4396e-01, 3.3440e-03, 4.9755e-03,
        2.2000e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 -0.018

[Epoch: 6, batch: 160/201] total loss per batch: 0.722
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0157, 0.0124, 0.0685, 0.0716, 0.0019, 0.2157, 0.6142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.021

[Epoch: 6, batch: 200/201] total loss per batch: 0.723
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.1015e-06, 3.9323e-05, 3.1131e-01, 1.0788e-04, 3.4759e-01, 1.7475e-05,
        3.4093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.894

[Epoch: 7, batch: 40/201] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0010, 0.0134, 0.0276, 0.0132, 0.0101, 0.8988, 0.0358],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.024

[Epoch: 7, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0218, 0.0562, 0.0104, 0.0301, 0.0707, 0.7888, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 7, batch: 120/201] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.7484e-03, 1.0182e-05, 6.7509e-02, 4.7835e-01, 8.5207e-03, 1.3935e-02,
        4.2192e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.016

[Epoch: 7, batch: 160/201] total loss per batch: 0.711
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0077, 0.0147, 0.1126, 0.1876, 0.0040, 0.1602, 0.5132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.028

[Epoch: 7, batch: 200/201] total loss per batch: 0.707
Policy (actual, predicted): 2 6
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.6136e-06, 2.8649e-05, 2.9846e-01, 5.8304e-05, 3.3850e-01, 9.5129e-06,
        3.6293e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.879

[Epoch: 8, batch: 40/201] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([6.6958e-04, 7.6364e-03, 1.5887e-02, 1.5337e-02, 1.3970e-02, 8.9849e-01,
        4.8006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.021

[Epoch: 8, batch: 80/201] total loss per batch: 0.690
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0441, 0.0434, 0.0070, 0.0245, 0.0616, 0.8111, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 8, batch: 120/201] total loss per batch: 0.675
Policy (actual, predicted): 3 6
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.3511e-02, 7.5292e-06, 7.6779e-02, 4.2992e-01, 1.1735e-02, 1.5870e-02,
        4.5218e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.014

[Epoch: 8, batch: 160/201] total loss per batch: 0.704
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0570, 0.0183, 0.0223, 0.0712, 0.0026, 0.2151, 0.6135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.020

[Epoch: 8, batch: 200/201] total loss per batch: 0.696
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.0645e-06, 4.9681e-05, 3.0288e-01, 1.6383e-04, 3.5316e-01, 1.5692e-05,
        3.4373e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.897

[Epoch: 9, batch: 40/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([6.2830e-04, 7.3799e-03, 1.1565e-02, 6.5158e-03, 8.1449e-03, 9.5605e-01,
        9.7163e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.027

[Epoch: 9, batch: 80/201] total loss per batch: 0.683
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0202, 0.0561, 0.0087, 0.0198, 0.0574, 0.8223, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 9, batch: 120/201] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.5469e-02, 1.5641e-05, 5.7465e-02, 6.1865e-01, 1.0199e-02, 1.6974e-02,
        2.8123e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.013

[Epoch: 9, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0045, 0.0122, 0.0765, 0.1297, 0.0027, 0.2337, 0.5408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.018

[Epoch: 9, batch: 200/201] total loss per batch: 0.690
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2779e-05, 3.9637e-05, 3.0769e-01, 1.1784e-04, 3.5494e-01, 1.2019e-05,
        3.3718e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.885

[Epoch: 10, batch: 40/201] total loss per batch: 0.639
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0010, 0.0070, 0.0131, 0.0177, 0.0101, 0.9162, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.017

[Epoch: 10, batch: 80/201] total loss per batch: 0.677
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0178, 0.0343, 0.0044, 0.0288, 0.0719, 0.8336, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 10, batch: 120/201] total loss per batch: 0.664
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0592e-02, 1.3513e-05, 5.8633e-02, 7.0721e-01, 8.1692e-03, 1.3296e-02,
        2.0209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.003

[Epoch: 10, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0099, 0.0142, 0.0709, 0.0859, 0.0023, 0.2237, 0.5931],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.012

[Epoch: 10, batch: 200/201] total loss per batch: 0.684
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.1837e-06, 4.5694e-05, 3.0491e-01, 6.1050e-05, 3.5999e-01, 9.5950e-06,
        3.3498e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.868

[Epoch: 11, batch: 40/201] total loss per batch: 0.635
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0010, 0.0088, 0.0121, 0.0074, 0.0097, 0.9492, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.013

[Epoch: 11, batch: 80/201] total loss per batch: 0.675
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0611, 0.0507, 0.0081, 0.0303, 0.0587, 0.7795, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 11, batch: 120/201] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0302e-02, 1.5090e-05, 4.0583e-02, 6.1305e-01, 8.7817e-03, 1.1524e-02,
        3.1575e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.014

[Epoch: 11, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0159, 0.0721, 0.1023, 0.0051, 0.3278, 0.4715],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.016

[Epoch: 11, batch: 200/201] total loss per batch: 0.683
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0050e-05, 6.0913e-05, 3.2450e-01, 1.0338e-04, 3.6227e-01, 1.7846e-05,
        3.1303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.912

[Epoch: 12, batch: 40/201] total loss per batch: 0.634
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0018, 0.0087, 0.0129, 0.0116, 0.0090, 0.9418, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.015

[Epoch: 12, batch: 80/201] total loss per batch: 0.676
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0287, 0.0470, 0.0076, 0.0445, 0.0419, 0.8161, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 12, batch: 120/201] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1662e-02, 2.0043e-05, 7.3273e-02, 5.9643e-01, 1.1752e-02, 2.4529e-02,
        2.8233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.016

[Epoch: 12, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0067, 0.0108, 0.1463, 0.1155, 0.0037, 0.0753, 0.6418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.009

[Epoch: 12, batch: 200/201] total loss per batch: 0.682
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.8545e-06, 4.2853e-05, 3.1320e-01, 8.9996e-05, 3.5538e-01, 1.5993e-05,
        3.3127e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.873

[Epoch: 13, batch: 40/201] total loss per batch: 0.634
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0011, 0.0104, 0.0133, 0.0113, 0.0112, 0.9398, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.011

[Epoch: 13, batch: 80/201] total loss per batch: 0.673
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0323, 0.0268, 0.0062, 0.0169, 0.0380, 0.8761, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 13, batch: 120/201] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.7913e-03, 1.7138e-05, 6.2183e-02, 5.4052e-01, 7.7061e-03, 8.1980e-03,
        3.7258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.012

[Epoch: 13, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0140, 0.0147, 0.0272, 0.0599, 0.0029, 0.2103, 0.6710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.009

[Epoch: 13, batch: 200/201] total loss per batch: 0.681
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.5996e-06, 1.0025e-04, 3.2279e-01, 1.1401e-04, 3.7849e-01, 2.3160e-05,
        2.9848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.888

[Epoch: 14, batch: 40/201] total loss per batch: 0.634
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0019, 0.0118, 0.0094, 0.0111, 0.0181, 0.9241, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.008

[Epoch: 14, batch: 80/201] total loss per batch: 0.669
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0297, 0.0332, 0.0076, 0.0348, 0.0497, 0.8344, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.054

[Epoch: 14, batch: 120/201] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1998e-02, 1.2181e-05, 4.6442e-02, 6.8146e-01, 6.0029e-03, 1.2374e-02,
        2.4171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.014

[Epoch: 14, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0161, 0.0106, 0.1776, 0.1593, 0.0022, 0.2824, 0.3518],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.004

[Epoch: 14, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4007e-05, 1.6370e-04, 3.3013e-01, 1.4856e-04, 3.5387e-01, 2.0530e-05,
        3.1565e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.866

[Epoch: 15, batch: 40/201] total loss per batch: 0.632
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([7.8579e-04, 7.2176e-03, 6.9487e-03, 4.6263e-03, 7.6739e-03, 9.6495e-01,
        7.8000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.000

[Epoch: 15, batch: 80/201] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0454, 0.0386, 0.0094, 0.0361, 0.0468, 0.8119, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 15, batch: 120/201] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([6.4205e-03, 1.0568e-05, 3.9522e-02, 6.9014e-01, 4.0392e-03, 5.9069e-03,
        2.5396e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.014

[Epoch: 15, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0094, 0.0116, 0.0117, 0.0766, 0.0024, 0.1250, 0.7632],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.003

[Epoch: 15, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0697e-05, 7.0139e-05, 3.3655e-01, 1.3921e-04, 3.3599e-01, 2.1049e-05,
        3.2723e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.907

[Epoch: 16, batch: 40/201] total loss per batch: 0.629
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0015, 0.0050, 0.0152, 0.0081, 0.0112, 0.9510, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.059

[Epoch: 16, batch: 80/201] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0180, 0.0430, 0.0049, 0.0490, 0.0412, 0.8383, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.067

[Epoch: 16, batch: 120/201] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1600e-02, 1.6553e-05, 1.0285e-01, 6.4828e-01, 8.3867e-03, 9.9532e-03,
        2.1891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.034

[Epoch: 16, batch: 160/201] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0128, 0.0130, 0.0540, 0.1169, 0.0032, 0.3386, 0.4615],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.000

[Epoch: 16, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.4220e-06, 2.0433e-04, 3.2631e-01, 3.3545e-04, 3.9002e-01, 3.5390e-05,
        2.8308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.867

[Epoch: 17, batch: 40/201] total loss per batch: 0.630
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0016, 0.0039, 0.0052, 0.0087, 0.0078, 0.9596, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.035

[Epoch: 17, batch: 80/201] total loss per batch: 0.667
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0489, 0.0458, 0.0063, 0.0430, 0.0499, 0.7941, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 17, batch: 120/201] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.4740e-02, 1.5342e-05, 6.0673e-02, 6.0200e-01, 5.8628e-03, 9.7845e-03,
        3.0693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.025

[Epoch: 17, batch: 160/201] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0069, 0.0081, 0.1108, 0.0886, 0.0051, 0.1058, 0.6747],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 17, batch: 200/201] total loss per batch: 0.675
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3845e-05, 9.8748e-05, 3.5071e-01, 2.8393e-04, 3.6545e-01, 2.9120e-05,
        2.8342e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.895

[Epoch: 18, batch: 40/201] total loss per batch: 0.629
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0015, 0.0057, 0.0083, 0.0080, 0.0101, 0.9558, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.002

[Epoch: 18, batch: 80/201] total loss per batch: 0.667
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0176, 0.0399, 0.0036, 0.0083, 0.0475, 0.8755, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.059

[Epoch: 18, batch: 120/201] total loss per batch: 0.658
Policy (actual, predicted): 3 6
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0660e-02, 1.7450e-05, 4.9583e-02, 4.5945e-01, 9.1842e-03, 8.0921e-03,
        4.6301e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.034

[Epoch: 18, batch: 160/201] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0184, 0.0171, 0.0279, 0.1288, 0.0035, 0.2790, 0.5253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 18, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.0573e-06, 6.1682e-05, 3.3879e-01, 1.8719e-04, 3.4596e-01, 3.0871e-05,
        3.1496e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.852

[Epoch: 19, batch: 40/201] total loss per batch: 0.630
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0029, 0.0033, 0.0048, 0.0071, 0.0055, 0.9656, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.131

[Epoch: 19, batch: 80/201] total loss per batch: 0.667
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0801, 0.0471, 0.0103, 0.0805, 0.0535, 0.7132, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.009

[Epoch: 19, batch: 120/201] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([5.1025e-03, 1.2150e-05, 5.1751e-02, 5.9100e-01, 8.2874e-03, 1.4225e-02,
        3.2962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.027

[Epoch: 19, batch: 160/201] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0034, 0.0117, 0.1083, 0.0949, 0.0049, 0.2157, 0.5611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 19, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.5019e-05, 1.3214e-04, 3.4376e-01, 2.5346e-04, 3.6190e-01, 4.5497e-05,
        2.9388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 20, batch: 40/201] total loss per batch: 0.630
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0020, 0.0116, 0.0081, 0.0095, 0.0134, 0.9360, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.009

[Epoch: 20, batch: 80/201] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0179, 0.0222, 0.0031, 0.0071, 0.0427, 0.8993, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.052

[Epoch: 20, batch: 120/201] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([6.6396e-03, 2.5107e-05, 5.1922e-02, 8.1812e-01, 8.5595e-03, 1.0074e-02,
        1.0466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.025

[Epoch: 20, batch: 160/201] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0121, 0.0116, 0.0511, 0.1570, 0.0037, 0.2084, 0.5561],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 20, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9292e-05, 2.6082e-04, 3.2522e-01, 1.7795e-04, 3.9282e-01, 2.1434e-05,
        2.8148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.854

[Epoch: 21, batch: 40/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0018, 0.0067, 0.0057, 0.0043, 0.0047, 0.9701, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.071

[Epoch: 21, batch: 80/201] total loss per batch: 0.674
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0250, 0.0417, 0.0078, 0.0429, 0.0352, 0.8412, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 21, batch: 120/201] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([2.5724e-02, 5.2348e-05, 9.2398e-02, 6.1339e-01, 8.0803e-03, 1.2347e-02,
        2.4801e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 21, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0087, 0.0170, 0.1086, 0.1032, 0.0063, 0.1669, 0.5892],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 21, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3626e-05, 7.9016e-05, 3.6061e-01, 1.2236e-04, 3.5713e-01, 5.9800e-05,
        2.8199e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 22, batch: 40/201] total loss per batch: 0.638
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0106, 0.0262, 0.0093, 0.0061, 0.9294, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.006

[Epoch: 22, batch: 80/201] total loss per batch: 0.673
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0494, 0.0661, 0.0134, 0.0129, 0.0719, 0.7771, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 22, batch: 120/201] total loss per batch: 0.666
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.0613e-03, 2.6912e-05, 4.5392e-02, 6.1743e-01, 8.9444e-03, 8.9406e-03,
        3.1220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.027

[Epoch: 22, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0096, 0.0120, 0.0621, 0.0652, 0.0052, 0.2518, 0.5941],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 22, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4279e-05, 8.7652e-05, 3.5025e-01, 5.1112e-04, 3.6132e-01, 3.6391e-05,
        2.8778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.869

[Epoch: 23, batch: 40/201] total loss per batch: 0.634
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0016, 0.0050, 0.0096, 0.0096, 0.0055, 0.9579, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.001

[Epoch: 23, batch: 80/201] total loss per batch: 0.671
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0283, 0.0397, 0.0022, 0.0270, 0.0237, 0.8755, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.053

[Epoch: 23, batch: 120/201] total loss per batch: 0.664
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([3.9179e-03, 2.1372e-05, 1.1551e-02, 6.6178e-01, 5.5946e-03, 7.2552e-03,
        3.0988e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.024

[Epoch: 23, batch: 160/201] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0099, 0.0092, 0.0961, 0.1325, 0.0044, 0.2411, 0.5067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.010

[Epoch: 23, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3353e-05, 8.4391e-05, 3.6567e-01, 1.5768e-04, 3.6826e-01, 3.7303e-05,
        2.6578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 24, batch: 40/201] total loss per batch: 0.632
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0048, 0.0113, 0.0150, 0.0103, 0.9379, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.011

[Epoch: 24, batch: 80/201] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0649, 0.0294, 0.0334, 0.0410, 0.0767, 0.7225, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.004

[Epoch: 24, batch: 120/201] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.2393e-03, 5.8937e-05, 5.0067e-02, 7.8335e-01, 5.5009e-03, 1.1157e-02,
        1.4163e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.019

[Epoch: 24, batch: 160/201] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0038, 0.0116, 0.0679, 0.0494, 0.0021, 0.2307, 0.6345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 24, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.7472e-05, 1.9400e-04, 3.3782e-01, 5.7311e-04, 3.8570e-01, 7.3001e-05,
        2.7562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.905

[Epoch: 25, batch: 40/201] total loss per batch: 0.627
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0014, 0.0028, 0.0102, 0.0049, 0.0049, 0.9699, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.096

[Epoch: 25, batch: 80/201] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0392, 0.0411, 0.0042, 0.0325, 0.0333, 0.8464, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 25, batch: 120/201] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1064e-02, 3.8674e-05, 5.5882e-02, 5.7021e-01, 7.9050e-03, 1.2469e-02,
        3.4243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 25, batch: 160/201] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0084, 0.0100, 0.1003, 0.2071, 0.0062, 0.2201, 0.4479],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 25, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2031e-05, 6.5074e-05, 3.4369e-01, 3.0103e-04, 3.7309e-01, 4.9154e-05,
        2.8279e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.908

[Epoch: 26, batch: 40/201] total loss per batch: 0.626
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0022, 0.0058, 0.0086, 0.0081, 0.0079, 0.9479, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.004

[Epoch: 26, batch: 80/201] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0201, 0.0294, 0.0067, 0.0479, 0.0334, 0.8570, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.053

[Epoch: 26, batch: 120/201] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2624e-02, 3.6599e-05, 6.0565e-02, 5.2386e-01, 7.9031e-03, 1.5408e-02,
        3.7960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.028

[Epoch: 26, batch: 160/201] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0065, 0.0079, 0.0440, 0.0855, 0.0034, 0.2619, 0.5907],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 26, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.0120e-05, 1.2423e-04, 3.5393e-01, 4.9931e-04, 3.6713e-01, 8.4770e-05,
        2.7822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.913

[Epoch: 27, batch: 40/201] total loss per batch: 0.622
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0019, 0.0038, 0.0109, 0.0070, 0.0050, 0.9635, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.007

[Epoch: 27, batch: 80/201] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0353, 0.0429, 0.0081, 0.0532, 0.0342, 0.8211, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 27, batch: 120/201] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.3579e-02, 2.9786e-05, 4.8419e-02, 5.3031e-01, 4.8814e-03, 8.2687e-03,
        3.9451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.026

[Epoch: 27, batch: 160/201] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0191, 0.0111, 0.1188, 0.1507, 0.0060, 0.1687, 0.5255],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 27, batch: 200/201] total loss per batch: 0.669
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.0513e-05, 1.1785e-04, 3.4943e-01, 2.6790e-04, 3.5004e-01, 4.8992e-05,
        3.0007e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.902

[Epoch: 28, batch: 40/201] total loss per batch: 0.621
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0022, 0.0063, 0.0058, 0.0078, 0.0059, 0.9656, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.102

[Epoch: 28, batch: 80/201] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0245, 0.0433, 0.0076, 0.0090, 0.0407, 0.8714, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.056

[Epoch: 28, batch: 120/201] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.5999e-03, 5.4217e-05, 5.7110e-02, 6.1430e-01, 6.6079e-03, 1.0239e-02,
        3.0409e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.030

[Epoch: 28, batch: 160/201] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0015, 0.0081, 0.0356, 0.0980, 0.0035, 0.2004, 0.6530],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 28, batch: 200/201] total loss per batch: 0.668
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.2574e-05, 1.3472e-04, 3.6384e-01, 5.6013e-04, 3.5113e-01, 1.0895e-04,
        2.8419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.901

[Epoch: 29, batch: 40/201] total loss per batch: 0.620
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0028, 0.0068, 0.0080, 0.0078, 0.0061, 0.9605, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.008

[Epoch: 29, batch: 80/201] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0758, 0.0351, 0.0043, 0.0640, 0.0284, 0.7857, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 29, batch: 120/201] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.6571e-03, 5.6259e-05, 7.3577e-02, 6.7945e-01, 6.5055e-03, 8.0312e-03,
        2.2472e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.028

[Epoch: 29, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0062, 0.1469, 0.1400, 0.0028, 0.2241, 0.4746],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 29, batch: 200/201] total loss per batch: 0.668
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6598e-05, 1.1476e-04, 3.3731e-01, 4.4069e-04, 3.8798e-01, 7.5920e-05,
        2.7406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 30, batch: 40/201] total loss per batch: 0.620
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0029, 0.0071, 0.0080, 0.0066, 0.0075, 0.9583, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.030

[Epoch: 30, batch: 80/201] total loss per batch: 0.657
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0101, 0.0266, 0.0081, 0.0157, 0.0539, 0.8796, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.058

[Epoch: 30, batch: 120/201] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.0755e-03, 6.6271e-05, 5.2469e-02, 6.3013e-01, 1.0038e-02, 8.4506e-03,
        2.9177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.035

[Epoch: 30, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0088, 0.0467, 0.0911, 0.0043, 0.2423, 0.6022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 30, batch: 200/201] total loss per batch: 0.667
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.3683e-05, 1.8071e-04, 3.5066e-01, 3.9281e-04, 3.6510e-01, 6.7018e-05,
        2.8356e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.894

[Epoch: 31, batch: 40/201] total loss per batch: 0.619
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0026, 0.0034, 0.0059, 0.0085, 0.0060, 0.9674, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.028

[Epoch: 31, batch: 80/201] total loss per batch: 0.656
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0342, 0.0394, 0.0041, 0.0345, 0.0264, 0.8567, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 31, batch: 120/201] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.0046e-03, 5.8071e-05, 6.3189e-02, 6.2092e-01, 7.5782e-03, 8.5394e-03,
        2.9071e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.027

[Epoch: 31, batch: 160/201] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0070, 0.0837, 0.1001, 0.0041, 0.1942, 0.6056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 31, batch: 200/201] total loss per batch: 0.666
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.1783e-05, 8.7303e-05, 3.4527e-01, 2.3334e-04, 3.5115e-01, 8.6613e-05,
        3.0314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.905

[Epoch: 32, batch: 40/201] total loss per batch: 0.620
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0034, 0.0042, 0.0051, 0.0050, 0.0067, 0.9672, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.098

[Epoch: 32, batch: 80/201] total loss per batch: 0.656
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0525, 0.0387, 0.0080, 0.0538, 0.0275, 0.8118, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 32, batch: 120/201] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.3919e-02, 8.9666e-05, 5.8621e-02, 6.1006e-01, 7.8677e-03, 7.9074e-03,
        3.0153e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.033

[Epoch: 32, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0121, 0.0637, 0.1216, 0.0059, 0.2783, 0.5141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 32, batch: 200/201] total loss per batch: 0.667
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.3537e-05, 1.6621e-04, 3.6970e-01, 2.8224e-04, 3.7128e-01, 5.4192e-05,
        2.5848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 33, batch: 40/201] total loss per batch: 0.619
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0026, 0.0041, 0.0046, 0.0051, 0.0090, 0.9693, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.009

[Epoch: 33, batch: 80/201] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0151, 0.0317, 0.0035, 0.0247, 0.0738, 0.8432, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 33, batch: 120/201] total loss per batch: 0.651
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.8883e-03, 8.7520e-05, 5.7453e-02, 6.2997e-01, 5.9427e-03, 6.0075e-03,
        2.9165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 33, batch: 160/201] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0090, 0.0688, 0.0928, 0.0029, 0.2137, 0.6078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 33, batch: 200/201] total loss per batch: 0.669
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.4702e-05, 4.2267e-05, 3.4975e-01, 1.6308e-04, 3.5948e-01, 7.6118e-05,
        2.9046e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 34, batch: 40/201] total loss per batch: 0.620
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0034, 0.0064, 0.0088, 0.0058, 0.0081, 0.9534, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.043

[Epoch: 34, batch: 80/201] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0301, 0.0379, 0.0059, 0.0343, 0.0257, 0.8593, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 34, batch: 120/201] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.6384e-03, 5.4172e-05, 5.9577e-02, 6.3560e-01, 6.8743e-03, 8.1323e-03,
        2.8012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 34, batch: 160/201] total loss per batch: 0.679
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0045, 0.0048, 0.1265, 0.1415, 0.0026, 0.1657, 0.5544],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 34, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.0301e-05, 6.8971e-05, 3.4591e-01, 6.9960e-04, 3.5519e-01, 4.1533e-05,
        2.9807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.880

[Epoch: 35, batch: 40/201] total loss per batch: 0.624
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0036, 0.0047, 0.0040, 0.0068, 0.0042, 0.9712, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.025

[Epoch: 35, batch: 80/201] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0532, 0.0318, 0.0065, 0.0406, 0.0312, 0.8277, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.013

[Epoch: 35, batch: 120/201] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.1659e-03, 4.4393e-05, 7.2330e-02, 6.5651e-01, 4.1147e-03, 6.7524e-03,
        2.5209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 35, batch: 160/201] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0102, 0.0116, 0.0399, 0.1106, 0.0082, 0.1908, 0.6287],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 35, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.8414e-05, 9.8667e-05, 3.6705e-01, 1.9999e-04, 3.4375e-01, 5.4080e-05,
        2.8882e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.936

[Epoch: 36, batch: 40/201] total loss per batch: 0.625
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0011, 0.0048, 0.0105, 0.0047, 0.0048, 0.9707, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.000

[Epoch: 36, batch: 80/201] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0295, 0.0415, 0.0042, 0.0316, 0.0447, 0.8443, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 36, batch: 120/201] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.5671e-03, 8.1163e-05, 7.7482e-02, 5.4998e-01, 7.4385e-03, 6.5103e-03,
        3.5094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 36, batch: 160/201] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0116, 0.1588, 0.0653, 0.0040, 0.3679, 0.3878],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 36, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.4993e-05, 1.1236e-04, 3.2662e-01, 2.9756e-04, 3.6009e-01, 7.4635e-05,
        3.1278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 37, batch: 40/201] total loss per batch: 0.627
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0076, 0.0075, 0.0085, 0.0118, 0.0091, 0.9365, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.017

[Epoch: 37, batch: 80/201] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0344, 0.0457, 0.0090, 0.0434, 0.0615, 0.7960, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 37, batch: 120/201] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1419e-02, 5.0904e-05, 5.2854e-02, 6.0436e-01, 1.0796e-02, 1.4305e-02,
        3.0621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 37, batch: 160/201] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0026, 0.0079, 0.0247, 0.2289, 0.0035, 0.1537, 0.5786],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.002

[Epoch: 37, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.1893e-05, 3.3526e-04, 3.4054e-01, 2.6773e-04, 3.8441e-01, 8.6200e-05,
        2.7433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.893

[Epoch: 38, batch: 40/201] total loss per batch: 0.628
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0024, 0.0036, 0.0134, 0.0019, 0.0031, 0.9732, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.005

[Epoch: 38, batch: 80/201] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0271, 0.0434, 0.0045, 0.0198, 0.0507, 0.8429, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.013

[Epoch: 38, batch: 120/201] total loss per batch: 0.657
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2860e-02, 6.3334e-05, 8.3411e-02, 4.9676e-01, 6.2895e-03, 7.5677e-03,
        3.9305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 38, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0027, 0.0158, 0.0971, 0.0819, 0.0038, 0.2862, 0.5125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 38, batch: 200/201] total loss per batch: 0.684
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.1478e-05, 4.1008e-04, 3.3458e-01, 7.6707e-04, 3.5267e-01, 3.2221e-05,
        3.1148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.927

[Epoch: 39, batch: 40/201] total loss per batch: 0.636
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0093, 0.0025, 0.0086, 0.0095, 0.0049, 0.9609, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.039

[Epoch: 39, batch: 80/201] total loss per batch: 0.671
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0614, 0.0311, 0.0062, 0.0567, 0.0422, 0.8001, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 39, batch: 120/201] total loss per batch: 0.663
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.0085e-03, 6.7772e-05, 4.4589e-02, 6.4636e-01, 4.0573e-03, 1.2009e-02,
        2.8591e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.011

[Epoch: 39, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0085, 0.0054, 0.0717, 0.1731, 0.0026, 0.1667, 0.5720],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.002

[Epoch: 39, batch: 200/201] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.5016e-05, 2.6470e-04, 3.4808e-01, 2.4544e-04, 3.4125e-01, 2.9273e-05,
        3.1012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.954

[Epoch: 40, batch: 40/201] total loss per batch: 0.639
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0070, 0.0068, 0.0098, 0.0029, 0.0082, 0.9553, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.005

[Epoch: 40, batch: 80/201] total loss per batch: 0.670
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0325, 0.0691, 0.0062, 0.0066, 0.0404, 0.8416, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 40, batch: 120/201] total loss per batch: 0.661
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([2.7084e-02, 3.7759e-05, 3.5734e-02, 6.4454e-01, 7.3149e-03, 1.3839e-02,
        2.7145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.033

[Epoch: 40, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0072, 0.0585, 0.1148, 0.0019, 0.2292, 0.5829],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 40, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.8367e-05, 1.7001e-04, 3.6196e-01, 1.4707e-04, 3.7802e-01, 6.0685e-05,
        2.5960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.921

[Epoch: 41, batch: 40/201] total loss per batch: 0.633
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0013, 0.0058, 0.0063, 0.0026, 0.0078, 0.9658, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.004

[Epoch: 41, batch: 80/201] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0358, 0.0424, 0.0087, 0.0263, 0.0314, 0.8508, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 41, batch: 120/201] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([6.9998e-03, 9.7122e-05, 8.3259e-02, 6.8665e-01, 8.4754e-03, 5.7213e-03,
        2.0879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.021

[Epoch: 41, batch: 160/201] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0032, 0.0146, 0.0931, 0.1202, 0.0020, 0.2372, 0.5296],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.005

[Epoch: 41, batch: 200/201] total loss per batch: 0.681
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6131e-05, 3.9484e-04, 3.4188e-01, 2.7462e-04, 3.5276e-01, 3.1027e-05,
        3.0464e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.927

[Epoch: 42, batch: 40/201] total loss per batch: 0.627
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0017, 0.0035, 0.0058, 0.0033, 0.0055, 0.9724, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.014

[Epoch: 42, batch: 80/201] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0418, 0.0367, 0.0049, 0.0210, 0.0593, 0.8332, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 42, batch: 120/201] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2244e-02, 9.8564e-05, 4.4559e-02, 6.2963e-01, 9.5216e-03, 4.1194e-03,
        2.9983e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.025

[Epoch: 42, batch: 160/201] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0095, 0.0076, 0.0829, 0.1095, 0.0038, 0.1934, 0.5934],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 42, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.2261e-05, 3.4400e-04, 3.6873e-01, 3.5331e-04, 3.4433e-01, 4.6807e-05,
        2.8614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.842

[Epoch: 43, batch: 40/201] total loss per batch: 0.622
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0014, 0.0055, 0.0050, 0.0032, 0.0097, 0.9641, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 0.007

[Epoch: 43, batch: 80/201] total loss per batch: 0.656
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0345, 0.0416, 0.0107, 0.0439, 0.0307, 0.8322, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 43, batch: 120/201] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.5598e-03, 4.9938e-05, 5.1469e-02, 6.0376e-01, 7.1947e-03, 1.1604e-02,
        3.1736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 43, batch: 160/201] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0106, 0.0569, 0.1089, 0.0032, 0.2341, 0.5812],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 43, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.0160e-05, 3.0758e-04, 3.5829e-01, 4.8232e-04, 3.5944e-01, 4.0893e-05,
        2.8141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.927

[Epoch: 44, batch: 40/201] total loss per batch: 0.617
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0027, 0.0039, 0.0040, 0.0049, 0.0061, 0.9703, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.001

[Epoch: 44, batch: 80/201] total loss per batch: 0.652
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0360, 0.0355, 0.0077, 0.0293, 0.0378, 0.8467, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.015

[Epoch: 44, batch: 120/201] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0114e-02, 7.4985e-05, 7.5029e-02, 6.0370e-01, 7.0080e-03, 9.1268e-03,
        2.9495e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.031

[Epoch: 44, batch: 160/201] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0089, 0.0096, 0.0873, 0.1302, 0.0038, 0.2730, 0.4872],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 44, batch: 200/201] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.6666e-05, 2.3056e-04, 3.6698e-01, 2.7418e-04, 3.4984e-01, 3.9056e-05,
        2.8261e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.905

[Epoch: 45, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0047, 0.0082, 0.0046, 0.0071, 0.9617, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.029

[Epoch: 45, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0281, 0.0349, 0.0049, 0.0362, 0.0321, 0.8591, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 45, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.9590e-03, 7.5911e-05, 4.0639e-02, 6.4654e-01, 5.3321e-03, 7.0052e-03,
        2.9145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.035

[Epoch: 45, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0058, 0.0067, 0.0700, 0.0863, 0.0029, 0.1491, 0.6792],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 45, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.7451e-05, 2.0698e-04, 3.7210e-01, 2.4859e-04, 3.6317e-01, 4.5024e-05,
        2.6421e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.934

[Epoch: 46, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0031, 0.0048, 0.0051, 0.0037, 0.0046, 0.9738, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.071

[Epoch: 46, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0396, 0.0325, 0.0079, 0.0318, 0.0336, 0.8487, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 46, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0953e-02, 8.2944e-05, 4.7731e-02, 6.4766e-01, 6.6240e-03, 6.7277e-03,
        2.8022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.032

[Epoch: 46, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0093, 0.0072, 0.0671, 0.1221, 0.0040, 0.2632, 0.5271],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 46, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6673e-05, 1.5264e-04, 3.6010e-01, 2.5544e-04, 3.4432e-01, 4.2746e-05,
        2.9511e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 47, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0048, 0.0068, 0.0053, 0.0068, 0.9660, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.011

[Epoch: 47, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0342, 0.0406, 0.0056, 0.0353, 0.0328, 0.8459, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 47, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0632e-02, 6.5857e-05, 5.5484e-02, 6.3172e-01, 5.5326e-03, 6.0185e-03,
        2.9055e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 47, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0070, 0.1004, 0.0923, 0.0036, 0.2154, 0.5761],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 47, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2822e-05, 1.6120e-04, 3.6711e-01, 2.4117e-04, 3.5056e-01, 4.1648e-05,
        2.8188e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 48, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.0053, 0.0043, 0.0039, 0.9724, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.025

[Epoch: 48, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0326, 0.0276, 0.0055, 0.0335, 0.0372, 0.8586, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 48, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2576e-02, 6.5177e-05, 7.1662e-02, 5.8626e-01, 6.9933e-03, 5.9136e-03,
        3.1653e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.032

[Epoch: 48, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0067, 0.0061, 0.0562, 0.1084, 0.0044, 0.2306, 0.5877],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 48, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3986e-05, 1.3069e-04, 3.4954e-01, 1.9665e-04, 3.6233e-01, 3.3384e-05,
        2.8776e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.921

[Epoch: 49, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0058, 0.0041, 0.0063, 0.0052, 0.0059, 0.9671, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.128

[Epoch: 49, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0424, 0.0400, 0.0080, 0.0371, 0.0404, 0.8266, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 49, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0581e-02, 5.5751e-05, 7.0689e-02, 6.0953e-01, 6.6831e-03, 6.2220e-03,
        2.9624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.039

[Epoch: 49, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0066, 0.0856, 0.1244, 0.0040, 0.2243, 0.5502],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 49, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.1212e-05, 2.2361e-04, 3.7292e-01, 3.8230e-04, 3.5338e-01, 6.4414e-05,
        2.7300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.911

[Epoch: 50, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0040, 0.0049, 0.0059, 0.0043, 0.0050, 0.9697, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.013

[Epoch: 50, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0281, 0.0278, 0.0050, 0.0287, 0.0449, 0.8607, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 50, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.4474e-03, 4.5701e-05, 3.8356e-02, 6.5776e-01, 3.6517e-03, 4.7016e-03,
        2.8704e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.032

[Epoch: 50, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0044, 0.0072, 0.0730, 0.0902, 0.0040, 0.2281, 0.5932],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.001

[Epoch: 50, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1910e-05, 1.0475e-04, 3.7088e-01, 1.4193e-04, 3.6102e-01, 3.3505e-05,
        2.6781e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 51, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0085, 0.0044, 0.0073, 0.0065, 0.0051, 0.9618, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.009

[Epoch: 51, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0419, 0.0421, 0.0075, 0.0403, 0.0266, 0.8358, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.015

[Epoch: 51, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3619e-03, 7.0567e-05, 4.6782e-02, 6.5921e-01, 7.2903e-03, 6.3462e-03,
        2.7094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.036

[Epoch: 51, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0068, 0.0065, 0.0844, 0.0967, 0.0046, 0.2366, 0.5644],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 51, batch: 200/201] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9704e-05, 1.5007e-04, 3.5403e-01, 2.5062e-04, 3.4540e-01, 4.9200e-05,
        3.0010e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 52, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0028, 0.0035, 0.0033, 0.0036, 0.0057, 0.9756, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.033

[Epoch: 52, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0328, 0.0345, 0.0054, 0.0402, 0.0369, 0.8456, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 52, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1433e-03, 5.9379e-05, 5.9802e-02, 6.3074e-01, 6.5776e-03, 6.7931e-03,
        2.8688e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 52, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0043, 0.0060, 0.0615, 0.1240, 0.0031, 0.2007, 0.6004],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 52, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1051e-05, 7.3174e-05, 3.6598e-01, 1.2488e-04, 3.3895e-01, 2.7489e-05,
        2.9483e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.935

[Epoch: 53, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0039, 0.0051, 0.0040, 0.0048, 0.0047, 0.9730, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.078

[Epoch: 53, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0311, 0.0338, 0.0056, 0.0215, 0.0326, 0.8692, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 53, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1400e-02, 5.1670e-05, 5.4414e-02, 5.9609e-01, 4.7127e-03, 5.0222e-03,
        3.2831e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 53, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0077, 0.0992, 0.0903, 0.0047, 0.2576, 0.5345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 53, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3127e-05, 1.1654e-04, 3.7252e-01, 2.0352e-04, 3.7496e-01, 3.3811e-05,
        2.5215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 54, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0044, 0.0054, 0.0055, 0.0055, 0.9692, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 54, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0489, 0.0331, 0.0052, 0.0697, 0.0456, 0.7924, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 54, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0084e-02, 7.0693e-05, 5.4265e-02, 6.3886e-01, 6.3200e-03, 6.4777e-03,
        2.8392e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 54, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0066, 0.0690, 0.1258, 0.0050, 0.2142, 0.5752],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 54, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9523e-05, 7.5195e-05, 3.6558e-01, 1.6032e-04, 3.4265e-01, 2.9432e-05,
        2.9149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 55, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0040, 0.0041, 0.0040, 0.0046, 0.0048, 0.9722, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.023

[Epoch: 55, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0164, 0.0376, 0.0074, 0.0074, 0.0248, 0.9007, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.059

[Epoch: 55, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0514e-02, 6.4465e-05, 5.6046e-02, 6.6268e-01, 6.4471e-03, 5.6357e-03,
        2.5862e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 55, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0070, 0.0049, 0.0927, 0.0740, 0.0036, 0.2137, 0.6041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 55, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3690e-05, 9.0052e-05, 3.5460e-01, 1.8274e-04, 3.5043e-01, 2.9826e-05,
        2.9465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 56, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0078, 0.0055, 0.0062, 0.0073, 0.0058, 0.9601, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 56, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0454, 0.0300, 0.0069, 0.0637, 0.0474, 0.7989, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 56, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.3020e-02, 4.7891e-05, 6.3117e-02, 5.7475e-01, 6.0083e-03, 6.9973e-03,
        3.3606e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 56, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0038, 0.0067, 0.0781, 0.1294, 0.0049, 0.2395, 0.5377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 56, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.7022e-05, 8.1154e-05, 3.8611e-01, 1.4748e-04, 3.4543e-01, 3.8856e-05,
        2.6818e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.930

[Epoch: 57, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0038, 0.0057, 0.0047, 0.0051, 0.0043, 0.9723, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.070

[Epoch: 57, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0381, 0.0355, 0.0054, 0.0123, 0.0307, 0.8738, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 57, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.8397e-03, 4.7417e-05, 4.3451e-02, 5.9293e-01, 4.6287e-03, 6.8257e-03,
        3.4428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 57, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0064, 0.0687, 0.1161, 0.0042, 0.2097, 0.5900],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 57, batch: 200/201] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6656e-05, 1.2663e-04, 3.6242e-01, 2.4717e-04, 3.4957e-01, 2.9037e-05,
        2.8758e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.898

[Epoch: 58, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0047, 0.0062, 0.0066, 0.0058, 0.9669, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.024

[Epoch: 58, batch: 80/201] total loss per batch: 0.653
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0435, 0.0297, 0.0052, 0.0614, 0.0273, 0.8280, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 58, batch: 120/201] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.5802e-03, 4.9470e-05, 4.3598e-02, 6.7585e-01, 4.5855e-03, 4.9758e-03,
        2.6237e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.037

[Epoch: 58, batch: 160/201] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0064, 0.0062, 0.0969, 0.0654, 0.0039, 0.2377, 0.5835],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 58, batch: 200/201] total loss per batch: 0.662
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.5557e-05, 1.1545e-04, 3.7144e-01, 2.1217e-04, 3.5084e-01, 6.7090e-05,
        2.7730e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 59, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0063, 0.0041, 0.0051, 0.0056, 0.9678, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.021

[Epoch: 59, batch: 80/201] total loss per batch: 0.654
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0260, 0.0327, 0.0094, 0.0243, 0.0548, 0.8490, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 59, batch: 120/201] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1323e-03, 1.0018e-04, 6.8572e-02, 6.4340e-01, 7.6149e-03, 6.9176e-03,
        2.6426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.023

[Epoch: 59, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0040, 0.0651, 0.1658, 0.0029, 0.2238, 0.5334],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 59, batch: 200/201] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.0214e-05, 1.1150e-04, 3.5620e-01, 2.2692e-04, 3.5173e-01, 5.7475e-05,
        2.9165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.933

[Epoch: 60, batch: 40/201] total loss per batch: 0.617
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0037, 0.0072, 0.0051, 0.0063, 0.9626, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.030

[Epoch: 60, batch: 80/201] total loss per batch: 0.655
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0368, 0.0471, 0.0051, 0.0227, 0.0500, 0.8299, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 60, batch: 120/201] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([5.0899e-03, 5.9930e-05, 4.2375e-02, 6.4983e-01, 5.3895e-03, 4.1142e-03,
        2.9314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.023

[Epoch: 60, batch: 160/201] total loss per batch: 0.672
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0034, 0.0179, 0.0800, 0.0966, 0.0031, 0.2050, 0.5941],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 60, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.8318e-05, 1.4831e-04, 3.4617e-01, 2.6309e-04, 3.4383e-01, 4.6585e-05,
        3.0953e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.884

[Epoch: 61, batch: 40/201] total loss per batch: 0.618
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0063, 0.0054, 0.0043, 0.0064, 0.0051, 0.9680, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.040

[Epoch: 61, batch: 80/201] total loss per batch: 0.655
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0393, 0.0369, 0.0071, 0.0891, 0.0353, 0.7852, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 61, batch: 120/201] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0441e-02, 8.2215e-05, 5.3982e-02, 5.7706e-01, 6.1381e-03, 7.1535e-03,
        3.4514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.037

[Epoch: 61, batch: 160/201] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0099, 0.0104, 0.0787, 0.1131, 0.0031, 0.2500, 0.5349],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 61, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9141e-05, 9.7729e-05, 3.7949e-01, 1.0874e-04, 3.5429e-01, 3.5096e-05,
        2.6596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.927

[Epoch: 62, batch: 40/201] total loss per batch: 0.618
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0029, 0.0051, 0.0050, 0.0038, 0.0044, 0.9707, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.085

[Epoch: 62, batch: 80/201] total loss per batch: 0.654
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0203, 0.0237, 0.0059, 0.0183, 0.0272, 0.9009, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 62, batch: 120/201] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1608e-02, 1.4905e-04, 5.4795e-02, 6.6492e-01, 9.4637e-03, 6.0200e-03,
        2.5304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 62, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0087, 0.0687, 0.1310, 0.0030, 0.1928, 0.5910],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.001

[Epoch: 62, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.3015e-05, 1.0541e-04, 3.5156e-01, 2.2163e-04, 3.6289e-01, 4.0880e-05,
        2.8516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 63, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0067, 0.0057, 0.0064, 0.0049, 0.0068, 0.9632, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.030

[Epoch: 63, batch: 80/201] total loss per batch: 0.652
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0367, 0.0354, 0.0082, 0.0343, 0.0318, 0.8474, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.012

[Epoch: 63, batch: 120/201] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.2616e-03, 1.2731e-04, 5.9124e-02, 6.6081e-01, 5.5659e-03, 3.8810e-03,
        2.6323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.037

[Epoch: 63, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0069, 0.0905, 0.1070, 0.0027, 0.2213, 0.5669],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 63, batch: 200/201] total loss per batch: 0.666
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.4592e-05, 2.9417e-04, 3.6948e-01, 2.1244e-04, 3.4467e-01, 3.1179e-05,
        2.8528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.940

[Epoch: 64, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0037, 0.0053, 0.0085, 0.0044, 0.0058, 0.9688, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.087

[Epoch: 64, batch: 80/201] total loss per batch: 0.652
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0407, 0.0344, 0.0053, 0.0334, 0.0454, 0.8317, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 64, batch: 120/201] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0709e-02, 9.0751e-05, 7.2587e-02, 6.5099e-01, 5.5968e-03, 4.3806e-03,
        2.5564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 64, batch: 160/201] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0063, 0.0066, 0.1015, 0.0805, 0.0035, 0.2409, 0.5607],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 64, batch: 200/201] total loss per batch: 0.667
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.8660e-06, 2.9635e-04, 3.5246e-01, 1.4486e-04, 3.5997e-01, 5.5515e-05,
        2.8706e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 65, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0032, 0.0041, 0.0044, 0.0090, 0.0082, 0.9655, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.012

[Epoch: 65, batch: 80/201] total loss per batch: 0.652
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0276, 0.0330, 0.0048, 0.0288, 0.0262, 0.8743, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 65, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.7197e-03, 6.3913e-05, 4.7771e-02, 6.0709e-01, 6.0175e-03, 6.2895e-03,
        3.2405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 65, batch: 160/201] total loss per batch: 0.671
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0452, 0.0069, 0.0610, 0.1334, 0.0050, 0.1188, 0.6297],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 65, batch: 200/201] total loss per batch: 0.666
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.6264e-06, 9.8684e-05, 3.6038e-01, 1.0116e-04, 3.7958e-01, 9.3876e-05,
        2.5974e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.936

[Epoch: 66, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0074, 0.0043, 0.0053, 0.0077, 0.0050, 0.9641, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.013

[Epoch: 66, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0495, 0.0390, 0.0081, 0.0480, 0.0487, 0.7999, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 66, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.2467e-03, 8.7660e-05, 5.7940e-02, 6.0329e-01, 8.3176e-03, 4.4324e-03,
        3.1669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 66, batch: 160/201] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0073, 0.0079, 0.0959, 0.0973, 0.0070, 0.3319, 0.4527],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 66, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3446e-05, 1.6196e-04, 3.4207e-01, 1.3372e-04, 3.4773e-01, 3.6533e-05,
        3.0986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.903

[Epoch: 67, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0042, 0.0124, 0.0070, 0.0055, 0.9558, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.016

[Epoch: 67, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0177, 0.0395, 0.0054, 0.0295, 0.0325, 0.8684, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 67, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0294e-02, 9.2929e-05, 5.9969e-02, 6.1904e-01, 6.4926e-03, 4.5623e-03,
        2.9955e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 67, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0066, 0.0083, 0.0891, 0.1061, 0.0067, 0.2156, 0.5676],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.003

[Epoch: 67, batch: 200/201] total loss per batch: 0.668
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6270e-05, 6.7878e-05, 3.0359e-01, 9.6206e-05, 3.9645e-01, 6.7698e-05,
        2.9971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.963

[Epoch: 68, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0032, 0.0045, 0.0034, 0.0044, 0.0063, 0.9737, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.044

[Epoch: 68, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0436, 0.0387, 0.0054, 0.0293, 0.0375, 0.8414, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 68, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.7064e-03, 5.3111e-05, 4.6255e-02, 6.5111e-01, 4.9525e-03, 4.0861e-03,
        2.8383e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 68, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0036, 0.0856, 0.1021, 0.0037, 0.2018, 0.5977],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.002

[Epoch: 68, batch: 200/201] total loss per batch: 0.663
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3581e-05, 1.5944e-04, 3.5565e-01, 3.6204e-04, 3.6770e-01, 4.8996e-05,
        2.7607e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.946

[Epoch: 69, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0038, 0.0041, 0.0045, 0.0071, 0.0046, 0.9687, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.033

[Epoch: 69, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0353, 0.0305, 0.0057, 0.0454, 0.0394, 0.8382, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 69, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.8120e-03, 5.0314e-05, 5.7049e-02, 5.9448e-01, 7.1570e-03, 6.1124e-03,
        3.2534e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 69, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0112, 0.0051, 0.0759, 0.1098, 0.0044, 0.2523, 0.5413],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 69, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.3288e-05, 1.3546e-04, 3.8277e-01, 9.6022e-05, 3.3916e-01, 3.0500e-05,
        2.7778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.945

[Epoch: 70, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0036, 0.0061, 0.0043, 0.9708, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.096

[Epoch: 70, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0333, 0.0362, 0.0056, 0.0309, 0.0321, 0.8574, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 70, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.7801e-03, 5.0360e-05, 4.7993e-02, 6.4086e-01, 6.1020e-03, 4.6709e-03,
        2.9055e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 70, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0055, 0.0739, 0.1018, 0.0038, 0.2218, 0.5886],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 70, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2807e-05, 7.1934e-05, 3.5971e-01, 1.0425e-04, 3.5418e-01, 2.6501e-05,
        2.8590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.927

[Epoch: 71, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0036, 0.0056, 0.0040, 0.0048, 0.0057, 0.9706, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.027

[Epoch: 71, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0302, 0.0320, 0.0063, 0.0356, 0.0338, 0.8575, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 71, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1221e-02, 6.7335e-05, 6.3940e-02, 6.0331e-01, 5.9985e-03, 5.2426e-03,
        3.1022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 71, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0051, 0.0786, 0.1086, 0.0037, 0.2200, 0.5786],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 71, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3625e-05, 1.3392e-04, 3.6530e-01, 1.2802e-04, 3.5896e-01, 3.4192e-05,
        2.7544e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.909

[Epoch: 72, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0039, 0.0047, 0.0050, 0.0051, 0.0047, 0.9722, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.032

[Epoch: 72, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0391, 0.0425, 0.0052, 0.0323, 0.0429, 0.8323, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 72, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.6095e-03, 4.9598e-05, 3.9655e-02, 6.4159e-01, 5.3092e-03, 4.3064e-03,
        2.9948e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 72, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0043, 0.0061, 0.0807, 0.1116, 0.0047, 0.2337, 0.5589],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 72, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3214e-05, 5.8354e-05, 3.6711e-01, 9.7784e-05, 3.4151e-01, 2.8496e-05,
        2.9118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.939

[Epoch: 73, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.0052, 0.0045, 0.0050, 0.9693, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.078

[Epoch: 73, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0388, 0.0253, 0.0064, 0.0391, 0.0342, 0.8510, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 73, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0577e-02, 7.7956e-05, 6.4336e-02, 6.3115e-01, 5.7869e-03, 4.3294e-03,
        2.8374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 73, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0065, 0.0046, 0.0766, 0.0890, 0.0046, 0.2224, 0.5963],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 73, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.3013e-06, 1.0670e-04, 3.6766e-01, 1.2263e-04, 3.6351e-01, 3.2494e-05,
        2.6856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.907

[Epoch: 74, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0058, 0.0051, 0.0056, 0.0060, 0.9677, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.026

[Epoch: 74, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0257, 0.0556, 0.0057, 0.0318, 0.0441, 0.8313, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 74, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0485e-02, 5.9003e-05, 5.6033e-02, 6.3916e-01, 5.7878e-03, 5.0678e-03,
        2.8341e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 74, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0041, 0.0050, 0.0865, 0.1100, 0.0047, 0.2596, 0.5302],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 74, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3673e-05, 7.1060e-05, 3.7314e-01, 8.7193e-05, 3.3799e-01, 3.0608e-05,
        2.8867e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.948

[Epoch: 75, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0059, 0.0063, 0.0035, 0.9670, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.039

[Epoch: 75, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0429, 0.0283, 0.0060, 0.0352, 0.0296, 0.8533, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 75, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1332e-02, 5.5780e-05, 6.8587e-02, 6.1891e-01, 7.1972e-03, 5.6911e-03,
        2.8822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 75, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0068, 0.0060, 0.0644, 0.1199, 0.0044, 0.1865, 0.6120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 75, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.5200e-06, 7.4733e-05, 3.6481e-01, 8.4225e-05, 3.5174e-01, 2.5266e-05,
        2.8326e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.901

[Epoch: 76, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0040, 0.0052, 0.0051, 0.0067, 0.9720, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.065

[Epoch: 76, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0259, 0.0278, 0.0044, 0.0286, 0.0251, 0.8838, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 76, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.8308e-03, 7.5584e-05, 4.9290e-02, 6.1352e-01, 4.8208e-03, 4.3490e-03,
        3.1811e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 76, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0055, 0.0907, 0.0723, 0.0043, 0.2368, 0.5848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 76, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9534e-05, 8.0788e-05, 3.7112e-01, 1.1699e-04, 3.5447e-01, 3.3381e-05,
        2.7416e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.939

[Epoch: 77, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0075, 0.0104, 0.0073, 0.0065, 0.0056, 0.9541, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.025

[Epoch: 77, batch: 80/201] total loss per batch: 0.652
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0488, 0.0357, 0.0062, 0.0444, 0.0449, 0.8155, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 77, batch: 120/201] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([6.2583e-03, 8.6076e-05, 6.1437e-02, 6.3130e-01, 9.9111e-03, 4.7002e-03,
        2.8631e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.032

[Epoch: 77, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0058, 0.0057, 0.0490, 0.1677, 0.0050, 0.2246, 0.5423],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 77, batch: 200/201] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0683e-05, 1.3004e-04, 3.7698e-01, 1.0869e-04, 3.2405e-01, 1.2802e-05,
        2.9870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.909

[Epoch: 78, batch: 40/201] total loss per batch: 0.617
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.0084, 0.0030, 0.0036, 0.9719, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.054

[Epoch: 78, batch: 80/201] total loss per batch: 0.654
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0359, 0.0296, 0.0058, 0.0427, 0.0316, 0.8483, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.017

[Epoch: 78, batch: 120/201] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1115e-02, 7.0501e-05, 6.0116e-02, 5.8227e-01, 5.8909e-03, 3.4415e-03,
        3.3710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 78, batch: 160/201] total loss per batch: 0.672
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0072, 0.0080, 0.1177, 0.0712, 0.0031, 0.1970, 0.5958],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 78, batch: 200/201] total loss per batch: 0.665
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1381e-05, 3.4555e-05, 3.3027e-01, 1.5995e-04, 3.4042e-01, 3.5918e-05,
        3.2906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 79, batch: 40/201] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0024, 0.0093, 0.0104, 0.0087, 0.0127, 0.9417, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.037

[Epoch: 79, batch: 80/201] total loss per batch: 0.732
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0382, 0.1309, 0.0045, 0.0157, 0.0692, 0.7390, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.017

[Epoch: 79, batch: 120/201] total loss per batch: 0.697
Policy (actual, predicted): 3 6
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([4.1899e-03, 3.3917e-04, 9.9639e-02, 3.8318e-01, 2.3028e-02, 1.6588e-02,
        4.7304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 79, batch: 160/201] total loss per batch: 0.740
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0195, 0.0076, 0.1046, 0.0965, 0.0100, 0.2174, 0.5444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 79, batch: 200/201] total loss per batch: 0.698
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9930e-05, 6.2703e-04, 3.3308e-01, 2.3730e-04, 3.8938e-01, 5.9397e-05,
        2.7659e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.869

[Epoch: 80, batch: 40/201] total loss per batch: 0.655
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0038, 0.0155, 0.0061, 0.0075, 0.0086, 0.9361, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 80, batch: 80/201] total loss per batch: 0.692
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0112, 0.0199, 0.0020, 0.0259, 0.0417, 0.8905, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 80, batch: 120/201] total loss per batch: 0.668
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1580e-02, 6.6089e-05, 3.9998e-02, 5.3853e-01, 1.0553e-02, 7.9275e-03,
        3.9134e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.026

[Epoch: 80, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0060, 0.0058, 0.0608, 0.0945, 0.0034, 0.2171, 0.6125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 80, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.6740e-06, 2.0522e-04, 3.2614e-01, 1.9540e-04, 4.1656e-01, 6.5346e-05,
        2.5683e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.838

[Epoch: 81, batch: 40/201] total loss per batch: 0.628
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0027, 0.0075, 0.0017, 0.0030, 0.0053, 0.9756, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.016

[Epoch: 81, batch: 80/201] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0361, 0.0420, 0.0050, 0.0429, 0.0532, 0.8141, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 81, batch: 120/201] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.4026e-02, 1.2306e-04, 5.8839e-02, 6.3140e-01, 5.5836e-03, 8.2298e-03,
        2.8180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.027

[Epoch: 81, batch: 160/201] total loss per batch: 0.672
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0043, 0.0794, 0.0809, 0.0047, 0.2269, 0.5989],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.001

[Epoch: 81, batch: 200/201] total loss per batch: 0.663
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.4809e-06, 1.1469e-04, 3.3658e-01, 9.3119e-05, 3.6728e-01, 3.7364e-05,
        2.9589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.871

[Epoch: 82, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0067, 0.0099, 0.0024, 0.0092, 0.0063, 0.9558, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.019

[Epoch: 82, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0408, 0.0364, 0.0070, 0.0375, 0.0354, 0.8352, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 82, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.8915e-03, 1.1491e-04, 4.7728e-02, 7.5580e-01, 5.8048e-03, 7.9008e-03,
        1.7476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.029

[Epoch: 82, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0060, 0.0066, 0.0862, 0.0960, 0.0062, 0.2311, 0.5680],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 82, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.6648e-06, 1.0333e-04, 3.4482e-01, 7.5185e-05, 3.6951e-01, 3.5344e-05,
        2.8545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.863

[Epoch: 83, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0094, 0.0028, 0.0076, 0.0065, 0.9604, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.023

[Epoch: 83, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0309, 0.0327, 0.0065, 0.0336, 0.0320, 0.8573, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 83, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2338e-02, 8.3838e-05, 5.1942e-02, 5.9725e-01, 6.1046e-03, 7.7103e-03,
        3.2458e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 83, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0061, 0.0059, 0.0780, 0.0987, 0.0054, 0.2251, 0.5808],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 83, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.4878e-06, 9.4268e-05, 3.4405e-01, 7.2031e-05, 3.7219e-01, 3.3967e-05,
        2.8354e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.875

[Epoch: 84, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0074, 0.0029, 0.0070, 0.0060, 0.9646, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.028

[Epoch: 84, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0327, 0.0339, 0.0061, 0.0337, 0.0320, 0.8552, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 84, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1132e-02, 8.2676e-05, 5.4409e-02, 6.0391e-01, 6.0963e-03, 7.4380e-03,
        3.1693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 84, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0060, 0.0779, 0.1030, 0.0056, 0.2246, 0.5770],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 84, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.9843e-06, 8.0917e-05, 3.4811e-01, 6.3209e-05, 3.6656e-01, 3.3094e-05,
        2.8515e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.886

[Epoch: 85, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0072, 0.0032, 0.0059, 0.0054, 0.9681, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.030

[Epoch: 85, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0332, 0.0323, 0.0055, 0.0341, 0.0328, 0.8557, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 85, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1512e-02, 7.7463e-05, 5.4871e-02, 6.1430e-01, 6.1854e-03, 6.9262e-03,
        3.0613e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 85, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0060, 0.0057, 0.0777, 0.1029, 0.0054, 0.2286, 0.5737],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 85, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.7366e-06, 7.2134e-05, 3.5457e-01, 5.3361e-05, 3.6105e-01, 2.8013e-05,
        2.8422e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.895

[Epoch: 86, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0065, 0.0031, 0.0061, 0.0052, 0.9687, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.033

[Epoch: 86, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0345, 0.0338, 0.0056, 0.0357, 0.0327, 0.8519, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 86, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1247e-02, 7.5758e-05, 5.5066e-02, 6.2007e-01, 5.9811e-03, 6.4933e-03,
        3.0106e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 86, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0058, 0.0764, 0.1037, 0.0056, 0.2263, 0.5762],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 86, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.0118e-06, 6.1747e-05, 3.5584e-01, 4.7862e-05, 3.6161e-01, 2.7186e-05,
        2.8241e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.898

[Epoch: 87, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0067, 0.0038, 0.0057, 0.0050, 0.9688, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.037

[Epoch: 87, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0331, 0.0333, 0.0053, 0.0345, 0.0328, 0.8552, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 87, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0951e-02, 7.2706e-05, 5.5035e-02, 6.2302e-01, 6.1666e-03, 6.3041e-03,
        2.9845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 87, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0058, 0.0056, 0.0786, 0.1039, 0.0054, 0.2303, 0.5704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 87, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.4864e-06, 5.8063e-05, 3.6132e-01, 4.5973e-05, 3.5473e-01, 2.4957e-05,
        2.8381e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.907

[Epoch: 88, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0061, 0.0038, 0.0059, 0.0049, 0.9690, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.038

[Epoch: 88, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0360, 0.0344, 0.0054, 0.0353, 0.0333, 0.8499, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 88, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1308e-02, 6.7319e-05, 5.4302e-02, 6.2306e-01, 5.5874e-03, 5.9364e-03,
        2.9974e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 88, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0055, 0.0057, 0.0774, 0.1056, 0.0059, 0.2239, 0.5761],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 88, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.7925e-06, 5.0199e-05, 3.6169e-01, 3.9007e-05, 3.5870e-01, 2.3074e-05,
        2.7950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.906

[Epoch: 89, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0066, 0.0042, 0.0050, 0.0045, 0.9699, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.041

[Epoch: 89, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0345, 0.0339, 0.0049, 0.0360, 0.0325, 0.8530, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 89, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1032e-02, 6.6910e-05, 5.5381e-02, 6.2471e-01, 5.8599e-03, 5.8466e-03,
        2.9710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 89, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0058, 0.0058, 0.0806, 0.1045, 0.0056, 0.2282, 0.5695],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 89, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.4906e-06, 4.5525e-05, 3.6608e-01, 3.6461e-05, 3.4786e-01, 1.9657e-05,
        2.8595e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.910

[Epoch: 90, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.0045, 0.0065, 0.0048, 0.9676, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.041

[Epoch: 90, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0332, 0.0338, 0.0054, 0.0348, 0.0371, 0.8500, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 90, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1446e-02, 6.2787e-05, 5.2979e-02, 6.2275e-01, 5.6673e-03, 5.6077e-03,
        3.0149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 90, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0057, 0.0796, 0.1075, 0.0059, 0.2260, 0.5700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 90, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.3629e-06, 4.1105e-05, 3.5937e-01, 3.9494e-05, 3.6567e-01, 1.9800e-05,
        2.7486e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.916

[Epoch: 91, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0061, 0.0046, 0.0045, 0.0044, 0.9706, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 91, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0381, 0.0337, 0.0047, 0.0387, 0.0345, 0.8449, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 91, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0856e-02, 6.3285e-05, 5.8110e-02, 6.2241e-01, 5.7951e-03, 6.6448e-03,
        2.9612e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 91, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0061, 0.0740, 0.1020, 0.0054, 0.2262, 0.5806],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 91, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.0036e-06, 4.0460e-05, 3.7756e-01, 2.9905e-05, 3.3768e-01, 1.7834e-05,
        2.8467e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.918

[Epoch: 92, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0043, 0.0063, 0.0047, 0.9693, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.043

[Epoch: 92, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0289, 0.0373, 0.0056, 0.0346, 0.0391, 0.8476, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 92, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1280e-02, 5.7977e-05, 5.7069e-02, 6.3900e-01, 5.8820e-03, 4.9754e-03,
        2.8174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 92, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0052, 0.0892, 0.1025, 0.0052, 0.2205, 0.5715],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 92, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.1815e-06, 3.5853e-05, 3.5490e-01, 3.4081e-05, 3.5984e-01, 1.6996e-05,
        2.8517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.903

[Epoch: 93, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0066, 0.0052, 0.0050, 0.9676, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 93, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0393, 0.0287, 0.0046, 0.0356, 0.0294, 0.8583, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 93, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1162e-02, 5.9079e-05, 4.9563e-02, 5.6775e-01, 6.0899e-03, 8.1583e-03,
        3.5721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 93, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0068, 0.0706, 0.1074, 0.0047, 0.2381, 0.5675],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 93, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.7464e-06, 5.6899e-05, 3.7753e-01, 4.6116e-05, 3.5933e-01, 3.0961e-05,
        2.6300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.906

[Epoch: 94, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0057, 0.0028, 0.0036, 0.0051, 0.9735, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 94, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0453, 0.0444, 0.0062, 0.0448, 0.0505, 0.8012, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 94, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1183e-02, 8.1834e-05, 4.9882e-02, 6.1487e-01, 8.4626e-03, 7.8789e-03,
        3.0764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 94, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0060, 0.0867, 0.1212, 0.0076, 0.2189, 0.5550],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 94, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.3283e-06, 2.9888e-05, 3.5668e-01, 3.8895e-05, 3.5065e-01, 1.6609e-05,
        2.9259e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.911

[Epoch: 95, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0062, 0.0057, 0.0056, 0.0070, 0.0053, 0.9615, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.039

[Epoch: 95, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0167, 0.0253, 0.0038, 0.0245, 0.0250, 0.9013, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.054

[Epoch: 95, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0110e-02, 1.0220e-04, 7.3905e-02, 6.1920e-01, 6.2063e-03, 6.2278e-03,
        2.8425e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 95, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0079, 0.0059, 0.0704, 0.0808, 0.0039, 0.2433, 0.5879],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 95, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.2739e-06, 5.5330e-05, 3.7912e-01, 4.5064e-05, 3.4380e-01, 3.4237e-05,
        2.7694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 96, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0033, 0.0048, 0.0051, 0.0046, 0.0046, 0.9734, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.054

[Epoch: 96, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0663, 0.0419, 0.0068, 0.0353, 0.0457, 0.7977, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 96, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0090e-02, 8.2746e-05, 4.8698e-02, 6.1223e-01, 4.6357e-03, 6.7914e-03,
        3.1747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 96, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0045, 0.0040, 0.0941, 0.1259, 0.0066, 0.1979, 0.5670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 96, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.9011e-06, 3.5218e-05, 3.5857e-01, 4.1442e-05, 3.6034e-01, 2.7796e-05,
        2.8098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.905

[Epoch: 97, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0033, 0.0039, 0.0034, 0.0033, 0.9777, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.057

[Epoch: 97, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0109, 0.0300, 0.0049, 0.0350, 0.0270, 0.8869, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 97, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1047e-02, 8.1953e-05, 5.0948e-02, 6.3090e-01, 5.4785e-03, 4.9420e-03,
        2.9660e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 97, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0087, 0.0078, 0.0703, 0.0984, 0.0039, 0.2315, 0.5795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 97, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2068e-05, 9.4858e-05, 3.7159e-01, 7.7274e-05, 3.4644e-01, 5.1383e-05,
        2.8174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.919

[Epoch: 98, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0059, 0.0067, 0.0076, 0.0053, 0.9612, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.055

[Epoch: 98, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0460, 0.0378, 0.0055, 0.0425, 0.0395, 0.8204, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 98, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1077e-02, 8.1557e-05, 6.4573e-02, 6.2371e-01, 4.6217e-03, 6.3582e-03,
        2.8958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 98, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0042, 0.0901, 0.1112, 0.0046, 0.1964, 0.5885],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 98, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1285e-05, 4.3264e-05, 3.7547e-01, 4.5886e-05, 3.4253e-01, 2.9483e-05,
        2.8187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.918

[Epoch: 99, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0044, 0.0044, 0.0057, 0.0052, 0.9724, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 99, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0225, 0.0292, 0.0063, 0.0303, 0.0349, 0.8715, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 99, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.4058e-02, 1.0920e-04, 4.7588e-02, 6.6067e-01, 6.6434e-03, 4.7563e-03,
        2.6618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 99, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0072, 0.0813, 0.1212, 0.0037, 0.2694, 0.5121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 99, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1089e-05, 5.8270e-05, 3.7522e-01, 6.2981e-05, 3.4111e-01, 2.4552e-05,
        2.8352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.910

[Epoch: 100, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0063, 0.0044, 0.0036, 0.0042, 0.9707, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 100, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0315, 0.0422, 0.0040, 0.0470, 0.0264, 0.8406, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 100, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.6884e-03, 6.6970e-05, 4.4782e-02, 6.3028e-01, 4.5049e-03, 5.9789e-03,
        3.0570e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 100, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0067, 0.0806, 0.0839, 0.0049, 0.2122, 0.6069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 100, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0278e-05, 3.2610e-05, 3.6349e-01, 4.0588e-05, 3.7049e-01, 2.7484e-05,
        2.6591e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 101, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0048, 0.0056, 0.0055, 0.0055, 0.9693, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 101, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0344, 0.0304, 0.0058, 0.0377, 0.0486, 0.8381, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 101, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1052e-02, 8.3942e-05, 5.6391e-02, 6.1400e-01, 5.9269e-03, 5.4003e-03,
        3.0715e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.051

[Epoch: 101, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0060, 0.0748, 0.1280, 0.0046, 0.2179, 0.5644],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 101, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4367e-05, 4.2575e-05, 3.7126e-01, 6.1039e-05, 3.4571e-01, 3.3525e-05,
        2.8288e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 102, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0061, 0.0055, 0.0058, 0.0046, 0.0040, 0.9692, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.055

[Epoch: 102, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0352, 0.0361, 0.0068, 0.0283, 0.0345, 0.8537, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 102, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.2868e-03, 7.1339e-05, 5.5940e-02, 6.3331e-01, 5.5778e-03, 5.5920e-03,
        2.9022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.053

[Epoch: 102, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0045, 0.0058, 0.0834, 0.1044, 0.0052, 0.2351, 0.5617],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 102, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2516e-05, 4.1633e-05, 3.7278e-01, 5.9748e-05, 3.4706e-01, 3.2611e-05,
        2.8001e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 103, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0055, 0.0053, 0.0053, 0.9689, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.058

[Epoch: 103, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0356, 0.0341, 0.0058, 0.0402, 0.0321, 0.8468, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 103, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0970e-02, 7.6808e-05, 5.6358e-02, 6.0335e-01, 5.4019e-03, 4.9946e-03,
        3.1885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 103, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0056, 0.0776, 0.1073, 0.0048, 0.2115, 0.5882],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 103, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2905e-05, 3.8911e-05, 3.7028e-01, 5.0204e-05, 3.4865e-01, 2.7590e-05,
        2.8094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 104, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0056, 0.0054, 0.0053, 0.0055, 0.9674, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.058

[Epoch: 104, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0374, 0.0372, 0.0055, 0.0355, 0.0361, 0.8427, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 104, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2486e-02, 7.2337e-05, 5.8231e-02, 6.1306e-01, 6.4015e-03, 5.5486e-03,
        3.0420e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 104, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0056, 0.0872, 0.0993, 0.0059, 0.2498, 0.5470],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 104, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.4973e-06, 3.7547e-05, 3.6938e-01, 4.9550e-05, 3.5095e-01, 3.2810e-05,
        2.7954e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 105, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0054, 0.0060, 0.0060, 0.9659, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 105, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0335, 0.0346, 0.0053, 0.0319, 0.0294, 0.8609, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 105, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0968e-02, 6.8182e-05, 6.1133e-02, 6.3742e-01, 6.9280e-03, 6.2236e-03,
        2.7726e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 105, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0054, 0.0687, 0.1092, 0.0047, 0.1976, 0.6093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 105, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.2734e-06, 3.2372e-05, 3.8074e-01, 4.0818e-05, 3.4672e-01, 2.5035e-05,
        2.7243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.926

[Epoch: 106, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0056, 0.0049, 0.0047, 0.0047, 0.9713, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 106, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0352, 0.0393, 0.0058, 0.0424, 0.0418, 0.8295, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 106, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1289e-02, 8.1661e-05, 5.8261e-02, 6.2971e-01, 5.9491e-03, 4.9656e-03,
        2.8975e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 106, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0057, 0.0902, 0.1046, 0.0051, 0.2519, 0.5372],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 106, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.5965e-06, 3.6203e-05, 3.7072e-01, 3.6900e-05, 3.4884e-01, 2.7261e-05,
        2.8033e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.923

[Epoch: 107, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0031, 0.0041, 0.0043, 0.0038, 0.0037, 0.9762, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 107, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0323, 0.0291, 0.0046, 0.0279, 0.0355, 0.8659, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 107, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2833e-02, 7.4827e-05, 5.4815e-02, 5.9559e-01, 7.1525e-03, 6.3431e-03,
        3.2319e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 107, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0039, 0.0039, 0.0616, 0.0994, 0.0030, 0.1962, 0.6320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 107, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.2576e-06, 3.6153e-05, 3.7879e-01, 4.2291e-05, 3.3936e-01, 2.3265e-05,
        2.8174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 108, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0079, 0.0066, 0.0054, 0.0068, 0.0052, 0.9614, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.060

[Epoch: 108, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0326, 0.0358, 0.0052, 0.0328, 0.0304, 0.8588, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 108, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1703e-03, 1.0476e-04, 5.2697e-02, 6.5542e-01, 4.3605e-03, 5.3551e-03,
        2.7289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 108, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0050, 0.1034, 0.0994, 0.0045, 0.2379, 0.5447],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 108, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.1509e-06, 3.4334e-05, 3.6583e-01, 3.9862e-05, 3.5498e-01, 3.1298e-05,
        2.7908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.926

[Epoch: 109, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0047, 0.0052, 0.0042, 0.0052, 0.9723, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.062

[Epoch: 109, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0309, 0.0395, 0.0058, 0.0347, 0.0379, 0.8471, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 109, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([2.2772e-03, 7.3115e-05, 5.9072e-02, 6.4895e-01, 7.9387e-03, 1.1745e-02,
        2.6995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.052

[Epoch: 109, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0063, 0.0604, 0.1210, 0.0034, 0.2179, 0.5864],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 109, batch: 200/201] total loss per batch: 0.662
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2631e-05, 4.5637e-05, 3.8572e-01, 6.1819e-05, 3.4612e-01, 5.5934e-05,
        2.6798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.910

[Epoch: 110, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.0046, 0.0102, 0.0040, 0.9646, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.063

[Epoch: 110, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0576, 0.0392, 0.0061, 0.0440, 0.0392, 0.8065, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 110, batch: 120/201] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([5.9732e-03, 4.8667e-05, 4.4596e-02, 6.0000e-01, 2.3565e-03, 2.1605e-03,
        3.4487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.036

[Epoch: 110, batch: 160/201] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0090, 0.0841, 0.0735, 0.0056, 0.2672, 0.5551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 110, batch: 200/201] total loss per batch: 0.664
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([3.0554e-05, 1.0828e-04, 3.5401e-01, 2.4117e-05, 3.5859e-01, 6.6629e-05,
        2.8717e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.942

[Epoch: 111, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0043, 0.0063, 0.0047, 0.0064, 0.9624, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 111, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0254, 0.0397, 0.0023, 0.0244, 0.0318, 0.8726, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 111, batch: 120/201] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1203e-02, 7.6431e-05, 6.2560e-02, 6.1303e-01, 6.1850e-03, 8.7612e-03,
        2.9818e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 111, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0073, 0.0053, 0.0739, 0.1272, 0.0038, 0.2095, 0.5731],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 111, batch: 200/201] total loss per batch: 0.661
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.4337e-05, 7.3617e-05, 3.6871e-01, 3.5473e-05, 3.3276e-01, 3.0999e-05,
        2.9837e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.912

[Epoch: 112, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0038, 0.0066, 0.0059, 0.0043, 0.0091, 0.9635, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.041

[Epoch: 112, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0426, 0.0285, 0.0048, 0.0431, 0.0379, 0.8373, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 112, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1499e-02, 6.1491e-05, 5.5239e-02, 6.1721e-01, 4.9472e-03, 5.6541e-03,
        3.0538e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.037

[Epoch: 112, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0060, 0.0059, 0.0924, 0.0961, 0.0058, 0.2208, 0.5730],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 112, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.7244e-05, 7.4179e-05, 3.7204e-01, 3.5137e-05, 3.4772e-01, 4.1057e-05,
        2.8007e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.930

[Epoch: 113, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0042, 0.0045, 0.0044, 0.0068, 0.9722, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.041

[Epoch: 113, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0266, 0.0360, 0.0034, 0.0308, 0.0349, 0.8633, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 113, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0778e-02, 8.2809e-05, 5.8445e-02, 6.0516e-01, 6.0517e-03, 5.0296e-03,
        3.1446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 113, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0063, 0.0063, 0.0854, 0.1127, 0.0065, 0.2280, 0.5548],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 113, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.5067e-05, 6.1988e-05, 3.7173e-01, 3.1554e-05, 3.4818e-01, 3.6660e-05,
        2.7995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 114, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.0046, 0.0045, 0.0061, 0.9698, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 114, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0377, 0.0388, 0.0050, 0.0410, 0.0399, 0.8321, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 114, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1589e-02, 7.3958e-05, 5.4361e-02, 6.3512e-01, 6.1176e-03, 5.2066e-03,
        2.8753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 114, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0061, 0.0796, 0.0974, 0.0052, 0.2185, 0.5880],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 114, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0770e-05, 5.4018e-05, 3.7190e-01, 3.5941e-05, 3.4982e-01, 3.2484e-05,
        2.7815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 115, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0052, 0.0045, 0.0045, 0.0057, 0.9699, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 115, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0305, 0.0309, 0.0048, 0.0347, 0.0374, 0.8571, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 115, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2394e-02, 6.5651e-05, 5.6238e-02, 6.1643e-01, 6.5268e-03, 5.5965e-03,
        3.0275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 115, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0044, 0.0825, 0.1032, 0.0050, 0.2262, 0.5732],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 115, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3426e-05, 5.5826e-05, 3.6640e-01, 2.5976e-05, 3.5386e-01, 2.4290e-05,
        2.7962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 116, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0052, 0.0058, 0.0049, 0.0055, 0.9703, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 116, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0387, 0.0390, 0.0050, 0.0377, 0.0360, 0.8369, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 116, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0380e-02, 6.4169e-05, 6.0075e-02, 6.0652e-01, 5.4181e-03, 5.5317e-03,
        3.1201e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 116, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0044, 0.0778, 0.1116, 0.0054, 0.2275, 0.5679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 116, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.9489e-06, 5.2092e-05, 3.7391e-01, 2.9603e-05, 3.4819e-01, 2.4738e-05,
        2.7778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.912

[Epoch: 117, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0048, 0.0048, 0.0058, 0.0049, 0.9683, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 117, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0364, 0.0369, 0.0061, 0.0342, 0.0361, 0.8449, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 117, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0500e-02, 6.8925e-05, 5.1310e-02, 6.5672e-01, 5.0185e-03, 4.4453e-03,
        2.7194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 117, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0041, 0.0049, 0.0761, 0.1008, 0.0040, 0.2215, 0.5887],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 117, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.5764e-05, 3.9207e-05, 3.6895e-01, 3.0241e-05, 3.4266e-01, 2.4126e-05,
        2.8828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 118, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0064, 0.0059, 0.0071, 0.0069, 0.0082, 0.9588, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.055

[Epoch: 118, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0282, 0.0304, 0.0046, 0.0347, 0.0299, 0.8681, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 118, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0409e-02, 6.5523e-05, 4.6514e-02, 6.4515e-01, 5.2275e-03, 5.0248e-03,
        2.8761e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.052

[Epoch: 118, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0050, 0.0845, 0.1048, 0.0050, 0.2152, 0.5807],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 118, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.2991e-06, 5.1369e-05, 3.8202e-01, 3.2620e-05, 3.5811e-01, 2.8031e-05,
        2.5975e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 119, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0047, 0.0049, 0.0045, 0.0048, 0.9731, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 119, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0388, 0.0388, 0.0044, 0.0379, 0.0393, 0.8344, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 119, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0518e-02, 6.3740e-05, 5.3765e-02, 6.0158e-01, 4.5285e-03, 4.1221e-03,
        3.2542e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 119, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0050, 0.0815, 0.1172, 0.0043, 0.2439, 0.5434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 119, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.7805e-05, 5.2968e-05, 3.6175e-01, 3.2396e-05, 3.3187e-01, 3.9254e-05,
        3.0624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.918

[Epoch: 120, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0055, 0.0055, 0.0060, 0.9661, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 120, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0389, 0.0386, 0.0050, 0.0318, 0.0349, 0.8456, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 120, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1744e-02, 7.1023e-05, 5.5999e-02, 6.1849e-01, 5.5322e-03, 5.6260e-03,
        3.0253e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.051

[Epoch: 120, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0068, 0.0054, 0.0800, 0.1050, 0.0047, 0.2036, 0.5944],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 120, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2606e-05, 6.1630e-05, 3.7677e-01, 4.0857e-05, 3.4940e-01, 3.8360e-05,
        2.7367e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.944

[Epoch: 121, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0035, 0.0039, 0.0035, 0.0036, 0.0033, 0.9797, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 121, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0203, 0.0287, 0.0028, 0.0290, 0.0383, 0.8761, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 121, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.2505e-03, 9.0924e-05, 4.7434e-02, 6.9433e-01, 4.0068e-03, 2.9938e-03,
        2.4189e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 121, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0060, 0.0723, 0.1000, 0.0047, 0.2326, 0.5794],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 121, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.0326e-05, 7.0361e-05, 3.6610e-01, 4.7971e-05, 3.5565e-01, 4.7079e-05,
        2.7807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.906

[Epoch: 122, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0034, 0.0040, 0.0037, 0.0027, 0.0034, 0.9781, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.040

[Epoch: 122, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0522, 0.0411, 0.0136, 0.0498, 0.0346, 0.8035, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 122, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1733e-02, 7.7526e-05, 5.7630e-02, 5.6932e-01, 6.5871e-03, 7.3275e-03,
        3.4733e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 122, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0067, 0.0063, 0.0836, 0.1256, 0.0065, 0.2412, 0.5300],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 122, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.3729e-05, 7.5594e-05, 3.6867e-01, 5.3014e-05, 3.5183e-01, 5.4228e-05,
        2.7930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 123, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0039, 0.0053, 0.0046, 0.0071, 0.9696, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.059

[Epoch: 123, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0228, 0.0351, 0.0033, 0.0256, 0.0391, 0.8674, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 123, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2792e-02, 1.4928e-04, 6.0787e-02, 5.7289e-01, 6.6252e-03, 5.5532e-03,
        3.4120e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 123, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0047, 0.0975, 0.0816, 0.0044, 0.1981, 0.6095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 123, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.6572e-05, 8.6331e-05, 3.8180e-01, 5.8885e-05, 3.3134e-01, 2.7527e-05,
        2.8667e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.902

[Epoch: 124, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0091, 0.0068, 0.0051, 0.0047, 0.0048, 0.9649, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 124, batch: 80/201] total loss per batch: 0.653
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0452, 0.0393, 0.0075, 0.0499, 0.0296, 0.8217, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 124, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.6207e-03, 1.1372e-04, 7.0976e-02, 6.1095e-01, 7.5959e-03, 6.8652e-03,
        2.9388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 124, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0043, 0.0700, 0.1147, 0.0055, 0.2399, 0.5596],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 124, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([2.4076e-05, 1.1712e-04, 3.6863e-01, 4.5737e-05, 3.5742e-01, 5.0661e-05,
        2.7371e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.908

[Epoch: 125, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.0081, 0.0096, 0.0084, 0.9508, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 125, batch: 80/201] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0292, 0.0337, 0.0065, 0.0277, 0.0513, 0.8450, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 125, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.2521e-02, 1.0097e-04, 7.1092e-02, 6.0768e-01, 9.5169e-03, 7.6012e-03,
        2.9149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 125, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0071, 0.0763, 0.1170, 0.0043, 0.1986, 0.5922],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 125, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2563e-05, 7.7305e-05, 3.7449e-01, 3.8016e-05, 3.4375e-01, 3.0367e-05,
        2.8161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.932

[Epoch: 126, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.0071, 0.0050, 0.0057, 0.9682, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 126, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0496, 0.0401, 0.0063, 0.0300, 0.0349, 0.8340, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 126, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0030e-02, 1.2895e-04, 5.5631e-02, 6.4259e-01, 5.7539e-03, 5.1087e-03,
        2.8076e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 126, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0054, 0.0812, 0.0868, 0.0050, 0.2448, 0.5720],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 126, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.5402e-05, 1.0290e-04, 3.6495e-01, 4.0625e-05, 3.5040e-01, 5.0215e-05,
        2.8444e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.908

[Epoch: 127, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0065, 0.0064, 0.0051, 0.0063, 0.0057, 0.9650, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 127, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0271, 0.0320, 0.0059, 0.0375, 0.0384, 0.8542, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 127, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1684e-03, 1.1559e-04, 5.3198e-02, 6.3289e-01, 4.6291e-03, 4.2509e-03,
        2.9575e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 127, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0079, 0.0836, 0.1258, 0.0051, 0.2247, 0.5471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 127, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.9899e-05, 8.9349e-05, 3.7491e-01, 4.5068e-05, 3.4523e-01, 4.4359e-05,
        2.7965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.926

[Epoch: 128, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0051, 0.0062, 0.0043, 0.0051, 0.9709, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.057

[Epoch: 128, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0328, 0.0330, 0.0042, 0.0309, 0.0339, 0.8614, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 128, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.7558e-03, 9.3757e-05, 4.5972e-02, 6.2669e-01, 4.2827e-03, 3.8807e-03,
        3.1032e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 128, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0050, 0.0763, 0.0985, 0.0055, 0.2222, 0.5878],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 128, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4582e-05, 8.5045e-05, 3.6900e-01, 4.0690e-05, 3.4774e-01, 4.4456e-05,
        2.8308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.893

[Epoch: 129, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0034, 0.0050, 0.0038, 0.0042, 0.0045, 0.9748, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 129, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0358, 0.0324, 0.0052, 0.0403, 0.0339, 0.8475, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 129, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.9255e-03, 1.2219e-04, 5.1608e-02, 6.4141e-01, 4.7151e-03, 5.1317e-03,
        2.8809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 129, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0066, 0.0068, 0.0828, 0.1110, 0.0058, 0.2169, 0.5702],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 129, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1980e-05, 7.8109e-05, 3.6052e-01, 4.3295e-05, 3.5260e-01, 4.7574e-05,
        2.8670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.936

[Epoch: 130, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0044, 0.0054, 0.0042, 0.0043, 0.9711, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 130, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0365, 0.0344, 0.0046, 0.0332, 0.0329, 0.8539, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 130, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0906e-02, 1.0130e-04, 5.8489e-02, 6.1439e-01, 5.4256e-03, 4.4412e-03,
        3.0624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 130, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0057, 0.0042, 0.0812, 0.1039, 0.0060, 0.2332, 0.5659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 130, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0769e-05, 6.3349e-05, 3.7476e-01, 4.3216e-05, 3.4344e-01, 3.5247e-05,
        2.8165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.913

[Epoch: 131, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0036, 0.0044, 0.0050, 0.0049, 0.0042, 0.9736, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 131, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0377, 0.0368, 0.0061, 0.0412, 0.0424, 0.8292, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 131, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.9681e-03, 9.5669e-05, 6.1703e-02, 6.0681e-01, 6.9295e-03, 8.1739e-03,
        3.0632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 131, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0065, 0.0066, 0.0810, 0.1056, 0.0052, 0.2380, 0.5571],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 131, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.0377e-06, 7.6957e-05, 3.7101e-01, 3.5670e-05, 3.5809e-01, 3.7362e-05,
        2.7074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.933

[Epoch: 132, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0055, 0.0050, 0.0062, 0.0060, 0.9645, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 132, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0318, 0.0324, 0.0049, 0.0322, 0.0299, 0.8644, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 132, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1758e-02, 1.0110e-04, 5.8086e-02, 6.2676e-01, 8.1991e-03, 6.9979e-03,
        2.8810e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 132, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0039, 0.0732, 0.1038, 0.0045, 0.2112, 0.5987],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 132, batch: 200/201] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.5407e-06, 8.5081e-05, 3.6731e-01, 5.1361e-05, 3.3913e-01, 2.7366e-05,
        2.9339e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.906

[Epoch: 133, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0067, 0.0030, 0.0052, 0.0042, 0.0077, 0.9691, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.039

[Epoch: 133, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0419, 0.0435, 0.0073, 0.0394, 0.0317, 0.8303, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.045

[Epoch: 133, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0105e-02, 1.1644e-04, 6.2591e-02, 6.3058e-01, 5.1020e-03, 6.6818e-03,
        2.8482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 133, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0053, 0.0720, 0.1017, 0.0046, 0.2349, 0.5757],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 133, batch: 200/201] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.6256e-06, 1.4601e-04, 3.6956e-01, 3.9057e-05, 3.6955e-01, 3.5092e-05,
        2.6066e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.950

[Epoch: 134, batch: 40/201] total loss per batch: 0.621
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0077, 0.0051, 0.0058, 0.0067, 0.0027, 0.9647, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.087

[Epoch: 134, batch: 80/201] total loss per batch: 0.654
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0211, 0.0231, 0.0055, 0.0406, 0.0533, 0.8489, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 134, batch: 120/201] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.2185e-03, 1.1250e-04, 6.1809e-02, 6.4555e-01, 5.0448e-03, 4.4949e-03,
        2.7577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 134, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0035, 0.0034, 0.0709, 0.0847, 0.0063, 0.2369, 0.5944],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 134, batch: 200/201] total loss per batch: 0.669
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.1164e-06, 5.3479e-05, 3.6966e-01, 1.8391e-05, 3.3466e-01, 3.4170e-05,
        2.9557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 135, batch: 40/201] total loss per batch: 0.625
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0043, 0.0071, 0.0097, 0.0087, 0.0077, 0.9536, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.071

[Epoch: 135, batch: 80/201] total loss per batch: 0.655
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0221, 0.0385, 0.0091, 0.0365, 0.0399, 0.8488, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 135, batch: 120/201] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([3.5673e-02, 5.6089e-05, 4.6917e-02, 5.7441e-01, 4.6071e-03, 3.4058e-03,
        3.3493e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.040

[Epoch: 135, batch: 160/201] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0057, 0.0054, 0.0767, 0.1184, 0.0063, 0.2027, 0.5847],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 135, batch: 200/201] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4472e-05, 6.8345e-05, 3.8988e-01, 7.1489e-05, 3.3275e-01, 3.5227e-05,
        2.7718e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.903

[Epoch: 136, batch: 40/201] total loss per batch: 0.616
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0042, 0.0041, 0.0042, 0.0050, 0.9726, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 136, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0568, 0.0398, 0.0039, 0.0305, 0.0315, 0.8310, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 136, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([4.3900e-03, 7.0892e-05, 4.3755e-02, 5.9219e-01, 4.4091e-03, 3.4532e-03,
        3.5174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 136, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0082, 0.0058, 0.0869, 0.1148, 0.0085, 0.2243, 0.5514],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 136, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0891e-05, 4.9930e-05, 3.6752e-01, 2.8041e-05, 3.4629e-01, 3.7464e-05,
        2.8606e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 137, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0040, 0.0052, 0.0028, 0.0051, 0.0054, 0.9718, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.045

[Epoch: 137, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0288, 0.0362, 0.0046, 0.0410, 0.0339, 0.8495, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 137, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.2260e-03, 7.9675e-05, 5.6030e-02, 6.3573e-01, 6.2035e-03, 5.9151e-03,
        2.8881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 137, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0065, 0.0065, 0.0788, 0.1153, 0.0072, 0.2223, 0.5634],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 137, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2797e-05, 5.2464e-05, 3.6436e-01, 2.4013e-05, 3.4571e-01, 3.2022e-05,
        2.8980e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.921

[Epoch: 138, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.0038, 0.0056, 0.0053, 0.9696, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.045

[Epoch: 138, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0339, 0.0317, 0.0047, 0.0389, 0.0349, 0.8499, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 138, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.3455e-03, 8.4856e-05, 5.6946e-02, 6.4344e-01, 6.0058e-03, 5.6589e-03,
        2.7952e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 138, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0055, 0.0050, 0.0749, 0.1076, 0.0062, 0.2220, 0.5789],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 138, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1065e-05, 5.0287e-05, 3.6920e-01, 2.2969e-05, 3.4488e-01, 2.8748e-05,
        2.8580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.926

[Epoch: 139, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0051, 0.0036, 0.0050, 0.0052, 0.9720, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.045

[Epoch: 139, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0364, 0.0342, 0.0053, 0.0373, 0.0360, 0.8448, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 139, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.2737e-03, 7.7448e-05, 5.6504e-02, 6.3443e-01, 5.8379e-03, 5.4077e-03,
        2.8947e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 139, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0049, 0.0783, 0.1096, 0.0061, 0.2292, 0.5665],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 139, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.3140e-06, 4.9582e-05, 3.7170e-01, 1.9629e-05, 3.4759e-01, 2.7244e-05,
        2.8061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 140, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0045, 0.0050, 0.0041, 0.0054, 0.0051, 0.9710, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 140, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0347, 0.0353, 0.0049, 0.0359, 0.0353, 0.8479, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 140, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.8186e-03, 7.2707e-05, 5.6049e-02, 6.3610e-01, 5.6960e-03, 5.2113e-03,
        2.8806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 140, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0048, 0.0780, 0.1037, 0.0060, 0.2226, 0.5799],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 140, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.1656e-06, 4.9404e-05, 3.7348e-01, 1.9975e-05, 3.4534e-01, 2.7372e-05,
        2.8107e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 141, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0043, 0.0052, 0.0050, 0.9703, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 141, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0356, 0.0331, 0.0050, 0.0353, 0.0345, 0.8511, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 141, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1183e-03, 6.7796e-05, 5.4503e-02, 6.2268e-01, 5.8124e-03, 5.2370e-03,
        3.0258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 141, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0050, 0.0799, 0.1044, 0.0060, 0.2228, 0.5767],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 141, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.7970e-06, 4.2712e-05, 3.7150e-01, 1.7012e-05, 3.4521e-01, 2.4728e-05,
        2.8320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 142, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0048, 0.0054, 0.0052, 0.9698, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 142, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0350, 0.0367, 0.0050, 0.0349, 0.0361, 0.8464, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 142, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.4571e-03, 5.9682e-05, 5.3869e-02, 6.1996e-01, 5.5099e-03, 5.1197e-03,
        3.0603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 142, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0045, 0.0779, 0.1034, 0.0053, 0.2275, 0.5762],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 142, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.9514e-06, 4.1117e-05, 3.7617e-01, 1.6867e-05, 3.4529e-01, 2.2967e-05,
        2.7845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 143, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0048, 0.0049, 0.0053, 0.9700, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 143, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0358, 0.0342, 0.0051, 0.0355, 0.0356, 0.8487, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 143, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1413e-03, 5.4171e-05, 5.3495e-02, 6.3041e-01, 5.2957e-03, 4.8722e-03,
        2.9673e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 143, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0051, 0.0809, 0.1060, 0.0057, 0.2203, 0.5772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 143, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.9300e-06, 3.7417e-05, 3.7291e-01, 1.7015e-05, 3.4990e-01, 2.2504e-05,
        2.7711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 144, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0049, 0.0050, 0.0052, 0.0050, 0.9703, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 144, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0347, 0.0361, 0.0048, 0.0344, 0.0346, 0.8500, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 144, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3034e-03, 5.5639e-05, 5.3685e-02, 6.3960e-01, 5.3128e-03, 5.0622e-03,
        2.8698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 144, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0047, 0.0816, 0.1076, 0.0056, 0.2258, 0.5696],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 144, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.9742e-06, 3.7886e-05, 3.7470e-01, 1.4918e-05, 3.3647e-01, 2.3406e-05,
        2.8874e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 145, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0055, 0.0051, 0.0046, 0.0048, 0.0051, 0.9699, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 145, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0348, 0.0326, 0.0049, 0.0358, 0.0355, 0.8514, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 145, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.5439e-03, 5.1971e-05, 5.6426e-02, 6.2790e-01, 4.9280e-03, 4.8841e-03,
        2.9626e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 145, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0051, 0.0771, 0.0986, 0.0050, 0.2176, 0.5919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 145, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.9208e-06, 2.9211e-05, 3.6999e-01, 1.7126e-05, 3.5702e-01, 2.2552e-05,
        2.7291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 146, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0056, 0.0052, 0.0048, 0.0048, 0.9707, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 146, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0380, 0.0397, 0.0057, 0.0346, 0.0379, 0.8387, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 146, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0619e-02, 5.9731e-05, 5.4497e-02, 6.0834e-01, 5.6826e-03, 5.4666e-03,
        3.1533e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 146, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0038, 0.0892, 0.1113, 0.0050, 0.2420, 0.5439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 146, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0390e-05, 3.9768e-05, 3.7752e-01, 1.5113e-05, 3.3761e-01, 1.8915e-05,
        2.8478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 147, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0055, 0.0059, 0.0057, 0.9654, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 147, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0319, 0.0303, 0.0047, 0.0340, 0.0308, 0.8641, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.050

[Epoch: 147, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.6668e-03, 4.4948e-05, 4.5661e-02, 6.4706e-01, 5.0586e-03, 4.7066e-03,
        2.8981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 147, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0061, 0.0729, 0.0958, 0.0057, 0.2101, 0.6044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 147, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.3745e-06, 4.7055e-05, 3.7674e-01, 2.4522e-05, 3.4785e-01, 3.1081e-05,
        2.7530e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 148, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0044, 0.0047, 0.0036, 0.0039, 0.9746, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.059

[Epoch: 148, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0499, 0.0361, 0.0049, 0.0430, 0.0392, 0.8188, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.021

[Epoch: 148, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1396e-02, 1.0903e-04, 6.2040e-02, 5.9005e-01, 6.2493e-03, 5.0358e-03,
        3.2512e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 148, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0050, 0.0808, 0.1428, 0.0047, 0.2416, 0.5193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 148, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1198e-05, 4.1363e-05, 3.6188e-01, 3.2889e-05, 3.4436e-01, 3.2115e-05,
        2.9364e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.893

[Epoch: 149, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0029, 0.0054, 0.0040, 0.0059, 0.0038, 0.9708, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.064

[Epoch: 149, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0231, 0.0374, 0.0054, 0.0230, 0.0385, 0.8686, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 149, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0040e-02, 5.6626e-05, 5.0292e-02, 6.6220e-01, 4.6875e-03, 7.1156e-03,
        2.6561e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 149, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0040, 0.0061, 0.0736, 0.0768, 0.0049, 0.2109, 0.6237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 149, batch: 200/201] total loss per batch: 0.659
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.4108e-05, 4.9646e-05, 3.6968e-01, 2.5262e-05, 3.3976e-01, 3.4190e-05,
        2.9044e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.936

[Epoch: 150, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0087, 0.0051, 0.0067, 0.0046, 0.0043, 0.9659, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 150, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0366, 0.0283, 0.0066, 0.0391, 0.0401, 0.8428, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 150, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.8635e-03, 6.8903e-05, 6.3150e-02, 6.0862e-01, 6.1703e-03, 5.0381e-03,
        3.0709e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 150, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0041, 0.0046, 0.0835, 0.1167, 0.0060, 0.2395, 0.5456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 150, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.4535e-06, 5.7892e-05, 3.7631e-01, 2.8027e-05, 3.4161e-01, 3.2272e-05,
        2.8195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.919

[Epoch: 151, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0033, 0.0049, 0.0034, 0.0035, 0.0062, 0.9753, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 151, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0405, 0.0382, 0.0041, 0.0350, 0.0278, 0.8501, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 151, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0306e-02, 5.8301e-05, 4.9781e-02, 6.1803e-01, 4.9897e-03, 4.1129e-03,
        3.1272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 151, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0076, 0.0776, 0.1121, 0.0054, 0.2326, 0.5597],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 151, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.2388e-05, 7.4820e-05, 3.6787e-01, 3.0845e-05, 3.5776e-01, 3.6007e-05,
        2.7422e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 152, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0035, 0.0048, 0.0051, 0.0042, 0.0054, 0.9729, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 152, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0358, 0.0393, 0.0059, 0.0368, 0.0357, 0.8415, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 152, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.0901e-03, 5.7466e-05, 5.8101e-02, 6.3901e-01, 5.0246e-03, 5.7475e-03,
        2.8297e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 152, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0041, 0.0054, 0.0743, 0.1101, 0.0056, 0.2146, 0.5858],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 152, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.3926e-06, 5.5531e-05, 3.7648e-01, 2.4298e-05, 3.4352e-01, 2.7810e-05,
        2.7989e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 153, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0061, 0.0053, 0.0049, 0.0044, 0.0055, 0.9689, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 153, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0330, 0.0321, 0.0045, 0.0348, 0.0357, 0.8554, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 153, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0009e-02, 6.7650e-05, 5.8541e-02, 6.3176e-01, 5.0756e-03, 5.4334e-03,
        2.8911e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 153, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0054, 0.0831, 0.1053, 0.0055, 0.2338, 0.5619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 153, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.1644e-06, 5.4023e-05, 3.7316e-01, 2.3042e-05, 3.4811e-01, 2.8413e-05,
        2.7862e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 154, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.0055, 0.0050, 0.0061, 0.9676, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.055

[Epoch: 154, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0362, 0.0344, 0.0047, 0.0349, 0.0368, 0.8486, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 154, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0023e-02, 5.6107e-05, 5.4257e-02, 6.1461e-01, 5.1813e-03, 5.7616e-03,
        3.1011e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 154, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0057, 0.0800, 0.1044, 0.0058, 0.2227, 0.5766],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 154, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.2981e-06, 4.9560e-05, 3.7536e-01, 2.3118e-05, 3.4458e-01, 2.5790e-05,
        2.7996e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.921

[Epoch: 155, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0049, 0.0046, 0.0054, 0.9711, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 155, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0360, 0.0369, 0.0047, 0.0351, 0.0345, 0.8479, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 155, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.9029e-03, 5.5964e-05, 5.4383e-02, 6.1879e-01, 5.2479e-03, 4.9966e-03,
        3.0662e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 155, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0051, 0.0793, 0.1028, 0.0057, 0.2242, 0.5778],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 155, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.0349e-06, 4.1939e-05, 3.7184e-01, 1.9667e-05, 3.4951e-01, 2.3706e-05,
        2.7856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 156, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0046, 0.0045, 0.0046, 0.9715, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.054

[Epoch: 156, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0316, 0.0322, 0.0047, 0.0343, 0.0341, 0.8586, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 156, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.7293e-03, 4.8988e-05, 5.4538e-02, 6.2184e-01, 4.9986e-03, 5.4033e-03,
        3.0344e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 156, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0055, 0.0790, 0.1048, 0.0053, 0.2274, 0.5728],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 156, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.2135e-06, 3.5823e-05, 3.7766e-01, 1.7793e-05, 3.4575e-01, 2.1597e-05,
        2.7652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 157, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.0050, 0.0048, 0.0051, 0.9700, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 157, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0385, 0.0380, 0.0054, 0.0368, 0.0368, 0.8397, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 157, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.5454e-03, 4.7773e-05, 5.6020e-02, 6.3657e-01, 5.3425e-03, 5.1290e-03,
        2.8734e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 157, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0044, 0.0776, 0.1078, 0.0053, 0.2242, 0.5759],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 157, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.1090e-06, 3.4656e-05, 3.7283e-01, 1.5562e-05, 3.4137e-01, 1.7412e-05,
        2.8573e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 158, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0047, 0.0053, 0.0054, 0.0055, 0.9687, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 158, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0319, 0.0310, 0.0050, 0.0338, 0.0320, 0.8616, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 158, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.9494e-03, 4.9314e-05, 5.4893e-02, 6.0914e-01, 5.2786e-03, 5.4876e-03,
        3.1521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 158, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0056, 0.0819, 0.1034, 0.0049, 0.2243, 0.5748],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 158, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.8383e-06, 3.3513e-05, 3.7099e-01, 1.4946e-05, 3.4877e-01, 2.1581e-05,
        2.8016e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 159, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.0049, 0.0060, 0.0060, 0.9673, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 159, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0346, 0.0393, 0.0043, 0.0352, 0.0370, 0.8441, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 159, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0159e-02, 4.8755e-05, 5.2373e-02, 6.0536e-01, 5.2249e-03, 5.0054e-03,
        3.2183e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 159, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0043, 0.0756, 0.1076, 0.0051, 0.2320, 0.5702],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 159, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.8675e-06, 3.0388e-05, 3.7316e-01, 1.8253e-05, 3.5107e-01, 1.8653e-05,
        2.7570e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.912

[Epoch: 160, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0041, 0.0067, 0.0040, 0.0048, 0.9703, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 160, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0434, 0.0346, 0.0066, 0.0347, 0.0366, 0.8383, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 160, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1668e-02, 5.1788e-05, 5.7582e-02, 6.4449e-01, 6.7723e-03, 5.8709e-03,
        2.7356e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.039

[Epoch: 160, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0068, 0.0917, 0.1048, 0.0053, 0.2165, 0.5703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 160, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.3132e-06, 4.3702e-05, 3.6905e-01, 2.6995e-05, 3.4157e-01, 2.5753e-05,
        2.8928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 161, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0033, 0.0039, 0.0027, 0.0048, 0.0040, 0.9768, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.042

[Epoch: 161, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0263, 0.0344, 0.0051, 0.0407, 0.0368, 0.8514, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 161, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3236e-03, 5.0000e-05, 7.7516e-02, 6.7325e-01, 6.3173e-03, 7.2009e-03,
        2.2634e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 161, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0039, 0.0033, 0.0535, 0.1040, 0.0041, 0.2369, 0.5941],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 161, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.4700e-06, 3.4805e-05, 3.6629e-01, 2.9692e-05, 3.5144e-01, 4.2907e-05,
        2.8216e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.939

[Epoch: 162, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0052, 0.0056, 0.0057, 0.0045, 0.0051, 0.9694, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.059

[Epoch: 162, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0459, 0.0319, 0.0063, 0.0356, 0.0440, 0.8305, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 162, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0146e-02, 5.9035e-05, 5.6215e-02, 6.0470e-01, 6.8493e-03, 8.0241e-03,
        3.1400e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 162, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0053, 0.0057, 0.1067, 0.1015, 0.0044, 0.2224, 0.5541],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 162, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.1291e-06, 7.5781e-05, 3.7419e-01, 2.8183e-05, 3.4122e-01, 3.2564e-05,
        2.8445e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.896

[Epoch: 163, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0084, 0.0073, 0.0072, 0.0064, 0.0071, 0.9556, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 163, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0212, 0.0311, 0.0033, 0.0289, 0.0230, 0.8884, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 163, batch: 120/201] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.0747e-03, 6.6329e-05, 3.9827e-02, 6.3730e-01, 3.4720e-03, 4.5640e-03,
        3.0669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 163, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0032, 0.0044, 0.0734, 0.1200, 0.0067, 0.2068, 0.5855],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 163, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.1254e-06, 6.7877e-05, 3.7791e-01, 3.9719e-05, 3.3832e-01, 3.5330e-05,
        2.8362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 164, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0037, 0.0049, 0.0039, 0.0041, 0.0041, 0.9765, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 164, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0449, 0.0449, 0.0062, 0.0472, 0.0440, 0.8071, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 164, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0074e-02, 4.9242e-05, 5.1737e-02, 5.9584e-01, 4.6028e-03, 4.8388e-03,
        3.3285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.058

[Epoch: 164, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0100, 0.0796, 0.0951, 0.0055, 0.2283, 0.5763],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 164, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 4
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0903e-05, 5.6888e-05, 3.6541e-01, 3.0963e-05, 3.6646e-01, 5.0741e-05,
        2.6798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.921

[Epoch: 165, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0036, 0.0040, 0.0041, 0.0032, 0.0035, 0.9783, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 165, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0241, 0.0258, 0.0049, 0.0272, 0.0297, 0.8833, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 165, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.5660e-03, 6.4665e-05, 5.3017e-02, 6.2535e-01, 5.2461e-03, 2.6274e-03,
        3.0413e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 165, batch: 160/201] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0080, 0.0098, 0.0908, 0.1130, 0.0043, 0.2278, 0.5464],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.007

[Epoch: 165, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0847e-05, 9.1840e-05, 3.7290e-01, 2.4224e-05, 3.4190e-01, 1.7610e-05,
        2.8506e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.899

[Epoch: 166, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0076, 0.0060, 0.0063, 0.0062, 0.9650, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.047

[Epoch: 166, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0381, 0.0473, 0.0057, 0.0341, 0.0402, 0.8245, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.020

[Epoch: 166, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1868e-02, 5.5335e-05, 4.5368e-02, 6.1725e-01, 7.5340e-03, 4.9712e-03,
        3.1296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.038

[Epoch: 166, batch: 160/201] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0038, 0.0058, 0.0600, 0.1043, 0.0040, 0.2311, 0.5909],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.001

[Epoch: 166, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.1300e-06, 6.4237e-05, 3.6481e-01, 3.2209e-05, 3.5054e-01, 4.0772e-05,
        2.8450e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.911

[Epoch: 167, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0061, 0.0052, 0.0044, 0.0064, 0.0063, 0.9653, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.031

[Epoch: 167, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0359, 0.0313, 0.0055, 0.0344, 0.0400, 0.8456, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 167, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3922e-03, 7.9744e-05, 5.4737e-02, 6.3372e-01, 6.5556e-03, 5.3357e-03,
        2.9018e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 167, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0049, 0.0086, 0.0693, 0.1244, 0.0044, 0.2144, 0.5741],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 167, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.1977e-05, 5.4270e-05, 3.7158e-01, 4.1406e-05, 3.4585e-01, 4.0003e-05,
        2.8242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.918

[Epoch: 168, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.0044, 0.0057, 0.0048, 0.9695, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.041

[Epoch: 168, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0311, 0.0329, 0.0056, 0.0307, 0.0332, 0.8610, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 168, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.9686e-03, 8.6171e-05, 5.3565e-02, 6.3064e-01, 5.8438e-03, 4.6167e-03,
        2.9528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 168, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0061, 0.0728, 0.1016, 0.0045, 0.2239, 0.5865],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 168, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.1545e-06, 5.0729e-05, 3.7397e-01, 4.6258e-05, 3.4648e-01, 3.6905e-05,
        2.7941e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 169, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0047, 0.0053, 0.0051, 0.9700, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 169, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0344, 0.0345, 0.0053, 0.0331, 0.0349, 0.8530, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 169, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.9823e-03, 7.1514e-05, 5.4961e-02, 6.2529e-01, 5.9735e-03, 4.8960e-03,
        2.9883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 169, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0059, 0.0810, 0.1064, 0.0046, 0.2273, 0.5702],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 169, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.3884e-06, 4.7700e-05, 3.7529e-01, 3.6765e-05, 3.4324e-01, 2.7326e-05,
        2.8135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.913

[Epoch: 170, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0065, 0.0058, 0.0055, 0.0059, 0.0052, 0.9652, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.049

[Epoch: 170, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0352, 0.0355, 0.0053, 0.0342, 0.0326, 0.8518, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 170, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.8731e-03, 7.0121e-05, 5.4612e-02, 6.2780e-01, 5.7219e-03, 4.8741e-03,
        2.9705e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.048

[Epoch: 170, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0050, 0.0811, 0.1032, 0.0045, 0.2182, 0.5831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 170, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.1588e-06, 4.8046e-05, 3.7468e-01, 3.2790e-05, 3.4415e-01, 2.8613e-05,
        2.8105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 171, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0053, 0.0048, 0.0049, 0.0052, 0.9714, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.053

[Epoch: 171, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0332, 0.0342, 0.0052, 0.0356, 0.0354, 0.8509, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 171, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0446e-02, 6.4127e-05, 5.4431e-02, 6.2542e-01, 5.5533e-03, 4.9503e-03,
        2.9914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 171, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0059, 0.0802, 0.1075, 0.0050, 0.2266, 0.5701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 171, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.1317e-06, 4.0443e-05, 3.7356e-01, 3.2311e-05, 3.4677e-01, 2.6643e-05,
        2.7957e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 172, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0050, 0.0052, 0.0048, 0.9691, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 172, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0377, 0.0371, 0.0053, 0.0370, 0.0348, 0.8428, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 172, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0424e-02, 5.9087e-05, 5.5732e-02, 6.1896e-01, 5.6230e-03, 5.5862e-03,
        3.0361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 172, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0045, 0.0044, 0.0772, 0.1025, 0.0048, 0.2270, 0.5796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 172, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.6935e-06, 3.7487e-05, 3.7107e-01, 3.0012e-05, 3.4464e-01, 2.5465e-05,
        2.8419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.919

[Epoch: 173, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0048, 0.0049, 0.0048, 0.9712, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 173, batch: 80/201] total loss per batch: 0.646
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0326, 0.0343, 0.0051, 0.0343, 0.0351, 0.8535, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 173, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0001e-02, 5.5227e-05, 5.6325e-02, 6.1671e-01, 5.6245e-03, 5.5195e-03,
        3.0577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 173, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0060, 0.0843, 0.1117, 0.0053, 0.2229, 0.5651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 173, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.3420e-06, 3.5288e-05, 3.7453e-01, 2.7001e-05, 3.4622e-01, 2.4932e-05,
        2.7916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 174, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0045, 0.0051, 0.0044, 0.9713, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 174, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0363, 0.0356, 0.0050, 0.0356, 0.0366, 0.8460, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 174, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0673e-02, 5.2518e-05, 5.4739e-02, 6.3539e-01, 5.2321e-03, 5.2761e-03,
        2.8864e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 174, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0043, 0.0728, 0.0935, 0.0047, 0.2229, 0.5967],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 174, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.3325e-06, 2.9040e-05, 3.6901e-01, 2.5853e-05, 3.4925e-01, 2.2949e-05,
        2.8166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.926

[Epoch: 175, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0053, 0.0048, 0.0057, 0.9693, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.050

[Epoch: 175, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0354, 0.0310, 0.0056, 0.0356, 0.0346, 0.8533, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 175, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0548e-02, 6.2415e-05, 5.9872e-02, 6.2351e-01, 6.4661e-03, 5.9339e-03,
        2.9361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 175, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0043, 0.0065, 0.0897, 0.1204, 0.0052, 0.2314, 0.5425],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 175, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.0666e-06, 3.3988e-05, 3.9119e-01, 2.5147e-05, 3.3481e-01, 2.4810e-05,
        2.7391e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 176, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0040, 0.0044, 0.0058, 0.0052, 0.0045, 0.9686, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.054

[Epoch: 176, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0358, 0.0412, 0.0055, 0.0393, 0.0389, 0.8324, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 176, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.2321e-03, 5.4952e-05, 5.9094e-02, 5.9866e-01, 5.8620e-03, 5.4160e-03,
        3.2168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 176, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0057, 0.0039, 0.0681, 0.0955, 0.0052, 0.2250, 0.5965],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 176, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([4.5789e-06, 2.2185e-05, 3.6610e-01, 2.8779e-05, 3.5848e-01, 2.5145e-05,
        2.7534e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.929

[Epoch: 177, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0042, 0.0049, 0.0038, 0.0037, 0.0039, 0.9762, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.055

[Epoch: 177, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0349, 0.0323, 0.0060, 0.0341, 0.0318, 0.8564, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 177, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.4100e-03, 5.4576e-05, 4.6061e-02, 6.4398e-01, 4.4709e-03, 4.1327e-03,
        2.9189e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 177, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0040, 0.0059, 0.0904, 0.1026, 0.0046, 0.2211, 0.5714],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 177, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.0533e-06, 3.2076e-05, 3.7520e-01, 3.3162e-05, 3.3675e-01, 2.6819e-05,
        2.8795e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.920

[Epoch: 178, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0066, 0.0055, 0.0047, 0.0048, 0.0043, 0.9678, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.046

[Epoch: 178, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0408, 0.0399, 0.0047, 0.0363, 0.0362, 0.8376, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 178, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0800e-02, 8.4699e-05, 5.1333e-02, 6.1614e-01, 6.0293e-03, 6.9205e-03,
        3.0870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.049

[Epoch: 178, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0037, 0.0724, 0.1172, 0.0044, 0.2210, 0.5765],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 178, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.3335e-06, 3.6624e-05, 3.7704e-01, 3.9185e-05, 3.4985e-01, 2.9483e-05,
        2.7299e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.923

[Epoch: 179, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0057, 0.0050, 0.0056, 0.0067, 0.0073, 0.9655, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.063

[Epoch: 179, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0222, 0.0287, 0.0042, 0.0302, 0.0400, 0.8674, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 179, batch: 120/201] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.0387e-03, 5.5290e-05, 4.6141e-02, 6.5060e-01, 4.6280e-03, 4.5589e-03,
        2.8598e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.051

[Epoch: 179, batch: 160/201] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0048, 0.0073, 0.0894, 0.1102, 0.0056, 0.2307, 0.5519],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 179, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.7189e-06, 2.5568e-05, 3.7018e-01, 4.1199e-05, 3.3643e-01, 2.8361e-05,
        2.9329e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.917

[Epoch: 180, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0064, 0.0060, 0.0053, 0.0054, 0.9660, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 180, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0463, 0.0429, 0.0052, 0.0442, 0.0358, 0.8205, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 180, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.5796e-03, 4.3106e-05, 5.0477e-02, 6.5610e-01, 4.3451e-03, 4.4061e-03,
        2.7605e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 180, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0062, 0.0805, 0.1034, 0.0053, 0.2180, 0.5824],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 180, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([9.7590e-06, 4.8737e-05, 3.7138e-01, 6.4547e-05, 3.5378e-01, 3.0485e-05,
        2.7469e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 181, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0044, 0.0056, 0.0056, 0.0057, 0.0047, 0.9684, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.061

[Epoch: 181, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0300, 0.0298, 0.0058, 0.0313, 0.0299, 0.8689, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 181, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0659e-02, 5.2876e-05, 5.9433e-02, 6.0664e-01, 5.4461e-03, 4.9144e-03,
        3.1286e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.041

[Epoch: 181, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0054, 0.0774, 0.1161, 0.0061, 0.2124, 0.5780],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 181, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.8565e-06, 3.9532e-05, 3.6844e-01, 4.8057e-05, 3.5284e-01, 3.0731e-05,
        2.7860e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 182, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0062, 0.0058, 0.0056, 0.0067, 0.0073, 0.9632, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 182, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0325, 0.0380, 0.0057, 0.0376, 0.0393, 0.8422, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 182, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.4121e-03, 4.8765e-05, 5.3028e-02, 6.2881e-01, 5.2407e-03, 5.4959e-03,
        2.9796e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 182, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0048, 0.0823, 0.1083, 0.0060, 0.2308, 0.5627],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 182, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.6743e-06, 3.6799e-05, 3.7723e-01, 3.7948e-05, 3.4384e-01, 2.2031e-05,
        2.7883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.923

[Epoch: 183, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0039, 0.0043, 0.0046, 0.0051, 0.0054, 0.9721, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 183, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0335, 0.0369, 0.0053, 0.0374, 0.0321, 0.8503, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 183, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0750e-02, 5.1116e-05, 5.4506e-02, 6.1689e-01, 5.1649e-03, 5.6783e-03,
        3.0696e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 183, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0050, 0.0050, 0.0756, 0.0967, 0.0055, 0.2314, 0.5809],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 183, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.2421e-06, 2.6710e-05, 3.7759e-01, 2.9817e-05, 3.4553e-01, 1.8880e-05,
        2.7680e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.913

[Epoch: 184, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0047, 0.0047, 0.0051, 0.9717, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 184, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0361, 0.0341, 0.0053, 0.0336, 0.0380, 0.8481, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 184, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0409e-02, 4.9028e-05, 5.6338e-02, 6.1113e-01, 5.5036e-03, 5.6970e-03,
        3.1088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 184, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0047, 0.0047, 0.0781, 0.1043, 0.0046, 0.2207, 0.5827],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 184, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.7138e-06, 2.9092e-05, 3.7696e-01, 2.7581e-05, 3.4640e-01, 1.8358e-05,
        2.7656e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.930

[Epoch: 185, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0050, 0.0055, 0.0054, 0.0050, 0.0052, 0.9680, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 185, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0346, 0.0364, 0.0054, 0.0342, 0.0340, 0.8505, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 185, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0922e-02, 4.5496e-05, 5.7038e-02, 6.2558e-01, 6.0669e-03, 5.8427e-03,
        2.9450e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 185, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0046, 0.0047, 0.0814, 0.0991, 0.0044, 0.2281, 0.5776],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 185, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.5091e-06, 2.7044e-05, 3.7608e-01, 2.6452e-05, 3.4613e-01, 1.6645e-05,
        2.7771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.915

[Epoch: 186, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0048, 0.0046, 0.0044, 0.9724, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.056

[Epoch: 186, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0363, 0.0343, 0.0053, 0.0341, 0.0366, 0.8485, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 186, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0750e-02, 4.3065e-05, 5.6965e-02, 6.2260e-01, 5.5947e-03, 5.7648e-03,
        2.9828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 186, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0046, 0.0789, 0.1119, 0.0042, 0.2234, 0.5728],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 186, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.0400e-06, 2.7705e-05, 3.6858e-01, 2.8986e-05, 3.4845e-01, 1.9926e-05,
        2.8289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.928

[Epoch: 187, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0059, 0.0050, 0.0053, 0.0050, 0.0050, 0.9693, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 187, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0347, 0.0349, 0.0055, 0.0368, 0.0322, 0.8508, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 187, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.1100e-02, 3.3697e-05, 5.7123e-02, 6.2335e-01, 5.9114e-03, 5.4778e-03,
        2.9700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 187, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0051, 0.0053, 0.0820, 0.1032, 0.0045, 0.2319, 0.5681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 187, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.4351e-06, 3.1930e-05, 3.7043e-01, 4.4287e-05, 3.4970e-01, 3.1946e-05,
        2.7975e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 188, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0046, 0.0054, 0.0054, 0.9680, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.057

[Epoch: 188, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0346, 0.0376, 0.0051, 0.0400, 0.0402, 0.8362, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 188, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([8.9152e-03, 3.8973e-05, 5.7328e-02, 6.1294e-01, 4.8601e-03, 5.3204e-03,
        3.1060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 188, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0042, 0.0045, 0.0809, 0.1050, 0.0045, 0.2174, 0.5835],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 188, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.7110e-06, 2.5772e-05, 3.6787e-01, 4.3304e-05, 3.5313e-01, 2.3283e-05,
        2.7890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 189, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0046, 0.0038, 0.0050, 0.0043, 0.0050, 0.9741, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 189, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0321, 0.0326, 0.0040, 0.0256, 0.0268, 0.8753, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 189, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0101e-02, 4.6275e-05, 4.6029e-02, 6.4960e-01, 4.5387e-03, 4.4383e-03,
        2.8524e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.046

[Epoch: 189, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0052, 0.0775, 0.1168, 0.0058, 0.2426, 0.5466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 189, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.7848e-06, 2.5438e-05, 3.6969e-01, 3.2059e-05, 3.3849e-01, 2.7283e-05,
        2.9172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.933

[Epoch: 190, batch: 40/201] total loss per batch: 0.613
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0041, 0.0056, 0.0041, 0.0034, 0.0051, 0.9711, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.045

[Epoch: 190, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0393, 0.0364, 0.0057, 0.0405, 0.0543, 0.8195, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 190, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0296e-02, 7.8153e-05, 5.6860e-02, 6.4810e-01, 4.9837e-03, 5.0527e-03,
        2.7463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.047

[Epoch: 190, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0062, 0.0722, 0.0913, 0.0055, 0.2054, 0.6141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 190, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.6778e-06, 3.2881e-05, 3.9115e-01, 4.1774e-05, 3.4204e-01, 2.4435e-05,
        2.6671e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.907

[Epoch: 191, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0019, 0.0033, 0.0044, 0.0031, 0.0028, 0.9821, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.070

[Epoch: 191, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0343, 0.0316, 0.0079, 0.0315, 0.0262, 0.8628, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 191, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3463e-03, 4.3253e-05, 4.6951e-02, 5.9906e-01, 5.0079e-03, 4.4300e-03,
        3.3516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 191, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0073, 0.0987, 0.1174, 0.0054, 0.2518, 0.5139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 191, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.7711e-06, 2.6250e-05, 3.5612e-01, 2.1840e-05, 3.4622e-01, 1.2910e-05,
        2.9760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.941

[Epoch: 192, batch: 40/201] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0101, 0.0066, 0.0110, 0.0142, 0.0098, 0.9366, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.048

[Epoch: 192, batch: 80/201] total loss per batch: 0.650
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0377, 0.0371, 0.0054, 0.0284, 0.0307, 0.8558, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 192, batch: 120/201] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([7.7564e-03, 6.7569e-05, 5.7587e-02, 5.7401e-01, 5.1401e-03, 6.2935e-03,
        3.4915e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.044

[Epoch: 192, batch: 160/201] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0056, 0.0043, 0.0657, 0.0928, 0.0049, 0.2146, 0.6121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 192, batch: 200/201] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.0131e-06, 4.8332e-05, 3.7647e-01, 3.6586e-05, 3.4180e-01, 2.7035e-05,
        2.8162e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.914

[Epoch: 193, batch: 40/201] total loss per batch: 0.614
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0067, 0.0069, 0.0056, 0.0044, 0.0079, 0.9646, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.040

[Epoch: 193, batch: 80/201] total loss per batch: 0.649
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0266, 0.0322, 0.0066, 0.0368, 0.0340, 0.8586, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 193, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.5819e-03, 1.0478e-04, 6.0461e-02, 6.2372e-01, 5.3299e-03, 5.4017e-03,
        2.9541e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.050

[Epoch: 193, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0055, 0.0084, 0.0789, 0.1091, 0.0060, 0.2240, 0.5681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 193, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0711e-05, 4.6991e-05, 3.6626e-01, 4.4904e-05, 3.4533e-01, 2.3272e-05,
        2.8828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.923

[Epoch: 194, batch: 40/201] total loss per batch: 0.612
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0056, 0.0041, 0.0040, 0.0038, 0.0065, 0.9726, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.051

[Epoch: 194, batch: 80/201] total loss per batch: 0.648
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0357, 0.0336, 0.0046, 0.0344, 0.0331, 0.8523, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 194, batch: 120/201] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0478e-02, 7.4042e-05, 5.4588e-02, 6.2720e-01, 5.0664e-03, 5.2940e-03,
        2.9730e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 194, batch: 160/201] total loss per batch: 0.665
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0059, 0.0050, 0.0827, 0.1058, 0.0058, 0.2317, 0.5631],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 194, batch: 200/201] total loss per batch: 0.657
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([1.0413e-05, 4.4894e-05, 3.8426e-01, 4.6704e-05, 3.4099e-01, 2.6369e-05,
        2.7463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.922

[Epoch: 195, batch: 40/201] total loss per batch: 0.611
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0053, 0.0056, 0.0039, 0.0047, 0.0055, 0.9684, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.045

[Epoch: 195, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0378, 0.0363, 0.0062, 0.0340, 0.0347, 0.8462, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 195, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0792e-02, 7.6639e-05, 5.7857e-02, 6.2121e-01, 5.6208e-03, 5.1470e-03,
        2.9929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 195, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0055, 0.0053, 0.0753, 0.1079, 0.0050, 0.2218, 0.5793],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 195, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.7315e-06, 3.8461e-05, 3.7941e-01, 4.3194e-05, 3.4188e-01, 2.5717e-05,
        2.7859e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 196, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0045, 0.0039, 0.0035, 0.0047, 0.9747, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 196, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0371, 0.0376, 0.0059, 0.0393, 0.0386, 0.8349, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 196, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([1.0732e-02, 7.0309e-05, 5.7346e-02, 6.2129e-01, 5.4414e-03, 5.6399e-03,
        2.9948e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 196, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0048, 0.0762, 0.1036, 0.0050, 0.2237, 0.5815],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 196, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([8.2590e-06, 2.9685e-05, 3.7551e-01, 3.2746e-05, 3.4481e-01, 2.1777e-05,
        2.7959e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.919

[Epoch: 197, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0038, 0.0050, 0.0047, 0.0043, 0.0046, 0.9722, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 197, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0326, 0.0340, 0.0056, 0.0360, 0.0339, 0.8532, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 197, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.6220e-03, 6.5116e-05, 5.5219e-02, 6.2062e-01, 5.2298e-03, 5.1050e-03,
        3.0414e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.045

[Epoch: 197, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0057, 0.0055, 0.0881, 0.1052, 0.0049, 0.2270, 0.5636],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 197, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([7.2303e-06, 3.1397e-05, 3.7872e-01, 3.3459e-05, 3.4132e-01, 2.3616e-05,
        2.7987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 198, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0046, 0.0045, 0.0048, 0.9714, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.052

[Epoch: 198, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0362, 0.0340, 0.0054, 0.0360, 0.0340, 0.8489, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 198, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.1981e-03, 6.1702e-05, 5.4276e-02, 6.2560e-01, 4.5930e-03, 4.7107e-03,
        3.0156e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 198, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0052, 0.0047, 0.0750, 0.1021, 0.0051, 0.2235, 0.5844],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 198, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.5885e-06, 2.2708e-05, 3.7483e-01, 3.0659e-05, 3.4125e-01, 1.8611e-05,
        2.8385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.925

[Epoch: 199, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0047, 0.0052, 0.0055, 0.0046, 0.0044, 0.9707, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.054

[Epoch: 199, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0314, 0.0359, 0.0045, 0.0355, 0.0309, 0.8573, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 199, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.7081e-03, 5.4444e-05, 5.1627e-02, 6.3159e-01, 4.9408e-03, 4.3953e-03,
        2.9768e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.042

[Epoch: 199, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0055, 0.0055, 0.0825, 0.1105, 0.0051, 0.2250, 0.5659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 199, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([6.8159e-06, 2.4642e-05, 3.7386e-01, 2.7073e-05, 3.4636e-01, 1.9833e-05,
        2.7971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

[Epoch: 200, batch: 40/201] total loss per batch: 0.610
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0049, 0.0055, 0.0056, 0.9668, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.052 -0.047

[Epoch: 200, batch: 80/201] total loss per batch: 0.647
Policy (actual, predicted): 5 5
Policy data: tensor([0.0350, 0.0350, 0.0050, 0.0350, 0.0350, 0.8500, 0.0050])
Policy pred: tensor([0.0346, 0.0352, 0.0049, 0.0350, 0.0392, 0.8460, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 200, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0000, 0.0550, 0.6250, 0.0050, 0.0050, 0.3000])
Policy pred: tensor([9.3964e-03, 5.5817e-05, 5.2702e-02, 6.2426e-01, 5.0517e-03, 4.2255e-03,
        3.0431e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.045 0.043

[Epoch: 200, batch: 160/201] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0800, 0.1050, 0.0050, 0.2250, 0.5750])
Policy pred: tensor([0.0054, 0.0047, 0.0755, 0.1011, 0.0050, 0.2213, 0.5871],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 200, batch: 200/201] total loss per batch: 0.656
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0000, 0.3750, 0.0000, 0.3450, 0.0000, 0.2800])
Policy pred: tensor([5.2317e-06, 2.3385e-05, 3.7930e-01, 2.2972e-05, 3.4482e-01, 1.8256e-05,
        2.7581e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.924 0.924

