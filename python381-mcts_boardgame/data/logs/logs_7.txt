Training set samples: 6220
Batch size: 32
[Epoch: 1, batch: 39/195] total loss per batch: 1.074
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([6.6856e-01, 4.4813e-02, 3.6104e-02, 9.2477e-08, 7.1075e-02, 1.7922e-01,
        2.2187e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.719

[Epoch: 1, batch: 78/195] total loss per batch: 1.053
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.7527e-02, 3.4156e-02, 4.8368e-07, 3.7507e-09, 3.7689e-02, 2.6427e-02,
        8.8420e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.132

[Epoch: 1, batch: 117/195] total loss per batch: 1.005
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.1560e-01, 8.7197e-09, 1.2738e-05, 2.0663e-11, 2.2548e-07, 5.8814e-11,
        1.8438e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.599

[Epoch: 1, batch: 156/195] total loss per batch: 1.105
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0069, 0.0135, 0.9625, 0.0033, 0.0053, 0.0056, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.023

[Epoch: 1, batch: 195/195] total loss per batch: 1.041
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.3472e-01, 1.2943e-01, 3.3814e-05, 1.3768e-06, 2.1105e-01, 4.5751e-01,
        6.7260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.208

[Epoch: 2, batch: 39/195] total loss per batch: 0.813
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([8.5431e-01, 2.3546e-02, 1.9483e-02, 2.8570e-08, 5.1185e-02, 5.1386e-02,
        9.4720e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.804

[Epoch: 2, batch: 78/195] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.8315e-03, 1.2103e-01, 2.6617e-07, 4.9076e-09, 4.0380e-02, 1.9239e-02,
        8.1652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.207

[Epoch: 2, batch: 117/195] total loss per batch: 0.756
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.2218e-01, 1.9821e-08, 1.7541e-05, 2.1350e-11, 2.4111e-07, 9.6270e-11,
        7.7799e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.625

[Epoch: 2, batch: 156/195] total loss per batch: 0.822
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4531e-03, 2.0793e-03, 9.9038e-01, 7.9693e-04, 1.3741e-03, 1.8923e-03,
        1.0229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.046

[Epoch: 2, batch: 195/195] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2400e-01, 1.3345e-01, 1.0692e-04, 3.4398e-06, 2.5685e-01, 4.0588e-01,
        7.9706e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.392

[Epoch: 3, batch: 39/195] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6677e-01, 7.6852e-03, 5.9778e-03, 1.2177e-08, 1.2182e-02, 7.3620e-03,
        2.5943e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.896

[Epoch: 3, batch: 78/195] total loss per batch: 0.672
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.0064e-03, 2.5777e-02, 2.7913e-07, 9.9347e-09, 2.7105e-02, 8.2472e-03,
        9.3786e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.300

[Epoch: 3, batch: 117/195] total loss per batch: 0.634
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6885e-01, 2.0883e-08, 1.4158e-05, 3.2672e-11, 1.3613e-07, 2.1916e-10,
        3.1138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.671

[Epoch: 3, batch: 156/195] total loss per batch: 0.660
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8289e-03, 1.3123e-03, 9.9402e-01, 4.2468e-04, 6.7578e-04, 9.2577e-04,
        8.1041e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.041

[Epoch: 3, batch: 195/195] total loss per batch: 0.636
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.2718e-02, 1.5694e-01, 1.4667e-04, 4.4655e-06, 3.5731e-01, 3.2046e-01,
        1.1242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.496

[Epoch: 4, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7463e-01, 7.0713e-03, 5.2294e-03, 2.6191e-08, 9.3123e-03, 3.7507e-03,
        5.6406e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.901

[Epoch: 4, batch: 78/195] total loss per batch: 0.611
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.4736e-04, 1.2082e-02, 1.7578e-07, 1.5717e-08, 2.0279e-02, 7.8408e-03,
        9.5905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.307

[Epoch: 4, batch: 117/195] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8331e-01, 1.9043e-08, 4.3563e-05, 6.5008e-11, 2.6107e-07, 4.8691e-10,
        1.6645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.707

[Epoch: 4, batch: 156/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2821e-03, 1.6344e-03, 9.9084e-01, 6.7952e-04, 9.4158e-04, 2.2517e-03,
        1.3699e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.049

[Epoch: 4, batch: 195/195] total loss per batch: 0.598
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([2.5555e-02, 1.7279e-01, 2.2980e-04, 4.3410e-06, 1.9598e-01, 3.9646e-01,
        2.0898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.584

[Epoch: 5, batch: 39/195] total loss per batch: 0.568
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5446e-01, 1.8449e-02, 6.6420e-03, 6.6733e-08, 1.1295e-02, 9.1216e-03,
        3.6633e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.934

[Epoch: 5, batch: 78/195] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([9.0297e-04, 1.2736e-02, 3.1423e-07, 3.6740e-08, 2.6346e-02, 6.8467e-03,
        9.5317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.369

[Epoch: 5, batch: 117/195] total loss per batch: 0.571
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9381e-01, 1.6706e-08, 2.2779e-05, 5.6661e-11, 1.3543e-07, 1.6516e-10,
        6.1663e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.562

[Epoch: 5, batch: 156/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0026, 0.9846, 0.0014, 0.0013, 0.0042, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 5, batch: 195/195] total loss per batch: 0.573
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.5945e-03, 4.7450e-02, 8.1785e-05, 1.7713e-06, 6.9126e-01, 1.7198e-01,
        8.0628e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.673

[Epoch: 6, batch: 39/195] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8493e-01, 3.9441e-03, 3.1269e-03, 1.9078e-08, 4.7559e-03, 3.2337e-03,
        6.7751e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.930

[Epoch: 6, batch: 78/195] total loss per batch: 0.580
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([9.0108e-04, 2.5147e-02, 2.5200e-07, 1.9965e-08, 1.6944e-02, 5.0595e-03,
        9.5195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.371

[Epoch: 6, batch: 117/195] total loss per batch: 0.557
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9379e-01, 1.8513e-08, 3.8845e-05, 1.1282e-10, 2.1254e-07, 3.3532e-10,
        6.1695e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.523

[Epoch: 6, batch: 156/195] total loss per batch: 0.572
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0030, 0.9859, 0.0016, 0.0013, 0.0033, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.055

[Epoch: 6, batch: 195/195] total loss per batch: 0.557
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([3.2426e-02, 1.6195e-01, 9.5445e-05, 2.1044e-06, 1.8912e-01, 4.1780e-01,
        1.9861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.706

[Epoch: 7, batch: 39/195] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5113e-01, 1.6257e-02, 8.6416e-03, 6.5636e-08, 1.7644e-02, 6.3076e-03,
        1.6604e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.943

[Epoch: 7, batch: 78/195] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.3533e-03, 1.8713e-02, 3.2567e-07, 5.4650e-08, 1.4945e-02, 4.6223e-03,
        9.6037e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.359

[Epoch: 7, batch: 117/195] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9242e-01, 1.1990e-08, 3.3164e-05, 4.1861e-11, 2.1132e-07, 1.8066e-10,
        7.5497e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 7, batch: 156/195] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0050, 0.0032, 0.9840, 0.0013, 0.0013, 0.0029, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.046

[Epoch: 7, batch: 195/195] total loss per batch: 0.546
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([2.0421e-02, 1.3444e-01, 1.0655e-04, 9.0853e-07, 3.0099e-01, 2.8900e-01,
        2.5505e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.722

[Epoch: 8, batch: 39/195] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7043e-01, 6.7493e-03, 5.2093e-03, 6.5493e-08, 1.1132e-02, 6.4679e-03,
        8.8720e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.940

[Epoch: 8, batch: 78/195] total loss per batch: 0.568
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.3828e-03, 9.2560e-03, 2.5238e-07, 3.0883e-08, 1.2169e-02, 3.4166e-03,
        9.7378e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.408

[Epoch: 8, batch: 117/195] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9235e-01, 2.6978e-08, 8.1883e-05, 1.7592e-10, 3.5618e-07, 2.9614e-10,
        7.5687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.569

[Epoch: 8, batch: 156/195] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0043, 0.9839, 0.0022, 0.0016, 0.0022, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.059

[Epoch: 8, batch: 195/195] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2787e-02, 1.2356e-01, 1.0582e-04, 1.7777e-06, 3.2292e-01, 3.9213e-01,
        1.4850e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.772

[Epoch: 9, batch: 39/195] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7743e-01, 5.5118e-03, 6.0581e-03, 1.5064e-08, 7.0879e-03, 3.9110e-03,
        3.4890e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.956

[Epoch: 9, batch: 78/195] total loss per batch: 0.564
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.6601e-03, 2.7928e-02, 3.4822e-07, 2.9476e-08, 1.5143e-02, 4.0340e-03,
        9.5123e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.346

[Epoch: 9, batch: 117/195] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9383e-01, 1.2316e-08, 2.7791e-05, 3.0780e-11, 1.0684e-07, 9.6062e-11,
        6.1389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.492

[Epoch: 9, batch: 156/195] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0054, 0.0047, 0.9797, 0.0027, 0.0020, 0.0029, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.050

[Epoch: 9, batch: 195/195] total loss per batch: 0.538
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2945e-02, 1.5502e-01, 8.0251e-05, 6.2134e-07, 3.0728e-01, 3.1481e-01,
        2.0987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 10, batch: 39/195] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([8.6908e-01, 2.5960e-02, 1.3334e-02, 9.7910e-08, 7.2126e-02, 1.9450e-02,
        5.1584e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.935

[Epoch: 10, batch: 78/195] total loss per batch: 0.561
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.9037e-03, 2.0273e-02, 1.3761e-07, 2.3511e-08, 1.1043e-02, 5.4579e-03,
        9.6132e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.415

[Epoch: 10, batch: 117/195] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9048e-01, 2.1377e-08, 4.2197e-05, 5.8932e-11, 2.1032e-07, 1.5544e-10,
        9.4795e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.531

[Epoch: 10, batch: 156/195] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0053, 0.9799, 0.0029, 0.0017, 0.0028, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.053

[Epoch: 10, batch: 195/195] total loss per batch: 0.536
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.1280e-03, 1.0813e-01, 9.8311e-05, 1.0717e-06, 3.9322e-01, 3.0221e-01,
        1.8821e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.754

[Epoch: 11, batch: 39/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.9207e-01, 1.9809e-03, 1.7925e-03, 6.4026e-09, 1.4895e-03, 2.6634e-03,
        2.8159e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.958

[Epoch: 11, batch: 78/195] total loss per batch: 0.559
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.7935e-03, 5.2373e-02, 5.0220e-07, 5.9991e-08, 1.2515e-02, 5.2719e-03,
        9.2805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.402

[Epoch: 11, batch: 117/195] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9452e-01, 1.8936e-08, 4.8900e-05, 6.4951e-11, 1.8999e-07, 2.2098e-10,
        5.4321e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 11, batch: 156/195] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0058, 0.0054, 0.9792, 0.0025, 0.0019, 0.0030, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.053

[Epoch: 11, batch: 195/195] total loss per batch: 0.535
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.9710e-03, 1.7550e-01, 6.8985e-05, 4.9379e-07, 2.2087e-01, 3.9492e-01,
        1.9968e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 12, batch: 39/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8635e-01, 3.0210e-03, 2.8487e-03, 8.9658e-09, 3.4516e-03, 4.3301e-03,
        3.4585e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.954

[Epoch: 12, batch: 78/195] total loss per batch: 0.555
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.4786e-03, 7.2429e-03, 1.8083e-07, 1.7887e-08, 9.6630e-03, 3.6888e-03,
        9.7793e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.376

[Epoch: 12, batch: 117/195] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8949e-01, 2.0951e-08, 4.8846e-05, 6.9684e-11, 1.5544e-07, 2.4688e-10,
        1.0460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.522

[Epoch: 12, batch: 156/195] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.0048, 0.9827, 0.0024, 0.0017, 0.0021, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.051

[Epoch: 12, batch: 195/195] total loss per batch: 0.533
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.5259e-03, 1.0847e-01, 8.9739e-05, 6.2598e-07, 4.2873e-01, 2.8329e-01,
        1.7090e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.753

[Epoch: 13, batch: 39/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8304e-01, 3.6651e-03, 2.9436e-03, 1.3307e-08, 4.4264e-03, 5.9274e-03,
        2.1699e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.960

[Epoch: 13, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.3621e-03, 1.2741e-02, 2.6483e-07, 2.3010e-08, 8.9215e-03, 3.0520e-03,
        9.7292e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.387

[Epoch: 13, batch: 117/195] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9445e-01, 1.6449e-08, 4.3570e-05, 5.4140e-11, 1.5530e-07, 2.1520e-10,
        5.5026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.436

[Epoch: 13, batch: 156/195] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0068, 0.0070, 0.9727, 0.0037, 0.0023, 0.0042, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 13, batch: 195/195] total loss per batch: 0.533
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.7959e-03, 1.6393e-01, 6.1539e-05, 8.8380e-07, 2.3203e-01, 3.5475e-01,
        2.4144e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 14, batch: 39/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7889e-01, 5.2593e-03, 3.9836e-03, 1.4062e-08, 5.4130e-03, 6.4452e-03,
        4.8058e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.931

[Epoch: 14, batch: 78/195] total loss per batch: 0.553
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.0371e-03, 2.0996e-02, 1.9081e-07, 2.8745e-08, 7.6884e-03, 4.3413e-03,
        9.6494e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.469

[Epoch: 14, batch: 117/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9188e-01, 2.2891e-08, 5.4468e-05, 1.0303e-10, 1.9101e-07, 4.8825e-10,
        8.0673e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.518

[Epoch: 14, batch: 156/195] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0050, 0.0048, 0.9800, 0.0029, 0.0020, 0.0033, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 14, batch: 195/195] total loss per batch: 0.533
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2289e-02, 1.2999e-01, 6.9878e-05, 8.2126e-07, 3.8114e-01, 3.6103e-01,
        1.1548e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.761

[Epoch: 15, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7256e-01, 4.2661e-03, 4.9047e-03, 1.1365e-08, 8.5776e-03, 9.6927e-03,
        3.0015e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 15, batch: 78/195] total loss per batch: 0.552
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.3836e-03, 2.6127e-02, 2.3692e-07, 2.2408e-08, 6.6918e-03, 4.0702e-03,
        9.6073e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.391

[Epoch: 15, batch: 117/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9554e-01, 2.3341e-08, 4.1156e-05, 8.1213e-11, 9.1781e-08, 1.9653e-10,
        4.4159e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.473

[Epoch: 15, batch: 156/195] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.0059, 0.9752, 0.0035, 0.0027, 0.0041, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 15, batch: 195/195] total loss per batch: 0.532
Policy (actual, predicted): 5 6
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2186e-02, 1.2379e-01, 5.4405e-05, 5.5253e-07, 2.7286e-01, 2.7165e-01,
        3.1946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.789

[Epoch: 16, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7039e-01, 5.7228e-03, 4.6734e-03, 1.9306e-08, 1.0110e-02, 9.1007e-03,
        6.2679e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.940

[Epoch: 16, batch: 78/195] total loss per batch: 0.552
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.1774e-03, 1.7543e-02, 1.9878e-07, 2.0174e-08, 5.0881e-03, 4.1812e-03,
        9.7101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.423

[Epoch: 16, batch: 117/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9201e-01, 6.1335e-08, 4.6000e-05, 1.4810e-10, 1.5275e-07, 3.0370e-10,
        7.9414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.470

[Epoch: 16, batch: 156/195] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0053, 0.9774, 0.0037, 0.0030, 0.0024, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 16, batch: 195/195] total loss per batch: 0.533
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.4219e-03, 1.4989e-01, 8.1792e-05, 7.1392e-07, 3.4351e-01, 3.9436e-01,
        1.0273e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.716

[Epoch: 17, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6967e-01, 4.2638e-03, 4.2722e-03, 1.6260e-08, 1.2387e-02, 9.4032e-03,
        3.4532e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 17, batch: 78/195] total loss per batch: 0.553
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.1843e-03, 2.2636e-02, 1.7707e-07, 1.6516e-08, 6.0892e-03, 3.9613e-03,
        9.6413e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.376

[Epoch: 17, batch: 117/195] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9376e-01, 4.4322e-08, 4.7635e-05, 1.5878e-10, 1.4802e-07, 3.4693e-10,
        6.1949e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 17, batch: 156/195] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0055, 0.0057, 0.9742, 0.0047, 0.0032, 0.0041, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 17, batch: 195/195] total loss per batch: 0.533
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.7203e-02, 1.4247e-01, 6.2887e-05, 8.6111e-07, 2.9749e-01, 2.6764e-01,
        2.7514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 18, batch: 39/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5939e-01, 6.9309e-03, 5.3277e-03, 1.5831e-08, 1.2483e-02, 1.5859e-02,
        3.9278e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.902

[Epoch: 18, batch: 78/195] total loss per batch: 0.553
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.8186e-03, 3.7807e-02, 2.5087e-07, 2.5677e-08, 4.8290e-03, 4.8868e-03,
        9.4966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.458

[Epoch: 18, batch: 117/195] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9404e-01, 4.4966e-08, 4.1462e-05, 1.1456e-10, 9.4188e-08, 3.4831e-10,
        5.9136e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.527

[Epoch: 18, batch: 156/195] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.0056, 0.9758, 0.0042, 0.0031, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 18, batch: 195/195] total loss per batch: 0.532
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.2372e-03, 1.2519e-01, 5.2349e-05, 1.6828e-06, 2.9477e-01, 4.1458e-01,
        1.6017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.744

[Epoch: 19, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7186e-01, 3.9528e-03, 4.5308e-03, 7.5786e-09, 1.0344e-02, 9.3124e-03,
        2.6188e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.953

[Epoch: 19, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9207e-03, 4.8546e-03, 2.4848e-07, 8.8367e-09, 5.1852e-03, 4.6314e-03,
        9.7841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.383

[Epoch: 19, batch: 117/195] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9086e-01, 6.9132e-08, 7.5453e-05, 4.6401e-10, 2.9896e-07, 6.4284e-10,
        9.0628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.448

[Epoch: 19, batch: 156/195] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0048, 0.9795, 0.0031, 0.0037, 0.0024, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 19, batch: 195/195] total loss per batch: 0.531
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0776e-02, 1.3586e-01, 7.0732e-05, 9.1765e-07, 3.8559e-01, 2.6123e-01,
        2.0647e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.819

[Epoch: 20, batch: 39/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5133e-01, 7.0061e-03, 6.3283e-03, 1.8805e-08, 1.9906e-02, 1.5422e-02,
        4.2561e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 20, batch: 78/195] total loss per batch: 0.553
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.3376e-03, 4.5502e-02, 1.0457e-07, 1.4483e-08, 7.7281e-03, 4.1920e-03,
        9.3824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.469

[Epoch: 20, batch: 117/195] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9274e-01, 4.1312e-08, 2.9129e-05, 1.2520e-10, 1.9753e-07, 6.1419e-10,
        7.2270e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.502

[Epoch: 20, batch: 156/195] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0050, 0.9775, 0.0041, 0.0028, 0.0036, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.043

[Epoch: 20, batch: 195/195] total loss per batch: 0.532
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.1525e-02, 1.5487e-01, 5.9282e-05, 1.1562e-06, 2.7276e-01, 3.8266e-01,
        1.7812e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.754

[Epoch: 21, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7639e-01, 5.8121e-03, 3.3316e-03, 3.0777e-09, 8.4196e-03, 6.0412e-03,
        7.2239e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.925

[Epoch: 21, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.7374e-03, 4.3589e-03, 1.8576e-07, 8.1054e-09, 1.1275e-02, 3.4683e-03,
        9.7716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.391

[Epoch: 21, batch: 117/195] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8921e-01, 4.7040e-08, 3.5923e-05, 1.5584e-10, 9.9053e-08, 9.4037e-11,
        1.0749e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.458

[Epoch: 21, batch: 156/195] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0059, 0.0038, 0.9762, 0.0059, 0.0031, 0.0027, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 21, batch: 195/195] total loss per batch: 0.534
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2074e-03, 1.3813e-01, 2.5896e-05, 3.0669e-07, 3.5665e-01, 2.7770e-01,
        2.2129e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.750

[Epoch: 22, batch: 39/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.4855e-01, 7.1845e-03, 4.7053e-03, 9.3364e-09, 1.3026e-02, 2.6528e-02,
        2.1990e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 22, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.6301e-03, 3.3385e-02, 1.2447e-07, 2.4928e-08, 6.9790e-03, 9.8419e-03,
        9.4716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.456

[Epoch: 22, batch: 117/195] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8387e-01, 8.9977e-08, 1.0904e-04, 3.4663e-10, 2.6722e-07, 1.7041e-09,
        1.6024e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.546

[Epoch: 22, batch: 156/195] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0050, 0.9780, 0.0030, 0.0035, 0.0042, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 22, batch: 195/195] total loss per batch: 0.536
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0651e-03, 1.3493e-01, 3.8926e-05, 3.9823e-07, 2.6704e-01, 4.0834e-01,
        1.8358e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.803

[Epoch: 23, batch: 39/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8133e-01, 5.6148e-03, 3.2752e-03, 1.1590e-08, 5.0065e-03, 4.7641e-03,
        7.5389e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.894

[Epoch: 23, batch: 78/195] total loss per batch: 0.557
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.4281e-03, 1.2226e-02, 1.7245e-07, 3.6935e-08, 5.8691e-03, 2.4497e-03,
        9.7403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.384

[Epoch: 23, batch: 117/195] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9700e-01, 1.7355e-08, 8.9281e-05, 5.3015e-11, 3.3943e-07, 2.5760e-10,
        2.9130e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.374

[Epoch: 23, batch: 156/195] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0071, 0.9738, 0.0026, 0.0021, 0.0051, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 23, batch: 195/195] total loss per batch: 0.557
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.3547e-02, 1.5328e-01, 1.6866e-04, 2.2856e-06, 3.7970e-01, 2.5447e-01,
        1.9883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.696

[Epoch: 24, batch: 39/195] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6092e-01, 7.3475e-03, 7.9209e-03, 2.9325e-08, 1.7180e-02, 6.6329e-03,
        2.5216e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.888

[Epoch: 24, batch: 78/195] total loss per batch: 0.578
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.8162e-03, 4.0235e-02, 4.2031e-08, 1.0729e-08, 4.4505e-03, 7.5513e-03,
        9.4495e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.358

[Epoch: 24, batch: 117/195] total loss per batch: 0.560
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8116e-01, 1.2836e-07, 1.1025e-04, 6.9115e-10, 5.1814e-07, 1.7870e-09,
        1.8728e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.645

[Epoch: 24, batch: 156/195] total loss per batch: 0.594
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0042, 0.9793, 0.0031, 0.0043, 0.0018, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.010

[Epoch: 24, batch: 195/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.9220e-03, 9.0971e-02, 1.2271e-04, 1.6545e-06, 3.5314e-01, 4.4231e-01,
        1.0353e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.688

[Epoch: 25, batch: 39/195] total loss per batch: 0.561
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.3785e-01, 7.9368e-03, 3.8091e-03, 1.3253e-08, 3.8283e-02, 1.2107e-02,
        1.1189e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.973

[Epoch: 25, batch: 78/195] total loss per batch: 0.599
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.4738e-03, 7.0724e-02, 1.1823e-06, 6.8006e-08, 8.4297e-03, 3.5609e-03,
        9.1181e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.319

[Epoch: 25, batch: 117/195] total loss per batch: 0.566
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7674e-01, 2.4284e-08, 1.2522e-04, 1.7788e-10, 7.7536e-07, 7.6477e-11,
        2.3133e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 25, batch: 156/195] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0052, 0.9810, 0.0015, 0.0042, 0.0013, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.017

[Epoch: 25, batch: 195/195] total loss per batch: 0.570
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([2.3968e-02, 1.7324e-01, 1.5934e-05, 8.4517e-07, 2.4670e-01, 3.2286e-01,
        2.3321e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 26, batch: 39/195] total loss per batch: 0.554
Policy (actual, predicted): 0 5
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([1.7867e-01, 3.7213e-02, 1.0852e-02, 5.9363e-07, 1.5519e-01, 6.1792e-01,
        1.5007e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.734

[Epoch: 26, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.1755e-03, 5.6212e-03, 1.0654e-07, 4.1712e-09, 6.6954e-03, 5.6175e-03,
        9.7889e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.484

[Epoch: 26, batch: 117/195] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9661e-01, 1.1313e-08, 5.8563e-05, 7.5578e-12, 4.1729e-07, 2.9259e-11,
        3.3331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.505

[Epoch: 26, batch: 156/195] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0064, 0.9781, 0.0029, 0.0037, 0.0016, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.024

[Epoch: 26, batch: 195/195] total loss per batch: 0.546
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.8110e-03, 1.1894e-01, 3.4326e-05, 1.6875e-06, 3.7142e-01, 3.2537e-01,
        1.7642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.807

[Epoch: 27, batch: 39/195] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.2191e-01, 1.8626e-02, 4.5008e-02, 1.3265e-06, 8.5785e-03, 5.4095e-03,
        4.6558e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.972

[Epoch: 27, batch: 78/195] total loss per batch: 0.568
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.6147e-03, 2.1514e-02, 3.5995e-07, 6.6617e-08, 1.0263e-02, 5.0810e-03,
        9.5853e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.402

[Epoch: 27, batch: 117/195] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9853e-01, 1.6858e-08, 1.0818e-04, 6.0417e-11, 1.4926e-07, 1.0131e-10,
        1.3586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.432

[Epoch: 27, batch: 156/195] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0044, 0.9824, 0.0020, 0.0037, 0.0015, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 27, batch: 195/195] total loss per batch: 0.534
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4505e-03, 1.6791e-01, 2.9442e-05, 1.7559e-06, 3.0059e-01, 3.3984e-01,
        1.8519e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.811

[Epoch: 28, batch: 39/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8274e-01, 5.5914e-03, 5.1728e-03, 2.6324e-07, 2.0901e-03, 4.3389e-03,
        6.9400e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.977

[Epoch: 28, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([2.7845e-03, 6.0939e-03, 3.0838e-07, 2.3305e-08, 6.2721e-03, 3.9648e-03,
        9.8088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.488

[Epoch: 28, batch: 117/195] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9691e-01, 4.3319e-08, 1.4573e-04, 7.3986e-11, 3.5013e-07, 5.3912e-11,
        2.9484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.462

[Epoch: 28, batch: 156/195] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0051, 0.9843, 0.0019, 0.0023, 0.0017, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 28, batch: 195/195] total loss per batch: 0.528
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5843e-03, 1.0489e-01, 2.9014e-05, 1.6683e-06, 3.3374e-01, 3.6234e-01,
        1.9242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.808

[Epoch: 29, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7632e-01, 8.9234e-03, 6.6420e-03, 1.7983e-07, 1.9583e-03, 6.0562e-03,
        9.9984e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.970

[Epoch: 29, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.2206e-03, 2.6095e-02, 3.4691e-07, 2.1175e-08, 7.0901e-03, 4.1210e-03,
        9.5847e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.374

[Epoch: 29, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9686e-01, 2.9826e-08, 7.9605e-05, 6.0835e-11, 2.4881e-07, 4.7969e-11,
        3.0630e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 29, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0049, 0.9810, 0.0027, 0.0037, 0.0017, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 29, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.3461e-03, 1.8582e-01, 2.3847e-05, 1.4238e-06, 3.0959e-01, 3.0223e-01,
        1.9499e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 30, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7049e-01, 8.6119e-03, 6.7446e-03, 1.6906e-07, 3.1807e-03, 1.0814e-02,
        1.6177e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.968

[Epoch: 30, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.4952e-03, 1.8832e-02, 4.0008e-07, 2.2626e-08, 5.7894e-03, 4.5837e-03,
        9.6630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.402

[Epoch: 30, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9584e-01, 5.3515e-08, 9.2467e-05, 6.9518e-11, 2.7673e-07, 4.0015e-11,
        4.0635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.482

[Epoch: 30, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0045, 0.9817, 0.0024, 0.0037, 0.0019, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 30, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.1031e-03, 1.0711e-01, 1.7379e-05, 7.6825e-07, 3.4503e-01, 3.5540e-01,
        1.8534e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 31, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6381e-01, 9.3696e-03, 8.1385e-03, 2.5527e-07, 3.9038e-03, 1.4661e-02,
        1.1790e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.961

[Epoch: 31, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.0008e-03, 2.2915e-02, 3.1268e-07, 2.1757e-08, 5.5918e-03, 3.9813e-03,
        9.6251e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.401

[Epoch: 31, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9602e-01, 3.2610e-08, 7.4527e-05, 4.3419e-11, 2.2129e-07, 2.9634e-11,
        3.9079e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 31, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0051, 0.9796, 0.0027, 0.0038, 0.0020, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 31, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.5032e-03, 1.5370e-01, 1.9471e-05, 8.5692e-07, 2.8352e-01, 3.1835e-01,
        2.3691e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 32, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6192e-01, 8.5492e-03, 7.5495e-03, 2.0013e-07, 5.5459e-03, 1.6302e-02,
        1.3254e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.956

[Epoch: 32, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.9418e-03, 2.2539e-02, 3.2739e-07, 1.7105e-08, 5.8347e-03, 3.8707e-03,
        9.6281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 32, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9583e-01, 3.3478e-08, 6.1708e-05, 4.5365e-11, 2.0272e-07, 3.4000e-11,
        4.1061e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.486

[Epoch: 32, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0047, 0.9802, 0.0027, 0.0039, 0.0020, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 32, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.9337e-03, 1.3624e-01, 1.7853e-05, 9.3429e-07, 3.7779e-01, 3.3291e-01,
        1.4611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.800

[Epoch: 33, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5923e-01, 9.0324e-03, 9.2523e-03, 3.4221e-07, 6.6423e-03, 1.5742e-02,
        1.0314e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.951

[Epoch: 33, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.8498e-03, 3.0359e-02, 2.7140e-07, 1.5044e-08, 4.8361e-03, 3.5624e-03,
        9.5639e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.426

[Epoch: 33, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9505e-01, 3.2745e-08, 6.1110e-05, 3.5202e-11, 1.7909e-07, 2.3183e-11,
        4.8895e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 33, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0043, 0.9815, 0.0028, 0.0031, 0.0020, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 33, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.3141e-03, 1.4277e-01, 1.8759e-05, 6.2765e-07, 2.4780e-01, 3.8524e-01,
        2.1586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.712

[Epoch: 34, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5940e-01, 8.3507e-03, 8.2174e-03, 2.3629e-07, 9.5051e-03, 1.4385e-02,
        1.3626e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.949

[Epoch: 34, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.5781e-03, 1.5870e-02, 3.5777e-07, 1.8190e-08, 5.1415e-03, 3.6979e-03,
        9.6971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.416

[Epoch: 34, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9480e-01, 4.8960e-08, 6.8253e-05, 6.6554e-11, 2.6950e-07, 5.0107e-11,
        5.1289e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.499

[Epoch: 34, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0052, 0.9782, 0.0027, 0.0044, 0.0022, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 34, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.7350e-03, 1.6330e-01, 1.6911e-05, 1.0996e-06, 3.8245e-01, 2.6468e-01,
        1.8081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.827

[Epoch: 35, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6425e-01, 8.2020e-03, 8.0931e-03, 2.4213e-07, 7.8705e-03, 1.1490e-02,
        9.7180e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.940

[Epoch: 35, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8212e-03, 3.7615e-02, 2.7898e-07, 1.8398e-08, 4.9675e-03, 3.9539e-03,
        9.4664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.419

[Epoch: 35, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9510e-01, 1.8985e-08, 4.4172e-05, 2.7657e-11, 1.0876e-07, 2.5332e-11,
        4.8579e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 35, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0045, 0.9800, 0.0029, 0.0034, 0.0023, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 35, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2986e-03, 1.0412e-01, 1.5460e-05, 8.1036e-07, 2.5515e-01, 4.3345e-01,
        2.0097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.757

[Epoch: 36, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5638e-01, 8.2050e-03, 7.9040e-03, 2.4261e-07, 1.0347e-02, 1.7060e-02,
        1.0105e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.930

[Epoch: 36, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.8095e-03, 9.8935e-03, 2.3872e-07, 1.1288e-08, 4.4297e-03, 3.4866e-03,
        9.7738e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.435

[Epoch: 36, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9414e-01, 4.9215e-08, 5.8570e-05, 4.7741e-11, 2.7456e-07, 6.2930e-11,
        5.8012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.516

[Epoch: 36, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0038, 0.9826, 0.0022, 0.0030, 0.0014, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 36, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.2956e-03, 1.7842e-01, 2.1103e-05, 1.3158e-06, 4.0004e-01, 2.3719e-01,
        1.7604e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.767

[Epoch: 37, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6474e-01, 7.5014e-03, 7.4792e-03, 1.5098e-07, 1.1219e-02, 8.8779e-03,
        1.8286e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.941

[Epoch: 37, batch: 78/195] total loss per batch: 0.548
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([9.4710e-03, 2.2998e-02, 3.2050e-07, 2.3811e-08, 5.1332e-03, 4.6078e-03,
        9.5779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 37, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9336e-01, 2.6954e-08, 5.2534e-05, 4.3857e-11, 2.1064e-07, 2.6561e-11,
        6.5892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.465

[Epoch: 37, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0037, 0.9781, 0.0034, 0.0056, 0.0023, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 37, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.2959e-03, 1.2334e-01, 3.5162e-05, 1.6122e-06, 2.5835e-01, 3.7933e-01,
        2.2965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.771

[Epoch: 38, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5293e-01, 9.0934e-03, 9.2225e-03, 5.7922e-07, 1.2360e-02, 1.6323e-02,
        6.5869e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.937

[Epoch: 38, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3320e-03, 4.1552e-02, 4.2060e-07, 2.0830e-08, 5.9774e-03, 4.6550e-03,
        9.4148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.469

[Epoch: 38, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9539e-01, 5.8964e-08, 5.2169e-05, 6.3313e-11, 1.5309e-07, 4.6961e-11,
        4.5530e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 38, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.0057, 0.9747, 0.0031, 0.0052, 0.0025, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 38, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.0530e-03, 1.5788e-01, 2.4926e-05, 1.4688e-06, 3.6724e-01, 2.9890e-01,
        1.6790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 39, batch: 39/195] total loss per batch: 0.516
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6382e-01, 7.7094e-03, 6.3097e-03, 2.8587e-07, 1.3853e-02, 8.1914e-03,
        1.1875e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 39, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.4046e-03, 3.3662e-03, 2.6166e-07, 6.3977e-09, 3.3879e-03, 3.3734e-03,
        9.8447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.347

[Epoch: 39, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9077e-01, 7.6334e-08, 5.7654e-05, 1.2892e-10, 2.7117e-07, 7.5160e-11,
        9.1733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.515

[Epoch: 39, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0053, 0.9807, 0.0036, 0.0019, 0.0026, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 39, batch: 195/195] total loss per batch: 0.528
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.2007e-03, 1.3142e-01, 3.9805e-05, 3.1946e-06, 3.0085e-01, 3.5185e-01,
        2.0863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 40, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6860e-01, 7.6201e-03, 5.0856e-03, 3.7762e-07, 7.6397e-03, 1.0961e-02,
        9.6428e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.939

[Epoch: 40, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.0603e-02, 1.3149e-02, 3.9298e-07, 1.7855e-08, 6.0761e-03, 5.1260e-03,
        9.6504e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.445

[Epoch: 40, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9594e-01, 5.3403e-08, 4.4631e-05, 1.2648e-10, 2.1986e-07, 4.7516e-11,
        4.0120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.454

[Epoch: 40, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.9844, 0.0027, 0.0027, 0.0016, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 40, batch: 195/195] total loss per batch: 0.529
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0968e-02, 1.4578e-01, 2.3132e-05, 5.6344e-07, 3.0747e-01, 3.4738e-01,
        1.8838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.773

[Epoch: 41, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5168e-01, 8.2670e-03, 7.4794e-03, 1.0698e-06, 1.9093e-02, 1.3329e-02,
        1.5497e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 41, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8391e-03, 2.1227e-02, 3.7878e-07, 2.0258e-08, 4.0299e-03, 4.6351e-03,
        9.6327e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.393

[Epoch: 41, batch: 117/195] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9384e-01, 6.4083e-08, 6.4862e-05, 1.6742e-10, 1.6699e-07, 1.2613e-10,
        6.0985e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.515

[Epoch: 41, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0034, 0.9787, 0.0035, 0.0049, 0.0019, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 41, batch: 195/195] total loss per batch: 0.529
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.7205e-03, 1.3376e-01, 6.4523e-05, 3.3561e-06, 3.9804e-01, 2.8900e-01,
        1.7141e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.740

[Epoch: 42, batch: 39/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5491e-01, 1.0082e-02, 9.8322e-03, 2.8568e-07, 1.0122e-02, 1.4998e-02,
        5.2924e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 42, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([9.4563e-03, 2.0551e-02, 1.9923e-07, 1.1607e-08, 3.9128e-03, 4.5784e-03,
        9.6150e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.396

[Epoch: 42, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9523e-01, 4.9309e-08, 2.5342e-05, 1.5429e-10, 9.9560e-08, 3.1771e-10,
        4.7472e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.414

[Epoch: 42, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.0046, 0.9753, 0.0044, 0.0048, 0.0031, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 42, batch: 195/195] total loss per batch: 0.530
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.8200e-03, 1.6728e-01, 3.3581e-05, 1.0650e-06, 2.5491e-01, 3.6710e-01,
        2.0485e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.772

[Epoch: 43, batch: 39/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7622e-01, 5.6905e-03, 4.8707e-03, 1.3824e-07, 9.6357e-03, 3.4948e-03,
        9.1509e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.933

[Epoch: 43, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.9888e-03, 1.6028e-02, 4.4851e-07, 1.4173e-08, 5.8837e-03, 4.4618e-03,
        9.6864e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.457

[Epoch: 43, batch: 117/195] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8785e-01, 6.5234e-08, 3.5476e-05, 1.3439e-10, 1.7028e-07, 4.3176e-11,
        1.2117e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.495

[Epoch: 43, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0040, 0.9802, 0.0037, 0.0027, 0.0031, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.021

[Epoch: 43, batch: 195/195] total loss per batch: 0.529
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.6956e-03, 7.5877e-02, 4.0142e-05, 3.6634e-06, 3.6454e-01, 3.5307e-01,
        2.0177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.787

[Epoch: 44, batch: 39/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.3751e-01, 1.1708e-02, 9.0554e-03, 6.2931e-07, 1.7873e-02, 2.3753e-02,
        1.0441e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.894

[Epoch: 44, batch: 78/195] total loss per batch: 0.551
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.6958e-03, 1.7071e-02, 8.9638e-08, 1.9367e-09, 2.7805e-03, 3.2907e-03,
        9.6816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.492

[Epoch: 44, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9104e-01, 3.4175e-08, 2.8233e-05, 7.8890e-11, 1.0519e-07, 9.1351e-10,
        8.9331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 44, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0059, 0.9804, 0.0031, 0.0022, 0.0019, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 44, batch: 195/195] total loss per batch: 0.528
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.4234e-02, 1.8332e-01, 2.4067e-05, 2.8137e-06, 2.4739e-01, 3.2579e-01,
        2.2925e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.723

[Epoch: 45, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6856e-01, 6.8276e-03, 7.7253e-03, 1.1876e-07, 8.4415e-03, 8.3996e-03,
        5.0274e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.936

[Epoch: 45, batch: 78/195] total loss per batch: 0.551
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3688e-03, 1.8398e-02, 5.3512e-07, 3.4751e-08, 4.1009e-03, 3.0410e-03,
        9.6809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.404

[Epoch: 45, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9403e-01, 1.6179e-07, 5.6954e-05, 4.2333e-10, 1.3767e-07, 1.1391e-10,
        5.9175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 45, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0027, 0.9809, 0.0032, 0.0049, 0.0025, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 45, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.0184e-03, 1.1553e-01, 3.4899e-05, 1.6693e-06, 4.6958e-01, 2.7525e-01,
        1.3558e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.792

[Epoch: 46, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6617e-01, 5.2992e-03, 4.9325e-03, 3.7869e-07, 8.6695e-03, 1.4870e-02,
        5.8816e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.932

[Epoch: 46, batch: 78/195] total loss per batch: 0.550
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.3873e-03, 2.5803e-02, 1.5091e-07, 7.1096e-09, 3.3285e-03, 5.7537e-03,
        9.5673e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.436

[Epoch: 46, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9157e-01, 2.7388e-08, 1.7149e-05, 4.4740e-11, 7.4373e-08, 1.4699e-10,
        8.4111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.507

[Epoch: 46, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0036, 0.9774, 0.0047, 0.0035, 0.0046, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 46, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.7901e-03, 1.5402e-01, 4.4004e-05, 3.1406e-06, 1.7073e-01, 4.5593e-01,
        2.1349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 47, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7170e-01, 3.8976e-03, 4.4229e-03, 1.2405e-07, 1.1919e-02, 8.0134e-03,
        4.2133e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 47, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.1610e-03, 1.9931e-02, 2.9659e-07, 2.0596e-08, 6.0753e-03, 4.6200e-03,
        9.6121e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.407

[Epoch: 47, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8918e-01, 1.2972e-07, 5.5552e-05, 6.9768e-10, 3.1526e-07, 7.2631e-10,
        1.0769e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.490

[Epoch: 47, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0061, 0.9775, 0.0038, 0.0024, 0.0031, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 47, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.4587e-02, 1.5203e-01, 4.6849e-05, 1.5831e-06, 3.4025e-01, 2.7363e-01,
        2.1944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.760

[Epoch: 48, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.4185e-01, 1.0028e-02, 1.1003e-02, 2.3207e-07, 1.6954e-02, 2.0094e-02,
        7.4993e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.915

[Epoch: 48, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6191e-03, 2.0728e-02, 2.7397e-07, 1.5625e-08, 2.7357e-03, 4.2384e-03,
        9.6668e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.444

[Epoch: 48, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8991e-01, 5.2511e-08, 4.4232e-05, 8.6406e-11, 1.7471e-07, 1.8553e-10,
        1.0048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.520

[Epoch: 48, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0030, 0.9802, 0.0038, 0.0035, 0.0025, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 48, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.4684e-03, 1.3822e-01, 5.2422e-05, 1.2168e-06, 3.7139e-01, 3.3845e-01,
        1.4742e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.736

[Epoch: 49, batch: 39/195] total loss per batch: 0.516
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.8013e-01, 2.9745e-03, 3.2488e-03, 3.0080e-07, 7.9038e-03, 5.7064e-03,
        4.0499e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 49, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.4318e-03, 3.4583e-02, 4.0767e-07, 4.0522e-08, 5.5657e-03, 6.7965e-03,
        9.4462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.441

[Epoch: 49, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9646e-01, 8.8631e-08, 8.8068e-05, 2.7710e-10, 1.6317e-07, 1.7299e-10,
        3.4478e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.479

[Epoch: 49, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.9798, 0.0036, 0.0031, 0.0036, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 49, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.9873e-03, 1.5074e-01, 1.0767e-04, 8.2879e-06, 2.8274e-01, 3.3465e-01,
        2.2276e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.843

[Epoch: 50, batch: 39/195] total loss per batch: 0.516
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6458e-01, 6.0770e-03, 3.9060e-03, 2.5576e-07, 1.0003e-02, 1.5390e-02,
        4.5774e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 50, batch: 78/195] total loss per batch: 0.548
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.0795e-03, 1.2325e-02, 1.8435e-07, 5.1317e-09, 3.9283e-03, 3.6764e-03,
        9.7499e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.406

[Epoch: 50, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9228e-01, 7.1794e-08, 4.8203e-05, 9.8447e-11, 1.0359e-07, 1.3929e-10,
        7.6723e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.473

[Epoch: 50, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.0052, 0.9773, 0.0030, 0.0023, 0.0041, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.020

[Epoch: 50, batch: 195/195] total loss per batch: 0.528
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.8475e-03, 9.2005e-02, 3.9256e-05, 7.0199e-07, 3.2593e-01, 3.8102e-01,
        1.9516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.750

[Epoch: 51, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.1540e-01, 1.3017e-02, 1.4310e-02, 2.4171e-07, 3.5211e-02, 2.1938e-02,
        1.2540e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.898

[Epoch: 51, batch: 78/195] total loss per batch: 0.548
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.8118e-03, 1.6311e-02, 2.3728e-07, 1.6742e-08, 3.0595e-03, 3.3606e-03,
        9.7246e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 51, batch: 117/195] total loss per batch: 0.523
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9161e-01, 6.8422e-08, 3.9132e-05, 1.3507e-10, 1.8145e-07, 2.3445e-10,
        8.3487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.503

[Epoch: 51, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0038, 0.9779, 0.0043, 0.0039, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.024

[Epoch: 51, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2085e-02, 1.6166e-01, 6.0095e-05, 3.9947e-06, 3.3982e-01, 2.9804e-01,
        1.8833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.795

[Epoch: 52, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7710e-01, 3.0686e-03, 3.2766e-03, 1.3854e-07, 8.1628e-03, 8.3686e-03,
        2.2027e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.937

[Epoch: 52, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6006e-03, 3.6574e-02, 3.2470e-07, 3.2735e-08, 3.2036e-03, 4.4381e-03,
        9.4918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.436

[Epoch: 52, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9367e-01, 7.3941e-08, 4.4491e-05, 1.7141e-10, 2.0974e-07, 3.6830e-10,
        6.2868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 52, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.9829, 0.0027, 0.0028, 0.0023, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 52, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.0512e-03, 1.2257e-01, 7.3482e-05, 4.1067e-06, 3.2710e-01, 3.5032e-01,
        1.9488e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.749

[Epoch: 53, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6868e-01, 4.1514e-03, 4.1289e-03, 1.6140e-07, 1.0948e-02, 1.2056e-02,
        3.1576e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.904

[Epoch: 53, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.1514e-03, 1.5392e-02, 3.3133e-07, 2.0779e-08, 3.9683e-03, 4.2216e-03,
        9.7127e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.421

[Epoch: 53, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9407e-01, 5.8588e-08, 3.9637e-05, 1.8382e-10, 2.1069e-07, 2.9174e-10,
        5.8875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 53, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0036, 0.9787, 0.0035, 0.0032, 0.0040, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 53, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.4412e-03, 1.5009e-01, 5.1660e-05, 3.5239e-06, 3.2245e-01, 3.2984e-01,
        1.8912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 54, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6593e-01, 4.8329e-03, 5.4833e-03, 1.2666e-07, 1.0164e-02, 1.3564e-02,
        2.1114e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.922

[Epoch: 54, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9491e-03, 2.8099e-02, 2.6271e-07, 1.6435e-08, 3.5793e-03, 5.1547e-03,
        9.5622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.431

[Epoch: 54, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9495e-01, 6.3398e-08, 3.3220e-05, 1.3508e-10, 1.2646e-07, 2.6524e-10,
        5.0120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.502

[Epoch: 54, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0049, 0.9795, 0.0035, 0.0031, 0.0030, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.021

[Epoch: 54, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.3874e-03, 1.2845e-01, 6.1370e-05, 3.6784e-06, 2.8846e-01, 3.6562e-01,
        2.1203e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.762

[Epoch: 55, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6396e-01, 5.4390e-03, 5.6007e-03, 1.5219e-07, 1.4008e-02, 1.0969e-02,
        2.1810e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.901

[Epoch: 55, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7024e-03, 1.4670e-02, 2.5693e-07, 6.7995e-09, 4.0199e-03, 4.3817e-03,
        9.7123e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.421

[Epoch: 55, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9446e-01, 4.8715e-08, 1.7689e-05, 9.0213e-11, 8.9994e-08, 1.5205e-10,
        5.5268e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.481

[Epoch: 55, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0033, 0.9819, 0.0030, 0.0025, 0.0029, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 55, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0801e-03, 1.4091e-01, 5.7459e-05, 3.5983e-06, 3.7289e-01, 3.0503e-01,
        1.7402e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 56, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5929e-01, 5.3226e-03, 5.1372e-03, 1.4120e-07, 1.4941e-02, 1.5292e-02,
        2.1401e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.904

[Epoch: 56, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3351e-03, 3.7737e-02, 3.5509e-07, 2.5710e-08, 3.4686e-03, 4.5269e-03,
        9.4793e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.405

[Epoch: 56, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9251e-01, 5.3446e-08, 3.4602e-05, 1.4953e-10, 1.3408e-07, 2.9998e-10,
        7.4580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.504

[Epoch: 56, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0044, 0.9783, 0.0033, 0.0034, 0.0036, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 56, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.1155e-03, 1.3157e-01, 5.4672e-05, 4.3707e-06, 2.9472e-01, 3.6363e-01,
        2.0290e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.759

[Epoch: 57, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6429e-01, 5.2833e-03, 5.9159e-03, 1.2453e-07, 1.1382e-02, 1.3110e-02,
        2.1012e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.926

[Epoch: 57, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2805e-03, 1.1427e-02, 1.7280e-07, 7.4986e-09, 3.8562e-03, 3.8364e-03,
        9.7460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.440

[Epoch: 57, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9491e-01, 4.7285e-08, 1.9976e-05, 1.0276e-10, 6.8850e-08, 1.0809e-10,
        5.0689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.476

[Epoch: 57, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0045, 0.9760, 0.0043, 0.0039, 0.0044, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 57, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.8621e-03, 1.4257e-01, 3.9536e-05, 4.1482e-06, 3.3309e-01, 3.1738e-01,
        1.9906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.789

[Epoch: 58, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6389e-01, 5.6903e-03, 4.9064e-03, 6.0276e-08, 1.2449e-02, 1.3047e-02,
        1.2162e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.938

[Epoch: 58, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0721e-03, 1.6467e-02, 2.9515e-07, 1.6764e-08, 3.2176e-03, 3.6190e-03,
        9.7062e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.391

[Epoch: 58, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9203e-01, 5.8814e-08, 3.6415e-05, 2.1562e-10, 1.4304e-07, 2.9253e-10,
        7.9382e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.473

[Epoch: 58, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0040, 0.9821, 0.0034, 0.0020, 0.0024, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 58, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.2806e-03, 1.4742e-01, 4.0473e-05, 2.7252e-06, 3.0628e-01, 3.3762e-01,
        2.0135e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 59, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5979e-01, 5.8973e-03, 6.2152e-03, 5.0735e-08, 1.5684e-02, 1.2379e-02,
        3.5221e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.915

[Epoch: 59, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9585e-03, 3.5511e-02, 1.5305e-07, 8.0052e-09, 5.0275e-03, 3.8259e-03,
        9.4868e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.492

[Epoch: 59, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9493e-01, 4.8933e-08, 1.8577e-05, 9.4893e-11, 9.5488e-08, 1.1259e-10,
        5.0494e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.503

[Epoch: 59, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.9839, 0.0028, 0.0033, 0.0021, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 59, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.3842e-03, 1.2519e-01, 3.8207e-05, 4.0198e-06, 3.5674e-01, 3.3547e-01,
        1.7618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.728

[Epoch: 60, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5798e-01, 7.4911e-03, 6.3721e-03, 6.2638e-08, 9.6313e-03, 1.8505e-02,
        1.8192e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.934

[Epoch: 60, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.7764e-03, 1.5323e-02, 1.4595e-07, 1.7464e-08, 4.2030e-03, 3.3883e-03,
        9.6931e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.359

[Epoch: 60, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9507e-01, 3.6605e-08, 2.2979e-05, 4.6057e-11, 4.0822e-08, 1.9687e-11,
        4.9088e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.539

[Epoch: 60, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0043, 0.9753, 0.0057, 0.0041, 0.0027, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.011

[Epoch: 60, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.7718e-03, 1.3895e-01, 4.0568e-05, 3.3521e-06, 3.1528e-01, 3.4160e-01,
        1.9635e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.848

[Epoch: 61, batch: 39/195] total loss per batch: 0.516
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6228e-01, 5.7512e-03, 7.4154e-03, 1.0299e-07, 1.6656e-02, 7.8806e-03,
        1.4897e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.900

[Epoch: 61, batch: 78/195] total loss per batch: 0.548
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.3155e-03, 1.6428e-02, 1.7344e-07, 1.7193e-08, 4.7413e-03, 2.8221e-03,
        9.7169e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.500

[Epoch: 61, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9374e-01, 5.3115e-08, 2.4552e-05, 1.7261e-10, 1.1011e-07, 1.3192e-10,
        6.2373e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.459

[Epoch: 61, batch: 156/195] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0025, 0.9859, 0.0024, 0.0016, 0.0025, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 61, batch: 195/195] total loss per batch: 0.529
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.6759e-03, 1.3816e-01, 6.2303e-05, 4.0863e-06, 3.0394e-01, 3.2339e-01,
        2.2476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.721

[Epoch: 62, batch: 39/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.3761e-01, 1.1549e-02, 6.0324e-03, 1.1916e-07, 2.4137e-02, 2.0639e-02,
        3.0978e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.892

[Epoch: 62, batch: 78/195] total loss per batch: 0.551
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4645e-03, 2.1909e-02, 2.0678e-07, 2.0471e-08, 3.4119e-03, 3.4073e-03,
        9.6481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 62, batch: 117/195] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9628e-01, 1.5974e-07, 3.1596e-05, 1.8477e-10, 7.6905e-08, 7.5429e-11,
        3.6890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 62, batch: 156/195] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0050, 0.0044, 0.9768, 0.0034, 0.0040, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.044

[Epoch: 62, batch: 195/195] total loss per batch: 0.540
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.4363e-03, 1.5638e-01, 1.1673e-04, 4.5127e-06, 3.1710e-01, 3.6347e-01,
        1.5549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.788

[Epoch: 63, batch: 39/195] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5704e-01, 8.1999e-03, 8.6352e-03, 1.3991e-07, 1.3106e-02, 1.2968e-02,
        4.5896e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.938

[Epoch: 63, batch: 78/195] total loss per batch: 0.558
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([1.1289e-02, 5.9598e-02, 1.1568e-06, 4.2410e-07, 8.4383e-03, 7.4270e-03,
        9.1325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.395

[Epoch: 63, batch: 117/195] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9356e-01, 7.4419e-08, 2.7235e-05, 1.6719e-10, 2.1435e-07, 8.7722e-11,
        6.4121e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 63, batch: 156/195] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0027, 0.9847, 0.0025, 0.0031, 0.0013, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 63, batch: 195/195] total loss per batch: 0.537
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.0760e-03, 1.3738e-01, 7.8495e-05, 5.3387e-06, 3.9425e-01, 2.7436e-01,
        1.8985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.816

[Epoch: 64, batch: 39/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7063e-01, 5.5520e-03, 5.6720e-03, 6.1281e-08, 6.7729e-03, 1.1328e-02,
        4.4646e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.898

[Epoch: 64, batch: 78/195] total loss per batch: 0.554
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.8130e-03, 5.5343e-03, 1.5449e-07, 1.7386e-07, 4.2939e-03, 3.8384e-03,
        9.8252e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 64, batch: 117/195] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9030e-01, 2.3127e-08, 1.7141e-05, 8.0878e-11, 9.8978e-08, 2.3529e-10,
        9.6802e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.440

[Epoch: 64, batch: 156/195] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0034, 0.9831, 0.0022, 0.0035, 0.0018, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.023

[Epoch: 64, batch: 195/195] total loss per batch: 0.531
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.1705e-03, 1.2643e-01, 6.8473e-05, 3.5430e-06, 2.6813e-01, 4.1876e-01,
        1.8143e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.751

[Epoch: 65, batch: 39/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6834e-01, 5.4271e-03, 5.9943e-03, 1.0246e-07, 9.0824e-03, 1.1140e-02,
        1.3788e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 65, batch: 78/195] total loss per batch: 0.549
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.4190e-03, 2.1447e-02, 8.9761e-08, 1.4490e-07, 4.6224e-03, 4.3704e-03,
        9.6514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.414

[Epoch: 65, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9622e-01, 2.5864e-08, 1.2684e-05, 3.7704e-11, 4.4754e-08, 9.6154e-11,
        3.7721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.469

[Epoch: 65, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0030, 0.9848, 0.0023, 0.0027, 0.0021, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.014

[Epoch: 65, batch: 195/195] total loss per batch: 0.530
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.8320e-03, 1.4194e-01, 3.6444e-05, 1.1773e-06, 3.3422e-01, 3.2132e-01,
        1.9465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.816

[Epoch: 66, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6780e-01, 1.1114e-02, 4.8130e-03, 1.0246e-07, 7.3155e-03, 8.9236e-03,
        2.9666e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.917

[Epoch: 66, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0032e-03, 1.8593e-02, 1.1164e-07, 1.2227e-07, 4.0815e-03, 3.6613e-03,
        9.6766e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.436

[Epoch: 66, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9682e-01, 3.5908e-08, 1.6335e-05, 7.1247e-11, 1.1442e-07, 1.1160e-10,
        3.1645e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.462

[Epoch: 66, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.9825, 0.0034, 0.0025, 0.0022, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 66, batch: 195/195] total loss per batch: 0.536
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0207e-02, 1.7449e-01, 8.0846e-05, 7.8947e-06, 3.1382e-01, 3.0479e-01,
        1.9661e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.778

[Epoch: 67, batch: 39/195] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5036e-01, 7.4291e-03, 7.0810e-03, 7.6552e-08, 1.4749e-02, 2.0354e-02,
        3.1552e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 67, batch: 78/195] total loss per batch: 0.548
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.3604e-03, 1.0387e-02, 7.8618e-08, 8.1996e-08, 6.0042e-03, 3.7275e-03,
        9.7652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.315

[Epoch: 67, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9584e-01, 5.8589e-08, 2.4835e-05, 1.1152e-10, 4.1417e-08, 7.8713e-11,
        4.1343e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.499

[Epoch: 67, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.9819, 0.0039, 0.0038, 0.0020, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 67, batch: 195/195] total loss per batch: 0.531
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.6930e-03, 1.1848e-01, 2.5424e-05, 3.7940e-06, 2.9835e-01, 3.6922e-01,
        2.0923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.740

[Epoch: 68, batch: 39/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5251e-01, 8.0710e-03, 6.1238e-03, 1.4888e-07, 1.1545e-02, 2.1701e-02,
        4.6768e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.918

[Epoch: 68, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.2804e-03, 2.7992e-02, 1.1658e-07, 2.9868e-08, 4.0636e-03, 7.1080e-03,
        9.5656e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.379

[Epoch: 68, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9350e-01, 3.0922e-07, 4.9081e-05, 3.8140e-10, 1.1578e-07, 9.1018e-11,
        6.4498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.505

[Epoch: 68, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.9804, 0.0040, 0.0037, 0.0026, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 68, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.7675e-03, 1.7175e-01, 2.6047e-05, 3.6448e-06, 3.4655e-01, 3.0212e-01,
        1.7278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.793

[Epoch: 69, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6496e-01, 6.9569e-03, 5.5176e-03, 1.5687e-07, 1.0220e-02, 1.2316e-02,
        3.4276e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.902

[Epoch: 69, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.0599e-03, 3.2982e-02, 1.5158e-07, 5.6317e-08, 5.1089e-03, 4.5682e-03,
        9.5328e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.409

[Epoch: 69, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9310e-01, 1.4547e-07, 3.6381e-05, 1.4288e-10, 7.3936e-08, 5.4825e-11,
        6.8635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.509

[Epoch: 69, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0037, 0.9792, 0.0041, 0.0041, 0.0024, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 69, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.8445e-03, 1.2059e-01, 2.9720e-05, 4.4097e-06, 3.0963e-01, 3.6408e-01,
        1.9882e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 70, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5279e-01, 7.8869e-03, 7.1384e-03, 1.5960e-07, 1.4492e-02, 1.7646e-02,
        5.0294e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 70, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.0835e-03, 2.1661e-02, 1.4529e-07, 3.5696e-08, 4.5653e-03, 4.9123e-03,
        9.6378e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.415

[Epoch: 70, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9434e-01, 1.1806e-07, 3.1723e-05, 1.6780e-10, 6.4796e-08, 7.5996e-11,
        5.6261e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 70, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0041, 0.9773, 0.0046, 0.0042, 0.0030, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 70, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.7297e-03, 1.3640e-01, 3.0471e-05, 3.9876e-06, 3.4183e-01, 3.2101e-01,
        1.9400e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.782

[Epoch: 71, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5602e-01, 7.0155e-03, 6.7498e-03, 1.3412e-07, 1.3955e-02, 1.6201e-02,
        5.8731e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 71, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.1438e-03, 2.3099e-02, 1.0875e-07, 3.0180e-08, 4.1604e-03, 4.2881e-03,
        9.6331e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 71, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9381e-01, 1.0711e-07, 3.1401e-05, 1.2736e-10, 6.1418e-08, 4.5256e-11,
        6.1540e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 71, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0039, 0.9782, 0.0041, 0.0041, 0.0028, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 71, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2212e-03, 1.4930e-01, 2.7094e-05, 4.6102e-06, 3.1730e-01, 3.3549e-01,
        1.9166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.765

[Epoch: 72, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5994e-01, 6.3399e-03, 6.1011e-03, 1.0044e-07, 1.2531e-02, 1.5037e-02,
        4.7277e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 72, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.5105e-03, 2.2148e-02, 1.0314e-07, 2.3689e-08, 3.9511e-03, 4.3297e-03,
        9.6406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.419

[Epoch: 72, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9287e-01, 8.7982e-08, 2.3449e-05, 1.0719e-10, 4.5518e-08, 4.6261e-11,
        7.1082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 72, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0040, 0.9786, 0.0040, 0.0039, 0.0030, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 72, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.7987e-03, 1.2833e-01, 2.3353e-05, 4.0549e-06, 3.1958e-01, 3.4442e-01,
        2.0085e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.781

[Epoch: 73, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5825e-01, 6.9510e-03, 6.6283e-03, 1.0402e-07, 1.4206e-02, 1.3919e-02,
        4.5384e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 73, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9550e-03, 2.3193e-02, 1.1988e-07, 2.9367e-08, 4.0088e-03, 4.1829e-03,
        9.6266e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.428

[Epoch: 73, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9345e-01, 1.0430e-07, 2.6942e-05, 1.2109e-10, 4.6237e-08, 3.8732e-11,
        6.5249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 73, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0039, 0.9780, 0.0042, 0.0039, 0.0030, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 73, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.9442e-03, 1.5389e-01, 2.4238e-05, 3.9892e-06, 3.2899e-01, 3.3006e-01,
        1.8008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 74, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5994e-01, 7.0390e-03, 6.6103e-03, 8.4851e-08, 1.1852e-02, 1.4512e-02,
        4.3100e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.918

[Epoch: 74, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1198e-03, 2.4069e-02, 7.8424e-08, 1.6525e-08, 3.7573e-03, 4.2705e-03,
        9.6178e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.417

[Epoch: 74, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9266e-01, 7.1756e-08, 1.9629e-05, 8.4510e-11, 3.6449e-08, 4.3184e-11,
        7.3214e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.499

[Epoch: 74, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0042, 0.9784, 0.0036, 0.0037, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 74, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2082e-03, 1.1387e-01, 1.6647e-05, 3.2749e-06, 3.1795e-01, 3.3905e-01,
        2.2291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.786

[Epoch: 75, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5727e-01, 7.0106e-03, 6.3652e-03, 9.9307e-08, 1.5642e-02, 1.3654e-02,
        5.9372e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 75, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6775e-03, 2.2599e-02, 1.5383e-07, 3.3245e-08, 4.1803e-03, 4.3575e-03,
        9.6219e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.432

[Epoch: 75, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9327e-01, 1.0450e-07, 2.5749e-05, 1.6798e-10, 5.1917e-08, 5.1172e-11,
        6.7039e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.492

[Epoch: 75, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.9816, 0.0036, 0.0037, 0.0027, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 75, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.7697e-03, 1.8124e-01, 2.9509e-05, 5.1279e-06, 3.2955e-01, 3.3108e-01,
        1.4832e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.758

[Epoch: 76, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6829e-01, 5.8632e-03, 5.1731e-03, 5.9473e-08, 1.0086e-02, 1.0567e-02,
        2.3991e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.918

[Epoch: 76, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2875e-03, 2.3020e-02, 6.3861e-08, 1.3622e-08, 3.6949e-03, 4.1634e-03,
        9.6283e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 76, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9465e-01, 4.8498e-08, 1.7011e-05, 5.7569e-11, 2.5569e-08, 2.2613e-11,
        5.3307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.493

[Epoch: 76, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0040, 0.9752, 0.0047, 0.0045, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 76, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.9134e-03, 1.0022e-01, 1.8384e-05, 2.3129e-06, 3.3233e-01, 3.3298e-01,
        2.2954e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.798

[Epoch: 77, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.3839e-01, 8.9155e-03, 9.4473e-03, 9.0768e-08, 2.3895e-02, 1.9302e-02,
        5.4039e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.915

[Epoch: 77, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4761e-03, 2.2223e-02, 1.7870e-07, 4.3088e-08, 3.9265e-03, 3.7973e-03,
        9.6358e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.419

[Epoch: 77, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9447e-01, 4.8558e-08, 1.4530e-05, 9.9751e-11, 2.4856e-08, 3.8334e-11,
        5.5171e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.479

[Epoch: 77, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0041, 0.9799, 0.0033, 0.0029, 0.0024, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 77, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.5902e-03, 1.7043e-01, 2.7952e-05, 5.1977e-06, 2.9720e-01, 3.4005e-01,
        1.8470e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.754

[Epoch: 78, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7128e-01, 5.0933e-03, 5.8731e-03, 1.0685e-07, 6.0783e-03, 1.1623e-02,
        5.6879e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 78, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9251e-03, 2.8715e-02, 1.4940e-07, 7.1102e-08, 4.6698e-03, 5.2174e-03,
        9.5447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.458

[Epoch: 78, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9317e-01, 8.1502e-08, 2.1155e-05, 2.8472e-10, 9.5132e-08, 1.5862e-10,
        6.8091e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.495

[Epoch: 78, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0024, 0.9850, 0.0025, 0.0035, 0.0018, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.045

[Epoch: 78, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0535e-02, 1.2078e-01, 2.0907e-05, 5.2919e-06, 3.4019e-01, 3.5155e-01,
        1.7691e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.792

[Epoch: 79, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6987e-01, 6.2069e-03, 4.0124e-03, 2.3658e-08, 1.0031e-02, 9.8600e-03,
        1.9815e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 79, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.0455e-03, 1.8879e-02, 1.1848e-07, 2.5352e-08, 2.5410e-03, 3.8945e-03,
        9.6964e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 79, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9318e-01, 7.9701e-08, 2.2778e-05, 3.5938e-10, 6.7353e-08, 1.3369e-10,
        6.8018e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.471

[Epoch: 79, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.0038, 0.9748, 0.0050, 0.0041, 0.0039, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 79, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2745e-03, 1.5043e-01, 2.8859e-05, 4.3267e-06, 3.1812e-01, 2.9974e-01,
        2.2540e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.704

[Epoch: 80, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5401e-01, 6.2722e-03, 6.0646e-03, 1.3409e-07, 1.4750e-02, 1.8861e-02,
        4.4840e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.917

[Epoch: 80, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.5643e-03, 1.6312e-02, 1.1661e-07, 7.1369e-08, 3.6374e-03, 4.2455e-03,
        9.7024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 80, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9578e-01, 9.9853e-08, 2.2741e-05, 2.3837e-10, 3.7429e-08, 6.5674e-11,
        4.1922e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.478

[Epoch: 80, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0040, 0.9815, 0.0024, 0.0032, 0.0029, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 80, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.2467e-03, 1.1935e-01, 2.4510e-05, 2.4960e-06, 3.3421e-01, 3.7225e-01,
        1.6592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.822

[Epoch: 81, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6012e-01, 6.9531e-03, 6.8973e-03, 1.7682e-07, 1.5083e-02, 1.0921e-02,
        2.8679e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 81, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.7806e-03, 4.2040e-02, 1.8662e-07, 2.3015e-08, 5.4936e-03, 4.0700e-03,
        9.4162e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.423

[Epoch: 81, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9187e-01, 3.4788e-08, 2.4236e-05, 2.0166e-10, 8.4808e-08, 1.8114e-10,
        8.1047e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.512

[Epoch: 81, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0027, 0.9843, 0.0028, 0.0034, 0.0018, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.047

[Epoch: 81, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.6844e-03, 1.8611e-01, 2.5872e-05, 4.8573e-06, 2.6883e-01, 3.3272e-01,
        2.0662e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.819

[Epoch: 82, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6947e-01, 5.2396e-03, 5.9705e-03, 6.1232e-08, 9.3178e-03, 9.9803e-03,
        2.5431e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 82, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.3923e-03, 1.5051e-02, 1.5462e-07, 2.0212e-07, 2.9050e-03, 4.2243e-03,
        9.7243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.415

[Epoch: 82, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8901e-01, 1.2027e-07, 4.7738e-05, 8.2122e-10, 1.5078e-07, 1.1104e-09,
        1.0940e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 82, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0044, 0.9755, 0.0034, 0.0045, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 82, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6029e-03, 8.7134e-02, 2.7505e-05, 6.7108e-06, 4.4167e-01, 3.0167e-01,
        1.6289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.686

[Epoch: 83, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5467e-01, 5.7620e-03, 6.6334e-03, 2.2712e-07, 1.6075e-02, 1.6807e-02,
        4.9266e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 83, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.7772e-03, 2.6766e-02, 7.4544e-08, 1.0427e-08, 3.4279e-03, 3.4742e-03,
        9.5855e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.448

[Epoch: 83, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9136e-01, 7.6790e-08, 2.2527e-05, 1.8677e-10, 4.4423e-08, 9.5241e-11,
        8.6169e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.464

[Epoch: 83, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0050, 0.9793, 0.0038, 0.0029, 0.0025, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 83, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.3373e-03, 1.6462e-01, 1.7597e-05, 4.4787e-06, 2.3691e-01, 3.5578e-01,
        2.3332e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.845

[Epoch: 84, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6578e-01, 5.4342e-03, 5.9100e-03, 1.3021e-07, 1.0825e-02, 1.2031e-02,
        1.4882e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.931

[Epoch: 84, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.3519e-03, 2.1443e-02, 2.2533e-07, 7.9419e-08, 3.5041e-03, 2.6588e-03,
        9.6704e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.420

[Epoch: 84, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9072e-01, 7.3771e-08, 4.9671e-05, 5.2085e-10, 1.5908e-07, 2.0653e-10,
        9.2253e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.511

[Epoch: 84, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0025, 0.9847, 0.0032, 0.0041, 0.0012, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 84, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.4039e-03, 1.4240e-01, 4.0972e-05, 4.5889e-06, 3.2618e-01, 3.5950e-01,
        1.6747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.769

[Epoch: 85, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5286e-01, 8.5634e-03, 7.3505e-03, 1.6779e-07, 1.8151e-02, 1.3053e-02,
        2.1829e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 85, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9313e-03, 1.9996e-02, 6.6242e-08, 1.9899e-08, 2.6865e-03, 3.4772e-03,
        9.6791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.435

[Epoch: 85, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9172e-01, 8.5495e-08, 3.0503e-05, 3.4958e-10, 7.3476e-08, 5.0153e-11,
        8.2499e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.516

[Epoch: 85, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.9799, 0.0036, 0.0040, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 85, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.9575e-03, 1.2703e-01, 3.3794e-05, 7.7533e-06, 3.5381e-01, 2.8992e-01,
        2.2324e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.780

[Epoch: 86, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6404e-01, 5.5942e-03, 5.7903e-03, 1.1407e-07, 1.2040e-02, 1.2521e-02,
        1.0988e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.931

[Epoch: 86, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.1311e-03, 2.6281e-02, 1.0037e-07, 3.2189e-08, 3.0134e-03, 3.6918e-03,
        9.5888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 86, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9433e-01, 8.2954e-08, 4.8405e-05, 7.2047e-10, 7.7943e-08, 1.4785e-10,
        5.6164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.450

[Epoch: 86, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0039, 0.9766, 0.0038, 0.0044, 0.0040, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 86, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6405e-03, 1.4791e-01, 2.2356e-05, 4.0923e-06, 2.8936e-01, 4.0015e-01,
        1.5591e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.753

[Epoch: 87, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6661e-01, 5.3787e-03, 5.2296e-03, 1.6293e-07, 1.1183e-02, 1.1571e-02,
        2.4430e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 87, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4508e-03, 2.3400e-02, 1.0726e-07, 2.2544e-08, 3.3807e-03, 3.0947e-03,
        9.6367e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 87, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9520e-01, 5.5181e-08, 2.4216e-05, 3.0062e-10, 4.5647e-08, 3.0414e-11,
        4.7726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 87, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0023, 0.9836, 0.0034, 0.0035, 0.0018, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 87, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.5284e-03, 1.4051e-01, 2.1654e-05, 9.9415e-07, 3.1646e-01, 3.0418e-01,
        2.3129e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.808

[Epoch: 88, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.4675e-01, 1.0292e-02, 1.1189e-02, 3.3771e-08, 1.7051e-02, 1.4686e-02,
        3.5245e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 88, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.4141e-03, 2.7047e-02, 8.7430e-08, 4.9680e-08, 3.1585e-03, 4.3007e-03,
        9.5808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.423

[Epoch: 88, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8986e-01, 2.9956e-07, 4.8064e-05, 1.0065e-09, 1.7773e-07, 1.6010e-10,
        1.0087e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.506

[Epoch: 88, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0037, 0.9786, 0.0035, 0.0041, 0.0034, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 88, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.4464e-03, 1.3747e-01, 6.0620e-05, 4.9910e-06, 3.6001e-01, 3.3120e-01,
        1.6582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.749

[Epoch: 89, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6276e-01, 5.0857e-03, 5.6962e-03, 7.4245e-08, 1.1281e-02, 1.5142e-02,
        3.0272e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 89, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.1291e-03, 1.6266e-02, 6.2610e-08, 8.8679e-09, 3.1940e-03, 2.9141e-03,
        9.7250e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.442

[Epoch: 89, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9424e-01, 6.1430e-08, 2.9306e-05, 5.4551e-10, 9.1950e-08, 1.7693e-10,
        5.7255e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.478

[Epoch: 89, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0040, 0.9768, 0.0038, 0.0044, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 89, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.1846e-03, 1.3810e-01, 2.5587e-05, 2.8688e-06, 2.9263e-01, 3.6367e-01,
        1.9939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.799

[Epoch: 90, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6840e-01, 6.9800e-03, 5.0644e-03, 2.4752e-08, 1.0008e-02, 9.5423e-03,
        8.8472e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.926

[Epoch: 90, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.1485e-03, 4.0278e-02, 1.6862e-07, 5.2425e-08, 4.2512e-03, 5.3922e-03,
        9.4293e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 90, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9400e-01, 1.8516e-07, 3.3164e-05, 5.9934e-10, 5.4683e-08, 1.1558e-10,
        5.9713e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 90, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0030, 0.9831, 0.0032, 0.0030, 0.0023, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 90, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0064e-02, 1.4469e-01, 2.7522e-05, 6.6403e-06, 3.2151e-01, 3.2170e-01,
        2.0200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.763

[Epoch: 91, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5416e-01, 6.4107e-03, 7.5901e-03, 8.6044e-08, 1.7099e-02, 1.4718e-02,
        2.5351e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 91, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.4886e-03, 1.0890e-02, 1.0142e-07, 1.7754e-08, 3.0415e-03, 3.0518e-03,
        9.7953e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 91, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9053e-01, 1.0329e-07, 1.7278e-05, 3.9073e-10, 9.8472e-08, 1.7932e-10,
        9.4525e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.483

[Epoch: 91, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0031, 0.9824, 0.0036, 0.0027, 0.0023, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 91, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.9134e-03, 1.2608e-01, 4.4453e-05, 3.4872e-06, 3.3807e-01, 3.4860e-01,
        1.8229e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 92, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6022e-01, 7.3228e-03, 6.7102e-03, 5.1275e-08, 1.3523e-02, 1.2186e-02,
        4.3281e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 92, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.8547e-03, 2.0557e-02, 1.6873e-07, 4.6013e-08, 3.9536e-03, 4.0238e-03,
        9.6361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.437

[Epoch: 92, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9288e-01, 1.2670e-07, 3.2392e-05, 3.7554e-10, 8.0502e-08, 2.6125e-10,
        7.0851e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.512

[Epoch: 92, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0041, 0.9776, 0.0041, 0.0047, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 92, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6931e-03, 1.5494e-01, 2.9573e-05, 5.5988e-06, 3.1786e-01, 3.2142e-01,
        1.9905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.785

[Epoch: 93, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6260e-01, 5.8987e-03, 6.3587e-03, 7.7865e-08, 1.3142e-02, 1.1977e-02,
        1.8438e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.908

[Epoch: 93, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8723e-03, 3.3811e-02, 8.2284e-08, 1.4244e-08, 3.0425e-03, 3.4022e-03,
        9.5287e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.430

[Epoch: 93, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9434e-01, 6.1322e-08, 1.9166e-05, 1.4262e-10, 6.4493e-08, 6.6672e-11,
        5.6425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.496

[Epoch: 93, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0038, 0.9798, 0.0029, 0.0039, 0.0024, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 93, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4544e-03, 1.2375e-01, 2.8232e-05, 3.5608e-06, 3.2867e-01, 3.4036e-01,
        2.0074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.771

[Epoch: 94, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5437e-01, 6.7995e-03, 7.9507e-03, 7.6962e-08, 1.4527e-02, 1.6329e-02,
        2.7840e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 94, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0064e-03, 1.7925e-02, 1.6002e-07, 3.8890e-08, 4.2950e-03, 4.5410e-03,
        9.6723e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.434

[Epoch: 94, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9431e-01, 8.0414e-08, 3.2030e-05, 3.2354e-10, 1.1797e-07, 3.2555e-10,
        5.6625e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 94, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.9810, 0.0032, 0.0031, 0.0034, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.046

[Epoch: 94, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.9813e-03, 1.5631e-01, 2.9961e-05, 1.0926e-05, 3.1027e-01, 3.7054e-01,
        1.5686e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.776

[Epoch: 95, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5691e-01, 7.6596e-03, 8.2147e-03, 5.7534e-08, 1.1865e-02, 1.5319e-02,
        3.4681e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.911

[Epoch: 95, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.8973e-03, 2.2852e-02, 6.1093e-08, 8.1734e-09, 3.4667e-03, 4.4635e-03,
        9.6132e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.399

[Epoch: 95, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9352e-01, 1.1041e-07, 1.2261e-05, 2.2937e-10, 3.7785e-08, 6.9413e-11,
        6.4724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.508

[Epoch: 95, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.9780, 0.0045, 0.0046, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 95, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.0034e-02, 1.2099e-01, 1.4440e-05, 1.1203e-06, 3.4198e-01, 2.9787e-01,
        2.2912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 96, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7452e-01, 4.0897e-03, 4.1948e-03, 6.2546e-08, 8.4923e-03, 8.6916e-03,
        7.2072e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 96, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.2063e-03, 1.7419e-02, 9.6690e-08, 2.8141e-08, 3.1167e-03, 3.6599e-03,
        9.7060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.472

[Epoch: 96, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9431e-01, 1.2637e-07, 3.2615e-05, 5.7152e-10, 1.1647e-07, 4.1121e-10,
        5.6621e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.457

[Epoch: 96, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0036, 0.9768, 0.0053, 0.0039, 0.0037, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 96, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.3783e-02, 1.4529e-01, 6.3345e-05, 9.1344e-06, 3.2401e-01, 3.1669e-01,
        2.0016e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.753

[Epoch: 97, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5365e-01, 6.7685e-03, 6.7437e-03, 7.0862e-08, 1.5502e-02, 1.7312e-02,
        2.0150e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.867

[Epoch: 97, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.5484e-03, 2.4689e-02, 8.8425e-08, 2.3702e-08, 5.1147e-03, 3.2927e-03,
        9.6036e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.382

[Epoch: 97, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9403e-01, 6.1847e-08, 1.4510e-05, 5.1617e-10, 6.2594e-08, 1.6944e-10,
        5.9598e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.496

[Epoch: 97, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0048, 0.9791, 0.0028, 0.0036, 0.0028, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 97, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.9978e-03, 1.2597e-01, 2.8527e-05, 2.6154e-06, 3.2267e-01, 3.7039e-01,
        1.7594e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 98, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.4752e-01, 9.3012e-03, 8.9750e-03, 4.0692e-08, 1.7402e-02, 1.6778e-02,
        1.8822e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.939

[Epoch: 98, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1655e-03, 3.8116e-02, 9.9638e-08, 2.8832e-08, 3.0958e-03, 5.1812e-03,
        9.4744e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.421

[Epoch: 98, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9004e-01, 2.2800e-07, 3.5346e-05, 8.6357e-10, 1.4341e-07, 5.9198e-10,
        9.9255e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.506

[Epoch: 98, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.9820, 0.0031, 0.0032, 0.0028, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 98, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.5993e-03, 1.8269e-01, 2.8225e-05, 3.2302e-06, 3.0616e-01, 3.0758e-01,
        1.9594e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.796

[Epoch: 99, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7307e-01, 5.0009e-03, 5.0404e-03, 3.9442e-08, 8.9635e-03, 7.9165e-03,
        1.2223e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.929

[Epoch: 99, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0303e-03, 8.8481e-03, 8.0112e-08, 2.6144e-08, 3.6940e-03, 2.4974e-03,
        9.7893e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 99, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9224e-01, 9.0870e-08, 2.6296e-05, 5.2530e-10, 9.0782e-08, 3.1786e-10,
        7.7309e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.463

[Epoch: 99, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0030, 0.9829, 0.0029, 0.0030, 0.0027, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 99, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.4394e-03, 9.4894e-02, 4.0731e-05, 1.1310e-05, 3.6195e-01, 3.3601e-01,
        2.0165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.759

[Epoch: 100, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6139e-01, 5.5280e-03, 6.3646e-03, 5.9739e-08, 1.3352e-02, 1.3350e-02,
        1.0107e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.914

[Epoch: 100, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.0431e-03, 2.6649e-02, 7.4093e-08, 2.8564e-08, 4.9373e-03, 5.3998e-03,
        9.5497e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.415

[Epoch: 100, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9364e-01, 1.5462e-07, 2.6588e-05, 1.0223e-09, 1.0481e-07, 3.5030e-10,
        6.3360e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 100, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0046, 0.9772, 0.0041, 0.0035, 0.0036, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 100, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.6281e-03, 2.0922e-01, 3.1904e-05, 1.8879e-06, 2.8333e-01, 3.2200e-01,
        1.7579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 101, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5051e-01, 8.0950e-03, 9.8453e-03, 2.2586e-07, 1.5571e-02, 1.5914e-02,
        6.0652e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 101, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.0009e-03, 1.8962e-02, 9.8475e-08, 3.1864e-08, 3.3544e-03, 2.5976e-03,
        9.6808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.434

[Epoch: 101, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9214e-01, 6.7185e-08, 1.5783e-05, 3.8433e-10, 8.3114e-08, 1.8385e-10,
        7.8451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.499

[Epoch: 101, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.9808, 0.0037, 0.0030, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 101, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.8951e-03, 1.2684e-01, 2.1599e-05, 8.4207e-07, 3.0531e-01, 3.6107e-01,
        2.0086e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.782

[Epoch: 102, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6542e-01, 5.2557e-03, 6.8650e-03, 7.3728e-08, 1.1403e-02, 1.1037e-02,
        2.3144e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.929

[Epoch: 102, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.2588e-03, 2.6598e-02, 8.8184e-08, 3.5849e-08, 3.1645e-03, 3.4257e-03,
        9.5955e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.445

[Epoch: 102, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9299e-01, 6.3067e-08, 2.7548e-05, 3.5737e-10, 8.9820e-08, 1.5795e-10,
        6.9832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.484

[Epoch: 102, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0038, 0.9783, 0.0033, 0.0040, 0.0037, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 102, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.5519e-03, 1.4054e-01, 4.7030e-05, 5.8689e-06, 3.4905e-01, 3.2318e-01,
        1.7962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.752

[Epoch: 103, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6138e-01, 6.3585e-03, 6.6480e-03, 1.6429e-07, 1.2129e-02, 1.3457e-02,
        2.9583e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.923

[Epoch: 103, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6699e-03, 1.9889e-02, 6.2283e-08, 2.0878e-08, 3.2762e-03, 2.9121e-03,
        9.6825e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.404

[Epoch: 103, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9370e-01, 7.8744e-08, 2.1185e-05, 5.7017e-10, 1.1382e-07, 2.3086e-10,
        6.2834e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 103, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0038, 0.9801, 0.0032, 0.0031, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 103, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.4984e-03, 1.3972e-01, 2.3779e-05, 3.2497e-06, 3.1364e-01, 3.4337e-01,
        1.9774e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.790

[Epoch: 104, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5786e-01, 6.5198e-03, 7.7108e-03, 1.0471e-07, 1.5347e-02, 1.2521e-02,
        4.6300e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.907

[Epoch: 104, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.0321e-03, 2.7078e-02, 1.0843e-07, 3.9692e-08, 4.0896e-03, 3.5590e-03,
        9.5724e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.430

[Epoch: 104, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9267e-01, 1.0908e-07, 2.7041e-05, 7.6672e-10, 1.6588e-07, 2.7582e-10,
        7.3056e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.484

[Epoch: 104, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.9817, 0.0033, 0.0029, 0.0028, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 104, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0541e-03, 1.3382e-01, 2.7031e-05, 2.3252e-06, 3.2929e-01, 3.3520e-01,
        1.9560e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.771

[Epoch: 105, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6170e-01, 6.7233e-03, 6.5600e-03, 1.0421e-07, 1.2542e-02, 1.2453e-02,
        1.8149e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 105, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7054e-03, 2.2066e-02, 7.5800e-08, 2.2731e-08, 3.5033e-03, 3.5592e-03,
        9.6517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.419

[Epoch: 105, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9319e-01, 5.8800e-08, 1.5834e-05, 4.2538e-10, 7.8315e-08, 1.5725e-10,
        6.7892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.478

[Epoch: 105, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.9811, 0.0033, 0.0032, 0.0031, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 105, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.4666e-03, 1.5340e-01, 2.2855e-05, 3.4210e-06, 3.1200e-01, 3.3356e-01,
        1.9255e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.772

[Epoch: 106, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5712e-01, 6.3957e-03, 7.1474e-03, 7.2320e-08, 1.4552e-02, 1.4761e-02,
        1.9979e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 106, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2585e-03, 2.0874e-02, 6.8642e-08, 2.2764e-08, 3.2909e-03, 2.9992e-03,
        9.6658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.450

[Epoch: 106, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9408e-01, 8.5991e-08, 2.1639e-05, 5.0764e-10, 1.1282e-07, 1.8400e-10,
        5.8946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.481

[Epoch: 106, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0041, 0.9770, 0.0039, 0.0038, 0.0040, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 106, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0816e-03, 1.2901e-01, 2.4992e-05, 2.6347e-06, 3.3080e-01, 3.4442e-01,
        1.8966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.778

[Epoch: 107, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6149e-01, 6.2920e-03, 6.8056e-03, 8.1266e-08, 1.1486e-02, 1.3887e-02,
        3.8881e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.925

[Epoch: 107, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.3057e-03, 2.7382e-02, 1.0133e-07, 3.5431e-08, 4.3296e-03, 3.8995e-03,
        9.5708e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.395

[Epoch: 107, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9248e-01, 4.8174e-08, 1.1868e-05, 2.5862e-10, 6.3895e-08, 1.4830e-10,
        7.5065e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 107, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.9798, 0.0036, 0.0033, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 107, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5801e-03, 1.4737e-01, 1.8216e-05, 2.5698e-06, 3.1329e-01, 3.2758e-01,
        2.0516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.778

[Epoch: 108, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5700e-01, 7.7234e-03, 7.0643e-03, 7.9195e-08, 1.4560e-02, 1.3627e-02,
        2.2582e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.923

[Epoch: 108, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7683e-03, 2.0030e-02, 6.7910e-08, 1.9724e-08, 3.0748e-03, 3.4679e-03,
        9.6766e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.444

[Epoch: 108, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9353e-01, 7.9699e-08, 2.9227e-05, 7.1493e-10, 9.7990e-08, 2.0304e-10,
        6.4405e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 108, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0031, 0.9828, 0.0029, 0.0031, 0.0027, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 108, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5248e-03, 1.3287e-01, 2.9283e-05, 3.9099e-06, 3.2038e-01, 3.6659e-01,
        1.7360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.768

[Epoch: 109, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6094e-01, 6.6630e-03, 6.4258e-03, 5.3641e-08, 1.3103e-02, 1.2860e-02,
        1.0716e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.922

[Epoch: 109, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4592e-03, 2.1902e-02, 5.4413e-08, 1.0291e-08, 2.7689e-03, 2.7488e-03,
        9.6612e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.429

[Epoch: 109, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9227e-01, 7.7731e-08, 1.0551e-05, 5.1961e-10, 6.7320e-08, 1.9247e-10,
        7.7222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 109, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0041, 0.9815, 0.0024, 0.0028, 0.0032, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 109, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.8301e-03, 1.5146e-01, 1.7423e-05, 3.0282e-06, 3.6280e-01, 2.5697e-01,
        2.2092e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.785

[Epoch: 110, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6561e-01, 5.4849e-03, 6.1297e-03, 1.4667e-07, 1.2126e-02, 1.0622e-02,
        2.9557e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.888

[Epoch: 110, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9173e-03, 2.9420e-02, 9.1900e-08, 6.6189e-08, 4.3481e-03, 3.0082e-03,
        9.5731e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.431

[Epoch: 110, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9386e-01, 4.4861e-08, 2.1525e-05, 2.5817e-10, 6.4954e-08, 4.1038e-10,
        6.1172e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 110, batch: 156/195] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0034, 0.9793, 0.0044, 0.0029, 0.0022, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 110, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5524e-03, 1.3873e-01, 2.5335e-05, 3.5921e-06, 2.6310e-01, 4.1845e-01,
        1.7314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.728

[Epoch: 111, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.4142e-01, 9.4428e-03, 8.5084e-03, 7.0129e-08, 2.0795e-02, 1.9810e-02,
        2.6622e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 111, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.6010e-03, 1.9257e-02, 1.0409e-07, 8.5887e-08, 4.7688e-03, 4.2935e-03,
        9.6308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.386

[Epoch: 111, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9530e-01, 7.4314e-08, 1.3705e-05, 4.3463e-10, 4.9895e-08, 1.9575e-10,
        4.6906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.510

[Epoch: 111, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.9784, 0.0034, 0.0047, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 111, batch: 195/195] total loss per batch: 0.525
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.0282e-03, 1.2487e-01, 2.8867e-05, 1.9543e-06, 3.3986e-01, 3.3731e-01,
        1.8991e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.848

[Epoch: 112, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6626e-01, 5.9756e-03, 6.4586e-03, 5.4134e-08, 1.1300e-02, 9.9956e-03,
        9.5949e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 112, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6195e-03, 1.9269e-02, 8.1957e-08, 2.4765e-08, 2.4079e-03, 3.2661e-03,
        9.6944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.455

[Epoch: 112, batch: 117/195] total loss per batch: 0.521
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9114e-01, 1.9612e-07, 4.5769e-05, 3.6141e-09, 1.9791e-07, 6.5424e-10,
        8.8107e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.507

[Epoch: 112, batch: 156/195] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0042, 0.9785, 0.0033, 0.0035, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.061

[Epoch: 112, batch: 195/195] total loss per batch: 0.537
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.2122e-03, 1.6274e-01, 2.2523e-05, 2.3145e-06, 3.3909e-01, 3.0748e-01,
        1.8646e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.706

[Epoch: 113, batch: 39/195] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.7319e-01, 7.2929e-03, 5.1368e-03, 7.7463e-08, 5.7241e-03, 8.5907e-03,
        6.7627e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.953

[Epoch: 113, batch: 78/195] total loss per batch: 0.558
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1479e-03, 2.0707e-02, 4.1034e-08, 1.1784e-08, 3.1728e-03, 3.8922e-03,
        9.6608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.383

[Epoch: 113, batch: 117/195] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9407e-01, 2.3272e-07, 1.3558e-05, 6.6871e-10, 1.9532e-08, 4.3511e-11,
        5.9177e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 113, batch: 156/195] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0027, 0.9858, 0.0021, 0.0022, 0.0015, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 113, batch: 195/195] total loss per batch: 0.541
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.7091e-03, 1.2696e-01, 1.9854e-05, 2.0773e-06, 2.8076e-01, 3.6620e-01,
        2.1635e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.827

[Epoch: 114, batch: 39/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5711e-01, 7.7203e-03, 9.8049e-03, 6.5370e-08, 1.3813e-02, 1.1511e-02,
        3.9717e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.956

[Epoch: 114, batch: 78/195] total loss per batch: 0.552
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7951e-03, 1.4943e-02, 6.0657e-08, 8.1197e-09, 3.7067e-03, 3.9091e-03,
        9.7165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.493

[Epoch: 114, batch: 117/195] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9252e-01, 4.3458e-07, 1.5122e-05, 4.7137e-09, 2.8999e-08, 8.0327e-11,
        7.4619e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.463

[Epoch: 114, batch: 156/195] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0037, 0.9833, 0.0021, 0.0031, 0.0019, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 114, batch: 195/195] total loss per batch: 0.527
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.8865e-03, 1.3398e-01, 3.5869e-05, 2.7519e-06, 3.5250e-01, 3.2433e-01,
        1.8426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.787

[Epoch: 115, batch: 39/195] total loss per batch: 0.515
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5853e-01, 1.1246e-02, 8.6177e-03, 3.4121e-08, 1.3555e-02, 7.9803e-03,
        7.4733e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.959

[Epoch: 115, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.8452e-03, 2.1603e-02, 1.1028e-07, 1.0428e-08, 3.6967e-03, 3.9296e-03,
        9.6493e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.439

[Epoch: 115, batch: 117/195] total loss per batch: 0.520
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9301e-01, 2.3337e-07, 1.4831e-05, 2.7832e-09, 5.4826e-08, 1.3105e-10,
        6.9735e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.482

[Epoch: 115, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0043, 0.9795, 0.0027, 0.0039, 0.0023, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 115, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.0753e-03, 1.4077e-01, 3.2206e-05, 3.0751e-06, 3.2545e-01, 3.3719e-01,
        1.8748e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.786

[Epoch: 116, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5361e-01, 1.1007e-02, 8.9328e-03, 3.7186e-08, 1.4547e-02, 1.1857e-02,
        4.1705e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.947

[Epoch: 116, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7877e-03, 2.4078e-02, 9.9669e-08, 8.6771e-09, 3.9015e-03, 3.9264e-03,
        9.6231e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 116, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9235e-01, 1.9457e-07, 1.1627e-05, 2.1647e-09, 4.7718e-08, 1.3842e-10,
        7.6423e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.481

[Epoch: 116, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0041, 0.9805, 0.0028, 0.0037, 0.0022, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 116, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.4840e-03, 1.3877e-01, 3.1168e-05, 3.1500e-06, 3.2341e-01, 3.3342e-01,
        1.9688e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.777

[Epoch: 117, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5409e-01, 1.0661e-02, 8.5146e-03, 3.8996e-08, 1.3805e-02, 1.2886e-02,
        4.7578e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.943

[Epoch: 117, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6667e-03, 2.3370e-02, 7.9755e-08, 7.6697e-09, 3.8896e-03, 3.8554e-03,
        9.6322e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.426

[Epoch: 117, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9232e-01, 1.8697e-07, 1.0144e-05, 1.7568e-09, 3.8648e-08, 1.0828e-10,
        7.6692e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 117, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0041, 0.9802, 0.0030, 0.0037, 0.0023, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 117, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.1083e-03, 1.3928e-01, 2.7229e-05, 2.7538e-06, 3.2385e-01, 3.3561e-01,
        1.9412e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 118, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5436e-01, 1.0219e-02, 8.2251e-03, 4.0328e-08, 1.3831e-02, 1.3316e-02,
        4.6975e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.939

[Epoch: 118, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.8465e-03, 2.4425e-02, 7.0167e-08, 6.5611e-09, 3.8515e-03, 3.7486e-03,
        9.6213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.426

[Epoch: 118, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9278e-01, 1.7045e-07, 9.5620e-06, 1.4995e-09, 3.3994e-08, 9.2694e-11,
        7.2106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.486

[Epoch: 118, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0040, 0.9802, 0.0030, 0.0037, 0.0023, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 118, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.8011e-03, 1.4061e-01, 2.5596e-05, 2.5413e-06, 3.2304e-01, 3.3708e-01,
        1.9244e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 119, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5447e-01, 9.8625e-03, 8.0606e-03, 3.9238e-08, 1.3689e-02, 1.3876e-02,
        4.6186e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.936

[Epoch: 119, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9634e-03, 2.4170e-02, 6.2548e-08, 5.6943e-09, 3.7797e-03, 3.6863e-03,
        9.6240e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 119, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9272e-01, 1.5380e-07, 8.7060e-06, 1.2690e-09, 3.0247e-08, 8.1786e-11,
        7.2703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 119, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0040, 0.9801, 0.0031, 0.0037, 0.0024, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 119, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6902e-03, 1.4015e-01, 2.3914e-05, 2.3466e-06, 3.2275e-01, 3.3627e-01,
        1.9410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 120, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5562e-01, 9.4510e-03, 7.8258e-03, 3.7461e-08, 1.3343e-02, 1.3720e-02,
        4.4893e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.933

[Epoch: 120, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0386e-03, 2.3669e-02, 5.7517e-08, 5.2557e-09, 3.6973e-03, 3.6350e-03,
        9.6296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 120, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9307e-01, 1.3800e-07, 8.2236e-06, 1.1209e-09, 2.7490e-08, 7.3757e-11,
        6.9255e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 120, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0039, 0.9801, 0.0031, 0.0037, 0.0025, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 120, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5868e-03, 1.3980e-01, 2.2512e-05, 2.1952e-06, 3.2271e-01, 3.3869e-01,
        1.9218e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 121, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5597e-01, 9.1446e-03, 7.7049e-03, 3.5153e-08, 1.3269e-02, 1.3863e-02,
        4.4201e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.930

[Epoch: 121, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2250e-03, 2.3666e-02, 5.1645e-08, 4.4899e-09, 3.6102e-03, 3.5884e-03,
        9.6291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 121, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9293e-01, 1.2407e-07, 7.2920e-06, 9.2863e-10, 2.4260e-08, 6.0784e-11,
        7.0629e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 121, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.9798, 0.0032, 0.0038, 0.0026, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 121, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5867e-03, 1.4054e-01, 2.1212e-05, 2.0425e-06, 3.2347e-01, 3.3556e-01,
        1.9382e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.773

[Epoch: 122, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5756e-01, 8.6969e-03, 7.4602e-03, 3.2934e-08, 1.2807e-02, 1.3430e-02,
        4.1888e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.926

[Epoch: 122, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1511e-03, 2.3424e-02, 4.7433e-08, 4.0962e-09, 3.5519e-03, 3.5490e-03,
        9.6332e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 122, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9330e-01, 1.1409e-07, 7.1696e-06, 8.6064e-10, 2.2561e-08, 5.9847e-11,
        6.6959e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 122, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0038, 0.9799, 0.0032, 0.0037, 0.0026, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 122, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6091e-03, 1.3956e-01, 1.9925e-05, 1.8917e-06, 3.2233e-01, 3.3688e-01,
        1.9460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.776

[Epoch: 123, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5686e-01, 8.5723e-03, 7.3464e-03, 3.0564e-08, 1.3315e-02, 1.3860e-02,
        4.1461e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 123, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4764e-03, 2.3562e-02, 4.2442e-08, 3.5330e-09, 3.5327e-03, 3.5201e-03,
        9.6291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 123, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9292e-01, 1.0031e-07, 6.1407e-06, 7.0395e-10, 2.0376e-08, 4.8984e-11,
        7.0774e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 123, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.9793, 0.0033, 0.0038, 0.0027, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 123, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5327e-03, 1.4051e-01, 1.8582e-05, 1.7297e-06, 3.2458e-01, 3.3842e-01,
        1.8994e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.772

[Epoch: 124, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5818e-01, 8.1634e-03, 7.3912e-03, 3.1055e-08, 1.2952e-02, 1.3278e-02,
        3.9928e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 124, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1320e-03, 2.3479e-02, 3.9757e-08, 3.1508e-09, 3.4074e-03, 3.3510e-03,
        9.6363e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 124, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9350e-01, 9.1644e-08, 6.0061e-06, 6.3300e-10, 1.7906e-08, 4.3565e-11,
        6.4923e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 124, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.9798, 0.0033, 0.0037, 0.0027, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 124, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.8128e-03, 1.4000e-01, 1.7322e-05, 1.5993e-06, 3.2084e-01, 3.3348e-01,
        1.9885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.778

[Epoch: 125, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5709e-01, 8.1616e-03, 7.0806e-03, 2.5157e-08, 1.3482e-02, 1.4153e-02,
        3.5361e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 125, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6074e-03, 2.3373e-02, 3.4786e-08, 2.8274e-09, 3.4736e-03, 3.5905e-03,
        9.6296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 125, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9266e-01, 8.1708e-08, 5.2138e-06, 5.3859e-10, 1.7643e-08, 4.2154e-11,
        7.3332e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.490

[Epoch: 125, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0039, 0.9791, 0.0034, 0.0038, 0.0029, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 125, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4253e-03, 1.4098e-01, 1.7912e-05, 1.6218e-06, 3.2734e-01, 3.3952e-01,
        1.8571e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 126, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6090e-01, 7.4068e-03, 6.7996e-03, 2.4494e-08, 1.2457e-02, 1.2398e-02,
        3.6345e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.917

[Epoch: 126, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1335e-03, 2.3213e-02, 3.4652e-08, 3.1157e-09, 3.3018e-03, 3.1198e-03,
        9.6423e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 126, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9455e-01, 7.8714e-08, 5.1720e-06, 4.9576e-10, 1.4660e-08, 2.6757e-11,
        5.4472e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 126, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.9804, 0.0032, 0.0035, 0.0027, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 126, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6017e-03, 1.3803e-01, 1.5965e-05, 1.4128e-06, 3.1419e-01, 3.3072e-01,
        2.1044e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 127, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5602e-01, 8.1305e-03, 7.0644e-03, 2.3408e-08, 1.3715e-02, 1.5042e-02,
        3.1748e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 127, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.1487e-03, 2.3858e-02, 2.5298e-08, 1.7705e-09, 3.4702e-03, 3.8227e-03,
        9.6170e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.420

[Epoch: 127, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9214e-01, 8.1421e-08, 4.6287e-06, 4.1710e-10, 1.7999e-08, 4.7711e-11,
        7.8549e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.492

[Epoch: 127, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0039, 0.9782, 0.0036, 0.0039, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 127, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.9219e-03, 1.4167e-01, 1.7012e-05, 1.4022e-06, 3.4892e-01, 3.4033e-01,
        1.6213e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.776

[Epoch: 128, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5873e-01, 7.3049e-03, 7.1028e-03, 2.3731e-08, 1.3318e-02, 1.3516e-02,
        2.4290e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.922

[Epoch: 128, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.0660e-03, 2.2848e-02, 2.1313e-08, 1.9342e-09, 3.5888e-03, 3.0573e-03,
        9.6344e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.419

[Epoch: 128, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9390e-01, 7.2737e-08, 5.2857e-06, 4.3675e-10, 1.5080e-08, 2.2384e-11,
        6.0925e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 128, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0040, 0.9791, 0.0034, 0.0036, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 128, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5644e-03, 1.4494e-01, 1.9305e-05, 1.6282e-06, 2.8343e-01, 3.3574e-01,
        2.2930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.767

[Epoch: 129, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6358e-01, 7.0341e-03, 5.6315e-03, 2.2113e-08, 1.1714e-02, 1.2006e-02,
        3.3137e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.915

[Epoch: 129, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0202e-03, 2.0639e-02, 3.5216e-08, 3.6753e-09, 3.4554e-03, 2.7379e-03,
        9.6715e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.435

[Epoch: 129, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9419e-01, 7.0711e-08, 4.0744e-06, 3.0019e-10, 1.2993e-08, 3.2403e-11,
        5.8106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 129, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0037, 0.9817, 0.0030, 0.0035, 0.0023, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 129, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.9775e-03, 1.3477e-01, 1.9520e-05, 2.2653e-06, 3.6668e-01, 3.2852e-01,
        1.6303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.799

[Epoch: 130, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5880e-01, 7.0932e-03, 6.5186e-03, 2.5952e-08, 1.4231e-02, 1.3338e-02,
        2.1767e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 130, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9960e-03, 2.7848e-02, 2.1823e-08, 2.9297e-09, 3.8999e-03, 3.7698e-03,
        9.5849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.407

[Epoch: 130, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9406e-01, 8.3982e-08, 4.4690e-06, 3.3144e-10, 1.3651e-08, 3.5427e-11,
        5.9349e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 130, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0039, 0.9766, 0.0042, 0.0033, 0.0041, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 130, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.1991e-03, 1.4402e-01, 2.2526e-05, 1.3913e-06, 2.8375e-01, 3.5610e-01,
        2.0791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.743

[Epoch: 131, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5987e-01, 6.6503e-03, 6.4886e-03, 4.2946e-08, 1.2089e-02, 1.4880e-02,
        1.7976e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 131, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4604e-03, 1.3291e-02, 2.9652e-08, 3.0102e-09, 3.7716e-03, 2.5240e-03,
        9.7395e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.440

[Epoch: 131, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9241e-01, 1.1373e-07, 8.0832e-06, 7.8797e-10, 2.6658e-08, 5.6067e-11,
        7.5828e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.479

[Epoch: 131, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.9794, 0.0036, 0.0043, 0.0024, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 131, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5132e-03, 1.4764e-01, 2.1378e-05, 2.5769e-06, 3.6344e-01, 3.0921e-01,
        1.7317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.803

[Epoch: 132, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5782e-01, 7.2854e-03, 6.3385e-03, 2.1074e-08, 1.4741e-02, 1.3766e-02,
        4.8828e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 132, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7290e-03, 3.3160e-02, 3.1153e-08, 7.1007e-09, 4.1355e-03, 2.9938e-03,
        9.5398e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.453

[Epoch: 132, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9271e-01, 1.0114e-07, 5.8720e-06, 3.0733e-10, 1.4000e-08, 5.0797e-11,
        7.2795e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.490

[Epoch: 132, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0037, 0.9818, 0.0032, 0.0029, 0.0030, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 132, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.2379e-03, 1.2397e-01, 1.8264e-05, 2.3500e-06, 2.7498e-01, 3.6697e-01,
        2.2683e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.767

[Epoch: 133, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6139e-01, 6.3024e-03, 6.4800e-03, 4.8606e-08, 1.2608e-02, 1.3201e-02,
        2.1029e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 133, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9085e-03, 1.6631e-02, 3.3505e-08, 2.5759e-09, 4.0848e-03, 3.4120e-03,
        9.6896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.379

[Epoch: 133, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9453e-01, 1.1693e-07, 6.1251e-06, 3.9782e-10, 1.7091e-08, 3.3047e-11,
        5.4645e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.482

[Epoch: 133, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0040, 0.9772, 0.0040, 0.0038, 0.0033, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 133, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0895e-03, 1.6400e-01, 1.7491e-05, 1.8473e-06, 3.5922e-01, 3.1020e-01,
        1.6048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.773

[Epoch: 134, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5982e-01, 7.1428e-03, 7.5041e-03, 2.3100e-08, 1.4003e-02, 1.1509e-02,
        2.6388e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 134, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9574e-03, 2.7968e-02, 1.4460e-08, 1.8983e-09, 3.1918e-03, 2.7459e-03,
        9.6014e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.460

[Epoch: 134, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9205e-01, 1.3905e-07, 9.3357e-06, 7.4560e-10, 2.8569e-08, 7.2442e-11,
        7.9453e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.501

[Epoch: 134, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.9798, 0.0033, 0.0033, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 134, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.7977e-03, 9.4795e-02, 2.4124e-05, 2.9426e-06, 3.1206e-01, 3.6990e-01,
        2.1542e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.761

[Epoch: 135, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6049e-01, 6.2517e-03, 6.5213e-03, 4.5751e-08, 1.2549e-02, 1.4172e-02,
        1.9692e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.908

[Epoch: 135, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.7461e-03, 2.0017e-02, 4.0681e-08, 5.6141e-09, 3.8208e-03, 2.6605e-03,
        9.6576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.396

[Epoch: 135, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9213e-01, 1.1236e-07, 6.7097e-06, 2.6239e-10, 1.4613e-08, 2.6896e-11,
        7.8637e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 135, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0041, 0.9816, 0.0028, 0.0032, 0.0029, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 135, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.7890e-03, 1.8382e-01, 1.9861e-05, 1.4724e-06, 3.1794e-01, 3.1048e-01,
        1.7896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.810

[Epoch: 136, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6077e-01, 6.1926e-03, 6.9503e-03, 5.4185e-08, 1.3104e-02, 1.2953e-02,
        2.6830e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 136, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6777e-03, 2.8318e-02, 3.9874e-08, 6.7835e-09, 3.4720e-03, 3.0700e-03,
        9.5946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.442

[Epoch: 136, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9535e-01, 1.4332e-07, 1.0619e-05, 7.6850e-10, 2.6144e-08, 5.8948e-11,
        4.6426e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 136, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0038, 0.9782, 0.0042, 0.0040, 0.0027, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 136, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.9834e-03, 1.3545e-01, 3.1866e-05, 2.2271e-06, 3.1885e-01, 3.4110e-01,
        1.9958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.730

[Epoch: 137, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5841e-01, 7.2214e-03, 7.3283e-03, 3.3384e-08, 1.3559e-02, 1.3464e-02,
        1.4879e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 137, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8453e-03, 2.3204e-02, 1.9401e-08, 2.0415e-09, 3.4805e-03, 3.6015e-03,
        9.6287e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 137, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9304e-01, 8.6358e-08, 4.5570e-06, 3.5595e-10, 2.0514e-08, 5.2199e-11,
        6.9573e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.495

[Epoch: 137, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0039, 0.9777, 0.0033, 0.0033, 0.0040, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 137, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5809e-03, 1.3215e-01, 1.8376e-05, 1.9647e-06, 3.4934e-01, 3.2903e-01,
        1.8287e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.840

[Epoch: 138, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5441e-01, 7.0236e-03, 6.9775e-03, 4.6375e-08, 1.6100e-02, 1.5462e-02,
        2.5967e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.911

[Epoch: 138, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9536e-03, 1.6535e-02, 3.8537e-08, 9.7231e-09, 3.0588e-03, 2.7055e-03,
        9.7175e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.422

[Epoch: 138, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8920e-01, 1.9601e-07, 1.1561e-05, 1.2704e-09, 4.4986e-08, 1.4101e-10,
        1.0791e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 138, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0033, 0.9826, 0.0028, 0.0028, 0.0030, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.041

[Epoch: 138, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.3285e-03, 1.5367e-01, 2.6438e-05, 1.9019e-06, 2.8979e-01, 3.3906e-01,
        2.0913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.742

[Epoch: 139, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6461e-01, 5.8986e-03, 5.2465e-03, 3.0566e-08, 1.2129e-02, 1.2096e-02,
        2.2928e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 139, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.4633e-03, 3.7044e-02, 2.6617e-08, 5.1656e-09, 4.5749e-03, 4.1791e-03,
        9.4674e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 139, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9559e-01, 9.3248e-08, 6.3064e-06, 3.2060e-10, 1.9394e-08, 4.1947e-11,
        4.4069e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.473

[Epoch: 139, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0033, 0.9805, 0.0037, 0.0042, 0.0024, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 139, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.4552e-03, 1.2887e-01, 2.4285e-05, 3.7241e-06, 3.4420e-01, 3.4505e-01,
        1.7739e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.797

[Epoch: 140, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5910e-01, 7.3288e-03, 6.8676e-03, 3.7884e-08, 1.2465e-02, 1.4229e-02,
        1.1931e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 140, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.5471e-03, 1.4153e-02, 2.0412e-08, 2.2816e-09, 2.7849e-03, 2.7398e-03,
        9.7578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.426

[Epoch: 140, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9330e-01, 1.5242e-07, 8.7770e-06, 5.7468e-10, 2.4370e-08, 3.6936e-11,
        6.6927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 140, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0036, 0.9791, 0.0028, 0.0029, 0.0038, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 140, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.3301e-03, 1.4221e-01, 2.0372e-05, 1.9522e-06, 3.4899e-01, 3.1091e-01,
        1.9154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.796

[Epoch: 141, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5552e-01, 7.3097e-03, 7.0354e-03, 5.0414e-08, 1.5567e-02, 1.4541e-02,
        2.2053e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 141, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3092e-03, 2.5863e-02, 4.1804e-08, 7.6339e-09, 2.9166e-03, 3.1726e-03,
        9.6174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.470

[Epoch: 141, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9150e-01, 9.9026e-08, 8.5027e-06, 6.3308e-10, 2.5308e-08, 4.9647e-11,
        8.4957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 141, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.9826, 0.0030, 0.0030, 0.0024, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 141, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([1.2871e-02, 1.4796e-01, 2.1791e-05, 1.4731e-06, 2.6716e-01, 3.5034e-01,
        2.2164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.743

[Epoch: 142, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6456e-01, 6.8884e-03, 6.2548e-03, 3.2857e-08, 1.0840e-02, 1.1446e-02,
        1.5849e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.891

[Epoch: 142, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.9020e-03, 2.3660e-02, 2.9614e-08, 4.8292e-09, 3.9855e-03, 3.5937e-03,
        9.6086e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.373

[Epoch: 142, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9332e-01, 9.0188e-08, 6.5210e-06, 4.1885e-10, 1.7512e-08, 4.9477e-11,
        6.6692e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.483

[Epoch: 142, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.9788, 0.0042, 0.0045, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 142, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([3.1423e-03, 1.3037e-01, 2.4947e-05, 3.7042e-06, 3.4316e-01, 3.5086e-01,
        1.7243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.799

[Epoch: 143, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5924e-01, 7.7549e-03, 6.9604e-03, 2.1865e-08, 1.1301e-02, 1.4736e-02,
        1.1552e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.935

[Epoch: 143, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1951e-03, 2.1415e-02, 1.7175e-08, 3.1493e-09, 3.3074e-03, 3.6279e-03,
        9.6545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.458

[Epoch: 143, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9424e-01, 7.7158e-08, 7.2516e-06, 6.5025e-10, 3.3606e-08, 4.8504e-11,
        5.7489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.489

[Epoch: 143, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0037, 0.9784, 0.0033, 0.0037, 0.0037, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 143, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6293e-03, 1.5705e-01, 2.0056e-05, 3.8097e-06, 3.3458e-01, 2.9945e-01,
        2.0227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.751

[Epoch: 144, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6166e-01, 6.3541e-03, 6.2329e-03, 8.2696e-08, 1.5441e-02, 1.0279e-02,
        3.1556e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 144, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.3934e-03, 2.2037e-02, 6.7599e-08, 1.8318e-08, 3.5461e-03, 3.6118e-03,
        9.6341e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.426

[Epoch: 144, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9394e-01, 8.3805e-08, 8.8997e-06, 6.3971e-10, 2.9446e-08, 4.5732e-11,
        6.0557e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.473

[Epoch: 144, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.9806, 0.0036, 0.0029, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 144, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4028e-03, 1.3844e-01, 1.4846e-05, 1.8903e-06, 3.1224e-01, 3.6083e-01,
        1.8207e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.787

[Epoch: 145, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5340e-01, 7.6938e-03, 7.5260e-03, 4.9760e-08, 1.5570e-02, 1.5790e-02,
        2.2455e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.914

[Epoch: 145, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6839e-03, 3.0424e-02, 2.1886e-08, 3.5841e-09, 3.4590e-03, 3.2542e-03,
        9.5618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 145, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9241e-01, 8.4278e-08, 8.6112e-06, 8.9822e-10, 3.7406e-08, 8.4612e-11,
        7.5788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.481

[Epoch: 145, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.9793, 0.0034, 0.0037, 0.0037, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 145, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4536e-03, 1.3688e-01, 1.4183e-05, 4.9342e-06, 3.1947e-01, 3.3779e-01,
        1.9939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.753

[Epoch: 146, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6605e-01, 6.7516e-03, 6.3495e-03, 4.8951e-08, 1.0422e-02, 1.0415e-02,
        9.2858e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.926

[Epoch: 146, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6869e-03, 1.6828e-02, 2.3024e-08, 5.5133e-09, 2.6761e-03, 2.8209e-03,
        9.7199e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.440

[Epoch: 146, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9362e-01, 6.0196e-08, 8.7693e-06, 5.2682e-10, 2.9555e-08, 9.8072e-11,
        6.3676e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.505

[Epoch: 146, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.9805, 0.0033, 0.0035, 0.0026, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 146, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.4179e-03, 1.4288e-01, 2.1128e-05, 5.0722e-06, 3.3616e-01, 3.1387e-01,
        1.9765e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.805

[Epoch: 147, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6368e-01, 5.9934e-03, 6.0113e-03, 3.5207e-08, 1.2531e-02, 1.1763e-02,
        1.7221e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.926

[Epoch: 147, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.2755e-03, 2.8289e-02, 3.6674e-08, 7.2546e-09, 5.7817e-03, 4.4718e-03,
        9.5418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.395

[Epoch: 147, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9275e-01, 9.6287e-08, 5.2581e-06, 4.1079e-10, 2.2919e-08, 7.5874e-11,
        7.2425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.496

[Epoch: 147, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.9798, 0.0034, 0.0031, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 147, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.4374e-03, 1.4362e-01, 1.8869e-05, 2.5561e-06, 3.1394e-01, 3.5266e-01,
        1.8533e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.750

[Epoch: 148, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5413e-01, 7.5752e-03, 7.3416e-03, 4.5145e-08, 1.4834e-02, 1.6108e-02,
        1.4974e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 148, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6096e-03, 1.9830e-02, 2.0706e-08, 4.3425e-09, 2.6220e-03, 3.3406e-03,
        9.6760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.461

[Epoch: 148, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9278e-01, 7.8956e-08, 7.1125e-06, 5.4024e-10, 2.3803e-08, 5.2083e-11,
        7.2170e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 148, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0032, 0.9766, 0.0044, 0.0042, 0.0038, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 148, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6805e-03, 1.3767e-01, 2.1322e-05, 4.3629e-06, 3.2233e-01, 3.4138e-01,
        1.9191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.796

[Epoch: 149, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6601e-01, 7.0299e-03, 6.3845e-03, 4.4592e-08, 1.0865e-02, 9.6985e-03,
        1.2557e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 149, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2716e-03, 2.5162e-02, 1.9303e-08, 3.8371e-09, 2.7437e-03, 2.9153e-03,
        9.6291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.420

[Epoch: 149, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9482e-01, 5.8840e-08, 5.4684e-06, 6.3697e-10, 2.3582e-08, 1.6323e-10,
        5.1765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.482

[Epoch: 149, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0036, 0.9806, 0.0032, 0.0035, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 149, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.8470e-03, 1.3106e-01, 1.4410e-05, 4.4253e-06, 3.2796e-01, 3.3692e-01,
        1.9720e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 150, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5876e-01, 6.1208e-03, 5.6700e-03, 4.6882e-08, 1.4697e-02, 1.4750e-02,
        3.8427e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 150, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3983e-03, 2.2479e-02, 4.0062e-08, 1.8286e-08, 4.3424e-03, 3.9927e-03,
        9.6279e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.406

[Epoch: 150, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9123e-01, 5.9624e-08, 1.0402e-05, 2.9961e-10, 2.1351e-08, 3.8182e-11,
        8.7622e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.516

[Epoch: 150, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.9799, 0.0035, 0.0034, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 150, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.4383e-03, 1.6176e-01, 1.3500e-05, 1.7948e-06, 3.1899e-01, 3.1082e-01,
        1.9997e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.782

[Epoch: 151, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5551e-01, 7.1504e-03, 7.8626e-03, 7.7828e-08, 1.4472e-02, 1.4997e-02,
        5.7420e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.898

[Epoch: 151, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7768e-03, 2.0822e-02, 1.6519e-08, 4.1753e-09, 3.1060e-03, 2.6251e-03,
        9.6767e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.455

[Epoch: 151, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9201e-01, 4.6800e-08, 3.4672e-06, 4.8531e-10, 2.1887e-08, 1.6997e-10,
        7.9908e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.469

[Epoch: 151, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0030, 0.9827, 0.0025, 0.0026, 0.0033, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 151, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.4866e-03, 1.3412e-01, 2.3807e-05, 8.2769e-06, 3.1865e-01, 3.6525e-01,
        1.7647e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.737

[Epoch: 152, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6061e-01, 7.1587e-03, 6.7786e-03, 5.1872e-08, 1.1236e-02, 1.4204e-02,
        1.0857e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 152, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.5990e-03, 3.1064e-02, 3.7727e-08, 7.5523e-09, 3.8606e-03, 3.6569e-03,
        9.5582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.415

[Epoch: 152, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9490e-01, 5.0276e-08, 5.7221e-06, 5.2611e-10, 2.2095e-08, 7.2357e-11,
        5.0910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.484

[Epoch: 152, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0038, 0.9801, 0.0037, 0.0031, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 152, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0794e-03, 1.3135e-01, 1.7513e-05, 3.1584e-06, 3.2639e-01, 3.4041e-01,
        1.9476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.797

[Epoch: 153, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6508e-01, 6.1283e-03, 6.1156e-03, 4.7729e-08, 1.2440e-02, 1.0230e-02,
        6.1937e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 153, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6327e-03, 2.1612e-02, 2.4271e-08, 6.4831e-09, 3.4845e-03, 2.9307e-03,
        9.6534e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.436

[Epoch: 153, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9369e-01, 4.1000e-08, 4.4130e-06, 4.4090e-10, 1.4838e-08, 4.3369e-11,
        6.3024e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.486

[Epoch: 153, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.9805, 0.0034, 0.0035, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 153, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2807e-03, 1.4140e-01, 1.7271e-05, 3.4082e-06, 3.2020e-01, 3.2695e-01,
        2.0515e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.781

[Epoch: 154, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5721e-01, 6.7351e-03, 6.7622e-03, 4.3176e-08, 1.6025e-02, 1.3265e-02,
        5.8122e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.918

[Epoch: 154, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3641e-03, 2.5376e-02, 3.5119e-08, 9.1536e-09, 3.5228e-03, 3.3522e-03,
        9.6139e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.437

[Epoch: 154, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9336e-01, 3.8207e-08, 5.2415e-06, 4.3124e-10, 1.6710e-08, 4.6647e-11,
        6.6339e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 154, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.9789, 0.0035, 0.0037, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 154, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0034e-03, 1.4268e-01, 1.7579e-05, 3.5352e-06, 3.2561e-01, 3.3690e-01,
        1.8778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.771

[Epoch: 155, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5930e-01, 6.5890e-03, 6.6099e-03, 3.9396e-08, 1.3332e-02, 1.4163e-02,
        7.2285e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 155, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3858e-03, 2.3953e-02, 2.0156e-08, 4.1056e-09, 3.3775e-03, 3.3514e-03,
        9.6293e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.409

[Epoch: 155, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9230e-01, 3.6481e-08, 3.8806e-06, 3.5271e-10, 1.3857e-08, 4.0681e-11,
        7.6929e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.499

[Epoch: 155, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0040, 0.9786, 0.0037, 0.0032, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 155, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0612e-03, 1.3775e-01, 1.4815e-05, 3.5379e-06, 3.1814e-01, 3.4431e-01,
        1.9372e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.769

[Epoch: 156, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6337e-01, 6.2901e-03, 6.3062e-03, 4.1264e-08, 1.1719e-02, 1.2313e-02,
        6.2674e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.922

[Epoch: 156, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4309e-03, 1.9671e-02, 3.5075e-08, 1.0147e-08, 3.4302e-03, 2.9927e-03,
        9.6748e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.428

[Epoch: 156, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9336e-01, 4.5573e-08, 5.2751e-06, 4.7284e-10, 1.8418e-08, 5.0435e-11,
        6.6299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.485

[Epoch: 156, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0033, 0.9811, 0.0035, 0.0030, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 156, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0310e-03, 1.4606e-01, 1.5486e-05, 3.0130e-06, 3.2727e-01, 3.2846e-01,
        1.9116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.782

[Epoch: 157, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5667e-01, 7.2328e-03, 6.6778e-03, 4.2259e-08, 1.4942e-02, 1.4470e-02,
        5.5755e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.901

[Epoch: 157, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.9945e-03, 2.6377e-02, 1.7598e-08, 3.3453e-09, 3.0699e-03, 3.2643e-03,
        9.6029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.433

[Epoch: 157, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9508e-01, 3.2714e-08, 3.7807e-06, 3.1130e-10, 1.1449e-08, 3.8867e-11,
        4.9164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.480

[Epoch: 157, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0027, 0.9823, 0.0032, 0.0029, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 157, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.9576e-03, 1.3455e-01, 1.7174e-05, 3.8483e-06, 3.2421e-01, 3.4048e-01,
        1.9377e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.761

[Epoch: 158, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6184e-01, 6.6767e-03, 6.6830e-03, 2.9481e-08, 1.3069e-02, 1.1725e-02,
        4.3685e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.914

[Epoch: 158, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.1246e-03, 2.2362e-02, 2.9830e-08, 6.7522e-09, 3.4663e-03, 3.5011e-03,
        9.6355e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 158, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9177e-01, 3.4058e-08, 4.6121e-06, 3.7468e-10, 1.5407e-08, 3.2608e-11,
        8.2286e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 158, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.9795, 0.0034, 0.0034, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 158, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.7470e-03, 1.3579e-01, 1.4125e-05, 2.9595e-06, 3.3068e-01, 3.3162e-01,
        1.9614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.798

[Epoch: 159, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5746e-01, 6.7216e-03, 7.0913e-03, 3.2854e-08, 1.4872e-02, 1.3845e-02,
        5.8674e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 159, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6879e-03, 2.5688e-02, 1.3676e-08, 3.1064e-09, 3.2967e-03, 3.5172e-03,
        9.6081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 159, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9363e-01, 2.4012e-08, 2.7689e-06, 2.0645e-10, 1.1163e-08, 3.8645e-11,
        6.3679e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.495

[Epoch: 159, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0039, 0.9784, 0.0034, 0.0035, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 159, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.3499e-03, 1.5084e-01, 1.4196e-05, 3.8287e-06, 3.0084e-01, 3.4488e-01,
        1.9607e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.741

[Epoch: 160, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6119e-01, 6.6020e-03, 6.5551e-03, 4.9511e-08, 1.1225e-02, 1.4420e-02,
        4.4925e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 160, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9269e-03, 1.8673e-02, 4.4782e-08, 9.5917e-09, 2.9121e-03, 3.3467e-03,
        9.6914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.463

[Epoch: 160, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9199e-01, 6.4128e-08, 8.2402e-06, 1.0812e-09, 3.6537e-08, 9.5580e-11,
        7.9980e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 160, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.9825, 0.0033, 0.0027, 0.0028, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 160, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0963e-03, 1.3889e-01, 1.4395e-05, 2.2149e-06, 3.3171e-01, 3.3187e-01,
        1.9042e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.809

[Epoch: 161, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5958e-01, 6.9658e-03, 5.8592e-03, 3.5649e-08, 1.2960e-02, 1.4630e-02,
        6.9221e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 161, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.2356e-03, 2.8095e-02, 1.5448e-08, 3.8149e-09, 3.2130e-03, 2.7611e-03,
        9.5870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 161, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9658e-01, 3.3834e-08, 3.3110e-06, 2.3780e-10, 1.0919e-08, 2.2324e-11,
        3.4131e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.466

[Epoch: 161, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0036, 0.9797, 0.0038, 0.0035, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 161, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.5579e-03, 1.3221e-01, 1.1692e-05, 1.4359e-06, 3.3926e-01, 3.4168e-01,
        1.8228e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.786

[Epoch: 162, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6849e-01, 6.4112e-03, 5.2811e-03, 3.5672e-08, 1.0077e-02, 9.7346e-03,
        5.7003e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.890

[Epoch: 162, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.7167e-03, 2.0870e-02, 4.0414e-08, 1.6533e-08, 4.5627e-03, 4.7626e-03,
        9.6209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.410

[Epoch: 162, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9089e-01, 5.1751e-08, 5.3564e-06, 4.0823e-10, 1.7472e-08, 7.9453e-11,
        9.1047e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.520

[Epoch: 162, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0039, 0.9751, 0.0047, 0.0040, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 162, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([9.9109e-03, 1.4414e-01, 1.7877e-05, 4.9197e-06, 3.0775e-01, 3.2228e-01,
        2.1590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.751

[Epoch: 163, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5686e-01, 5.7317e-03, 6.9281e-03, 7.4735e-08, 1.8423e-02, 1.2051e-02,
        2.2762e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.930

[Epoch: 163, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7073e-03, 2.5122e-02, 1.9310e-08, 5.3037e-09, 3.8212e-03, 3.3827e-03,
        9.6197e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 163, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9025e-01, 3.6120e-08, 4.1084e-06, 4.2867e-10, 4.4183e-08, 1.1548e-10,
        9.7462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.529

[Epoch: 163, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0028, 0.9837, 0.0026, 0.0025, 0.0029, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.024

[Epoch: 163, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4210e-03, 1.5890e-01, 1.9123e-05, 5.1764e-06, 3.0753e-01, 3.4711e-01,
        1.8001e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 164, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5670e-01, 6.3893e-03, 6.4826e-03, 3.6064e-08, 1.5065e-02, 1.5354e-02,
        4.9638e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.930

[Epoch: 164, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.3170e-03, 2.2712e-02, 3.0372e-08, 1.3298e-08, 3.8962e-03, 4.3033e-03,
        9.6077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.423

[Epoch: 164, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9535e-01, 5.3923e-08, 3.7579e-06, 5.0723e-10, 3.0420e-08, 1.9060e-10,
        4.6503e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.451

[Epoch: 164, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0041, 0.9792, 0.0034, 0.0037, 0.0029, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 164, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.8420e-03, 1.2657e-01, 3.0210e-05, 3.1080e-06, 3.3940e-01, 3.3738e-01,
        1.9078e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.779

[Epoch: 165, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5751e-01, 7.4039e-03, 6.4370e-03, 3.8213e-08, 1.5296e-02, 1.3352e-02,
        5.1290e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 165, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6357e-03, 1.9057e-02, 2.8724e-08, 8.1396e-09, 2.6407e-03, 3.1666e-03,
        9.6850e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.430

[Epoch: 165, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9371e-01, 6.3446e-08, 3.5479e-06, 3.8133e-10, 1.7905e-08, 1.0903e-10,
        6.2862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.476

[Epoch: 165, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.9770, 0.0041, 0.0046, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 165, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0526e-03, 1.4878e-01, 1.9556e-05, 3.6625e-06, 3.2493e-01, 3.3262e-01,
        1.8660e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 166, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6401e-01, 6.2090e-03, 5.7974e-03, 3.1502e-08, 1.1640e-02, 1.2338e-02,
        3.0982e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 166, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3504e-03, 2.4998e-02, 3.0928e-08, 7.7553e-09, 2.7107e-03, 3.6790e-03,
        9.6226e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 166, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9378e-01, 4.1572e-08, 2.8274e-06, 3.5582e-10, 1.3437e-08, 5.7282e-11,
        6.2125e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.492

[Epoch: 166, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0038, 0.9793, 0.0033, 0.0035, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 166, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5744e-03, 1.3644e-01, 1.9639e-05, 2.7153e-06, 3.1506e-01, 3.3654e-01,
        2.0537e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 167, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5670e-01, 7.5143e-03, 7.2135e-03, 2.8447e-08, 1.3759e-02, 1.4811e-02,
        3.7385e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.924

[Epoch: 167, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.0632e-03, 2.2722e-02, 2.7332e-08, 9.5424e-09, 3.4737e-03, 3.9015e-03,
        9.6284e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 167, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9306e-01, 5.2511e-08, 3.1720e-06, 3.7385e-10, 1.6661e-08, 1.0861e-10,
        6.9327e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.492

[Epoch: 167, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0037, 0.9788, 0.0034, 0.0038, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 167, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0194e-03, 1.3691e-01, 1.7916e-05, 2.4960e-06, 3.2438e-01, 3.4393e-01,
        1.8874e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 168, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5472e-01, 7.3474e-03, 7.0590e-03, 4.0369e-08, 1.6124e-02, 1.4742e-02,
        4.4611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.914

[Epoch: 168, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0032e-03, 2.3916e-02, 2.2270e-08, 6.9966e-09, 3.4742e-03, 3.4547e-03,
        9.6315e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.428

[Epoch: 168, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9346e-01, 5.9043e-08, 3.6391e-06, 4.9970e-10, 2.0653e-08, 1.0681e-10,
        6.5318e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.488

[Epoch: 168, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.9796, 0.0034, 0.0037, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 168, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.7029e-03, 1.4083e-01, 1.8387e-05, 2.0845e-06, 3.2920e-01, 3.3045e-01,
        1.9281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.780

[Epoch: 169, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6576e-01, 6.2330e-03, 6.2541e-03, 2.2838e-08, 1.1267e-02, 1.0478e-02,
        4.3525e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.921

[Epoch: 169, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6600e-03, 2.1952e-02, 1.9399e-08, 7.1431e-09, 3.4887e-03, 3.1604e-03,
        9.6474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.423

[Epoch: 169, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9323e-01, 4.2053e-08, 3.0016e-06, 3.0074e-10, 1.2378e-08, 6.5957e-11,
        6.7687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.490

[Epoch: 169, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.9802, 0.0032, 0.0034, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 169, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.3742e-03, 1.4118e-01, 1.7640e-05, 2.3599e-06, 3.1961e-01, 3.3982e-01,
        1.9299e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 170, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6168e-01, 5.9633e-03, 6.3447e-03, 2.6424e-08, 1.2574e-02, 1.3433e-02,
        4.2885e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 170, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.1797e-03, 2.4846e-02, 2.0314e-08, 5.7681e-09, 3.1342e-03, 3.4803e-03,
        9.6236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.422

[Epoch: 170, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9303e-01, 3.4229e-08, 2.4708e-06, 2.7832e-10, 1.1769e-08, 5.4681e-11,
        6.9720e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.497

[Epoch: 170, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.9798, 0.0032, 0.0036, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 170, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.7983e-03, 1.3888e-01, 1.8170e-05, 2.3043e-06, 3.2030e-01, 3.3456e-01,
        1.9945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.776

[Epoch: 171, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5485e-01, 7.6493e-03, 7.0175e-03, 2.6671e-08, 1.6187e-02, 1.4289e-02,
        4.1336e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 171, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8986e-03, 2.0692e-02, 2.3770e-08, 7.7805e-09, 3.4925e-03, 3.4294e-03,
        9.6549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.412

[Epoch: 171, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9298e-01, 3.7480e-08, 3.1121e-06, 3.8302e-10, 1.8766e-08, 7.7620e-11,
        7.0123e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.487

[Epoch: 171, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0036, 0.9786, 0.0034, 0.0042, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 171, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.4499e-03, 1.4531e-01, 1.6521e-05, 2.5998e-06, 3.2453e-01, 3.3661e-01,
        1.8608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 172, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6284e-01, 6.7583e-03, 6.2241e-03, 2.9628e-08, 1.1141e-02, 1.3036e-02,
        3.0867e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 172, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.0646e-03, 2.8001e-02, 1.7601e-08, 6.0033e-09, 3.5216e-03, 3.1852e-03,
        9.5923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.439

[Epoch: 172, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9323e-01, 5.2490e-08, 3.1674e-06, 4.1509e-10, 1.6392e-08, 1.0983e-10,
        6.7718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.478

[Epoch: 172, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0039, 0.9773, 0.0039, 0.0035, 0.0040, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 172, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.0864e-03, 1.3732e-01, 1.8414e-05, 2.5698e-06, 3.3397e-01, 3.3281e-01,
        1.8979e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.734

[Epoch: 173, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6705e-01, 5.5850e-03, 5.7380e-03, 3.4652e-08, 1.0943e-02, 1.0679e-02,
        4.3930e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 173, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.3258e-03, 1.9957e-02, 1.7282e-08, 6.1408e-09, 3.5866e-03, 3.0631e-03,
        9.6707e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.434

[Epoch: 173, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9452e-01, 4.2449e-08, 3.8556e-06, 2.8315e-10, 1.4334e-08, 3.4636e-11,
        5.4767e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.469

[Epoch: 173, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0039, 0.9798, 0.0034, 0.0025, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 173, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.7723e-03, 1.3073e-01, 1.5109e-05, 2.3038e-06, 3.1050e-01, 3.5711e-01,
        1.9587e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.824

[Epoch: 174, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5486e-01, 7.5244e-03, 7.1759e-03, 3.7837e-08, 1.5351e-02, 1.5085e-02,
        3.9620e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.912

[Epoch: 174, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.5737e-03, 3.0315e-02, 2.5687e-08, 8.1924e-09, 3.4906e-03, 3.8133e-03,
        9.5381e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.406

[Epoch: 174, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9157e-01, 3.7303e-08, 3.3575e-06, 4.0746e-10, 1.0887e-08, 8.0821e-11,
        8.4288e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.510

[Epoch: 174, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0023, 0.9867, 0.0022, 0.0024, 0.0019, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 174, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.9874e-03, 1.5864e-01, 1.0946e-05, 1.0550e-06, 3.0519e-01, 3.1376e-01,
        2.1342e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 175, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5687e-01, 6.7802e-03, 7.4711e-03, 4.5301e-08, 1.6061e-02, 1.2818e-02,
        2.2721e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.929

[Epoch: 175, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.4548e-03, 1.4068e-02, 1.8657e-08, 3.6967e-09, 3.0911e-03, 3.1734e-03,
        9.7421e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.430

[Epoch: 175, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9287e-01, 5.2932e-08, 3.9127e-06, 4.3940e-10, 1.6655e-08, 1.6674e-10,
        7.1247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.504

[Epoch: 175, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.9773, 0.0045, 0.0052, 0.0026, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 175, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6800e-03, 1.2683e-01, 2.9486e-05, 4.3422e-06, 3.7586e-01, 3.2461e-01,
        1.6599e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.755

[Epoch: 176, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5600e-01, 8.4492e-03, 7.1306e-03, 2.6149e-08, 1.1028e-02, 1.7394e-02,
        2.9060e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.927

[Epoch: 176, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.2907e-03, 3.0503e-02, 1.5551e-08, 5.3834e-09, 4.5907e-03, 3.0947e-03,
        9.5552e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.417

[Epoch: 176, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9285e-01, 5.9556e-08, 3.6505e-06, 1.9254e-10, 7.5019e-09, 5.3453e-11,
        7.1487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 176, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0045, 0.9747, 0.0036, 0.0039, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 176, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2457e-03, 1.4045e-01, 1.7284e-05, 4.3016e-06, 2.9241e-01, 3.6110e-01,
        1.9978e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 177, batch: 39/195] total loss per batch: 0.514
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5764e-01, 4.0479e-03, 7.0556e-03, 3.0540e-07, 2.2217e-02, 9.0334e-03,
        9.6829e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.908

[Epoch: 177, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6138e-03, 2.1936e-02, 2.9261e-08, 9.3828e-09, 3.1670e-03, 2.5819e-03,
        9.6670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.435

[Epoch: 177, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8467e-01, 3.2880e-07, 1.8924e-05, 1.5570e-09, 6.1736e-08, 1.9073e-10,
        1.5308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.455

[Epoch: 177, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0027, 0.9826, 0.0030, 0.0027, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 177, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.3428e-03, 1.5739e-01, 1.0232e-05, 1.3390e-06, 2.9614e-01, 3.4153e-01,
        1.9759e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.799

[Epoch: 178, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6406e-01, 6.5638e-03, 6.3517e-03, 1.8197e-08, 8.4566e-03, 1.4561e-02,
        8.2564e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.928

[Epoch: 178, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7258e-03, 2.2380e-02, 1.7229e-08, 4.6830e-09, 3.4410e-03, 3.6242e-03,
        9.6483e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 178, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9406e-01, 1.8172e-07, 1.3261e-05, 6.3276e-10, 1.7265e-08, 1.4297e-10,
        5.9276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.438

[Epoch: 178, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.9795, 0.0039, 0.0032, 0.0034, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 178, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.2200e-03, 1.3134e-01, 2.1263e-05, 3.5212e-06, 3.5376e-01, 3.1051e-01,
        1.9714e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.765

[Epoch: 179, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6720e-01, 6.4955e-03, 5.4444e-03, 2.6175e-08, 9.8328e-03, 1.1020e-02,
        4.8163e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 179, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.1744e-03, 2.5872e-02, 2.3561e-08, 7.0231e-09, 4.1179e-03, 3.4503e-03,
        9.5939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.393

[Epoch: 179, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9389e-01, 1.4206e-07, 9.8730e-06, 5.0005e-10, 1.6748e-08, 9.1674e-11,
        6.0992e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 179, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0036, 0.9784, 0.0037, 0.0034, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 179, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4465e-03, 1.4080e-01, 1.8753e-05, 3.4168e-06, 3.1528e-01, 3.4571e-01,
        1.9174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.769

[Epoch: 180, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6260e-01, 6.4402e-03, 5.9133e-03, 3.8977e-08, 1.2668e-02, 1.2373e-02,
        6.2573e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.917

[Epoch: 180, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8268e-03, 2.1695e-02, 1.7299e-08, 3.1900e-09, 3.5473e-03, 3.2001e-03,
        9.6473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.435

[Epoch: 180, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9411e-01, 1.3065e-07, 9.3452e-06, 5.4029e-10, 2.1149e-08, 1.1751e-10,
        5.8795e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.472

[Epoch: 180, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0032, 0.9807, 0.0033, 0.0031, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 180, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.3687e-03, 1.3815e-01, 1.7226e-05, 2.7951e-06, 3.2067e-01, 3.4372e-01,
        1.9108e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.776

[Epoch: 181, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6316e-01, 6.2278e-03, 5.9077e-03, 2.3194e-08, 1.2318e-02, 1.2385e-02,
        4.6303e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.914

[Epoch: 181, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.4494e-03, 2.2649e-02, 2.2070e-08, 5.4915e-09, 3.4404e-03, 2.9878e-03,
        9.6447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.432

[Epoch: 181, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9362e-01, 1.2808e-07, 8.7721e-06, 4.6571e-10, 1.6707e-08, 8.3130e-11,
        6.3723e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.474

[Epoch: 181, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.9805, 0.0033, 0.0032, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 181, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4157e-03, 1.3919e-01, 1.7522e-05, 3.0189e-06, 3.2762e-01, 3.3466e-01,
        1.9209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 182, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6058e-01, 6.6328e-03, 6.2879e-03, 2.4848e-08, 1.3052e-02, 1.3447e-02,
        4.4727e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.918

[Epoch: 182, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8277e-03, 2.2906e-02, 1.8481e-08, 3.8510e-09, 3.4813e-03, 3.3302e-03,
        9.6345e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 182, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9340e-01, 1.1121e-07, 7.7484e-06, 4.1468e-10, 1.5120e-08, 8.8666e-11,
        6.5967e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.483

[Epoch: 182, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.9790, 0.0036, 0.0034, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 182, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5840e-03, 1.4000e-01, 1.5822e-05, 2.9407e-06, 3.2027e-01, 3.3669e-01,
        1.9644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.781

[Epoch: 183, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5652e-01, 7.1676e-03, 6.8281e-03, 3.1083e-08, 1.4594e-02, 1.4886e-02,
        5.2845e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.919

[Epoch: 183, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.6823e-03, 2.4127e-02, 2.0918e-08, 4.6252e-09, 3.5291e-03, 3.4650e-03,
        9.6220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.427

[Epoch: 183, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9271e-01, 1.0956e-07, 8.0722e-06, 4.6128e-10, 1.6654e-08, 1.0077e-10,
        7.2786e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.498

[Epoch: 183, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.9795, 0.0034, 0.0032, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 183, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6657e-03, 1.4290e-01, 1.6254e-05, 2.8788e-06, 3.2380e-01, 3.3379e-01,
        1.9282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.762

[Epoch: 184, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6084e-01, 7.0320e-03, 6.5065e-03, 2.4772e-08, 1.3094e-02, 1.2528e-02,
        4.5145e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.923

[Epoch: 184, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([6.8646e-03, 2.2713e-02, 1.5839e-08, 3.0614e-09, 3.2073e-03, 3.3139e-03,
        9.6390e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.425

[Epoch: 184, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9372e-01, 1.1343e-07, 6.9141e-06, 4.6729e-10, 1.7380e-08, 9.3685e-11,
        6.2732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.494

[Epoch: 184, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.9798, 0.0034, 0.0032, 0.0034, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 184, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.0206e-03, 1.3995e-01, 1.4191e-05, 2.3441e-06, 3.2005e-01, 3.3641e-01,
        1.9655e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.792

[Epoch: 185, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5978e-01, 7.0583e-03, 6.1738e-03, 1.9150e-08, 1.2763e-02, 1.4215e-02,
        5.1308e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.920

[Epoch: 185, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.9478e-03, 2.4288e-02, 1.9273e-08, 4.6950e-09, 3.4429e-03, 3.0206e-03,
        9.6330e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.429

[Epoch: 185, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9299e-01, 1.2141e-07, 8.9214e-06, 5.6648e-10, 1.7818e-08, 1.1068e-10,
        6.9980e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.475

[Epoch: 185, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0034, 0.9784, 0.0042, 0.0037, 0.0035, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 185, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.4535e-03, 1.3277e-01, 1.3375e-05, 2.2477e-06, 3.3099e-01, 3.4240e-01,
        1.8737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.765

[Epoch: 186, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6257e-01, 6.6570e-03, 6.2209e-03, 2.1260e-08, 1.3093e-02, 1.1450e-02,
        4.5614e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.913

[Epoch: 186, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([8.5571e-03, 1.9766e-02, 1.5879e-08, 3.6465e-09, 3.2321e-03, 3.7064e-03,
        9.6474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.412

[Epoch: 186, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9490e-01, 7.6944e-08, 4.3035e-06, 2.0944e-10, 8.7509e-09, 6.5975e-11,
        5.0942e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.491

[Epoch: 186, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0028, 0.9803, 0.0037, 0.0033, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 186, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.2990e-03, 1.4905e-01, 1.1953e-05, 1.4674e-06, 3.1829e-01, 3.2834e-01,
        1.9801e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.784

[Epoch: 187, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6358e-01, 6.1527e-03, 6.0412e-03, 1.5731e-08, 1.2381e-02, 1.1838e-02,
        4.7481e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.903

[Epoch: 187, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.8105e-03, 2.9163e-02, 1.3206e-08, 2.6186e-09, 3.1425e-03, 2.6537e-03,
        9.6023e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.471

[Epoch: 187, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9262e-01, 7.4346e-08, 8.9897e-06, 4.3853e-10, 1.3685e-08, 6.7302e-11,
        7.3733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.467

[Epoch: 187, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.9820, 0.0031, 0.0025, 0.0028, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 187, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.4466e-03, 1.3597e-01, 1.9363e-05, 4.2955e-06, 3.1790e-01, 3.4796e-01,
        1.9070e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.755

[Epoch: 188, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5763e-01, 5.6836e-03, 7.2336e-03, 2.7906e-08, 1.4584e-02, 1.4869e-02,
        3.9160e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.923

[Epoch: 188, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.0629e-03, 1.5606e-02, 1.6235e-08, 3.9563e-09, 3.2013e-03, 4.0106e-03,
        9.7012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.362

[Epoch: 188, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9465e-01, 6.7844e-08, 5.3223e-06, 2.2126e-10, 1.2047e-08, 8.3012e-11,
        5.3433e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.523

[Epoch: 188, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0043, 0.9780, 0.0031, 0.0037, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.017

[Epoch: 188, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.1310e-03, 1.3792e-01, 1.2008e-05, 2.4013e-06, 3.2315e-01, 3.2261e-01,
        2.1017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.787

[Epoch: 189, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5163e-01, 9.1273e-03, 8.9395e-03, 1.3487e-08, 1.5616e-02, 1.4687e-02,
        3.0191e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.923

[Epoch: 189, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.7214e-03, 2.3477e-02, 3.6872e-08, 1.5356e-08, 3.4787e-03, 3.1358e-03,
        9.6419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.467

[Epoch: 189, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9295e-01, 1.7610e-07, 1.1549e-05, 8.9131e-10, 2.7166e-08, 3.7448e-10,
        7.0422e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.461

[Epoch: 189, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0029, 0.9795, 0.0033, 0.0037, 0.0039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.044

[Epoch: 189, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([8.4185e-03, 1.5606e-01, 2.0693e-05, 2.8415e-06, 3.2563e-01, 3.3802e-01,
        1.7185e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.755

[Epoch: 190, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6218e-01, 6.2231e-03, 6.8856e-03, 7.6570e-08, 1.3634e-02, 1.1072e-02,
        4.4294e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.909

[Epoch: 190, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.3118e-03, 2.9632e-02, 2.1643e-08, 6.4311e-09, 2.9920e-03, 3.5852e-03,
        9.5648e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.454

[Epoch: 190, batch: 117/195] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9484e-01, 9.9760e-08, 6.7456e-06, 5.2002e-10, 1.3544e-08, 1.4127e-10,
        5.1581e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.486

[Epoch: 190, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0028, 0.9808, 0.0038, 0.0029, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.043

[Epoch: 190, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([4.9079e-03, 1.2686e-01, 1.6180e-05, 7.7062e-06, 3.2888e-01, 3.4535e-01,
        1.9398e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.780

[Epoch: 191, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6260e-01, 5.5994e-03, 6.1708e-03, 4.3960e-08, 1.2098e-02, 1.3523e-02,
        5.0181e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.911

[Epoch: 191, batch: 78/195] total loss per batch: 0.544
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.4182e-03, 2.2592e-02, 1.8201e-08, 6.8701e-09, 3.7689e-03, 3.4076e-03,
        9.6281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.396

[Epoch: 191, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9380e-01, 7.1951e-08, 8.6564e-06, 3.0388e-10, 1.8928e-08, 9.0940e-11,
        6.1911e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.501

[Epoch: 191, batch: 156/195] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0037, 0.9791, 0.0040, 0.0031, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 191, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.4831e-03, 1.3118e-01, 1.0212e-05, 3.2432e-06, 3.0617e-01, 3.4635e-01,
        2.1081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.783

[Epoch: 192, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.5607e-01, 6.3855e-03, 6.3379e-03, 3.7147e-08, 1.6785e-02, 1.4420e-02,
        3.4844e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.925

[Epoch: 192, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6959e-03, 2.7295e-02, 5.9355e-08, 3.0179e-08, 1.8175e-03, 5.7679e-03,
        9.5942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.436

[Epoch: 192, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9254e-01, 1.3567e-07, 1.0189e-05, 6.6170e-09, 4.1219e-08, 7.5459e-10,
        7.4485e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.504

[Epoch: 192, batch: 156/195] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0030, 0.9790, 0.0043, 0.0032, 0.0031, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.045

[Epoch: 192, batch: 195/195] total loss per batch: 0.524
Policy (actual, predicted): 5 4
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([7.9958e-03, 1.5106e-01, 1.4668e-05, 8.6526e-06, 3.4231e-01, 2.9980e-01,
        1.9881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 193, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6957e-01, 6.3330e-03, 4.7234e-03, 5.2690e-08, 7.8277e-03, 1.1516e-02,
        2.6052e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.906

[Epoch: 193, batch: 78/195] total loss per batch: 0.547
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([7.6065e-03, 2.5305e-02, 8.7880e-08, 1.4704e-08, 3.5734e-03, 6.9905e-03,
        9.5652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 193, batch: 117/195] total loss per batch: 0.524
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9520e-01, 1.0626e-07, 9.3108e-06, 3.4521e-09, 3.9289e-08, 1.8304e-10,
        4.7897e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.453

[Epoch: 193, batch: 156/195] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.9791, 0.0036, 0.0032, 0.0037, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 193, batch: 195/195] total loss per batch: 0.526
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.3455e-03, 1.4191e-01, 1.9728e-05, 2.6544e-05, 3.2321e-01, 3.3805e-01,
        1.9143e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.797

[Epoch: 194, batch: 39/195] total loss per batch: 0.513
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6086e-01, 9.2390e-03, 7.2525e-03, 4.1424e-07, 1.0183e-02, 1.2442e-02,
        2.6069e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.827

[Epoch: 194, batch: 78/195] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.7705e-03, 2.2063e-02, 4.6343e-08, 4.0812e-08, 4.1919e-03, 3.3106e-03,
        9.6666e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.438

[Epoch: 194, batch: 117/195] total loss per batch: 0.522
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9452e-01, 7.5572e-08, 1.7467e-05, 2.1021e-09, 5.5170e-08, 3.1399e-10,
        5.4638e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.478

[Epoch: 194, batch: 156/195] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0029, 0.9795, 0.0041, 0.0030, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 194, batch: 195/195] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([5.6087e-03, 1.4084e-01, 1.4431e-05, 1.3100e-05, 3.2983e-01, 3.3297e-01,
        1.9071e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.771

[Epoch: 195, batch: 39/195] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6636e-01, 7.2774e-03, 6.1461e-03, 1.4385e-07, 9.1374e-03, 1.1041e-02,
        3.3356e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.911

[Epoch: 195, batch: 78/195] total loss per batch: 0.545
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([3.6942e-03, 3.8441e-02, 5.6424e-08, 4.4512e-08, 3.3422e-03, 3.8221e-03,
        9.5070e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.429

[Epoch: 195, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9316e-01, 8.9135e-08, 1.3074e-05, 2.3049e-09, 5.2448e-08, 3.2507e-10,
        6.8222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.470

[Epoch: 195, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0030, 0.9801, 0.0040, 0.0029, 0.0030, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 195, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.1627e-03, 1.3838e-01, 1.3076e-05, 1.1193e-05, 3.2937e-01, 3.3490e-01,
        1.9116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.770

[Epoch: 196, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6548e-01, 6.9208e-03, 6.1292e-03, 1.3180e-07, 9.7025e-03, 1.1744e-02,
        2.7745e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 196, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([4.4991e-03, 2.5556e-02, 4.9880e-08, 4.4500e-08, 3.4761e-03, 4.1301e-03,
        9.6234e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.418

[Epoch: 196, batch: 117/195] total loss per batch: 0.518
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9333e-01, 9.7850e-08, 1.3871e-05, 2.5291e-09, 5.4652e-08, 2.9333e-10,
        6.6594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.477

[Epoch: 196, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0030, 0.9800, 0.0039, 0.0030, 0.0031, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 196, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.5332e-03, 1.3898e-01, 1.2517e-05, 1.0534e-05, 3.2679e-01, 3.3780e-01,
        1.8987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 197, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6329e-01, 7.0531e-03, 6.3106e-03, 1.3866e-07, 1.0672e-02, 1.2655e-02,
        2.3176e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 197, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.0360e-03, 2.4369e-02, 4.5589e-08, 4.3808e-08, 3.5011e-03, 4.0363e-03,
        9.6306e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.421

[Epoch: 197, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9329e-01, 9.8106e-08, 1.3748e-05, 2.5552e-09, 5.3209e-08, 2.7588e-10,
        6.6966e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.479

[Epoch: 197, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.9800, 0.0038, 0.0030, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 197, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6317e-03, 1.3908e-01, 1.1961e-05, 1.0235e-05, 3.2451e-01, 3.3880e-01,
        1.9095e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 198, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6196e-01, 7.1148e-03, 6.3889e-03, 1.3294e-07, 1.1367e-02, 1.3153e-02,
        2.0762e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 198, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.3096e-03, 2.3702e-02, 4.2368e-08, 4.2298e-08, 3.4861e-03, 3.9719e-03,
        9.6353e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.421

[Epoch: 198, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9323e-01, 9.4924e-08, 1.3290e-05, 2.4189e-09, 5.0634e-08, 2.5015e-10,
        6.7586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.481

[Epoch: 198, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.9799, 0.0038, 0.0030, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 198, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6803e-03, 1.3936e-01, 1.1367e-05, 9.6580e-06, 3.2364e-01, 3.3842e-01,
        1.9188e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

[Epoch: 199, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6103e-01, 7.1429e-03, 6.4294e-03, 1.2783e-07, 1.1901e-02, 1.3478e-02,
        1.8945e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 199, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.6369e-03, 2.3740e-02, 4.0162e-08, 4.1863e-08, 3.5017e-03, 3.9566e-03,
        9.6316e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.422

[Epoch: 199, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9329e-01, 9.2567e-08, 1.2774e-05, 2.2905e-09, 4.7874e-08, 2.2608e-10,
        6.6953e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.482

[Epoch: 199, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0032, 0.9800, 0.0037, 0.0030, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 199, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6667e-03, 1.3963e-01, 1.0974e-05, 9.3033e-06, 3.2338e-01, 3.3811e-01,
        1.9220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.774

[Epoch: 200, batch: 39/195] total loss per batch: 0.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.9600, 0.0067, 0.0067, 0.0000, 0.0133, 0.0133, 0.0000])
Policy pred: tensor([9.6047e-01, 7.1221e-03, 6.4486e-03, 1.2251e-07, 1.2303e-02, 1.3639e-02,
        1.7183e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.917 -0.916

[Epoch: 200, batch: 78/195] total loss per batch: 0.543
Policy (actual, predicted): 6 6
Policy data: tensor([0.0067, 0.0233, 0.0000, 0.0000, 0.0033, 0.0033, 0.9633])
Policy pred: tensor([5.8285e-03, 2.3261e-02, 3.8194e-08, 4.0033e-08, 3.5116e-03, 3.8620e-03,
        9.6354e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.425 -0.424

[Epoch: 200, batch: 117/195] total loss per batch: 0.517
Policy (actual, predicted): 0 0
Policy data: tensor([0.9933, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9323e-01, 8.8802e-08, 1.2092e-05, 2.1230e-09, 4.4845e-08, 2.0629e-10,
        6.7557e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.489 0.483

[Epoch: 200, batch: 156/195] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9800, 0.0033, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0032, 0.9799, 0.0036, 0.0031, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 200, batch: 195/195] total loss per batch: 0.522
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.1400, 0.0000, 0.0000, 0.3233, 0.3367, 0.1933])
Policy pred: tensor([6.6676e-03, 1.3982e-01, 1.0521e-05, 8.8649e-06, 3.2333e-01, 3.3732e-01,
        1.9284e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.775 0.775

