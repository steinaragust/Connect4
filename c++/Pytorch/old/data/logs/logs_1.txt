Training set samples: 6011
Batch size: 32
[Epoch: 1, batch: 37/188] total loss per batch: 1.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.1098, 0.0428, 0.1939, 0.2974, 0.2011, 0.1070, 0.0478],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 1, batch: 74/188] total loss per batch: 1.557
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.1018e-01, 5.4378e-02, 4.1198e-01, 2.7935e-01, 1.5112e-04, 3.6713e-04,
        1.4359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.035

[Epoch: 1, batch: 111/188] total loss per batch: 1.506
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0693, 0.1466, 0.0831, 0.0932, 0.0994, 0.4823, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 1, batch: 148/188] total loss per batch: 1.447
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0534, 0.0230, 0.3405, 0.0606, 0.4272, 0.0507, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 1, batch: 185/188] total loss per batch: 1.538
Policy (actual, predicted): 4 5
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0362, 0.0184, 0.0522, 0.2043, 0.2113, 0.3561, 0.1215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 2, batch: 37/188] total loss per batch: 1.323
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0704, 0.0672, 0.1275, 0.5285, 0.0679, 0.0805, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 2, batch: 74/188] total loss per batch: 1.300
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([7.3149e-02, 6.1736e-02, 4.2825e-01, 3.0947e-01, 1.2525e-04, 6.3385e-05,
        1.2720e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.020

[Epoch: 2, batch: 111/188] total loss per batch: 1.241
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0259, 0.3151, 0.0689, 0.0531, 0.0640, 0.4554, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.031

[Epoch: 2, batch: 148/188] total loss per batch: 1.197
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0821, 0.0104, 0.2097, 0.0301, 0.6418, 0.0113, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.019

[Epoch: 2, batch: 185/188] total loss per batch: 1.292
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0119, 0.0087, 0.0263, 0.0569, 0.8124, 0.0414, 0.0423],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 3, batch: 37/188] total loss per batch: 1.147
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0294, 0.0096, 0.0222, 0.9098, 0.0117, 0.0096, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 3, batch: 74/188] total loss per batch: 1.141
Policy (actual, predicted): 3 6
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([9.4388e-02, 3.7755e-02, 2.7615e-01, 2.4950e-01, 2.3728e-04, 4.0181e-05,
        3.4192e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.003

[Epoch: 3, batch: 111/188] total loss per batch: 1.101
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0098, 0.0567, 0.0180, 0.0064, 0.0127, 0.8914, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 3, batch: 148/188] total loss per batch: 1.076
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0418, 0.0060, 0.0782, 0.0352, 0.8186, 0.0053, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.007

[Epoch: 3, batch: 185/188] total loss per batch: 1.156
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0034, 0.0283, 0.0113, 0.8856, 0.0412, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.012

[Epoch: 4, batch: 37/188] total loss per batch: 1.072
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0321, 0.0436, 0.0858, 0.7345, 0.0107, 0.0364, 0.0569],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.019

[Epoch: 4, batch: 74/188] total loss per batch: 1.076
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7979e-01, 6.0961e-02, 4.5098e-01, 2.0705e-01, 2.7744e-04, 1.9932e-04,
        1.0074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.186

[Epoch: 4, batch: 111/188] total loss per batch: 1.069
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0197, 0.1126, 0.1099, 0.0306, 0.0664, 0.6451, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 4, batch: 148/188] total loss per batch: 1.040
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.1036, 0.0129, 0.1902, 0.0178, 0.6487, 0.0030, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.009

[Epoch: 4, batch: 185/188] total loss per batch: 1.101
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0032, 0.0035, 0.0120, 0.0176, 0.9363, 0.0207, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.003

[Epoch: 5, batch: 37/188] total loss per batch: 1.033
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0610, 0.0684, 0.1974, 0.3645, 0.0135, 0.2647, 0.0306],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.020

[Epoch: 5, batch: 74/188] total loss per batch: 1.045
Policy (actual, predicted): 3 6
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([0.1490, 0.0217, 0.2611, 0.2092, 0.0005, 0.0008, 0.3578],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.215

[Epoch: 5, batch: 111/188] total loss per batch: 1.010
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0112, 0.0877, 0.0379, 0.0137, 0.0162, 0.8087, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 5, batch: 148/188] total loss per batch: 0.976
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0432, 0.0073, 0.1523, 0.0050, 0.7740, 0.0012, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 5, batch: 185/188] total loss per batch: 1.046
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0045, 0.0204, 0.0118, 0.9107, 0.0351, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 6, batch: 37/188] total loss per batch: 1.001
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0184, 0.0293, 0.0387, 0.8767, 0.0073, 0.0237, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 6, batch: 74/188] total loss per batch: 1.005
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.2606e-01, 8.6971e-03, 1.7384e-01, 6.4368e-01, 7.3744e-04, 2.2845e-04,
        4.6761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.120

[Epoch: 6, batch: 111/188] total loss per batch: 0.992
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0164, 0.0585, 0.0352, 0.0033, 0.0215, 0.8539, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 6, batch: 148/188] total loss per batch: 0.965
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0562, 0.0088, 0.2135, 0.0209, 0.6826, 0.0043, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 6, batch: 185/188] total loss per batch: 1.033
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0084, 0.0025, 0.0503, 0.0116, 0.8439, 0.0722, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.009

[Epoch: 7, batch: 37/188] total loss per batch: 0.977
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0305, 0.0281, 0.1026, 0.7745, 0.0225, 0.0322, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.014

[Epoch: 7, batch: 74/188] total loss per batch: 0.986
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([0.2083, 0.0166, 0.3286, 0.1559, 0.0004, 0.0014, 0.2888],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.016

[Epoch: 7, batch: 111/188] total loss per batch: 0.966
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.0699, 0.0114, 0.0039, 0.0074, 0.8993, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 7, batch: 148/188] total loss per batch: 0.930
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0907, 0.0036, 0.0577, 0.0128, 0.8186, 0.0025, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.011

[Epoch: 7, batch: 185/188] total loss per batch: 0.994
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0046, 0.0035, 0.0314, 0.0459, 0.8298, 0.0512, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.017

[Epoch: 8, batch: 37/188] total loss per batch: 0.951
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0187, 0.0262, 0.0311, 0.8363, 0.0179, 0.0516, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 8, batch: 74/188] total loss per batch: 0.947
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([0.2409, 0.0465, 0.1097, 0.3391, 0.0021, 0.0005, 0.2611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.325

[Epoch: 8, batch: 111/188] total loss per batch: 0.939
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0225, 0.2048, 0.0538, 0.0071, 0.0187, 0.6679, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 8, batch: 148/188] total loss per batch: 0.918
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0129, 0.0097, 0.0732, 0.0066, 0.8882, 0.0010, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.016

[Epoch: 8, batch: 185/188] total loss per batch: 0.985
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0030, 0.0025, 0.0115, 0.0033, 0.9631, 0.0110, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.011

[Epoch: 9, batch: 37/188] total loss per batch: 0.945
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0117, 0.0108, 0.0783, 0.8344, 0.0272, 0.0096, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 9, batch: 74/188] total loss per batch: 0.938
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.6350e-01, 9.2131e-03, 3.4470e-01, 2.9712e-01, 3.7766e-04, 8.4427e-05,
        8.5013e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.030

[Epoch: 9, batch: 111/188] total loss per batch: 0.931
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0071, 0.2189, 0.0931, 0.0045, 0.0121, 0.6407, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.031

[Epoch: 9, batch: 148/188] total loss per batch: 0.902
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0593, 0.0131, 0.1852, 0.0589, 0.6514, 0.0095, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.008

[Epoch: 9, batch: 185/188] total loss per batch: 0.973
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0075, 0.0026, 0.0351, 0.0164, 0.8870, 0.0385, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 10, batch: 37/188] total loss per batch: 0.923
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0141, 0.0554, 0.1721, 0.4142, 0.0453, 0.2666, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.019

[Epoch: 10, batch: 74/188] total loss per batch: 0.933
Policy (actual, predicted): 3 2
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.4398e-01, 4.1107e-02, 2.9129e-01, 1.4108e-01, 2.3298e-04, 1.6179e-03,
        2.8069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.071

[Epoch: 10, batch: 111/188] total loss per batch: 0.911
Policy (actual, predicted): 5 1
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0197, 0.5857, 0.0229, 0.0087, 0.0046, 0.3507, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 10, batch: 148/188] total loss per batch: 0.884
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0301, 0.0034, 0.0429, 0.0082, 0.8883, 0.0026, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 10, batch: 185/188] total loss per batch: 0.955
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0100, 0.0072, 0.0377, 0.0240, 0.8119, 0.0604, 0.0487],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.019

[Epoch: 11, batch: 37/188] total loss per batch: 0.908
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0153, 0.0227, 0.0857, 0.8089, 0.0093, 0.0301, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.020

[Epoch: 11, batch: 74/188] total loss per batch: 0.915
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([0.2700, 0.0707, 0.1763, 0.3346, 0.0005, 0.0004, 0.1475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.004

[Epoch: 11, batch: 111/188] total loss per batch: 0.895
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0111, 0.0293, 0.0162, 0.0019, 0.0053, 0.9315, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 11, batch: 148/188] total loss per batch: 0.864
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0310, 0.0168, 0.5234, 0.0224, 0.3947, 0.0045, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.024

[Epoch: 11, batch: 185/188] total loss per batch: 0.928
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0018, 0.0014, 0.0056, 0.0012, 0.9772, 0.0093, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.016

[Epoch: 12, batch: 37/188] total loss per batch: 0.876
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0130, 0.0194, 0.0672, 0.8285, 0.0234, 0.0274, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.012

[Epoch: 12, batch: 74/188] total loss per batch: 0.880
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1284e-01, 4.8279e-02, 1.8378e-01, 3.7756e-01, 2.6416e-04, 3.8356e-04,
        1.7689e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.011

[Epoch: 12, batch: 111/188] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0042, 0.0794, 0.0234, 0.0022, 0.0037, 0.8786, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 12, batch: 148/188] total loss per batch: 0.830
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0205, 0.0074, 0.0187, 0.0095, 0.9360, 0.0030, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 12, batch: 185/188] total loss per batch: 0.895
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0033, 0.0198, 0.0052, 0.9313, 0.0279, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.018

[Epoch: 13, batch: 37/188] total loss per batch: 0.864
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0149, 0.0764, 0.0775, 0.6177, 0.0291, 0.1466, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 13, batch: 74/188] total loss per batch: 0.874
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.6591e-01, 5.5675e-02, 2.6607e-01, 3.3575e-01, 4.1063e-04, 3.0604e-04,
        1.7588e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.001

[Epoch: 13, batch: 111/188] total loss per batch: 0.861
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0079, 0.1688, 0.0239, 0.0042, 0.0057, 0.7818, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 13, batch: 148/188] total loss per batch: 0.823
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0333, 0.0055, 0.0625, 0.0062, 0.8793, 0.0021, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.011

[Epoch: 13, batch: 185/188] total loss per batch: 0.891
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0041, 0.0206, 0.0045, 0.9400, 0.0100, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.018

[Epoch: 14, batch: 37/188] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0119, 0.0269, 0.0594, 0.6948, 0.0137, 0.1317, 0.0615],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 14, batch: 74/188] total loss per batch: 0.871
Policy (actual, predicted): 3 0
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([3.7283e-01, 5.4443e-02, 8.2490e-02, 2.7992e-01, 1.9423e-04, 9.3912e-05,
        2.1002e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.030

[Epoch: 14, batch: 111/188] total loss per batch: 0.861
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0046, 0.0455, 0.0196, 0.0037, 0.0078, 0.9113, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.004

[Epoch: 14, batch: 148/188] total loss per batch: 0.827
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0294, 0.0107, 0.0929, 0.0122, 0.8413, 0.0035, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.005

[Epoch: 14, batch: 185/188] total loss per batch: 0.889
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0034, 0.0042, 0.0210, 0.0041, 0.9307, 0.0245, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.012

[Epoch: 15, batch: 37/188] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0115, 0.0461, 0.0796, 0.6304, 0.0329, 0.1414, 0.0583],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.019

[Epoch: 15, batch: 74/188] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.0365e-01, 4.8869e-02, 3.4190e-01, 3.5432e-01, 2.7046e-04, 1.5411e-04,
        1.5084e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.023

[Epoch: 15, batch: 111/188] total loss per batch: 0.859
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0024, 0.1169, 0.0257, 0.0015, 0.0025, 0.8472, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 15, batch: 148/188] total loss per batch: 0.825
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0275, 0.0076, 0.1165, 0.0162, 0.8186, 0.0017, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.008

[Epoch: 15, batch: 185/188] total loss per batch: 0.891
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0035, 0.0049, 0.0246, 0.0028, 0.9374, 0.0131, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.016

[Epoch: 16, batch: 37/188] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0104, 0.0165, 0.0563, 0.7811, 0.0094, 0.0814, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.000

[Epoch: 16, batch: 74/188] total loss per batch: 0.869
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.2904e-01, 8.5282e-02, 1.6181e-01, 4.6594e-01, 1.8171e-04, 5.5703e-05,
        1.5770e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.044

[Epoch: 16, batch: 111/188] total loss per batch: 0.858
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1931, 0.0278, 0.0042, 0.0069, 0.7589, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 16, batch: 148/188] total loss per batch: 0.822
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0266, 0.0103, 0.1359, 0.0100, 0.7985, 0.0069, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 16, batch: 185/188] total loss per batch: 0.888
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0086, 0.0042, 0.0284, 0.0159, 0.8999, 0.0233, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.016

[Epoch: 17, batch: 37/188] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0095, 0.0240, 0.0496, 0.7003, 0.0080, 0.1510, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 17, batch: 74/188] total loss per batch: 0.863
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.5912e-01, 1.1584e-01, 1.1582e-01, 2.6589e-01, 2.8984e-04, 1.1067e-04,
        2.4294e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.102

[Epoch: 17, batch: 111/188] total loss per batch: 0.850
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0026, 0.0911, 0.0107, 0.0034, 0.0054, 0.8837, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 17, batch: 148/188] total loss per batch: 0.823
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0207, 0.0034, 0.1136, 0.0059, 0.8385, 0.0014, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 17, batch: 185/188] total loss per batch: 0.890
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0028, 0.0064, 0.0200, 0.0033, 0.9335, 0.0130, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 18, batch: 37/188] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0129, 0.0327, 0.1517, 0.6180, 0.0135, 0.1040, 0.0673],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 18, batch: 74/188] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.8162e-01, 6.3725e-02, 1.6629e-01, 3.6812e-01, 1.9544e-04, 1.2196e-04,
        1.1992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.012

[Epoch: 18, batch: 111/188] total loss per batch: 0.850
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0046, 0.1757, 0.0438, 0.0040, 0.0088, 0.7576, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 18, batch: 148/188] total loss per batch: 0.818
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0254, 0.0163, 0.1383, 0.0173, 0.7848, 0.0056, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.000

[Epoch: 18, batch: 185/188] total loss per batch: 0.892
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0027, 0.0019, 0.0074, 0.0039, 0.9725, 0.0075, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.017

[Epoch: 19, batch: 37/188] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0074, 0.0161, 0.0315, 0.7837, 0.0133, 0.1178, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.013

[Epoch: 19, batch: 74/188] total loss per batch: 0.864
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([7.8855e-02, 9.2812e-02, 2.7153e-01, 3.9996e-01, 3.3487e-04, 3.4914e-05,
        1.5648e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.024

[Epoch: 19, batch: 111/188] total loss per batch: 0.851
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0073, 0.1117, 0.0240, 0.0099, 0.0073, 0.8320, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 19, batch: 148/188] total loss per batch: 0.820
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0267, 0.0084, 0.2802, 0.0082, 0.6639, 0.0011, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.015

[Epoch: 19, batch: 185/188] total loss per batch: 0.886
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0067, 0.0060, 0.0202, 0.0059, 0.9356, 0.0093, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.020

[Epoch: 20, batch: 37/188] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0121, 0.0389, 0.0981, 0.5681, 0.0124, 0.1908, 0.0795],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 20, batch: 74/188] total loss per batch: 0.864
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.9984e-01, 5.8451e-02, 1.6599e-01, 3.1953e-01, 3.7843e-04, 1.0840e-04,
        1.5569e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.100

[Epoch: 20, batch: 111/188] total loss per batch: 0.851
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.1191, 0.0143, 0.0032, 0.0082, 0.8466, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 20, batch: 148/188] total loss per batch: 0.823
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0131, 0.0070, 0.0833, 0.0234, 0.8629, 0.0064, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 20, batch: 185/188] total loss per batch: 0.889
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0082, 0.0092, 0.0265, 0.0110, 0.8778, 0.0500, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.014

[Epoch: 21, batch: 37/188] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0125, 0.0363, 0.0486, 0.5530, 0.0340, 0.2669, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 21, batch: 74/188] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1498e-01, 1.3268e-01, 1.1542e-01, 4.1252e-01, 1.5156e-04, 1.4160e-04,
        1.2410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.027

[Epoch: 21, batch: 111/188] total loss per batch: 0.844
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0028, 0.2203, 0.0289, 0.0039, 0.0030, 0.7359, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 21, batch: 148/188] total loss per batch: 0.817
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0200, 0.0043, 0.1048, 0.0071, 0.8470, 0.0028, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.013

[Epoch: 21, batch: 185/188] total loss per batch: 0.884
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0110, 0.0080, 0.0407, 0.0193, 0.8649, 0.0232, 0.0329],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.012

[Epoch: 22, batch: 37/188] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0094, 0.0161, 0.0579, 0.7883, 0.0095, 0.0654, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.005

[Epoch: 22, batch: 74/188] total loss per batch: 0.840
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.6141e-01, 8.0857e-02, 1.4710e-01, 4.0027e-01, 4.5267e-04, 1.0476e-04,
        1.0981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.029

[Epoch: 22, batch: 111/188] total loss per batch: 0.825
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.0998, 0.0216, 0.0033, 0.0045, 0.8636, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 22, batch: 148/188] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0213, 0.0081, 0.0990, 0.0105, 0.8533, 0.0028, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.007

[Epoch: 22, batch: 185/188] total loss per batch: 0.862
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0037, 0.0052, 0.0145, 0.0034, 0.9533, 0.0085, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.013

[Epoch: 23, batch: 37/188] total loss per batch: 0.826
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0064, 0.0162, 0.0372, 0.7131, 0.0115, 0.1646, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 23, batch: 74/188] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.2731e-01, 7.5636e-02, 2.1799e-01, 4.1448e-01, 1.5892e-04, 3.2660e-05,
        1.6439e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.056

[Epoch: 23, batch: 111/188] total loss per batch: 0.819
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0019, 0.1792, 0.0127, 0.0030, 0.0043, 0.7958, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 23, batch: 148/188] total loss per batch: 0.791
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0178, 0.0089, 0.1420, 0.0090, 0.8070, 0.0037, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.014

[Epoch: 23, batch: 185/188] total loss per batch: 0.857
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0043, 0.0042, 0.0192, 0.0063, 0.9466, 0.0078, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.009

[Epoch: 24, batch: 37/188] total loss per batch: 0.824
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0129, 0.0299, 0.0851, 0.5791, 0.0114, 0.2187, 0.0629],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 24, batch: 74/188] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.9761e-01, 1.1169e-01, 1.4784e-01, 3.3279e-01, 3.6434e-04, 1.0823e-04,
        1.0959e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.061

[Epoch: 24, batch: 111/188] total loss per batch: 0.819
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0067, 0.1472, 0.0261, 0.0045, 0.0076, 0.8007, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 24, batch: 148/188] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0137, 0.0055, 0.1092, 0.0117, 0.8519, 0.0035, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.010

[Epoch: 24, batch: 185/188] total loss per batch: 0.855
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0083, 0.0164, 0.0079, 0.9238, 0.0167, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.010

[Epoch: 25, batch: 37/188] total loss per batch: 0.826
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0066, 0.0177, 0.0610, 0.7137, 0.0099, 0.1218, 0.0694],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 25, batch: 74/188] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.0107e-01, 4.9894e-02, 1.6899e-01, 5.6297e-01, 4.0781e-04, 5.4989e-05,
        1.1662e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.085

[Epoch: 25, batch: 111/188] total loss per batch: 0.821
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0023, 0.0997, 0.0134, 0.0037, 0.0029, 0.8741, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 25, batch: 148/188] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0168, 0.0086, 0.1727, 0.0121, 0.7788, 0.0037, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.011

[Epoch: 25, batch: 185/188] total loss per batch: 0.859
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0042, 0.0059, 0.0227, 0.0055, 0.9334, 0.0136, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.009

[Epoch: 26, batch: 37/188] total loss per batch: 0.827
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0059, 0.0223, 0.0745, 0.7375, 0.0095, 0.1032, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 26, batch: 74/188] total loss per batch: 0.839
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.8478e-01, 1.2558e-01, 1.4635e-01, 2.8939e-01, 5.4588e-04, 1.5628e-04,
        1.5320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.064

[Epoch: 26, batch: 111/188] total loss per batch: 0.826
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0071, 0.1754, 0.0316, 0.0055, 0.0076, 0.7659, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 26, batch: 148/188] total loss per batch: 0.798
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0152, 0.0057, 0.1761, 0.0081, 0.7867, 0.0028, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.021

[Epoch: 26, batch: 185/188] total loss per batch: 0.863
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0071, 0.0095, 0.0074, 0.9487, 0.0102, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 27, batch: 37/188] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0093, 0.0227, 0.0767, 0.6454, 0.0154, 0.1676, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 27, batch: 74/188] total loss per batch: 0.842
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.6759e-01, 1.3247e-01, 2.9432e-01, 3.1327e-01, 2.9023e-04, 9.3646e-05,
        9.1970e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.056

[Epoch: 27, batch: 111/188] total loss per batch: 0.829
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0026, 0.1680, 0.0218, 0.0034, 0.0023, 0.7978, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 27, batch: 148/188] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0208, 0.0070, 0.1600, 0.0130, 0.7852, 0.0049, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.017

[Epoch: 27, batch: 185/188] total loss per batch: 0.866
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0104, 0.0110, 0.0296, 0.0121, 0.8912, 0.0174, 0.0283],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.001

[Epoch: 28, batch: 37/188] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0138, 0.0167, 0.0866, 0.6724, 0.0116, 0.1506, 0.0484],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.005

[Epoch: 28, batch: 74/188] total loss per batch: 0.844
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.8696e-01, 4.6919e-02, 1.0467e-01, 4.0713e-01, 2.4260e-04, 1.2746e-04,
        1.5395e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.089

[Epoch: 28, batch: 111/188] total loss per batch: 0.831
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1480, 0.0133, 0.0045, 0.0050, 0.8202, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 28, batch: 148/188] total loss per batch: 0.799
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0168, 0.0020, 0.0792, 0.0031, 0.8925, 0.0021, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.017

[Epoch: 28, batch: 185/188] total loss per batch: 0.867
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0034, 0.0085, 0.0138, 0.0048, 0.9443, 0.0092, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.000

[Epoch: 29, batch: 37/188] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0064, 0.0264, 0.0663, 0.6641, 0.0147, 0.1642, 0.0579],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 29, batch: 74/188] total loss per batch: 0.841
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9850e-01, 1.4867e-01, 2.2499e-01, 3.0892e-01, 9.0790e-04, 1.4629e-04,
        1.1786e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.074

[Epoch: 29, batch: 111/188] total loss per batch: 0.830
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0033, 0.1672, 0.0219, 0.0050, 0.0060, 0.7890, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 29, batch: 148/188] total loss per batch: 0.800
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0120, 0.0082, 0.2589, 0.0115, 0.6987, 0.0035, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.013

[Epoch: 29, batch: 185/188] total loss per batch: 0.865
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0075, 0.0107, 0.0344, 0.0128, 0.9002, 0.0115, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 30, batch: 37/188] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0075, 0.0230, 0.1032, 0.6407, 0.0082, 0.1701, 0.0473],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 30, batch: 74/188] total loss per batch: 0.842
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.4865e-01, 5.1922e-02, 1.0785e-01, 4.5051e-01, 1.3577e-04, 7.8312e-05,
        1.4085e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.051

[Epoch: 30, batch: 111/188] total loss per batch: 0.829
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0087, 0.1869, 0.0275, 0.0072, 0.0067, 0.7529, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.016

[Epoch: 30, batch: 148/188] total loss per batch: 0.799
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0178, 0.0061, 0.1771, 0.0089, 0.7824, 0.0021, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.009

[Epoch: 30, batch: 185/188] total loss per batch: 0.864
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0042, 0.0085, 0.0198, 0.0032, 0.9277, 0.0219, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 31, batch: 37/188] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0106, 0.0265, 0.8623, 0.0087, 0.0630, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 31, batch: 74/188] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9714e-01, 1.2958e-01, 2.5507e-01, 3.1113e-01, 3.1679e-04, 9.8932e-05,
        1.0667e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.089

[Epoch: 31, batch: 111/188] total loss per batch: 0.825
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0030, 0.1305, 0.0178, 0.0043, 0.0036, 0.8365, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 31, batch: 148/188] total loss per batch: 0.794
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0110, 0.0045, 0.0620, 0.0054, 0.9118, 0.0023, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.015

[Epoch: 31, batch: 185/188] total loss per batch: 0.859
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0041, 0.0046, 0.0105, 0.0042, 0.9554, 0.0134, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 32, batch: 37/188] total loss per batch: 0.817
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0059, 0.0230, 0.0848, 0.6579, 0.0075, 0.1828, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 32, batch: 74/188] total loss per batch: 0.825
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2145e-01, 1.0629e-01, 1.5138e-01, 3.8720e-01, 1.1778e-04, 5.6503e-05,
        1.3350e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.049

[Epoch: 32, batch: 111/188] total loss per batch: 0.812
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.1751, 0.0181, 0.0037, 0.0042, 0.7904, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.011

[Epoch: 32, batch: 148/188] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0044, 0.1514, 0.0090, 0.8186, 0.0028, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.008

[Epoch: 32, batch: 185/188] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0054, 0.0087, 0.0167, 0.0071, 0.9254, 0.0142, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.003

[Epoch: 33, batch: 37/188] total loss per batch: 0.813
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0077, 0.0232, 0.0678, 0.6362, 0.0147, 0.2066, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.012

[Epoch: 33, batch: 74/188] total loss per batch: 0.821
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7822e-01, 8.4790e-02, 2.0822e-01, 4.2380e-01, 1.4805e-04, 1.0330e-04,
        1.0471e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.065

[Epoch: 33, batch: 111/188] total loss per batch: 0.809
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1259, 0.0157, 0.0033, 0.0037, 0.8447, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.014

[Epoch: 33, batch: 148/188] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0043, 0.1141, 0.0073, 0.8572, 0.0034, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.020

[Epoch: 33, batch: 185/188] total loss per batch: 0.843
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0034, 0.0068, 0.0153, 0.0041, 0.9422, 0.0094, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 34, batch: 37/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0069, 0.0145, 0.0465, 0.7664, 0.0056, 0.1170, 0.0430],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 34, batch: 74/188] total loss per batch: 0.820
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.5894e-01, 1.3258e-01, 1.7279e-01, 2.8084e-01, 1.2696e-04, 2.9842e-05,
        1.5469e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.094

[Epoch: 34, batch: 111/188] total loss per batch: 0.808
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0029, 0.1952, 0.0142, 0.0057, 0.0052, 0.7737, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.014

[Epoch: 34, batch: 148/188] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0126, 0.0055, 0.1804, 0.0093, 0.7850, 0.0026, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.025

[Epoch: 34, batch: 185/188] total loss per batch: 0.843
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0081, 0.0166, 0.0050, 0.9373, 0.0109, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 35, batch: 37/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0098, 0.0214, 0.0573, 0.5936, 0.0180, 0.2580, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 35, batch: 74/188] total loss per batch: 0.821
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.4675e-01, 7.9634e-02, 2.3476e-01, 4.4463e-01, 8.9231e-05, 8.8729e-05,
        9.4052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.073

[Epoch: 35, batch: 111/188] total loss per batch: 0.810
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0027, 0.1064, 0.0135, 0.0027, 0.0047, 0.8659, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 35, batch: 148/188] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0109, 0.0055, 0.1088, 0.0063, 0.8572, 0.0046, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.026

[Epoch: 35, batch: 185/188] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0062, 0.0089, 0.0232, 0.0055, 0.9083, 0.0141, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 36, batch: 37/188] total loss per batch: 0.815
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0065, 0.0173, 0.0525, 0.7819, 0.0059, 0.0984, 0.0375],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.013

[Epoch: 36, batch: 74/188] total loss per batch: 0.824
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.7521e-01, 8.0380e-02, 1.0965e-01, 3.9687e-01, 1.1192e-04, 2.6572e-05,
        1.3775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.091

[Epoch: 36, batch: 111/188] total loss per batch: 0.813
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.2297, 0.0137, 0.0044, 0.0043, 0.7410, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.014

[Epoch: 36, batch: 148/188] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0052, 0.1711, 0.0088, 0.7997, 0.0022, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.025

[Epoch: 36, batch: 185/188] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0038, 0.0086, 0.0151, 0.0046, 0.9376, 0.0101, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.011

[Epoch: 37, batch: 37/188] total loss per batch: 0.818
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0070, 0.0227, 0.0839, 0.6341, 0.0229, 0.1733, 0.0561],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.018

[Epoch: 37, batch: 74/188] total loss per batch: 0.827
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.2792e-01, 1.0492e-01, 2.6185e-01, 3.9974e-01, 1.0020e-04, 1.0254e-04,
        1.0537e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.043

[Epoch: 37, batch: 111/188] total loss per batch: 0.817
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0030, 0.1090, 0.0196, 0.0024, 0.0034, 0.8591, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.015

[Epoch: 37, batch: 148/188] total loss per batch: 0.786
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0136, 0.0089, 0.1662, 0.0083, 0.7913, 0.0045, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.040

[Epoch: 37, batch: 185/188] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0067, 0.0129, 0.0064, 0.9344, 0.0159, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 38, batch: 37/188] total loss per batch: 0.820
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0103, 0.0176, 0.0381, 0.6940, 0.0133, 0.1840, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 38, batch: 74/188] total loss per batch: 0.828
Policy (actual, predicted): 3 0
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([3.2701e-01, 1.2309e-01, 1.3356e-01, 2.7694e-01, 1.0236e-04, 4.9726e-05,
        1.3925e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.058

[Epoch: 38, batch: 111/188] total loss per batch: 0.818
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0021, 0.1569, 0.0143, 0.0051, 0.0052, 0.8124, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.017

[Epoch: 38, batch: 148/188] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0045, 0.1381, 0.0107, 0.8249, 0.0036, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.036

[Epoch: 38, batch: 185/188] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0077, 0.0186, 0.0072, 0.9285, 0.0148, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.011

[Epoch: 39, batch: 37/188] total loss per batch: 0.820
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0068, 0.0198, 0.0671, 0.7327, 0.0162, 0.1110, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 39, batch: 74/188] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([9.4363e-02, 1.0063e-01, 1.6434e-01, 5.1374e-01, 1.7289e-04, 7.0455e-05,
        1.2668e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.019

[Epoch: 39, batch: 111/188] total loss per batch: 0.816
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0044, 0.2053, 0.0251, 0.0051, 0.0059, 0.7494, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.016

[Epoch: 39, batch: 148/188] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0145, 0.0097, 0.1602, 0.0103, 0.7936, 0.0061, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.028

[Epoch: 39, batch: 185/188] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0070, 0.0143, 0.0314, 0.0093, 0.8791, 0.0156, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 40, batch: 37/188] total loss per batch: 0.820
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0057, 0.0239, 0.0743, 0.6608, 0.0085, 0.1647, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 40, batch: 74/188] total loss per batch: 0.827
Policy (actual, predicted): 3 0
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([3.1747e-01, 1.0668e-01, 1.6937e-01, 2.4390e-01, 8.5312e-05, 5.3483e-05,
        1.6245e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.135

[Epoch: 40, batch: 111/188] total loss per batch: 0.816
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0026, 0.0728, 0.0085, 0.0033, 0.0039, 0.9065, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.011

[Epoch: 40, batch: 148/188] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0117, 0.0048, 0.1332, 0.0070, 0.8329, 0.0024, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.015

[Epoch: 40, batch: 185/188] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0036, 0.0050, 0.0107, 0.0041, 0.9570, 0.0094, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.002

[Epoch: 41, batch: 37/188] total loss per batch: 0.819
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0073, 0.0267, 0.0731, 0.6163, 0.0132, 0.2075, 0.0558],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 41, batch: 74/188] total loss per batch: 0.826
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7002e-01, 6.5860e-02, 1.8008e-01, 4.5986e-01, 1.6539e-04, 9.5818e-05,
        1.2391e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.051

[Epoch: 41, batch: 111/188] total loss per batch: 0.814
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.2288, 0.0191, 0.0041, 0.0041, 0.7349, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.011

[Epoch: 41, batch: 148/188] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0129, 0.0049, 0.0991, 0.0096, 0.8653, 0.0047, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.024

[Epoch: 41, batch: 185/188] total loss per batch: 0.849
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0043, 0.0088, 0.0186, 0.0047, 0.9358, 0.0108, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 42, batch: 37/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0050, 0.0169, 0.0449, 0.7731, 0.0094, 0.1001, 0.0506],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 42, batch: 74/188] total loss per batch: 0.817
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0674e-01, 1.3858e-01, 2.1773e-01, 3.2610e-01, 1.2509e-04, 2.2905e-05,
        1.1070e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.089

[Epoch: 42, batch: 111/188] total loss per batch: 0.806
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0052, 0.1148, 0.0161, 0.0035, 0.0035, 0.8533, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.009

[Epoch: 42, batch: 148/188] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0177, 0.0071, 0.2371, 0.0095, 0.7154, 0.0046, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.031

[Epoch: 42, batch: 185/188] total loss per batch: 0.840
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0053, 0.0083, 0.0130, 0.0062, 0.9437, 0.0107, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 43, batch: 37/188] total loss per batch: 0.806
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0068, 0.0220, 0.0576, 0.6604, 0.0097, 0.1977, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 43, batch: 74/188] total loss per batch: 0.814
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7062e-01, 8.9684e-02, 1.4698e-01, 4.7850e-01, 1.1758e-04, 4.8504e-05,
        1.1405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.041

[Epoch: 43, batch: 111/188] total loss per batch: 0.803
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1885, 0.0145, 0.0047, 0.0046, 0.7801, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 43, batch: 148/188] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.0043, 0.1074, 0.0065, 0.8676, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.023

[Epoch: 43, batch: 185/188] total loss per batch: 0.837
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0042, 0.0092, 0.0190, 0.0059, 0.9317, 0.0119, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.002

[Epoch: 44, batch: 37/188] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0194, 0.0656, 0.7261, 0.0112, 0.1235, 0.0480],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 44, batch: 74/188] total loss per batch: 0.813
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.7248e-01, 8.6892e-02, 2.1960e-01, 2.8906e-01, 8.1873e-05, 2.9458e-05,
        1.3186e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.085

[Epoch: 44, batch: 111/188] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1713, 0.0161, 0.0039, 0.0034, 0.7967, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 44, batch: 148/188] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0034, 0.1453, 0.0066, 0.8315, 0.0020, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.023

[Epoch: 44, batch: 185/188] total loss per batch: 0.837
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0104, 0.0141, 0.0053, 0.9376, 0.0103, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 45, batch: 37/188] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0044, 0.0232, 0.0622, 0.6596, 0.0114, 0.1994, 0.0398],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 45, batch: 74/188] total loss per batch: 0.814
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.6743e-01, 1.0663e-01, 1.5864e-01, 4.4413e-01, 1.1298e-04, 5.1808e-05,
        1.2301e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.022

[Epoch: 45, batch: 111/188] total loss per batch: 0.803
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0017, 0.0916, 0.0104, 0.0034, 0.0026, 0.8871, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 45, batch: 148/188] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0053, 0.1555, 0.0075, 0.8122, 0.0041, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.010

[Epoch: 45, batch: 185/188] total loss per batch: 0.838
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0038, 0.0086, 0.0228, 0.0054, 0.9195, 0.0146, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.001

[Epoch: 46, batch: 37/188] total loss per batch: 0.806
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0071, 0.0190, 0.0588, 0.7274, 0.0108, 0.1219, 0.0549],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 46, batch: 74/188] total loss per batch: 0.815
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.7949e-01, 9.9985e-02, 1.3936e-01, 3.6548e-01, 9.1795e-05, 3.5727e-05,
        1.1557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.074

[Epoch: 46, batch: 111/188] total loss per batch: 0.804
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.2076, 0.0168, 0.0042, 0.0043, 0.7569, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 46, batch: 148/188] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0036, 0.1580, 0.0066, 0.8189, 0.0023, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.025

[Epoch: 46, batch: 185/188] total loss per batch: 0.840
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0091, 0.0130, 0.0049, 0.9450, 0.0094, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.008

[Epoch: 47, batch: 37/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0252, 0.0639, 0.6619, 0.0150, 0.1890, 0.0389],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 47, batch: 74/188] total loss per batch: 0.818
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7913e-01, 9.8137e-02, 2.7695e-01, 3.0731e-01, 1.6571e-04, 3.1869e-05,
        1.3828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.055

[Epoch: 47, batch: 111/188] total loss per batch: 0.807
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0023, 0.1418, 0.0096, 0.0039, 0.0033, 0.8352, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 47, batch: 148/188] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0063, 0.1360, 0.0072, 0.8317, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.003

[Epoch: 47, batch: 185/188] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0042, 0.0087, 0.0173, 0.0074, 0.9229, 0.0144, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.007

[Epoch: 48, batch: 37/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0066, 0.0173, 0.0751, 0.7062, 0.0156, 0.1302, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 48, batch: 74/188] total loss per batch: 0.820
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9265e-01, 8.4206e-02, 8.7962e-02, 5.2494e-01, 1.0885e-04, 4.9087e-05,
        1.1008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.099

[Epoch: 48, batch: 111/188] total loss per batch: 0.809
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0029, 0.1473, 0.0149, 0.0037, 0.0040, 0.8218, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 48, batch: 148/188] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0027, 0.1289, 0.0068, 0.8475, 0.0028, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.017

[Epoch: 48, batch: 185/188] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0043, 0.0081, 0.0195, 0.0050, 0.9354, 0.0112, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 49, batch: 37/188] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0093, 0.0233, 0.0822, 0.6268, 0.0227, 0.1840, 0.0516],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 49, batch: 74/188] total loss per batch: 0.823
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.5698e-01, 8.5962e-02, 2.2637e-01, 2.9209e-01, 1.5154e-04, 3.8977e-05,
        1.3841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.079

[Epoch: 49, batch: 111/188] total loss per batch: 0.810
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0045, 0.2264, 0.0111, 0.0066, 0.0062, 0.7407, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 49, batch: 148/188] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0120, 0.0091, 0.1920, 0.0134, 0.7612, 0.0064, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 49, batch: 185/188] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0113, 0.0116, 0.0058, 0.9366, 0.0123, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 50, batch: 37/188] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0042, 0.0134, 0.0580, 0.7799, 0.0087, 0.1027, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 50, batch: 74/188] total loss per batch: 0.823
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.6193e-01, 1.5683e-01, 1.0243e-01, 4.3682e-01, 1.4476e-04, 3.9480e-05,
        1.4181e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.028

[Epoch: 50, batch: 111/188] total loss per batch: 0.810
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0034, 0.1815, 0.0170, 0.0066, 0.0034, 0.7814, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.005

[Epoch: 50, batch: 148/188] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0025, 0.0985, 0.0057, 0.8794, 0.0027, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 50, batch: 185/188] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0041, 0.0116, 0.0285, 0.0065, 0.9201, 0.0101, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 51, batch: 37/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0079, 0.0240, 0.0707, 0.5887, 0.0261, 0.2336, 0.0492],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 51, batch: 74/188] total loss per batch: 0.819
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.6291e-01, 6.8003e-02, 2.5206e-01, 3.2416e-01, 7.5642e-05, 2.4841e-05,
        9.2766e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.090

[Epoch: 51, batch: 111/188] total loss per batch: 0.807
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0047, 0.1148, 0.0105, 0.0046, 0.0048, 0.8553, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 51, batch: 148/188] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0108, 0.0080, 0.1811, 0.0104, 0.7796, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.006

[Epoch: 51, batch: 185/188] total loss per batch: 0.841
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0054, 0.0095, 0.0149, 0.0054, 0.9330, 0.0124, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 52, batch: 37/188] total loss per batch: 0.804
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0170, 0.0552, 0.7246, 0.0166, 0.1376, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 52, batch: 74/188] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9679e-01, 1.2666e-01, 1.2971e-01, 4.0575e-01, 8.4360e-05, 3.8998e-05,
        1.4096e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.057

[Epoch: 52, batch: 111/188] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1379, 0.0130, 0.0050, 0.0048, 0.8304, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 52, batch: 148/188] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0044, 0.1222, 0.0078, 0.8508, 0.0032, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 52, batch: 185/188] total loss per batch: 0.835
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0122, 0.0198, 0.0055, 0.9317, 0.0093, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 53, batch: 37/188] total loss per batch: 0.802
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0057, 0.0213, 0.0634, 0.6815, 0.0170, 0.1645, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 53, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3112e-01, 1.0450e-01, 1.7938e-01, 3.7536e-01, 5.8863e-05, 3.0215e-05,
        1.0955e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.084

[Epoch: 53, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.1337, 0.0108, 0.0040, 0.0043, 0.8391, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 53, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0043, 0.1365, 0.0081, 0.8352, 0.0033, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 53, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0095, 0.0176, 0.0069, 0.9314, 0.0102, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 54, batch: 37/188] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0064, 0.0195, 0.0622, 0.7091, 0.0146, 0.1439, 0.0442],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 54, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0997e-01, 1.1751e-01, 1.7720e-01, 3.7618e-01, 5.2183e-05, 2.8902e-05,
        1.1906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.046

[Epoch: 54, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.1859, 0.0123, 0.0056, 0.0062, 0.7808, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 54, batch: 148/188] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0053, 0.1757, 0.0079, 0.7964, 0.0034, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 54, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0044, 0.0106, 0.0170, 0.0047, 0.9351, 0.0108, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.005

[Epoch: 55, batch: 37/188] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0234, 0.0674, 0.6326, 0.0194, 0.2029, 0.0483],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 55, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1614e-01, 1.0057e-01, 1.8949e-01, 3.7725e-01, 5.5937e-05, 2.4395e-05,
        1.1647e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.079

[Epoch: 55, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0029, 0.1216, 0.0119, 0.0035, 0.0040, 0.8519, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 55, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0039, 0.1215, 0.0060, 0.8566, 0.0026, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 55, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0098, 0.0174, 0.0069, 0.9268, 0.0116, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 56, batch: 37/188] total loss per batch: 0.802
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0060, 0.0172, 0.0620, 0.7339, 0.0123, 0.1250, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 56, batch: 74/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0815e-01, 1.0891e-01, 1.5271e-01, 3.9666e-01, 4.3598e-05, 2.6838e-05,
        1.3350e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.045

[Epoch: 56, batch: 111/188] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.1773, 0.0137, 0.0053, 0.0054, 0.7890, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 56, batch: 148/188] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0078, 0.0057, 0.1958, 0.0072, 0.7739, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.002

[Epoch: 56, batch: 185/188] total loss per batch: 0.836
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0051, 0.0114, 0.0175, 0.0051, 0.9287, 0.0137, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.003

[Epoch: 57, batch: 37/188] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0234, 0.0727, 0.6556, 0.0190, 0.1683, 0.0553],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 57, batch: 74/188] total loss per batch: 0.814
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.6366e-01, 9.3293e-02, 2.0981e-01, 3.3011e-01, 7.9127e-05, 2.1425e-05,
        1.0303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.094

[Epoch: 57, batch: 111/188] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1411, 0.0112, 0.0034, 0.0051, 0.8299, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 57, batch: 148/188] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0042, 0.1121, 0.0057, 0.8641, 0.0030, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 57, batch: 185/188] total loss per batch: 0.838
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0105, 0.0173, 0.0072, 0.9243, 0.0122, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 58, batch: 37/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0071, 0.0181, 0.0553, 0.7100, 0.0127, 0.1593, 0.0374],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 58, batch: 74/188] total loss per batch: 0.817
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.2852e-01, 1.1892e-01, 1.3977e-01, 4.5037e-01, 4.1910e-05, 2.7073e-05,
        1.6236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.060

[Epoch: 58, batch: 111/188] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.1633, 0.0107, 0.0049, 0.0054, 0.8066, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 58, batch: 148/188] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0086, 0.0045, 0.1631, 0.0074, 0.8081, 0.0033, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 58, batch: 185/188] total loss per batch: 0.841
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0066, 0.0108, 0.0153, 0.0070, 0.9292, 0.0113, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 59, batch: 37/188] total loss per batch: 0.809
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0078, 0.0245, 0.0795, 0.6614, 0.0199, 0.1621, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 59, batch: 74/188] total loss per batch: 0.817
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.9885e-01, 8.7080e-02, 1.7990e-01, 3.3826e-01, 1.1204e-04, 2.4442e-05,
        9.5772e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.083

[Epoch: 59, batch: 111/188] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1929, 0.0106, 0.0043, 0.0041, 0.7771, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.005

[Epoch: 59, batch: 148/188] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0053, 0.1742, 0.0067, 0.7974, 0.0029, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 59, batch: 185/188] total loss per batch: 0.840
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0044, 0.0079, 0.0105, 0.0047, 0.9497, 0.0102, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 60, batch: 37/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0059, 0.0180, 0.0440, 0.7389, 0.0114, 0.1342, 0.0477],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 60, batch: 74/188] total loss per batch: 0.815
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.6366e-01, 1.1225e-01, 1.6267e-01, 4.2822e-01, 4.3666e-05, 2.7364e-05,
        1.3314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.068

[Epoch: 60, batch: 111/188] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0042, 0.1386, 0.0090, 0.0057, 0.0058, 0.8321, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 60, batch: 148/188] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0049, 0.1529, 0.0065, 0.8220, 0.0024, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 60, batch: 185/188] total loss per batch: 0.839
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0073, 0.0148, 0.0037, 0.9424, 0.0089, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 61, batch: 37/188] total loss per batch: 0.806
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0066, 0.0230, 0.0686, 0.6743, 0.0208, 0.1700, 0.0367],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 61, batch: 74/188] total loss per batch: 0.814
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.4395e-01, 1.0576e-01, 2.0616e-01, 3.4713e-01, 1.1236e-04, 3.7731e-05,
        9.6845e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 61, batch: 111/188] total loss per batch: 0.803
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1645, 0.0107, 0.0042, 0.0040, 0.8081, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 61, batch: 148/188] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.0052, 0.1675, 0.0073, 0.8034, 0.0023, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 61, batch: 185/188] total loss per batch: 0.838
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0058, 0.0109, 0.0155, 0.0072, 0.9193, 0.0134, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 62, batch: 37/188] total loss per batch: 0.802
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0059, 0.0172, 0.0561, 0.7357, 0.0115, 0.1307, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 62, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9378e-01, 9.8597e-02, 1.7396e-01, 4.1651e-01, 4.8586e-05, 2.2296e-05,
        1.1708e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.071

[Epoch: 62, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0032, 0.1338, 0.0102, 0.0049, 0.0047, 0.8374, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 62, batch: 148/188] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0045, 0.1196, 0.0060, 0.8560, 0.0029, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 62, batch: 185/188] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0090, 0.0146, 0.0054, 0.9340, 0.0110, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 63, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0059, 0.0201, 0.0629, 0.6573, 0.0170, 0.1930, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 63, batch: 74/188] total loss per batch: 0.809
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1876e-01, 1.1241e-01, 1.8619e-01, 3.6633e-01, 5.7313e-05, 2.7673e-05,
        1.1622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.068

[Epoch: 63, batch: 111/188] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0026, 0.1564, 0.0089, 0.0038, 0.0040, 0.8194, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 63, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0046, 0.1540, 0.0067, 0.8210, 0.0024, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 63, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0053, 0.0089, 0.0138, 0.0051, 0.9366, 0.0092, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 64, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0065, 0.0191, 0.0633, 0.7043, 0.0155, 0.1448, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 64, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0656e-01, 9.5517e-02, 1.8130e-01, 3.8448e-01, 4.3740e-05, 1.7961e-05,
        1.3207e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.064

[Epoch: 64, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0032, 0.1619, 0.0092, 0.0046, 0.0053, 0.8100, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 64, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0045, 0.1393, 0.0060, 0.8368, 0.0030, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 64, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0084, 0.0134, 0.0049, 0.9402, 0.0097, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 65, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0053, 0.0177, 0.0666, 0.6704, 0.0158, 0.1790, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 65, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2190e-01, 1.1147e-01, 1.7216e-01, 3.8999e-01, 4.9124e-05, 2.2664e-05,
        1.0441e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.068

[Epoch: 65, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0026, 0.1436, 0.0088, 0.0039, 0.0041, 0.8320, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 65, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0045, 0.1522, 0.0062, 0.8231, 0.0028, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 65, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0057, 0.0111, 0.0188, 0.0061, 0.9223, 0.0112, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 66, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0064, 0.0216, 0.0637, 0.6861, 0.0145, 0.1565, 0.0511],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 66, batch: 74/188] total loss per batch: 0.809
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0709e-01, 9.8934e-02, 1.9295e-01, 3.6352e-01, 4.1215e-05, 1.5826e-05,
        1.3745e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.054

[Epoch: 66, batch: 111/188] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0034, 0.1675, 0.0082, 0.0048, 0.0055, 0.8042, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 66, batch: 148/188] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0045, 0.1485, 0.0051, 0.8289, 0.0027, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 66, batch: 185/188] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0081, 0.0116, 0.0038, 0.9481, 0.0069, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 67, batch: 37/188] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0051, 0.0142, 0.0673, 0.7166, 0.0144, 0.1426, 0.0398],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 67, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3381e-01, 1.0732e-01, 1.6265e-01, 3.9657e-01, 4.5146e-05, 1.7896e-05,
        9.9589e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.078

[Epoch: 67, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0033, 0.1649, 0.0094, 0.0049, 0.0051, 0.8067, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 67, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0048, 0.1437, 0.0069, 0.8277, 0.0040, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 67, batch: 185/188] total loss per batch: 0.835
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0054, 0.0119, 0.0214, 0.0081, 0.9080, 0.0140, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 68, batch: 37/188] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0066, 0.0293, 0.0799, 0.6558, 0.0167, 0.1530, 0.0587],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 68, batch: 74/188] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0301e-01, 1.0039e-01, 1.5934e-01, 3.9105e-01, 3.6854e-05, 1.5661e-05,
        1.4615e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.049

[Epoch: 68, batch: 111/188] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0032, 0.1248, 0.0075, 0.0039, 0.0039, 0.8513, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 68, batch: 148/188] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0052, 0.1902, 0.0047, 0.7869, 0.0026, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.016

[Epoch: 68, batch: 185/188] total loss per batch: 0.837
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0043, 0.0074, 0.0129, 0.0031, 0.9505, 0.0083, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 69, batch: 37/188] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0052, 0.0123, 0.0557, 0.7245, 0.0127, 0.1637, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 69, batch: 74/188] total loss per batch: 0.814
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0702e-01, 1.1724e-01, 2.4193e-01, 3.2178e-01, 5.7385e-05, 2.4151e-05,
        1.1194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.093

[Epoch: 69, batch: 111/188] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0051, 0.2037, 0.0121, 0.0062, 0.0070, 0.7590, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 69, batch: 148/188] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0049, 0.0829, 0.0069, 0.8863, 0.0052, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 69, batch: 185/188] total loss per batch: 0.837
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0051, 0.0090, 0.0143, 0.0064, 0.9362, 0.0095, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.006

[Epoch: 70, batch: 37/188] total loss per batch: 0.805
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0069, 0.0248, 0.0667, 0.6602, 0.0188, 0.1630, 0.0596],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 70, batch: 74/188] total loss per batch: 0.813
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3064e-01, 9.1957e-02, 1.2026e-01, 4.2413e-01, 3.5909e-05, 1.9260e-05,
        1.3295e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.033

[Epoch: 70, batch: 111/188] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0028, 0.1420, 0.0077, 0.0034, 0.0034, 0.8363, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 70, batch: 148/188] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0051, 0.2126, 0.0064, 0.7618, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.005

[Epoch: 70, batch: 185/188] total loss per batch: 0.836
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0076, 0.0155, 0.0048, 0.9404, 0.0095, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 71, batch: 37/188] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0178, 0.0632, 0.7038, 0.0151, 0.1559, 0.0384],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 71, batch: 74/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0755e-01, 1.0652e-01, 2.0681e-01, 3.6511e-01, 4.6269e-05, 1.7686e-05,
        1.1395e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.076

[Epoch: 71, batch: 111/188] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.1455, 0.0103, 0.0051, 0.0057, 0.8232, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 71, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0048, 0.0910, 0.0049, 0.8820, 0.0040, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 71, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0051, 0.0085, 0.0146, 0.0059, 0.9397, 0.0085, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 72, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0054, 0.0186, 0.0547, 0.6967, 0.0159, 0.1699, 0.0388],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 72, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1056e-01, 9.8279e-02, 1.7715e-01, 3.9492e-01, 3.7902e-05, 2.0884e-05,
        1.1904e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.048

[Epoch: 72, batch: 111/188] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1594, 0.0086, 0.0041, 0.0045, 0.8152, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 72, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0053, 0.1690, 0.0058, 0.8057, 0.0038, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 72, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0089, 0.0150, 0.0052, 0.9343, 0.0113, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 73, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0061, 0.0203, 0.0659, 0.6853, 0.0157, 0.1640, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 73, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1830e-01, 1.1491e-01, 1.8290e-01, 3.6155e-01, 3.1635e-05, 1.5462e-05,
        1.2228e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.070

[Epoch: 73, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0033, 0.1537, 0.0082, 0.0041, 0.0047, 0.8204, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 73, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0048, 0.1329, 0.0050, 0.8435, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 73, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0090, 0.0155, 0.0053, 0.9350, 0.0096, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 74, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0054, 0.0178, 0.0590, 0.6948, 0.0146, 0.1678, 0.0406],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 74, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1475e-01, 9.6510e-02, 1.7207e-01, 3.9934e-01, 3.0161e-05, 1.7641e-05,
        1.1728e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.067

[Epoch: 74, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0034, 0.1540, 0.0082, 0.0042, 0.0048, 0.8197, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 74, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0048, 0.1458, 0.0056, 0.8296, 0.0037, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 74, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0046, 0.0084, 0.0133, 0.0053, 0.9404, 0.0092, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 75, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0065, 0.0194, 0.0670, 0.6898, 0.0151, 0.1575, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 75, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3346e-01, 1.0954e-01, 1.8664e-01, 3.5508e-01, 3.3292e-05, 1.6210e-05,
        1.1523e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.066

[Epoch: 75, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0034, 0.1521, 0.0083, 0.0043, 0.0052, 0.8207, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 75, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0047, 0.1507, 0.0052, 0.8259, 0.0034, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 75, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0099, 0.0170, 0.0056, 0.9308, 0.0101, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 76, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0054, 0.0181, 0.0614, 0.6824, 0.0150, 0.1713, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 76, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9414e-01, 9.8002e-02, 1.7544e-01, 4.1499e-01, 2.6838e-05, 1.5344e-05,
        1.1739e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.064

[Epoch: 76, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.1578, 0.0079, 0.0043, 0.0046, 0.8161, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 76, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0049, 0.1516, 0.0055, 0.8232, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 76, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0088, 0.0141, 0.0057, 0.9362, 0.0102, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 77, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0066, 0.0196, 0.0661, 0.6977, 0.0170, 0.1466, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 77, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.4438e-01, 1.1445e-01, 1.7593e-01, 3.5383e-01, 3.3275e-05, 1.4382e-05,
        1.1137e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.072

[Epoch: 77, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.1475, 0.0074, 0.0045, 0.0058, 0.8248, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 77, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0046, 0.1464, 0.0057, 0.8288, 0.0038, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 77, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0105, 0.0201, 0.0055, 0.9282, 0.0094, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 78, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0053, 0.0187, 0.0667, 0.6990, 0.0128, 0.1544, 0.0430],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 78, batch: 74/188] total loss per batch: 0.809
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.8947e-01, 8.9179e-02, 2.0255e-01, 3.9012e-01, 2.3482e-05, 1.6991e-05,
        1.2863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 78, batch: 111/188] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0031, 0.1393, 0.0084, 0.0039, 0.0041, 0.8361, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 78, batch: 148/188] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0053, 0.1639, 0.0055, 0.8096, 0.0041, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 78, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0046, 0.0088, 0.0123, 0.0056, 0.9382, 0.0105, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 79, batch: 37/188] total loss per batch: 0.802
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0067, 0.0225, 0.0626, 0.6604, 0.0198, 0.1783, 0.0497],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 79, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3015e-01, 1.2228e-01, 1.5405e-01, 3.7859e-01, 3.9542e-05, 1.0874e-05,
        1.1489e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.079

[Epoch: 79, batch: 111/188] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0044, 0.1622, 0.0069, 0.0047, 0.0068, 0.8081, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 79, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0040, 0.1245, 0.0048, 0.8536, 0.0034, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 79, batch: 185/188] total loss per batch: 0.836
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0053, 0.0103, 0.0205, 0.0055, 0.9232, 0.0124, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 80, batch: 37/188] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0055, 0.0206, 0.0724, 0.7261, 0.0117, 0.1218, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 80, batch: 74/188] total loss per batch: 0.812
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.8804e-01, 8.6745e-02, 2.0044e-01, 3.9057e-01, 2.7366e-05, 1.5838e-05,
        1.3416e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.060

[Epoch: 80, batch: 111/188] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0033, 0.1518, 0.0104, 0.0044, 0.0048, 0.8194, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 80, batch: 148/188] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0067, 0.1691, 0.0060, 0.8026, 0.0039, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 80, batch: 185/188] total loss per batch: 0.836
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0041, 0.0080, 0.0117, 0.0050, 0.9456, 0.0088, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 81, batch: 37/188] total loss per batch: 0.802
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0191, 0.0617, 0.6516, 0.0169, 0.2024, 0.0426],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 81, batch: 74/188] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3909e-01, 1.0809e-01, 1.6913e-01, 3.6604e-01, 3.8420e-05, 1.3906e-05,
        1.1759e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.087

[Epoch: 81, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1664, 0.0078, 0.0046, 0.0058, 0.8053, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 81, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0048, 0.1480, 0.0056, 0.8269, 0.0043, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 81, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0053, 0.0102, 0.0181, 0.0050, 0.9265, 0.0101, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 82, batch: 37/188] total loss per batch: 0.800
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0065, 0.0193, 0.0719, 0.6937, 0.0158, 0.1456, 0.0472],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 82, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0956e-01, 9.7380e-02, 1.8223e-01, 3.8724e-01, 3.1516e-05, 1.4799e-05,
        1.2354e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.061

[Epoch: 82, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0032, 0.1523, 0.0074, 0.0037, 0.0043, 0.8239, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 82, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0053, 0.1411, 0.0056, 0.8328, 0.0040, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 82, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0092, 0.0155, 0.0048, 0.9353, 0.0099, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 83, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0062, 0.0187, 0.0661, 0.6819, 0.0157, 0.1673, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 83, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2799e-01, 1.0513e-01, 1.7336e-01, 3.6927e-01, 2.7915e-05, 1.3095e-05,
        1.2421e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.075

[Epoch: 83, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0035, 0.1649, 0.0077, 0.0043, 0.0051, 0.8090, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 83, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0049, 0.1547, 0.0053, 0.8210, 0.0038, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 83, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0044, 0.0087, 0.0146, 0.0044, 0.9413, 0.0084, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 84, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0063, 0.0192, 0.0695, 0.6866, 0.0156, 0.1539, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 84, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0563e-01, 1.0265e-01, 1.8204e-01, 3.9352e-01, 2.7330e-05, 1.3677e-05,
        1.1611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.060

[Epoch: 84, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0034, 0.1546, 0.0076, 0.0038, 0.0046, 0.8203, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 84, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0049, 0.1441, 0.0053, 0.8307, 0.0043, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 84, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0095, 0.0162, 0.0049, 0.9333, 0.0104, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 85, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0061, 0.0193, 0.0640, 0.6866, 0.0152, 0.1630, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 85, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2058e-01, 1.0951e-01, 1.7592e-01, 3.7428e-01, 2.3574e-05, 1.1554e-05,
        1.1968e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 85, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.1575, 0.0071, 0.0037, 0.0045, 0.8183, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 85, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0049, 0.1510, 0.0052, 0.8249, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 85, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0093, 0.0154, 0.0049, 0.9376, 0.0091, 0.0192],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 86, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0064, 0.0191, 0.0682, 0.6842, 0.0153, 0.1587, 0.0481],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 86, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.1544e-01, 1.0045e-01, 1.8130e-01, 3.8532e-01, 2.4078e-05, 1.2781e-05,
        1.1746e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.063

[Epoch: 86, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0036, 0.1527, 0.0076, 0.0039, 0.0048, 0.8215, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 86, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0048, 0.1534, 0.0053, 0.8217, 0.0043, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 86, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0096, 0.0155, 0.0049, 0.9350, 0.0098, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 87, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0057, 0.0190, 0.0618, 0.6957, 0.0144, 0.1574, 0.0460],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 87, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0681e-01, 1.1122e-01, 1.7769e-01, 3.8221e-01, 2.1927e-05, 1.0147e-05,
        1.2204e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.065

[Epoch: 87, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1634, 0.0069, 0.0038, 0.0046, 0.8117, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 87, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0047, 0.1473, 0.0052, 0.8287, 0.0041, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 87, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0099, 0.0157, 0.0050, 0.9367, 0.0085, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 88, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0060, 0.0188, 0.0687, 0.6736, 0.0150, 0.1729, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 88, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3115e-01, 9.7910e-02, 1.8401e-01, 3.7088e-01, 2.1820e-05, 1.2884e-05,
        1.1602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.068

[Epoch: 88, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.1426, 0.0071, 0.0039, 0.0050, 0.8320, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 88, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0049, 0.1616, 0.0050, 0.8135, 0.0039, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 88, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0045, 0.0094, 0.0155, 0.0053, 0.9342, 0.0104, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 89, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0053, 0.0195, 0.0592, 0.7180, 0.0142, 0.1354, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 89, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.9342e-01, 1.1482e-01, 1.7591e-01, 3.8688e-01, 1.8433e-05, 8.5811e-06,
        1.2894e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.063

[Epoch: 89, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0043, 0.1806, 0.0068, 0.0040, 0.0041, 0.7944, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 89, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.1341, 0.0050, 0.8425, 0.0045, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 89, batch: 185/188] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0052, 0.0095, 0.0169, 0.0048, 0.9352, 0.0090, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 90, batch: 37/188] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0063, 0.0197, 0.0636, 0.6649, 0.0166, 0.1828, 0.0460],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 90, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.5308e-01, 8.8509e-02, 1.7429e-01, 3.7683e-01, 2.6804e-05, 1.4623e-05,
        1.0725e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.079

[Epoch: 90, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.1461, 0.0077, 0.0044, 0.0058, 0.8263, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 90, batch: 148/188] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0054, 0.1668, 0.0050, 0.8076, 0.0033, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 90, batch: 185/188] total loss per batch: 0.834
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0043, 0.0093, 0.0149, 0.0046, 0.9378, 0.0095, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 91, batch: 37/188] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0051, 0.0233, 0.0657, 0.6919, 0.0138, 0.1478, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 91, batch: 74/188] total loss per batch: 0.810
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.7493e-01, 1.3308e-01, 1.8595e-01, 3.6762e-01, 2.0930e-05, 9.0422e-06,
        1.3839e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.054

[Epoch: 91, batch: 111/188] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1534, 0.0063, 0.0042, 0.0047, 0.8218, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 91, batch: 148/188] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0043, 0.1358, 0.0052, 0.8398, 0.0050, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 91, batch: 185/188] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0089, 0.0167, 0.0054, 0.9353, 0.0091, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 92, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0057, 0.0175, 0.0591, 0.7223, 0.0154, 0.1411, 0.0389],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 92, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2432e-01, 9.4776e-02, 1.7754e-01, 3.8915e-01, 2.4101e-05, 1.2057e-05,
        1.1418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.077

[Epoch: 92, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1571, 0.0072, 0.0043, 0.0051, 0.8163, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 92, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0050, 0.1577, 0.0052, 0.8165, 0.0047, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 92, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0096, 0.0165, 0.0050, 0.9332, 0.0103, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 93, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0200, 0.0684, 0.6646, 0.0149, 0.1794, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 93, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0555e-01, 1.1211e-01, 1.7999e-01, 3.7968e-01, 2.2177e-05, 1.0345e-05,
        1.2264e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.071

[Epoch: 93, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0037, 0.1516, 0.0069, 0.0041, 0.0049, 0.8232, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 93, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0049, 0.1397, 0.0051, 0.8348, 0.0046, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 93, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0094, 0.0162, 0.0052, 0.9346, 0.0100, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 94, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0056, 0.0182, 0.0617, 0.7008, 0.0143, 0.1559, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 94, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2223e-01, 1.0188e-01, 1.8080e-01, 3.7841e-01, 1.9260e-05, 9.9710e-06,
        1.1665e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 94, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1489, 0.0070, 0.0042, 0.0049, 0.8253, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 94, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0048, 0.1564, 0.0050, 0.8189, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 94, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0046, 0.0092, 0.0157, 0.0048, 0.9354, 0.0101, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 95, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0195, 0.0662, 0.6843, 0.0155, 0.1624, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 95, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0971e-01, 1.1017e-01, 1.8051e-01, 3.8104e-01, 2.0894e-05, 9.2415e-06,
        1.1853e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 95, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1573, 0.0068, 0.0041, 0.0050, 0.8173, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 95, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0048, 0.1413, 0.0051, 0.8338, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 95, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0049, 0.0096, 0.0159, 0.0051, 0.9342, 0.0102, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 96, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0055, 0.0186, 0.0644, 0.6930, 0.0145, 0.1595, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 96, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.2400e-01, 9.9172e-02, 1.8036e-01, 3.7540e-01, 1.8071e-05, 9.1825e-06,
        1.2104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 96, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0039, 0.1453, 0.0068, 0.0042, 0.0049, 0.8293, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 96, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0049, 0.1626, 0.0050, 0.8130, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 96, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0094, 0.0159, 0.0050, 0.9349, 0.0099, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 97, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0058, 0.0192, 0.0653, 0.6884, 0.0153, 0.1593, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 97, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.0404e-01, 1.1079e-01, 1.7490e-01, 3.9190e-01, 1.7953e-05, 8.4000e-06,
        1.1835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.068

[Epoch: 97, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0041, 0.1664, 0.0065, 0.0041, 0.0053, 0.8074, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 97, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0046, 0.1332, 0.0047, 0.8429, 0.0045, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 97, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0097, 0.0161, 0.0053, 0.9322, 0.0112, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 98, batch: 37/188] total loss per batch: 0.798
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0054, 0.0193, 0.0646, 0.6875, 0.0143, 0.1651, 0.0437],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 98, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.3314e-01, 9.9208e-02, 1.8252e-01, 3.6433e-01, 1.7899e-05, 8.9730e-06,
        1.2077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.069

[Epoch: 98, batch: 111/188] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0040, 0.1366, 0.0069, 0.0041, 0.0047, 0.8381, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 98, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0052, 0.1712, 0.0053, 0.8035, 0.0045, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 98, batch: 185/188] total loss per batch: 0.831
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0047, 0.0099, 0.0156, 0.0049, 0.9356, 0.0095, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 99, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0057, 0.0190, 0.0663, 0.6969, 0.0147, 0.1517, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 99, batch: 74/188] total loss per batch: 0.807
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([1.8761e-01, 1.0633e-01, 1.8594e-01, 4.0051e-01, 1.7868e-05, 8.1628e-06,
        1.1958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.064

[Epoch: 99, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0042, 0.1706, 0.0061, 0.0046, 0.0055, 0.8026, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 99, batch: 148/188] total loss per batch: 0.767
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0042, 0.1314, 0.0046, 0.8460, 0.0041, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.007

[Epoch: 99, batch: 185/188] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0048, 0.0093, 0.0154, 0.0050, 0.9342, 0.0108, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

[Epoch: 100, batch: 37/188] total loss per batch: 0.799
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0200, 0.0650, 0.6900, 0.0150, 0.1600, 0.0450])
Policy pred: tensor([0.0056, 0.0188, 0.0613, 0.6832, 0.0158, 0.1728, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 100, batch: 74/188] total loss per batch: 0.808
Policy (actual, predicted): 3 3
Policy data: tensor([0.2150, 0.1050, 0.1800, 0.3800, 0.0000, 0.0000, 0.1200])
Policy pred: tensor([2.4285e-01, 1.0655e-01, 1.6245e-01, 3.6285e-01, 1.9537e-05, 9.9709e-06,
        1.2527e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.068 0.079

[Epoch: 100, batch: 111/188] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.1550, 0.0050, 0.0050, 0.0050, 0.8200, 0.0050])
Policy pred: tensor([0.0046, 0.1414, 0.0074, 0.0041, 0.0054, 0.8316, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 100, batch: 148/188] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.8250, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0057, 0.1625, 0.0053, 0.8105, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.006

[Epoch: 100, batch: 185/188] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0100, 0.0150, 0.0050, 0.9350, 0.0100, 0.0200])
Policy pred: tensor([0.0050, 0.0107, 0.0173, 0.0051, 0.9321, 0.0102, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.007

