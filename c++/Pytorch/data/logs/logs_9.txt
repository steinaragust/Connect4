Training set samples: 7537
Batch size: 32
[Epoch: 1, batch: 47/236] total loss per batch: 0.941
Policy (actual, predicted): 1 2
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.0400e-05, 4.3685e-01, 4.6788e-01, 4.0124e-06, 9.5241e-02, 1.1404e-06,
        1.1132e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.024

[Epoch: 1, batch: 94/236] total loss per batch: 0.892
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6467e-07, 1.1036e-08, 5.4955e-08, 1.6228e-08, 1.0000e+00, 8.4885e-10,
        9.3315e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 1, batch: 141/236] total loss per batch: 0.885
Policy (actual, predicted): 1 4
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.3716e-01, 1.1994e-01, 1.5794e-01, 4.5030e-06, 2.4968e-01, 1.6096e-06,
        2.3527e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.301

[Epoch: 1, batch: 188/236] total loss per batch: 0.877
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3163e-02, 1.3525e-02, 1.2249e-02, 3.1693e-06, 1.0724e-02, 5.7376e-02,
        8.8296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.024

[Epoch: 1, batch: 235/236] total loss per batch: 0.890
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0235, 0.0338, 0.0205, 0.0394, 0.0141, 0.8630, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.001

[Epoch: 2, batch: 47/236] total loss per batch: 0.656
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.9120e-06, 5.9372e-01, 3.4567e-01, 1.1596e-06, 6.0600e-02, 6.1618e-07,
        9.3976e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.291

[Epoch: 2, batch: 94/236] total loss per batch: 0.632
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4111e-07, 1.4284e-08, 4.8937e-08, 1.0036e-08, 1.0000e+00, 1.9093e-09,
        7.8326e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.555

[Epoch: 2, batch: 141/236] total loss per batch: 0.641
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.5105e-01, 1.4711e-01, 1.0718e-01, 1.6343e-06, 2.4802e-01, 5.6067e-07,
        2.4664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.426

[Epoch: 2, batch: 188/236] total loss per batch: 0.618
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([8.2152e-03, 6.6802e-03, 8.0837e-03, 1.0663e-06, 6.4885e-03, 2.4942e-02,
        9.4559e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.029

[Epoch: 2, batch: 235/236] total loss per batch: 0.621
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0088, 0.0064, 0.0108, 0.0086, 0.0050, 0.9583, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 3, batch: 47/236] total loss per batch: 0.512
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2761e-06, 8.1820e-01, 1.4266e-01, 2.6302e-07, 3.9140e-02, 8.9291e-08,
        2.0088e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.381

[Epoch: 3, batch: 94/236] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.6302e-08, 4.8659e-09, 4.3521e-09, 2.5577e-09, 1.0000e+00, 2.1352e-10,
        6.1756e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.563

[Epoch: 3, batch: 141/236] total loss per batch: 0.514
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.6300e-01, 2.3134e-01, 6.4953e-02, 5.4413e-07, 1.9406e-01, 2.5239e-07,
        2.4664e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.470

[Epoch: 3, batch: 188/236] total loss per batch: 0.496
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.6754e-03, 4.5722e-03, 4.6957e-03, 5.0019e-07, 4.4195e-03, 1.5976e-02,
        9.6366e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.034

[Epoch: 3, batch: 235/236] total loss per batch: 0.493
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([4.1202e-03, 3.5626e-03, 4.3364e-03, 3.5562e-03, 2.8056e-03, 9.8085e-01,
        7.7090e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.048

[Epoch: 4, batch: 47/236] total loss per batch: 0.449
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.0042e-07, 8.5240e-01, 1.2285e-01, 1.5247e-07, 2.4753e-02, 4.9272e-08,
        3.0647e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.108

[Epoch: 4, batch: 94/236] total loss per batch: 0.453
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.6210e-09, 3.0219e-09, 1.9004e-09, 1.1155e-09, 1.0000e+00, 1.2423e-10,
        3.0493e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 4, batch: 141/236] total loss per batch: 0.468
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.9263e-01, 3.1242e-01, 4.1274e-02, 3.6043e-07, 1.7487e-01, 1.3162e-07,
        1.7881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.406

[Epoch: 4, batch: 188/236] total loss per batch: 0.449
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.6432e-03, 7.8534e-03, 4.0874e-03, 5.6643e-07, 4.1205e-03, 1.6182e-02,
        9.6111e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 4, batch: 235/236] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.0626e-03, 1.3644e-03, 3.4110e-03, 4.4015e-03, 3.7458e-03, 9.8422e-01,
        7.9640e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 5, batch: 47/236] total loss per batch: 0.428
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.7173e-07, 9.2759e-01, 5.2237e-02, 7.9293e-08, 2.0171e-02, 5.5790e-08,
        1.8292e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.020

[Epoch: 5, batch: 94/236] total loss per batch: 0.433
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.7123e-09, 3.1128e-09, 9.9291e-10, 1.2875e-09, 1.0000e+00, 9.5957e-11,
        8.3487e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.536

[Epoch: 5, batch: 141/236] total loss per batch: 0.451
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3831e-01, 3.2036e-01, 3.5741e-02, 3.2446e-07, 1.3734e-01, 1.5581e-07,
        1.6824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.383

[Epoch: 5, batch: 188/236] total loss per batch: 0.428
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.9117e-03, 3.7447e-03, 2.8852e-03, 3.6808e-07, 2.5642e-03, 2.4506e-03,
        9.8444e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 5, batch: 235/236] total loss per batch: 0.422
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0021, 0.0029, 0.0056, 0.0045, 0.0045, 0.9791, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 6, batch: 47/236] total loss per batch: 0.417
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.1698e-07, 8.9456e-01, 9.3349e-02, 4.7496e-08, 1.2091e-02, 9.3918e-08,
        1.5704e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.438

[Epoch: 6, batch: 94/236] total loss per batch: 0.417
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2143e-08, 5.3342e-09, 7.6393e-10, 6.7885e-10, 1.0000e+00, 1.7430e-10,
        2.6959e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.533

[Epoch: 6, batch: 141/236] total loss per batch: 0.431
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3410e-01, 3.3308e-01, 5.2607e-02, 3.1372e-07, 1.2428e-01, 1.4069e-07,
        1.5593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.326

[Epoch: 6, batch: 188/236] total loss per batch: 0.412
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8317e-03, 4.2707e-03, 3.0179e-03, 3.8880e-07, 2.9474e-03, 2.8596e-03,
        9.8407e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 6, batch: 235/236] total loss per batch: 0.406
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0022, 0.0044, 0.0059, 0.0039, 0.0060, 0.9760, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 7, batch: 47/236] total loss per batch: 0.406
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.2184e-07, 8.6667e-01, 1.1936e-01, 1.3743e-07, 1.3973e-02, 8.1587e-08,
        2.7000e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.196

[Epoch: 7, batch: 94/236] total loss per batch: 0.405
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.3346e-09, 3.4687e-09, 7.2805e-10, 6.2928e-10, 1.0000e+00, 1.3739e-10,
        3.0498e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.525

[Epoch: 7, batch: 141/236] total loss per batch: 0.420
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.8931e-01, 3.2862e-01, 3.9598e-02, 3.1493e-07, 1.3308e-01, 1.0366e-07,
        1.0940e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.334

[Epoch: 7, batch: 188/236] total loss per batch: 0.402
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6302e-03, 3.4596e-03, 2.7436e-03, 3.3068e-07, 2.8473e-03, 1.1841e-03,
        9.8713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 7, batch: 235/236] total loss per batch: 0.399
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.7003e-03, 1.9518e-03, 3.1822e-03, 5.0819e-03, 4.0657e-03, 9.8209e-01,
        9.2886e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 8, batch: 47/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.2080e-07, 9.3680e-01, 5.1073e-02, 5.2790e-08, 1.2124e-02, 1.1957e-07,
        7.5052e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.343

[Epoch: 8, batch: 94/236] total loss per batch: 0.403
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.2432e-08, 8.1053e-08, 8.0929e-09, 4.8874e-09, 1.0000e+00, 1.7657e-09,
        3.1802e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.519

[Epoch: 8, batch: 141/236] total loss per batch: 0.419
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2353e-01, 3.5613e-01, 6.5235e-02, 4.0540e-07, 1.1175e-01, 1.2628e-07,
        1.4336e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.295

[Epoch: 8, batch: 188/236] total loss per batch: 0.402
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8136e-03, 8.3469e-03, 3.1314e-03, 6.5584e-07, 6.3124e-03, 1.2665e-03,
        9.7813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 8, batch: 235/236] total loss per batch: 0.392
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0026, 0.0047, 0.0030, 0.0057, 0.0039, 0.9788, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.036

[Epoch: 9, batch: 47/236] total loss per batch: 0.397
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.6192e-07, 8.5533e-01, 1.3064e-01, 1.4087e-07, 1.4037e-02, 9.0625e-08,
        5.4097e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.005

[Epoch: 9, batch: 94/236] total loss per batch: 0.400
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.6347e-08, 2.7553e-08, 2.9302e-09, 3.5492e-09, 1.0000e+00, 7.8419e-10,
        6.9533e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.475

[Epoch: 9, batch: 141/236] total loss per batch: 0.415
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2036e-01, 3.4546e-01, 4.6243e-02, 4.5629e-07, 9.8996e-02, 1.7055e-07,
        1.8894e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.287

[Epoch: 9, batch: 188/236] total loss per batch: 0.396
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2901e-03, 6.4485e-03, 2.0028e-03, 5.3976e-07, 4.4468e-03, 2.5398e-03,
        9.8227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 9, batch: 235/236] total loss per batch: 0.389
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0027, 0.0012, 0.0031, 0.0051, 0.0036, 0.9828, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 10, batch: 47/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.1463e-07, 9.4312e-01, 4.8173e-02, 7.9936e-08, 8.7070e-03, 5.2957e-08,
        9.0671e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.113

[Epoch: 10, batch: 94/236] total loss per batch: 0.396
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.8848e-08, 4.4289e-08, 7.0375e-09, 1.6378e-08, 1.0000e+00, 2.0735e-09,
        1.2427e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.450

[Epoch: 10, batch: 141/236] total loss per batch: 0.412
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6761e-01, 4.1554e-01, 3.9178e-02, 7.3661e-07, 8.2367e-02, 2.2188e-07,
        9.5310e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.284

[Epoch: 10, batch: 188/236] total loss per batch: 0.396
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.0816e-03, 3.9228e-03, 2.0356e-03, 3.7798e-07, 7.4555e-03, 2.1673e-03,
        9.8234e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.019

[Epoch: 10, batch: 235/236] total loss per batch: 0.388
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.2520e-03, 1.0041e-03, 1.6939e-03, 4.3658e-03, 3.9361e-03, 9.8615e-01,
        5.9634e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 11, batch: 47/236] total loss per batch: 0.398
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2914e-06, 8.8786e-01, 9.9348e-02, 2.0876e-07, 1.2788e-02, 1.1103e-07,
        1.7781e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.071

[Epoch: 11, batch: 94/236] total loss per batch: 0.397
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0594e-07, 6.8069e-08, 1.2619e-08, 1.8498e-08, 1.0000e+00, 2.0409e-09,
        1.4273e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.517

[Epoch: 11, batch: 141/236] total loss per batch: 0.408
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3599e-01, 3.5174e-01, 2.6894e-02, 8.4440e-07, 9.8328e-02, 3.8675e-07,
        1.8705e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.246

[Epoch: 11, batch: 188/236] total loss per batch: 0.393
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7316e-03, 3.7326e-03, 1.0699e-03, 8.3235e-07, 2.2122e-03, 1.1697e-03,
        9.8908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.018

[Epoch: 11, batch: 235/236] total loss per batch: 0.388
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0030, 0.0023, 0.0023, 0.0045, 0.0035, 0.9834, 0.0010],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 12, batch: 47/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.8199e-07, 9.5694e-01, 3.7344e-02, 1.9012e-07, 5.7160e-03, 6.5677e-08,
        3.9124e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.081

[Epoch: 12, batch: 94/236] total loss per batch: 0.391
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.4572e-07, 4.3775e-07, 6.1397e-08, 1.1371e-07, 1.0000e+00, 8.0687e-09,
        1.5033e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.481

[Epoch: 12, batch: 141/236] total loss per batch: 0.403
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7549e-01, 3.7875e-01, 5.1513e-02, 1.3166e-06, 7.6703e-02, 4.6855e-07,
        1.1754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.281

[Epoch: 12, batch: 188/236] total loss per batch: 0.389
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6321e-03, 3.8095e-03, 2.3646e-03, 5.9459e-07, 3.2608e-03, 2.1772e-03,
        9.8576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 12, batch: 235/236] total loss per batch: 0.383
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.7127e-03, 1.9116e-03, 1.7309e-03, 4.7284e-03, 1.8799e-03, 9.8718e-01,
        8.5312e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.036

[Epoch: 13, batch: 47/236] total loss per batch: 0.389
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1494e-06, 8.3843e-01, 1.5274e-01, 4.0716e-07, 8.8321e-03, 1.5340e-07,
        4.9797e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.079

[Epoch: 13, batch: 94/236] total loss per batch: 0.388
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3596e-07, 8.2226e-08, 2.2820e-08, 4.9636e-08, 1.0000e+00, 2.0748e-09,
        2.9607e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.501

[Epoch: 13, batch: 141/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.1728e-01, 3.8287e-01, 3.9923e-02, 1.0178e-06, 9.2788e-02, 5.2499e-07,
        1.6714e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.255

[Epoch: 13, batch: 188/236] total loss per batch: 0.388
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.1908e-03, 5.5876e-03, 1.3812e-03, 6.6582e-07, 3.9692e-03, 1.8151e-03,
        9.8506e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 13, batch: 235/236] total loss per batch: 0.382
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.5144e-03, 2.4915e-03, 1.2171e-03, 2.2980e-03, 1.6555e-03, 9.8906e-01,
        7.5983e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.028

[Epoch: 14, batch: 47/236] total loss per batch: 0.388
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.4293e-07, 9.5811e-01, 3.5018e-02, 2.1471e-07, 6.8676e-03, 9.2951e-08,
        5.9734e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.166

[Epoch: 14, batch: 94/236] total loss per batch: 0.387
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.3206e-07, 3.2267e-07, 4.9576e-08, 6.3544e-08, 1.0000e+00, 4.1349e-09,
        8.4677e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.487

[Epoch: 14, batch: 141/236] total loss per batch: 0.400
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6935e-01, 3.3358e-01, 5.4002e-02, 9.6977e-07, 9.7453e-02, 4.2409e-07,
        1.4561e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 14, batch: 188/236] total loss per batch: 0.389
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5330e-03, 4.8135e-03, 3.3661e-03, 7.6248e-07, 4.3602e-03, 1.9866e-03,
        9.8194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 14, batch: 235/236] total loss per batch: 0.389
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([3.1757e-03, 2.2379e-03, 1.1077e-03, 8.2776e-03, 2.6590e-03, 9.8193e-01,
        6.1665e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.045

[Epoch: 15, batch: 47/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0737e-06, 9.4758e-01, 4.6695e-02, 3.1869e-07, 5.7227e-03, 7.6683e-08,
        3.4573e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.103

[Epoch: 15, batch: 94/236] total loss per batch: 0.390
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0386e-07, 1.0414e-07, 1.6318e-08, 2.5758e-08, 1.0000e+00, 3.5412e-09,
        2.3742e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.484

[Epoch: 15, batch: 141/236] total loss per batch: 0.404
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.1619e-01, 3.9805e-01, 3.3782e-02, 1.1268e-06, 9.4989e-02, 7.2493e-07,
        1.5698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.265

[Epoch: 15, batch: 188/236] total loss per batch: 0.390
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5358e-03, 5.4729e-03, 1.4995e-03, 6.8128e-07, 2.5176e-03, 2.7517e-03,
        9.8522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 15, batch: 235/236] total loss per batch: 0.391
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([3.1868e-03, 6.5753e-03, 2.3586e-03, 2.1382e-03, 1.6680e-03, 9.8367e-01,
        4.0504e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.024

[Epoch: 16, batch: 47/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.6177e-06, 7.2956e-01, 2.5845e-01, 3.2390e-07, 1.1985e-02, 1.4846e-07,
        1.7331e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.161

[Epoch: 16, batch: 94/236] total loss per batch: 0.397
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.7229e-07, 4.1790e-07, 5.3899e-08, 7.2118e-08, 1.0000e+00, 6.3032e-09,
        9.5224e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.471

[Epoch: 16, batch: 141/236] total loss per batch: 0.407
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6876e-01, 3.4141e-01, 5.2975e-02, 6.8367e-07, 6.7998e-02, 4.1228e-07,
        1.6885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.296

[Epoch: 16, batch: 188/236] total loss per batch: 0.393
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.9536e-03, 2.9682e-03, 1.2139e-03, 8.6834e-07, 3.8806e-03, 3.5755e-03,
        9.8341e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 16, batch: 235/236] total loss per batch: 0.387
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0019, 0.0018, 0.0019, 0.0091, 0.0034, 0.9808, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 17, batch: 47/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.1271e-07, 9.9308e-01, 5.2249e-03, 1.3455e-07, 1.6956e-03, 4.1660e-08,
        1.7544e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.161

[Epoch: 17, batch: 94/236] total loss per batch: 0.390
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.5186e-07, 8.7123e-08, 4.8869e-08, 6.2084e-08, 1.0000e+00, 6.5639e-09,
        1.5556e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.479

[Epoch: 17, batch: 141/236] total loss per batch: 0.414
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7442e-01, 3.2429e-01, 5.5158e-02, 9.4137e-07, 1.0976e-01, 1.0044e-06,
        1.3636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.240

[Epoch: 17, batch: 188/236] total loss per batch: 0.394
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.7982e-03, 1.1032e-03, 2.5177e-03, 6.2817e-07, 1.5515e-03, 1.6890e-03,
        9.9134e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 17, batch: 235/236] total loss per batch: 0.386
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.4929e-03, 7.0514e-03, 3.2910e-03, 2.8206e-03, 4.8052e-03, 9.7901e-01,
        5.2945e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 18, batch: 47/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0840e-06, 9.6177e-01, 3.2540e-02, 3.0382e-07, 5.6908e-03, 1.2501e-07,
        1.4200e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.060

[Epoch: 18, batch: 94/236] total loss per batch: 0.388
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.5718e-07, 1.3573e-07, 3.1927e-08, 1.2141e-07, 1.0000e+00, 2.1341e-08,
        4.3373e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.468

[Epoch: 18, batch: 141/236] total loss per batch: 0.411
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6993e-01, 4.3560e-01, 4.9698e-02, 2.0919e-06, 7.7028e-02, 6.4228e-07,
        6.7743e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.271

[Epoch: 18, batch: 188/236] total loss per batch: 0.392
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([5.7390e-03, 3.1759e-03, 2.0719e-03, 7.1155e-07, 3.4507e-03, 2.4708e-03,
        9.8309e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.021

[Epoch: 18, batch: 235/236] total loss per batch: 0.385
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0030, 0.0011, 0.0040, 0.0020, 0.9869, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 19, batch: 47/236] total loss per batch: 0.388
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.2084e-06, 9.3060e-01, 5.8369e-02, 8.1759e-07, 1.1027e-02, 3.2883e-07,
        3.0829e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.023

[Epoch: 19, batch: 94/236] total loss per batch: 0.386
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.4824e-07, 7.5051e-08, 1.9501e-08, 2.5727e-08, 1.0000e+00, 7.4838e-09,
        2.4496e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 19, batch: 141/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4372e-01, 3.6622e-01, 4.0484e-02, 3.3140e-06, 1.0521e-01, 1.0295e-06,
        1.4435e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 19, batch: 188/236] total loss per batch: 0.388
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.8943e-03, 2.5050e-03, 1.8715e-03, 1.7573e-06, 3.6775e-03, 2.5059e-03,
        9.8554e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 19, batch: 235/236] total loss per batch: 0.382
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0024, 0.0034, 0.0014, 0.0033, 0.0042, 0.9841, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 20, batch: 47/236] total loss per batch: 0.386
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.4564e-06, 8.7063e-01, 1.2348e-01, 6.4895e-07, 5.8832e-03, 2.3977e-07,
        5.4443e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.058

[Epoch: 20, batch: 94/236] total loss per batch: 0.384
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2946e-06, 2.2048e-07, 5.5628e-08, 9.4696e-08, 1.0000e+00, 1.6062e-08,
        2.2263e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 20, batch: 141/236] total loss per batch: 0.399
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.9703e-01, 3.7581e-01, 5.0850e-02, 5.2033e-06, 5.7362e-02, 1.5967e-06,
        1.1894e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.266

[Epoch: 20, batch: 188/236] total loss per batch: 0.386
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.0681e-03, 2.5971e-03, 1.6611e-03, 2.5520e-06, 2.4624e-03, 1.5873e-03,
        9.8762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.014

[Epoch: 20, batch: 235/236] total loss per batch: 0.380
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0026, 0.0028, 0.0014, 0.0063, 0.0022, 0.9832, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 21, batch: 47/236] total loss per batch: 0.385
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.6686e-06, 9.4699e-01, 4.6003e-02, 4.5234e-07, 7.0054e-03, 1.6558e-07,
        9.0239e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.077

[Epoch: 21, batch: 94/236] total loss per batch: 0.384
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([8.7682e-07, 1.4474e-07, 4.1181e-08, 7.8016e-08, 1.0000e+00, 1.4047e-08,
        2.4583e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.524

[Epoch: 21, batch: 141/236] total loss per batch: 0.398
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5196e-01, 3.4876e-01, 3.1471e-02, 3.2491e-06, 1.0929e-01, 1.3437e-06,
        1.5851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.232

[Epoch: 21, batch: 188/236] total loss per batch: 0.384
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7037e-03, 2.7092e-03, 1.5489e-03, 1.9369e-06, 3.7727e-03, 1.8932e-03,
        9.8637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 21, batch: 235/236] total loss per batch: 0.387
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.8579e-03, 6.2885e-04, 1.6807e-03, 4.2048e-03, 4.4910e-03, 9.8646e-01,
        6.7541e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 22, batch: 47/236] total loss per batch: 0.402
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3604e-06, 9.3159e-01, 6.4306e-02, 5.0694e-07, 4.0979e-03, 2.2176e-07,
        1.4603e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.241

[Epoch: 22, batch: 94/236] total loss per batch: 0.408
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.0853e-06, 2.2703e-06, 8.3677e-08, 2.9764e-07, 9.9999e-01, 9.7072e-09,
        1.2939e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.409

[Epoch: 22, batch: 141/236] total loss per batch: 0.423
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.9841e-01, 4.2531e-01, 8.2087e-03, 3.0964e-06, 1.8198e-01, 2.1164e-06,
        8.6085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.224

[Epoch: 22, batch: 188/236] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6669e-03, 9.3648e-03, 2.4367e-03, 2.5002e-06, 1.6302e-03, 3.0201e-03,
        9.8088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.016

[Epoch: 22, batch: 235/236] total loss per batch: 0.449
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([6.1321e-03, 1.0018e-03, 1.1270e-03, 3.2273e-03, 5.0134e-03, 9.8320e-01,
        3.0261e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.041

[Epoch: 23, batch: 47/236] total loss per batch: 0.440
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0826e-06, 7.8889e-01, 1.9930e-01, 9.1220e-07, 1.1804e-02, 1.0887e-06,
        1.5126e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.226

[Epoch: 23, batch: 94/236] total loss per batch: 0.447
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.6640e-08, 3.6767e-08, 1.4192e-09, 5.0220e-08, 1.0000e+00, 7.6650e-09,
        2.6941e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 23, batch: 141/236] total loss per batch: 0.450
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([4.3611e-01, 1.8617e-01, 6.0554e-02, 2.1708e-06, 1.1798e-01, 3.6966e-06,
        1.9919e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.369

[Epoch: 23, batch: 188/236] total loss per batch: 0.430
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0146e-03, 4.4204e-03, 3.9088e-04, 1.4624e-07, 6.0675e-03, 3.4126e-04,
        9.8577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 23, batch: 235/236] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([4.7126e-03, 1.1157e-03, 9.9332e-04, 1.4030e-03, 5.5328e-03, 9.8581e-01,
        4.3340e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 24, batch: 47/236] total loss per batch: 0.413
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7878e-06, 9.4869e-01, 3.5049e-02, 5.2446e-07, 1.6259e-02, 4.9537e-07,
        2.5950e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.121

[Epoch: 24, batch: 94/236] total loss per batch: 0.405
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.9236e-08, 4.3959e-07, 1.6467e-08, 5.2270e-08, 1.0000e+00, 5.7663e-10,
        9.1053e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 24, batch: 141/236] total loss per batch: 0.419
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.0387e-01, 3.9444e-01, 7.4875e-02, 5.2403e-06, 1.1479e-01, 1.3279e-05,
        1.1201e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.289

[Epoch: 24, batch: 188/236] total loss per batch: 0.400
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.7581e-03, 2.0826e-03, 7.8977e-04, 4.0718e-07, 1.6627e-03, 8.1936e-04,
        9.9289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.018

[Epoch: 24, batch: 235/236] total loss per batch: 0.392
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0049, 0.0013, 0.0018, 0.0062, 0.0064, 0.9784, 0.0010],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.005

[Epoch: 25, batch: 47/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8918e-06, 9.2271e-01, 6.7611e-02, 3.3797e-07, 9.6798e-03, 3.2192e-07,
        8.6102e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.148

[Epoch: 25, batch: 94/236] total loss per batch: 0.390
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.8404e-08, 1.9913e-07, 5.9012e-09, 4.2279e-08, 1.0000e+00, 1.7050e-09,
        1.2316e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.550

[Epoch: 25, batch: 141/236] total loss per batch: 0.404
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2621e-01, 3.9781e-01, 6.1033e-02, 2.6887e-06, 7.7836e-02, 7.5509e-06,
        1.3710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 25, batch: 188/236] total loss per batch: 0.387
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.0798e-03, 1.8128e-03, 9.0877e-04, 5.8564e-07, 2.9077e-03, 1.6618e-03,
        9.9163e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.022

[Epoch: 25, batch: 235/236] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0039, 0.0012, 0.0016, 0.0050, 0.0061, 0.9806, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 26, batch: 47/236] total loss per batch: 0.385
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7545e-06, 9.1800e-01, 7.5398e-02, 2.9233e-07, 6.5953e-03, 3.4932e-07,
        1.2656e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.124

[Epoch: 26, batch: 94/236] total loss per batch: 0.383
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3223e-07, 3.8900e-07, 6.7261e-09, 5.9397e-08, 1.0000e+00, 1.4818e-09,
        9.4365e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.516

[Epoch: 26, batch: 141/236] total loss per batch: 0.397
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2639e-01, 4.2488e-01, 5.9108e-02, 2.7063e-06, 8.7486e-02, 7.3592e-06,
        1.0212e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.263

[Epoch: 26, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.2959e-03, 2.4739e-03, 1.1488e-03, 5.7476e-07, 2.6905e-03, 1.4097e-03,
        9.9098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 26, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0036, 0.0016, 0.0013, 0.0040, 0.0040, 0.9839, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 27, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4320e-06, 9.1945e-01, 7.3889e-02, 2.6281e-07, 6.6631e-03, 3.5587e-07,
        2.5497e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 27, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.5436e-07, 3.7547e-07, 4.6762e-09, 3.8846e-08, 1.0000e+00, 1.2927e-09,
        1.5813e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 27, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4718e-01, 3.6457e-01, 4.5039e-02, 2.8514e-06, 8.7742e-02, 6.6552e-06,
        1.5547e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.281

[Epoch: 27, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.5051e-03, 1.8564e-03, 8.4878e-04, 5.1581e-07, 2.4168e-03, 1.2201e-03,
        9.9215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.014

[Epoch: 27, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0031, 0.0011, 0.0015, 0.0045, 0.0036, 0.9848, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 28, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4497e-06, 9.0527e-01, 8.8856e-02, 2.0904e-07, 5.8728e-03, 3.2246e-07,
        2.6770e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.057

[Epoch: 28, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0743e-07, 2.7319e-07, 4.4067e-09, 3.4702e-08, 1.0000e+00, 1.3869e-09,
        1.0394e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.509

[Epoch: 28, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4640e-01, 3.9102e-01, 3.8252e-02, 2.4842e-06, 7.8529e-02, 6.1335e-06,
        1.4579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.226

[Epoch: 28, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.7499e-03, 2.3016e-03, 8.7163e-04, 5.0302e-07, 2.1991e-03, 1.3998e-03,
        9.9148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 28, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0028, 0.0013, 0.0018, 0.0032, 0.0031, 0.9863, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.003

[Epoch: 29, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2832e-06, 9.2579e-01, 6.9272e-02, 1.4166e-07, 4.9384e-03, 2.1174e-07,
        1.8294e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.152

[Epoch: 29, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.8448e-08, 1.9542e-07, 2.7517e-09, 1.9108e-08, 1.0000e+00, 6.1763e-10,
        1.3481e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 29, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6172e-01, 3.8946e-01, 4.4099e-02, 2.3828e-06, 8.0570e-02, 5.7534e-06,
        1.2415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 29, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.6243e-03, 1.6797e-03, 8.0337e-04, 3.7819e-07, 2.1400e-03, 9.8373e-04,
        9.9277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.011

[Epoch: 29, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0029, 0.0012, 0.0015, 0.0035, 0.0036, 0.9861, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 30, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1544e-06, 9.0178e-01, 9.2672e-02, 2.1812e-07, 5.5477e-03, 3.0601e-07,
        3.3201e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.003

[Epoch: 30, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3307e-07, 3.6139e-07, 5.0708e-09, 2.9693e-08, 1.0000e+00, 1.4139e-09,
        1.3585e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 30, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4239e-01, 3.7048e-01, 3.7743e-02, 2.2332e-06, 8.6902e-02, 4.7042e-06,
        1.6248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.268

[Epoch: 30, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.5485e-03, 1.9811e-03, 6.5755e-04, 3.9831e-07, 2.8592e-03, 1.3852e-03,
        9.9157e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.010

[Epoch: 30, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0024, 0.0019, 0.0018, 0.0026, 0.0029, 0.9870, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 31, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5567e-06, 9.1912e-01, 7.6964e-02, 2.5573e-07, 3.9088e-03, 2.6955e-07,
        1.8358e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.144

[Epoch: 31, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.5561e-08, 1.2259e-07, 1.9705e-09, 1.4261e-08, 1.0000e+00, 5.5845e-10,
        1.5280e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 31, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5828e-01, 4.0201e-01, 4.3087e-02, 2.2553e-06, 7.8711e-02, 4.9375e-06,
        1.1791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.241

[Epoch: 31, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3250e-03, 1.6495e-03, 1.0826e-03, 5.5786e-07, 2.0420e-03, 1.2061e-03,
        9.9169e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 31, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0033, 0.0011, 0.0014, 0.0038, 0.0034, 0.9858, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 32, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1548e-06, 9.2447e-01, 7.1531e-02, 2.0568e-07, 4.0013e-03, 3.2916e-07,
        1.7029e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.042

[Epoch: 32, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.5495e-07, 6.3729e-07, 9.0501e-09, 4.9803e-08, 1.0000e+00, 2.2291e-09,
        1.9990e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 32, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2730e-01, 3.7071e-01, 4.4400e-02, 3.6052e-06, 8.9598e-02, 7.1976e-06,
        1.6798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.245

[Epoch: 32, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2896e-03, 1.6532e-03, 8.5587e-04, 6.2916e-07, 4.2339e-03, 1.2995e-03,
        9.8967e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 32, batch: 235/236] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0024, 0.0016, 0.0021, 0.0021, 0.0025, 0.9882, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 33, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9749e-06, 8.9619e-01, 9.9886e-02, 4.1446e-07, 3.9256e-03, 5.5275e-07,
        5.2191e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.075

[Epoch: 33, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.2476e-08, 1.1620e-07, 2.7131e-09, 1.7328e-08, 1.0000e+00, 1.1456e-09,
        1.9481e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.513

[Epoch: 33, batch: 141/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6900e-01, 3.8471e-01, 3.4005e-02, 2.2761e-06, 8.2989e-02, 3.3472e-06,
        1.2929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.274

[Epoch: 33, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.1551e-03, 1.9360e-03, 8.5598e-04, 1.0244e-06, 2.1769e-03, 1.5524e-03,
        9.9132e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 33, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0034, 0.0013, 0.0013, 0.0047, 0.0032, 0.9842, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 34, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7353e-06, 9.3852e-01, 5.8435e-02, 2.7158e-07, 3.0400e-03, 3.6318e-07,
        1.1503e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 34, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.0921e-07, 1.0233e-06, 1.4018e-08, 9.6859e-08, 1.0000e+00, 6.1590e-09,
        1.7634e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.517

[Epoch: 34, batch: 141/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2299e-01, 4.1297e-01, 4.8725e-02, 6.5071e-06, 7.3306e-02, 9.5061e-06,
        1.4199e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.263

[Epoch: 34, batch: 188/236] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5838e-03, 1.6617e-03, 1.2610e-03, 1.1942e-06, 3.3874e-03, 1.1800e-03,
        9.8892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 34, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0020, 0.0027, 0.0022, 0.0031, 0.0028, 0.9851, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 35, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.1644e-06, 8.9872e-01, 9.6676e-02, 3.6307e-07, 4.6030e-03, 5.0681e-07,
        8.2617e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.096

[Epoch: 35, batch: 94/236] total loss per batch: 0.382
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4953e-07, 1.0354e-07, 5.6380e-09, 2.5792e-08, 1.0000e+00, 1.8983e-09,
        1.6676e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 35, batch: 141/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5570e-01, 3.9135e-01, 3.9576e-02, 3.3273e-06, 9.2098e-02, 6.2525e-06,
        1.2126e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 35, batch: 188/236] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3567e-03, 1.2841e-03, 1.2609e-03, 1.3906e-06, 2.3993e-03, 1.2346e-03,
        9.9146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 35, batch: 235/236] total loss per batch: 0.378
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0022, 0.0010, 0.0013, 0.0052, 0.0021, 0.9864, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 36, batch: 47/236] total loss per batch: 0.384
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.5840e-06, 9.3217e-01, 6.4564e-02, 4.1866e-07, 3.2602e-03, 4.7393e-07,
        3.7169e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.190

[Epoch: 36, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.7345e-07, 9.4453e-07, 1.2854e-08, 6.8373e-08, 1.0000e+00, 9.8106e-09,
        4.3185e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.527

[Epoch: 36, batch: 141/236] total loss per batch: 0.397
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4976e-01, 3.3083e-01, 4.8544e-02, 4.9730e-06, 8.3023e-02, 7.4337e-06,
        1.8784e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.244

[Epoch: 36, batch: 188/236] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.7731e-03, 1.9117e-03, 1.2985e-03, 1.6507e-06, 3.3827e-03, 1.1419e-03,
        9.8749e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 36, batch: 235/236] total loss per batch: 0.378
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0026, 0.0036, 0.0020, 0.0024, 0.0015, 0.9863, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.009

[Epoch: 37, batch: 47/236] total loss per batch: 0.385
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.6454e-06, 9.0741e-01, 8.8860e-02, 5.0696e-07, 3.7277e-03, 4.5165e-07,
        7.4970e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.076

[Epoch: 37, batch: 94/236] total loss per batch: 0.382
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.5793e-07, 5.0549e-07, 1.3760e-08, 1.0304e-07, 1.0000e+00, 8.1105e-09,
        8.6946e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.482

[Epoch: 37, batch: 141/236] total loss per batch: 0.398
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2764e-01, 4.6629e-01, 3.0950e-02, 5.0017e-06, 7.2181e-02, 3.3586e-06,
        1.0293e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.272

[Epoch: 37, batch: 188/236] total loss per batch: 0.384
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.9538e-03, 1.6600e-03, 1.8691e-03, 2.7254e-06, 2.0794e-03, 1.7451e-03,
        9.9069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 37, batch: 235/236] total loss per batch: 0.380
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0045, 0.0033, 0.0021, 0.0050, 0.0033, 0.9802, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 38, batch: 47/236] total loss per batch: 0.385
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9050e-06, 9.3975e-01, 5.5858e-02, 3.3455e-07, 4.3858e-03, 5.1917e-07,
        4.4212e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.124

[Epoch: 38, batch: 94/236] total loss per batch: 0.387
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.0813e-07, 1.0841e-06, 3.1017e-08, 1.4270e-07, 1.0000e+00, 7.2731e-08,
        4.8686e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 38, batch: 141/236] total loss per batch: 0.399
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3191e-01, 3.3924e-01, 5.0660e-02, 4.3470e-06, 1.0759e-01, 2.7662e-06,
        1.7059e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.201

[Epoch: 38, batch: 188/236] total loss per batch: 0.387
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0317e-03, 1.9271e-03, 1.2423e-03, 1.3248e-06, 3.2176e-03, 1.2149e-03,
        9.8937e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 38, batch: 235/236] total loss per batch: 0.380
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.3362e-03, 1.4133e-03, 1.1355e-03, 3.8005e-03, 1.0163e-03, 9.8937e-01,
        9.3267e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 39, batch: 47/236] total loss per batch: 0.387
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.2704e-06, 8.8178e-01, 1.1042e-01, 6.2779e-07, 7.7928e-03, 2.7459e-06,
        6.6541e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 39, batch: 94/236] total loss per batch: 0.390
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.8833e-07, 4.4775e-07, 1.0260e-08, 6.0115e-08, 1.0000e+00, 1.4786e-08,
        4.5604e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 39, batch: 141/236] total loss per batch: 0.413
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.1944e-01, 3.3045e-01, 5.8952e-02, 3.1137e-06, 7.8029e-02, 1.4730e-06,
        2.1313e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.294

[Epoch: 39, batch: 188/236] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.1288e-03, 5.7124e-03, 1.2015e-03, 7.8530e-07, 4.0959e-03, 8.9313e-04,
        9.8697e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 39, batch: 235/236] total loss per batch: 0.430
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.5070e-02, 1.7004e-03, 2.3350e-04, 5.1810e-03, 9.2563e-03, 9.6687e-01,
        1.6917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 40, batch: 47/236] total loss per batch: 0.432
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0734e-06, 8.7724e-01, 1.1869e-01, 9.1786e-07, 4.0587e-03, 4.7129e-06,
        3.9777e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.119

[Epoch: 40, batch: 94/236] total loss per batch: 0.425
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.6829e-07, 3.0894e-06, 1.6851e-07, 3.8921e-09, 1.0000e+00, 7.9225e-08,
        3.6965e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.481

[Epoch: 40, batch: 141/236] total loss per batch: 0.457
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.9688e-01, 3.8984e-01, 6.7855e-02, 3.9175e-06, 1.2185e-01, 7.1420e-06,
        1.2357e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.341

[Epoch: 40, batch: 188/236] total loss per batch: 0.446
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([8.9256e-03, 6.0564e-03, 1.5066e-03, 4.6990e-06, 2.9801e-02, 2.1716e-02,
        9.3199e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.020

[Epoch: 40, batch: 235/236] total loss per batch: 0.432
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0059, 0.0023, 0.0017, 0.0221, 0.0199, 0.9422, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.050

[Epoch: 41, batch: 47/236] total loss per batch: 0.419
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.1491e-06, 8.9346e-01, 1.0443e-01, 8.8821e-07, 2.1055e-03, 3.4681e-07,
        1.3414e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.289

[Epoch: 41, batch: 94/236] total loss per batch: 0.408
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.5112e-07, 7.9678e-08, 6.3382e-08, 1.1315e-08, 1.0000e+00, 1.8627e-09,
        2.0725e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.440

[Epoch: 41, batch: 141/236] total loss per batch: 0.426
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([4.2199e-01, 3.0222e-01, 3.3981e-02, 2.0965e-06, 1.0636e-01, 1.9877e-06,
        1.3544e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.314

[Epoch: 41, batch: 188/236] total loss per batch: 0.403
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.6914e-03, 5.2313e-03, 2.6436e-03, 4.3024e-06, 1.6534e-02, 1.0670e-02,
        9.5823e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 41, batch: 235/236] total loss per batch: 0.391
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0031, 0.0111, 0.0027, 0.0051, 0.0023, 0.9738, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.043

[Epoch: 42, batch: 47/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.1071e-06, 9.2249e-01, 7.6119e-02, 7.5467e-07, 1.3863e-03, 2.7936e-07,
        3.4225e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.054

[Epoch: 42, batch: 94/236] total loss per batch: 0.388
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.8129e-07, 1.8613e-07, 5.5011e-07, 6.3394e-08, 1.0000e+00, 4.5021e-09,
        1.0685e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.484

[Epoch: 42, batch: 141/236] total loss per batch: 0.402
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([4.6317e-01, 3.3000e-01, 3.1464e-02, 2.8323e-06, 7.9645e-02, 3.1490e-06,
        9.5715e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.329

[Epoch: 42, batch: 188/236] total loss per batch: 0.390
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([7.2480e-03, 4.5158e-03, 1.9273e-03, 7.7234e-06, 7.8145e-03, 5.2709e-03,
        9.7322e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 42, batch: 235/236] total loss per batch: 0.380
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0029, 0.0055, 0.0021, 0.0025, 0.0020, 0.9826, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.039

[Epoch: 43, batch: 47/236] total loss per batch: 0.384
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.6971e-06, 9.2649e-01, 7.1537e-02, 1.0618e-06, 1.9643e-03, 3.3691e-07,
        1.1722e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.121

[Epoch: 43, batch: 94/236] total loss per batch: 0.382
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.3985e-07, 1.3764e-07, 5.3189e-07, 2.1177e-08, 1.0000e+00, 2.1757e-09,
        5.6156e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.488

[Epoch: 43, batch: 141/236] total loss per batch: 0.396
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.8427e-01, 3.7146e-01, 3.0172e-02, 3.6461e-06, 8.5334e-02, 2.7916e-06,
        1.2875e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.295

[Epoch: 43, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.5468e-03, 4.4524e-03, 1.7026e-03, 5.0898e-06, 7.6766e-03, 2.8097e-03,
        9.7881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 43, batch: 235/236] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0021, 0.0036, 0.0017, 0.0024, 0.0016, 0.9863, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.036

[Epoch: 44, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.1817e-06, 9.2021e-01, 7.7980e-02, 7.4518e-07, 1.8080e-03, 3.0875e-07,
        1.2657e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.105

[Epoch: 44, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.1657e-07, 9.0277e-08, 3.7629e-07, 1.4625e-08, 1.0000e+00, 1.8819e-09,
        9.4671e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.483

[Epoch: 44, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6057e-01, 3.8871e-01, 3.6491e-02, 2.9631e-06, 8.3186e-02, 2.1824e-06,
        1.3104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.298

[Epoch: 44, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7813e-03, 3.4455e-03, 1.4761e-03, 4.2222e-06, 6.3611e-03, 2.3376e-03,
        9.8259e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 44, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0019, 0.0033, 0.0018, 0.0025, 0.0024, 0.9858, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.037

[Epoch: 45, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.8074e-06, 9.1466e-01, 8.3197e-02, 5.5833e-07, 2.1386e-03, 2.5348e-07,
        1.0694e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.067

[Epoch: 45, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.6536e-07, 7.0713e-08, 2.8320e-07, 1.1285e-08, 1.0000e+00, 1.4409e-09,
        3.0727e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.486

[Epoch: 45, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5506e-01, 3.8319e-01, 4.1911e-02, 2.6148e-06, 7.9758e-02, 1.9783e-06,
        1.4008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.279

[Epoch: 45, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1890e-03, 2.8373e-03, 1.9032e-03, 2.6437e-06, 6.3333e-03, 1.9382e-03,
        9.8380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 45, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0026, 0.0017, 0.0019, 0.0023, 0.9876, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.038

[Epoch: 46, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.3331e-06, 9.2282e-01, 7.5571e-02, 5.2411e-07, 1.6016e-03, 2.6516e-07,
        1.2061e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 46, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8898e-07, 5.7707e-08, 2.2834e-07, 8.7281e-09, 1.0000e+00, 1.4027e-09,
        5.4922e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.487

[Epoch: 46, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5676e-01, 3.9410e-01, 4.1316e-02, 2.1766e-06, 8.4005e-02, 1.6432e-06,
        1.2381e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 46, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2964e-03, 2.6214e-03, 1.3852e-03, 2.3145e-06, 6.2633e-03, 2.0161e-03,
        9.8442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 46, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0019, 0.0030, 0.0019, 0.0023, 0.0022, 0.9863, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.032

[Epoch: 47, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.5508e-06, 9.0255e-01, 9.5271e-02, 4.1891e-07, 2.1777e-03, 3.0364e-07,
        8.7862e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.111

[Epoch: 47, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1072e-07, 6.9576e-08, 2.2446e-07, 8.6572e-09, 1.0000e+00, 1.6174e-09,
        3.0208e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 47, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4976e-01, 3.8141e-01, 3.5904e-02, 2.0275e-06, 7.8508e-02, 1.9309e-06,
        1.5442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.262

[Epoch: 47, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1428e-03, 2.5030e-03, 1.8928e-03, 2.4214e-06, 4.7896e-03, 1.9949e-03,
        9.8567e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 47, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0019, 0.0016, 0.0018, 0.0021, 0.9885, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.038

[Epoch: 48, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7422e-06, 9.3609e-01, 6.2340e-02, 3.8105e-07, 1.5690e-03, 1.7429e-07,
        1.1429e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.052

[Epoch: 48, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1665e-07, 6.7279e-08, 2.1068e-07, 9.1972e-09, 1.0000e+00, 1.6325e-09,
        4.1257e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 48, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6888e-01, 3.7069e-01, 4.5374e-02, 2.3165e-06, 8.1209e-02, 1.9672e-06,
        1.3385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.243

[Epoch: 48, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.8653e-03, 2.4063e-03, 1.3591e-03, 2.0402e-06, 4.9603e-03, 1.5376e-03,
        9.8587e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 48, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0024, 0.0016, 0.0020, 0.0027, 0.9872, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 49, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.0646e-06, 8.7876e-01, 1.1841e-01, 4.0971e-07, 2.8279e-03, 3.3847e-07,
        9.9676e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.131

[Epoch: 49, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8394e-07, 6.1448e-08, 2.3848e-07, 1.0168e-08, 1.0000e+00, 1.8778e-09,
        4.5746e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.493

[Epoch: 49, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3662e-01, 4.0703e-01, 3.1704e-02, 2.4661e-06, 8.4061e-02, 2.9535e-06,
        1.4057e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.272

[Epoch: 49, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.4289e-03, 2.4649e-03, 1.7800e-03, 2.0392e-06, 4.8326e-03, 2.3060e-03,
        9.8619e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 49, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0021, 0.0017, 0.0016, 0.0021, 0.0019, 0.9875, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.038

[Epoch: 50, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.0713e-06, 9.2655e-01, 7.1177e-02, 6.5666e-07, 2.2734e-03, 2.7976e-07,
        5.9553e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.028

[Epoch: 50, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.0438e-07, 1.3711e-07, 3.0236e-07, 1.8669e-08, 1.0000e+00, 1.9518e-09,
        4.8820e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 50, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7062e-01, 3.6735e-01, 4.7437e-02, 2.1959e-06, 7.1638e-02, 2.1290e-06,
        1.4296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.237

[Epoch: 50, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9431e-03, 2.2631e-03, 1.3413e-03, 2.3500e-06, 4.5155e-03, 1.5187e-03,
        9.8742e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 50, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0020, 0.0014, 0.0019, 0.0022, 0.9890, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.030

[Epoch: 51, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.6555e-06, 9.1552e-01, 8.1946e-02, 5.2375e-07, 2.5325e-03, 3.4770e-07,
        1.1677e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.141

[Epoch: 51, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.1097e-07, 1.6094e-07, 4.4390e-07, 2.5375e-08, 1.0000e+00, 3.6882e-09,
        3.9483e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.493

[Epoch: 51, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4301e-01, 4.1149e-01, 3.3845e-02, 3.3948e-06, 8.8306e-02, 3.9726e-06,
        1.2334e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.307

[Epoch: 51, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5917e-03, 1.8635e-03, 1.6489e-03, 2.2529e-06, 4.7762e-03, 1.7512e-03,
        9.8737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 51, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0021, 0.0016, 0.0016, 0.0020, 0.0018, 0.9880, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.038

[Epoch: 52, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.2867e-06, 9.0055e-01, 9.7069e-02, 9.1992e-07, 2.3744e-03, 3.7317e-07,
        1.0780e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.003

[Epoch: 52, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.0823e-07, 1.0495e-07, 2.1775e-07, 1.2723e-08, 1.0000e+00, 1.3612e-09,
        2.9389e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 52, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5114e-01, 3.7166e-01, 3.9403e-02, 3.4499e-06, 8.8803e-02, 3.3632e-06,
        1.4898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 52, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3847e-03, 1.6756e-03, 1.3328e-03, 2.1770e-06, 3.3093e-03, 1.5361e-03,
        9.8976e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 52, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0022, 0.0018, 0.0016, 0.0028, 0.0021, 0.9871, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 53, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.3667e-06, 9.3480e-01, 6.2837e-02, 6.5345e-07, 2.3583e-03, 3.3082e-07,
        1.6955e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.174

[Epoch: 53, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.9190e-07, 1.2294e-07, 2.8846e-07, 1.9440e-08, 1.0000e+00, 2.9421e-09,
        4.1641e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 53, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3043e-01, 4.1081e-01, 3.6566e-02, 2.4245e-06, 7.6103e-02, 2.3638e-06,
        1.4608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.263

[Epoch: 53, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9232e-03, 1.6638e-03, 1.5114e-03, 1.8913e-06, 3.9142e-03, 1.5636e-03,
        9.8842e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 53, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0015, 0.0012, 0.0015, 0.0015, 0.9907, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.037

[Epoch: 54, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.8377e-06, 9.0071e-01, 9.6388e-02, 6.9967e-07, 2.8948e-03, 4.6407e-07,
        1.2817e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.035

[Epoch: 54, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.2847e-07, 9.8367e-08, 2.6169e-07, 1.0768e-08, 1.0000e+00, 1.5280e-09,
        4.4424e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 54, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5879e-01, 3.8287e-01, 3.8139e-02, 3.5010e-06, 7.9974e-02, 3.1929e-06,
        1.4022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.255

[Epoch: 54, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7895e-03, 1.5363e-03, 1.4763e-03, 1.5377e-06, 3.2081e-03, 1.4946e-03,
        9.8949e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 54, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0021, 0.0016, 0.0019, 0.0017, 0.9888, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.032

[Epoch: 55, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.4236e-06, 9.0969e-01, 8.7865e-02, 6.0137e-07, 2.4422e-03, 6.7427e-07,
        7.8089e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.070

[Epoch: 55, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.2108e-07, 1.1287e-07, 2.7643e-07, 1.7295e-08, 1.0000e+00, 2.8668e-09,
        7.4728e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.486

[Epoch: 55, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4673e-01, 3.8537e-01, 4.7735e-02, 2.1306e-06, 8.6370e-02, 2.0257e-06,
        1.3379e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 55, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.1054e-03, 1.3753e-03, 1.4282e-03, 2.1423e-06, 3.5112e-03, 1.4730e-03,
        9.9010e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 55, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0016, 0.0014, 0.0020, 0.0018, 0.9893, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.029

[Epoch: 56, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.3016e-06, 9.4582e-01, 5.2324e-02, 6.3350e-07, 1.8526e-03, 3.5018e-07,
        1.1515e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.128

[Epoch: 56, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.6465e-07, 1.3126e-07, 3.8761e-07, 1.4991e-08, 1.0000e+00, 2.4026e-09,
        7.1441e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.501

[Epoch: 56, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5820e-01, 3.6712e-01, 3.3006e-02, 3.3368e-06, 8.0278e-02, 2.8768e-06,
        1.6140e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.261

[Epoch: 56, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2859e-03, 1.8546e-03, 1.4220e-03, 1.9978e-06, 3.8152e-03, 1.4871e-03,
        9.8913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 56, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0018, 0.0018, 0.0015, 0.0024, 0.9890, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.029

[Epoch: 57, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.1043e-06, 8.5775e-01, 1.3833e-01, 7.4626e-07, 3.9138e-03, 8.5471e-07,
        9.6966e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.066

[Epoch: 57, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.0198e-07, 1.8869e-07, 4.6419e-07, 2.1901e-08, 1.0000e+00, 9.8619e-09,
        4.4987e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 57, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6199e-01, 3.7021e-01, 5.1795e-02, 5.0041e-06, 9.9111e-02, 4.6377e-06,
        1.1688e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.231

[Epoch: 57, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0080e-03, 1.2928e-03, 1.7895e-03, 3.8616e-06, 2.6463e-03, 2.9162e-03,
        9.8834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 57, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0023, 0.0011, 0.0022, 0.0011, 0.9899, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.041

[Epoch: 58, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.4568e-06, 9.3280e-01, 6.5146e-02, 9.1055e-07, 2.0516e-03, 5.6061e-07,
        1.5581e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 58, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3062e-06, 2.5649e-07, 1.1077e-06, 3.7696e-08, 1.0000e+00, 7.1151e-09,
        1.7814e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 58, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3958e-01, 4.0344e-01, 3.4552e-02, 4.2046e-06, 5.8237e-02, 2.8215e-06,
        1.6419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.301

[Epoch: 58, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5038e-03, 1.5279e-03, 1.2786e-03, 2.0003e-06, 3.3341e-03, 1.0680e-03,
        9.9029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 58, batch: 235/236] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0015, 0.0019, 0.0022, 0.0026, 0.9874, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 59, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.5935e-06, 9.2612e-01, 7.1338e-02, 9.7700e-07, 2.5315e-03, 5.6484e-07,
        1.0161e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 59, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.9502e-07, 1.2131e-07, 3.0534e-07, 2.4887e-08, 1.0000e+00, 5.8366e-09,
        2.4488e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.510

[Epoch: 59, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6973e-01, 3.4685e-01, 4.3350e-02, 7.3776e-06, 1.0099e-01, 5.2526e-06,
        1.3906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.253

[Epoch: 59, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2422e-03, 1.7867e-03, 1.5290e-03, 2.3691e-06, 3.2868e-03, 3.2928e-03,
        9.8686e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 59, batch: 235/236] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0030, 0.0018, 0.0015, 0.0019, 0.9886, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 60, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.8806e-06, 9.0064e-01, 9.7500e-02, 1.2169e-06, 1.8473e-03, 5.1177e-07,
        2.4947e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.148

[Epoch: 60, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([8.4605e-07, 2.1710e-07, 4.2939e-07, 1.9779e-08, 1.0000e+00, 6.0444e-09,
        5.9809e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.482

[Epoch: 60, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4129e-01, 3.7107e-01, 5.0160e-02, 5.6366e-06, 7.1610e-02, 4.3888e-06,
        1.6586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.296

[Epoch: 60, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3867e-03, 1.9718e-03, 1.4212e-03, 2.5830e-06, 2.3680e-03, 9.5811e-04,
        9.8989e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 60, batch: 235/236] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0016, 0.0014, 0.0025, 0.0027, 0.9863, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.029

[Epoch: 61, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.1096e-06, 9.1228e-01, 8.5136e-02, 1.3558e-06, 2.5731e-03, 8.6787e-07,
        2.0682e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.003

[Epoch: 61, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.0633e-06, 5.5832e-07, 1.2533e-06, 3.0544e-08, 1.0000e+00, 1.8688e-08,
        1.2307e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.519

[Epoch: 61, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7024e-01, 3.8882e-01, 3.9594e-02, 4.8489e-06, 8.4030e-02, 2.8186e-06,
        1.1731e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 61, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.5648e-03, 2.1467e-03, 1.7082e-03, 3.4011e-06, 3.6333e-03, 2.7861e-03,
        9.8816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 61, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0018, 0.0020, 0.0019, 0.0012, 0.9901, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 62, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.5767e-06, 9.2570e-01, 7.2940e-02, 4.8720e-07, 1.3605e-03, 4.2565e-07,
        4.9588e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.152

[Epoch: 62, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.1800e-07, 1.8707e-07, 4.0893e-07, 1.3851e-08, 1.0000e+00, 8.7518e-09,
        8.8055e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 62, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.0721e-01, 4.1237e-01, 4.9972e-02, 8.2286e-06, 8.8946e-02, 6.5985e-06,
        1.4149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.224

[Epoch: 62, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3799e-03, 2.8559e-03, 1.1083e-03, 5.6101e-06, 3.8478e-03, 2.0092e-03,
        9.8679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 62, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0022, 0.0019, 0.0012, 0.0038, 0.0019, 0.9850, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 63, batch: 47/236] total loss per batch: 0.384
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.4990e-06, 8.8617e-01, 1.0929e-01, 6.9027e-07, 4.5387e-03, 9.3301e-07,
        1.4294e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.003

[Epoch: 63, batch: 94/236] total loss per batch: 0.386
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([8.3995e-07, 5.0119e-07, 9.4498e-07, 1.6086e-08, 1.0000e+00, 7.9129e-09,
        1.6848e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 63, batch: 141/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5478e-01, 4.0275e-01, 3.1990e-02, 2.2187e-06, 9.2487e-02, 3.1833e-06,
        1.1799e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.234

[Epoch: 63, batch: 188/236] total loss per batch: 0.395
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.0629e-03, 5.1854e-03, 1.4725e-03, 3.9269e-06, 2.3512e-03, 1.5651e-03,
        9.8836e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 63, batch: 235/236] total loss per batch: 0.383
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([6.7236e-04, 6.7739e-04, 1.0376e-03, 2.1472e-03, 2.3218e-03, 9.9177e-01,
        1.3689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 64, batch: 47/236] total loss per batch: 0.388
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.3364e-06, 9.4461e-01, 5.2112e-02, 1.6840e-06, 3.2725e-03, 6.0023e-07,
        4.2852e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.068

[Epoch: 64, batch: 94/236] total loss per batch: 0.392
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.4248e-06, 1.8469e-07, 8.3536e-07, 6.0471e-08, 1.0000e+00, 4.3836e-08,
        3.6524e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.517

[Epoch: 64, batch: 141/236] total loss per batch: 0.405
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.0886e-01, 4.2076e-01, 2.3835e-02, 1.6796e-06, 9.1558e-02, 8.4040e-06,
        1.5498e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.239

[Epoch: 64, batch: 188/236] total loss per batch: 0.387
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.3189e-03, 3.4989e-03, 1.1623e-03, 4.0755e-06, 3.5811e-03, 2.5025e-03,
        9.8793e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 64, batch: 235/236] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.6452e-04, 7.6371e-04, 1.6055e-03, 2.0226e-03, 1.9766e-03, 9.9155e-01,
        1.1163e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 65, batch: 47/236] total loss per batch: 0.383
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.3564e-06, 9.2072e-01, 7.6660e-02, 6.1546e-07, 2.6119e-03, 3.8275e-07,
        5.1256e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.140

[Epoch: 65, batch: 94/236] total loss per batch: 0.382
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4414e-06, 2.6408e-07, 8.4692e-07, 3.1425e-08, 1.0000e+00, 1.1833e-08,
        2.1203e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.507

[Epoch: 65, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4721e-01, 3.5950e-01, 4.6389e-02, 3.6036e-06, 1.1049e-01, 7.7782e-06,
        1.3641e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.286

[Epoch: 65, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2203e-03, 2.7933e-03, 1.3355e-03, 2.7368e-06, 3.4042e-03, 1.9248e-03,
        9.8832e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 65, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.8488e-03, 9.4077e-04, 2.4614e-03, 3.8071e-03, 2.2456e-03, 9.8598e-01,
        2.7195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.027

[Epoch: 66, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.4773e-06, 9.1828e-01, 7.8257e-02, 4.8925e-07, 3.4610e-03, 4.4365e-07,
        2.7677e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.039

[Epoch: 66, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0899e-06, 2.0678e-07, 1.1523e-06, 4.3524e-08, 1.0000e+00, 1.7790e-08,
        4.7334e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.503

[Epoch: 66, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5387e-01, 4.0054e-01, 3.8057e-02, 2.8210e-06, 7.1037e-02, 5.1237e-06,
        1.3649e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.280

[Epoch: 66, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2667e-03, 3.3487e-03, 1.1650e-03, 1.7300e-06, 3.7064e-03, 1.6603e-03,
        9.8685e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 66, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.6141e-03, 1.4171e-03, 2.2715e-03, 3.7490e-03, 2.4409e-03, 9.8773e-01,
        7.8114e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 67, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.0784e-06, 9.1707e-01, 8.1322e-02, 3.9272e-07, 1.6082e-03, 2.6296e-07,
        1.5439e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.096

[Epoch: 67, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0263e-06, 2.0736e-07, 9.4059e-07, 2.9607e-08, 1.0000e+00, 1.4573e-08,
        1.0970e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.487

[Epoch: 67, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4937e-01, 3.7670e-01, 4.5139e-02, 2.3118e-06, 7.9179e-02, 4.2788e-06,
        1.4961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 67, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3900e-03, 3.4441e-03, 1.3379e-03, 2.7841e-06, 4.2408e-03, 1.6981e-03,
        9.8589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 67, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0011, 0.0013, 0.0027, 0.0023, 0.9898, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 68, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.2327e-06, 9.2820e-01, 6.9778e-02, 3.7735e-07, 2.0167e-03, 2.4890e-07,
        1.4832e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.094

[Epoch: 68, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.9048e-07, 2.0633e-07, 8.6937e-07, 2.5647e-08, 1.0000e+00, 1.1920e-08,
        1.1064e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.480

[Epoch: 68, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5129e-01, 3.8073e-01, 3.8201e-02, 2.1487e-06, 8.5957e-02, 3.5755e-06,
        1.4382e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 68, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2400e-03, 2.5502e-03, 1.3658e-03, 2.4614e-06, 3.8316e-03, 1.5124e-03,
        9.8750e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 68, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0015, 0.0019, 0.0029, 0.0024, 0.9882, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 69, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.0847e-06, 9.0425e-01, 9.3386e-02, 2.9907e-07, 2.3614e-03, 2.9672e-07,
        1.3889e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.101

[Epoch: 69, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.7843e-07, 1.8541e-07, 5.4601e-07, 1.8806e-08, 1.0000e+00, 8.8797e-09,
        5.3275e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.489

[Epoch: 69, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5577e-01, 3.9050e-01, 3.9127e-02, 1.7851e-06, 7.3870e-02, 3.5188e-06,
        1.4072e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 69, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0094e-03, 2.5340e-03, 1.3714e-03, 1.7713e-06, 3.4877e-03, 1.7084e-03,
        9.8789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 69, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0010, 0.0013, 0.0024, 0.0018, 0.9906, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 70, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.9842e-06, 9.2722e-01, 7.0753e-02, 3.2933e-07, 2.0217e-03, 2.0547e-07,
        1.3708e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 70, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.3924e-07, 1.2425e-07, 6.7694e-07, 1.5979e-08, 1.0000e+00, 8.7955e-09,
        6.2865e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 70, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5336e-01, 3.8345e-01, 3.5946e-02, 1.5614e-06, 8.7892e-02, 2.9686e-06,
        1.3935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.260

[Epoch: 70, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7445e-03, 2.4279e-03, 1.4650e-03, 1.7000e-06, 4.2254e-03, 1.4640e-03,
        9.8667e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 70, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0013, 0.0020, 0.0026, 0.0019, 0.9892, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 71, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.8193e-06, 9.0182e-01, 9.5863e-02, 2.5973e-07, 2.3120e-03, 2.3286e-07,
        1.5465e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.098

[Epoch: 71, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([8.3020e-07, 1.8744e-07, 6.9146e-07, 2.4438e-08, 1.0000e+00, 1.2011e-08,
        6.9385e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.488

[Epoch: 71, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5816e-01, 3.8136e-01, 4.3691e-02, 2.0054e-06, 7.4278e-02, 3.3453e-06,
        1.4251e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.253

[Epoch: 71, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2428e-03, 2.0326e-03, 1.1970e-03, 1.3958e-06, 3.3189e-03, 1.4392e-03,
        9.8877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 71, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0012, 0.0015, 0.0018, 0.0017, 0.9910, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 72, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.8353e-06, 9.3905e-01, 5.9043e-02, 3.0438e-07, 1.9053e-03, 2.4579e-07,
        6.7497e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.082

[Epoch: 72, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.3595e-07, 1.1797e-07, 4.1654e-07, 1.0055e-08, 1.0000e+00, 5.1501e-09,
        1.8297e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 72, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4506e-01, 3.8874e-01, 3.5868e-02, 1.2360e-06, 8.7625e-02, 2.6397e-06,
        1.4270e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.243

[Epoch: 72, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5162e-03, 2.0987e-03, 1.8119e-03, 1.6565e-06, 4.0931e-03, 1.7933e-03,
        9.8669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 72, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0017, 0.0019, 0.0032, 0.0022, 0.9876, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 73, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.2531e-06, 8.8644e-01, 1.1116e-01, 2.8220e-07, 2.4014e-03, 2.1526e-07,
        2.3546e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.084

[Epoch: 73, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.8035e-07, 2.0263e-07, 4.4668e-07, 2.5705e-08, 1.0000e+00, 1.7570e-08,
        1.0762e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 73, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6012e-01, 3.9195e-01, 3.9152e-02, 1.6856e-06, 7.6604e-02, 3.6482e-06,
        1.3217e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.271

[Epoch: 73, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4208e-03, 2.4590e-03, 1.5104e-03, 1.6228e-06, 3.6812e-03, 1.7259e-03,
        9.8720e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 73, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0013, 0.0018, 0.0019, 0.0018, 0.9902, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 74, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.3409e-06, 9.3153e-01, 6.6238e-02, 4.9337e-07, 2.2220e-03, 4.5558e-07,
        9.7736e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.119

[Epoch: 74, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1881e-06, 3.1835e-07, 1.6029e-06, 2.9004e-08, 1.0000e+00, 1.7684e-08,
        5.9425e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 74, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7467e-01, 3.7723e-01, 3.6170e-02, 1.7100e-06, 7.3055e-02, 4.7553e-06,
        1.3886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.262

[Epoch: 74, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9382e-03, 3.0173e-03, 1.3007e-03, 1.5517e-06, 2.6018e-03, 2.2507e-03,
        9.8789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 74, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0026, 0.0015, 0.0013, 0.0026, 0.0016, 0.9884, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 75, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.6170e-06, 8.9506e-01, 1.0231e-01, 4.6512e-07, 2.6328e-03, 4.4302e-07,
        2.5423e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.046

[Epoch: 75, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2485e-06, 2.2696e-07, 8.2375e-07, 4.3371e-08, 1.0000e+00, 1.3803e-08,
        1.5701e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.511

[Epoch: 75, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.8055e-01, 4.1988e-01, 4.8711e-02, 2.7952e-06, 1.0727e-01, 5.1425e-06,
        1.4359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.287

[Epoch: 75, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.0707e-03, 1.2654e-03, 9.8503e-04, 7.5908e-07, 3.1973e-03, 8.9860e-04,
        9.9158e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 75, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0014, 0.0015, 0.0014, 0.0021, 0.9905, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 76, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.0614e-06, 9.1668e-01, 8.1103e-02, 6.2783e-07, 2.2109e-03, 5.2587e-07,
        3.3958e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.122

[Epoch: 76, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.8441e-07, 1.5375e-07, 5.4598e-07, 4.2963e-08, 1.0000e+00, 1.1781e-08,
        5.8034e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 76, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([4.1206e-01, 3.2230e-01, 4.2127e-02, 3.5506e-06, 8.5665e-02, 6.2363e-06,
        1.3785e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.261

[Epoch: 76, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.2564e-03, 3.8986e-03, 1.1545e-03, 2.5714e-06, 3.7038e-03, 2.3514e-03,
        9.8263e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.021

[Epoch: 76, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.7229e-03, 9.3815e-04, 1.0307e-03, 2.0498e-03, 1.6398e-03, 9.9131e-01,
        1.3135e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 77, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.3988e-06, 9.0912e-01, 8.7615e-02, 6.4899e-07, 3.2629e-03, 6.9342e-07,
        6.6202e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.048

[Epoch: 77, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1586e-06, 4.4167e-07, 1.5174e-06, 7.7427e-08, 1.0000e+00, 2.5398e-08,
        3.1362e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 77, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5740e-01, 3.6554e-01, 3.2103e-02, 7.2254e-06, 8.4949e-02, 1.6139e-05,
        1.5998e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.234

[Epoch: 77, batch: 188/236] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.7143e-03, 1.7674e-03, 1.2526e-03, 7.2015e-07, 3.3080e-03, 1.2121e-03,
        9.8774e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 77, batch: 235/236] total loss per batch: 0.378
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0010, 0.0010, 0.0014, 0.0014, 0.0043, 0.9886, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 78, batch: 47/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.3986e-06, 9.2338e-01, 7.5191e-02, 3.0179e-07, 1.4276e-03, 7.5049e-07,
        6.1185e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.211

[Epoch: 78, batch: 94/236] total loss per batch: 0.390
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6183e-07, 1.2641e-07, 4.6501e-07, 2.8020e-08, 1.0000e+00, 9.3321e-09,
        6.7146e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.515

[Epoch: 78, batch: 141/236] total loss per batch: 0.401
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3302e-01, 4.0633e-01, 4.6637e-02, 2.8670e-06, 1.1359e-01, 4.7344e-06,
        1.0042e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.221

[Epoch: 78, batch: 188/236] total loss per batch: 0.393
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2814e-03, 1.4512e-02, 7.0790e-04, 3.5101e-06, 5.0611e-03, 3.3753e-03,
        9.7406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 78, batch: 235/236] total loss per batch: 0.394
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0020, 0.0021, 0.0026, 0.0034, 0.0047, 0.9830, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 79, batch: 47/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.7211e-06, 8.7542e-01, 1.1357e-01, 1.5639e-06, 1.1008e-02, 7.0314e-07,
        1.5068e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.044

[Epoch: 79, batch: 94/236] total loss per batch: 0.395
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.9598e-07, 3.7393e-08, 1.2906e-06, 3.2063e-09, 1.0000e+00, 1.2868e-08,
        4.5754e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.615

[Epoch: 79, batch: 141/236] total loss per batch: 0.404
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([2.7169e-01, 4.7499e-01, 2.8226e-02, 3.9165e-06, 6.9676e-02, 9.0986e-06,
        1.5541e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 79, batch: 188/236] total loss per batch: 0.386
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([8.7898e-04, 2.1535e-03, 1.1823e-03, 5.3448e-07, 1.6856e-03, 1.3388e-03,
        9.9276e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 79, batch: 235/236] total loss per batch: 0.382
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.3858e-03, 5.6689e-03, 1.9486e-03, 1.3874e-03, 8.9165e-04, 9.8710e-01,
        1.6139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 80, batch: 47/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.2985e-06, 9.1370e-01, 8.4446e-02, 6.4420e-07, 1.8476e-03, 3.7919e-07,
        2.2445e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.151

[Epoch: 80, batch: 94/236] total loss per batch: 0.385
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1628e-07, 8.0114e-07, 6.4088e-07, 4.0082e-08, 1.0000e+00, 3.4272e-08,
        1.0245e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.460

[Epoch: 80, batch: 141/236] total loss per batch: 0.395
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6258e-01, 3.9621e-01, 2.6316e-02, 5.2453e-06, 7.0467e-02, 6.8594e-06,
        1.4441e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.276

[Epoch: 80, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.7028e-03, 3.2650e-03, 1.5718e-03, 8.2501e-07, 3.6780e-03, 1.1300e-03,
        9.8865e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 80, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.5155e-03, 5.1021e-03, 1.7363e-03, 1.2078e-03, 9.0206e-04, 9.8767e-01,
        1.8670e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 81, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.2558e-06, 9.2045e-01, 7.8189e-02, 4.9093e-07, 1.3556e-03, 2.7148e-07,
        1.0073e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.046

[Epoch: 81, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6022e-07, 2.9843e-07, 3.6464e-07, 1.4514e-08, 1.0000e+00, 1.9934e-08,
        3.0204e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.434

[Epoch: 81, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5668e-01, 3.9348e-01, 4.0401e-02, 4.9117e-06, 7.8782e-02, 8.9696e-06,
        1.3065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.245

[Epoch: 81, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0342e-03, 2.3947e-03, 1.5684e-03, 8.3103e-07, 2.4947e-03, 1.0049e-03,
        9.8950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 81, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.6969e-03, 4.4264e-03, 1.5140e-03, 1.0661e-03, 9.2085e-04, 9.8814e-01,
        2.2362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 82, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9464e-06, 9.1763e-01, 8.0968e-02, 4.8065e-07, 1.3965e-03, 1.9391e-07,
        9.4564e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.095

[Epoch: 82, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.2156e-07, 4.3249e-07, 3.9413e-07, 1.9995e-08, 1.0000e+00, 2.4052e-08,
        4.7132e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.440

[Epoch: 82, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6059e-01, 3.8487e-01, 3.8595e-02, 5.1763e-06, 7.3154e-02, 9.0145e-06,
        1.4277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.246

[Epoch: 82, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6396e-03, 2.3277e-03, 1.4651e-03, 7.3877e-07, 2.5378e-03, 1.0163e-03,
        9.9001e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 82, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.8779e-03, 3.3199e-03, 1.4935e-03, 1.1340e-03, 9.2079e-04, 9.8919e-01,
        2.0668e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 83, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8680e-06, 9.1617e-01, 8.2399e-02, 3.9537e-07, 1.4320e-03, 1.7422e-07,
        1.1275e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.097

[Epoch: 83, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8600e-07, 2.7900e-07, 2.7032e-07, 1.1685e-08, 1.0000e+00, 1.8541e-08,
        3.5893e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.446

[Epoch: 83, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5679e-01, 3.8594e-01, 3.9408e-02, 3.9161e-06, 7.9345e-02, 7.0212e-06,
        1.3851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 83, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9643e-03, 2.1638e-03, 1.3938e-03, 6.1987e-07, 2.7897e-03, 1.1962e-03,
        9.8949e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 83, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.6032e-03, 2.7165e-03, 1.6380e-03, 1.2331e-03, 9.4390e-04, 9.8993e-01,
        1.9381e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 84, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.6480e-06, 9.1545e-01, 8.3102e-02, 3.1187e-07, 1.4467e-03, 1.4596e-07,
        8.4897e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.097

[Epoch: 84, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.7779e-07, 3.4176e-07, 3.0573e-07, 1.1569e-08, 1.0000e+00, 1.9704e-08,
        5.1667e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.454

[Epoch: 84, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5360e-01, 3.9316e-01, 3.9057e-02, 3.5560e-06, 7.6384e-02, 6.5008e-06,
        1.3779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.260

[Epoch: 84, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7557e-03, 2.0463e-03, 1.3021e-03, 5.9736e-07, 3.0070e-03, 1.1057e-03,
        9.8978e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 84, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0025, 0.0016, 0.0012, 0.0011, 0.9899, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 85, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5353e-06, 9.1486e-01, 8.3661e-02, 2.8501e-07, 1.4818e-03, 1.3278e-07,
        7.8439e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.084

[Epoch: 85, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.5654e-07, 2.2349e-07, 2.2731e-07, 8.0650e-09, 1.0000e+00, 1.5241e-08,
        2.9104e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.460

[Epoch: 85, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5866e-01, 3.8815e-01, 3.5923e-02, 3.0701e-06, 8.0567e-02, 5.8355e-06,
        1.3669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 85, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8844e-03, 2.2092e-03, 1.4234e-03, 5.4743e-07, 2.9795e-03, 1.2351e-03,
        9.8927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 85, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0023, 0.0017, 0.0013, 0.0010, 0.9901, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 86, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3009e-06, 9.1935e-01, 7.9153e-02, 2.0937e-07, 1.4955e-03, 1.0348e-07,
        7.4078e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.097

[Epoch: 86, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3824e-07, 2.0607e-07, 2.1647e-07, 7.8069e-09, 1.0000e+00, 1.3694e-08,
        4.9620e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.468

[Epoch: 86, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4658e-01, 3.9202e-01, 4.2392e-02, 2.5225e-06, 7.7364e-02, 5.0940e-06,
        1.4164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 86, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2554e-03, 1.9314e-03, 1.2583e-03, 5.0816e-07, 3.3797e-03, 1.2717e-03,
        9.8890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 86, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0020, 0.0017, 0.0013, 0.0012, 0.9903, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 87, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4194e-06, 9.0766e-01, 9.0726e-02, 2.6046e-07, 1.6102e-03, 1.1608e-07,
        7.8099e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 87, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3229e-07, 1.6040e-07, 1.7014e-07, 5.3899e-09, 1.0000e+00, 1.2634e-08,
        2.4035e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.467

[Epoch: 87, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5829e-01, 3.8906e-01, 3.3136e-02, 2.8992e-06, 8.3012e-02, 5.4837e-06,
        1.3650e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.238

[Epoch: 87, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5616e-03, 2.0087e-03, 1.5923e-03, 4.8028e-07, 2.6904e-03, 1.2438e-03,
        9.8990e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 87, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0022, 0.0018, 0.0016, 0.0011, 0.9897, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 88, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0715e-06, 9.2678e-01, 7.1768e-02, 1.6056e-07, 1.4472e-03, 8.9087e-08,
        8.2990e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.104

[Epoch: 88, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3155e-07, 1.8473e-07, 2.0849e-07, 6.3230e-09, 1.0000e+00, 1.1789e-08,
        5.4112e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.484

[Epoch: 88, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4487e-01, 3.8749e-01, 4.6617e-02, 1.9078e-06, 7.6791e-02, 3.5553e-06,
        1.4422e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.274

[Epoch: 88, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.0246e-03, 1.8968e-03, 1.3149e-03, 5.9903e-07, 3.9141e-03, 1.2967e-03,
        9.8755e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 88, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0022, 0.0017, 0.0014, 0.0016, 0.9892, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 89, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5345e-06, 9.0363e-01, 9.4407e-02, 2.6551e-07, 1.9601e-03, 1.3330e-07,
        6.6351e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.072

[Epoch: 89, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.5932e-07, 1.1642e-07, 2.4091e-07, 6.9842e-09, 1.0000e+00, 1.3015e-08,
        2.6253e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.472

[Epoch: 89, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6841e-01, 3.7341e-01, 2.9197e-02, 3.5828e-06, 8.4989e-02, 7.5143e-06,
        1.4398e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.232

[Epoch: 89, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5788e-03, 2.2331e-03, 1.5026e-03, 5.7545e-07, 3.1752e-03, 1.3845e-03,
        9.8913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 89, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.9239e-03, 2.2342e-03, 1.9702e-03, 1.4874e-03, 9.7809e-04, 9.8987e-01,
        1.5346e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 90, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2984e-06, 9.2503e-01, 7.3197e-02, 1.9419e-07, 1.7723e-03, 1.4848e-07,
        9.8694e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.120

[Epoch: 90, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.5053e-07, 1.6823e-07, 2.5278e-07, 7.4861e-09, 1.0000e+00, 1.7027e-08,
        2.2531e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.489

[Epoch: 90, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5223e-01, 3.8950e-01, 5.6019e-02, 3.1205e-06, 7.9861e-02, 4.9703e-06,
        1.2238e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.275

[Epoch: 90, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5321e-03, 1.9831e-03, 1.5643e-03, 5.6940e-07, 2.3854e-03, 1.0286e-03,
        9.9051e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 90, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0013, 0.0013, 0.0016, 0.0018, 0.9907, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 91, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.4392e-06, 8.8413e-01, 1.1338e-01, 4.8694e-07, 2.4820e-03, 2.2140e-07,
        8.7473e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.063

[Epoch: 91, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.3121e-07, 2.4736e-07, 5.5372e-07, 2.2346e-08, 1.0000e+00, 2.5740e-08,
        2.2456e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.484

[Epoch: 91, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5316e-01, 3.8030e-01, 3.2566e-02, 3.7218e-06, 7.9488e-02, 8.6110e-06,
        1.5448e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.221

[Epoch: 91, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4970e-03, 2.4742e-03, 1.6842e-03, 7.1783e-07, 4.3329e-03, 2.1431e-03,
        9.8587e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 91, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0019, 0.0027, 0.0018, 0.0016, 0.0015, 0.9888, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 92, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7395e-06, 9.2952e-01, 6.8663e-02, 3.9587e-07, 1.8091e-03, 2.9366e-07,
        1.1714e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.157

[Epoch: 92, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.3276e-07, 1.5262e-07, 3.7561e-07, 1.1515e-08, 1.0000e+00, 2.3300e-08,
        3.8668e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.479

[Epoch: 92, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.8130e-01, 3.8516e-01, 5.4331e-02, 4.2543e-06, 7.1521e-02, 5.5249e-06,
        1.0768e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 92, batch: 188/236] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.3199e-03, 2.4233e-03, 2.2821e-03, 4.7626e-06, 2.9320e-03, 1.8454e-03,
        9.8619e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 92, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0027, 0.0012, 0.0055, 0.0043, 0.0039, 0.9797, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 93, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([4.4081e-06, 9.1440e-01, 8.2930e-02, 3.3037e-07, 2.6604e-03, 5.9437e-07,
        8.9132e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.014

[Epoch: 93, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0889e-06, 3.0087e-07, 1.4015e-06, 9.1792e-08, 1.0000e+00, 4.8969e-08,
        2.0650e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 93, batch: 141/236] total loss per batch: 0.396
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2402e-01, 3.7369e-01, 5.4715e-02, 4.3085e-06, 7.4739e-02, 7.5590e-06,
        1.7283e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.276

[Epoch: 93, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([5.1265e-03, 2.9377e-03, 1.4083e-03, 2.0846e-06, 2.6237e-03, 1.6845e-03,
        9.8622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 93, batch: 235/236] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.0545e-03, 7.8955e-03, 2.9145e-03, 8.0054e-04, 4.3664e-03, 9.7887e-01,
        3.0977e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 94, batch: 47/236] total loss per batch: 0.384
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([3.1654e-06, 9.1971e-01, 7.8918e-02, 1.2577e-06, 1.3649e-03, 2.0657e-07,
        1.1139e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.175

[Epoch: 94, batch: 94/236] total loss per batch: 0.380
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.1235e-07, 1.4681e-07, 1.9321e-06, 3.0219e-08, 1.0000e+00, 2.6134e-08,
        1.2468e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 94, batch: 141/236] total loss per batch: 0.410
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5444e-01, 4.0381e-01, 4.2397e-02, 6.0710e-06, 9.2762e-02, 5.7509e-06,
        1.0658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.276

[Epoch: 94, batch: 188/236] total loss per batch: 0.404
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2090e-03, 1.6305e-03, 1.3071e-03, 5.7250e-06, 2.4126e-03, 1.7939e-03,
        9.9064e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 94, batch: 235/236] total loss per batch: 0.393
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([8.2079e-04, 5.0670e-04, 1.2610e-03, 1.1300e-03, 2.9884e-03, 9.9084e-01,
        2.4516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.004

[Epoch: 95, batch: 47/236] total loss per batch: 0.389
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8630e-06, 8.9842e-01, 1.0064e-01, 2.0857e-07, 9.4325e-04, 3.1024e-07,
        6.7190e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.026

[Epoch: 95, batch: 94/236] total loss per batch: 0.383
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([8.9375e-07, 7.2076e-07, 2.8818e-06, 5.6300e-08, 1.0000e+00, 2.0923e-07,
        1.2951e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 95, batch: 141/236] total loss per batch: 0.401
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7418e-01, 3.7386e-01, 3.1815e-02, 4.7951e-06, 1.1019e-01, 3.9219e-06,
        1.0995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.226

[Epoch: 95, batch: 188/236] total loss per batch: 0.384
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.4787e-03, 1.4152e-03, 1.8470e-03, 5.2197e-06, 1.9856e-03, 1.1116e-03,
        9.9216e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 95, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([7.9944e-04, 3.7013e-03, 2.1226e-03, 1.5968e-03, 2.0150e-03, 9.8671e-01,
        3.0527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 96, batch: 47/236] total loss per batch: 0.382
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.8178e-06, 9.2944e-01, 6.9445e-02, 4.7027e-07, 1.1088e-03, 3.3571e-07,
        2.1273e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.084

[Epoch: 96, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.2555e-07, 4.9264e-07, 2.0283e-06, 3.9834e-08, 1.0000e+00, 2.0436e-07,
        3.2747e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.482

[Epoch: 96, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5307e-01, 3.8783e-01, 3.9124e-02, 3.7463e-06, 8.7275e-02, 2.7855e-06,
        1.3269e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.234

[Epoch: 96, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.5621e-03, 1.8688e-03, 1.5969e-03, 4.6286e-06, 1.3555e-03, 1.2983e-03,
        9.9231e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 96, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([8.6790e-04, 1.8126e-03, 2.0008e-03, 1.6590e-03, 2.1822e-03, 9.8817e-01,
        3.3074e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 97, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8069e-06, 9.2123e-01, 7.7663e-02, 3.3664e-07, 1.1023e-03, 2.0610e-07,
        1.0959e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.099

[Epoch: 97, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.5894e-07, 2.3704e-07, 1.2351e-06, 2.4126e-08, 1.0000e+00, 1.2989e-07,
        3.7337e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.475

[Epoch: 97, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7125e-01, 3.7627e-01, 3.6924e-02, 2.9830e-06, 8.1091e-02, 2.7536e-06,
        1.3445e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.260

[Epoch: 97, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5479e-03, 1.7165e-03, 1.3492e-03, 5.8612e-06, 2.3457e-03, 1.3470e-03,
        9.9069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 97, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.2041e-04, 1.6640e-03, 1.8884e-03, 1.5673e-03, 1.5658e-03, 9.8985e-01,
        2.5411e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 98, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7448e-06, 9.1502e-01, 8.3868e-02, 3.3565e-07, 1.1110e-03, 2.2141e-07,
        1.6613e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.126

[Epoch: 98, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.6288e-07, 1.8309e-07, 1.1222e-06, 1.9780e-08, 1.0000e+00, 1.0679e-07,
        1.7845e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.485

[Epoch: 98, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4789e-01, 3.8861e-01, 4.0844e-02, 2.4728e-06, 7.9137e-02, 2.8266e-06,
        1.4351e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.255

[Epoch: 98, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5396e-03, 1.7131e-03, 1.3956e-03, 4.8750e-06, 2.3748e-03, 1.3552e-03,
        9.9062e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 98, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0017, 0.0019, 0.0015, 0.0020, 0.9892, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 99, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5011e-06, 9.1062e-01, 8.7878e-02, 3.7556e-07, 1.5028e-03, 2.0859e-07,
        1.5423e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.072

[Epoch: 99, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.4434e-07, 1.3814e-07, 7.6771e-07, 1.1618e-08, 1.0000e+00, 7.9125e-08,
        1.4705e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.486

[Epoch: 99, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5467e-01, 3.9105e-01, 3.5786e-02, 2.1465e-06, 7.9859e-02, 2.4181e-06,
        1.3863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.257

[Epoch: 99, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8611e-03, 1.7659e-03, 1.5100e-03, 4.7758e-06, 3.0108e-03, 1.4045e-03,
        9.8944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 99, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0017, 0.0018, 0.0012, 0.0018, 0.9899, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 100, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2067e-06, 9.2269e-01, 7.6121e-02, 2.3137e-07, 1.1899e-03, 1.5485e-07,
        1.4324e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.100

[Epoch: 100, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.6881e-07, 1.1660e-07, 6.7753e-07, 9.5773e-09, 1.0000e+00, 6.5590e-08,
        1.7441e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.488

[Epoch: 100, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5258e-01, 3.8157e-01, 4.3155e-02, 1.8127e-06, 8.1247e-02, 2.2426e-06,
        1.4145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 100, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7933e-03, 1.7700e-03, 1.5970e-03, 4.5518e-06, 2.9321e-03, 1.4047e-03,
        9.8950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 100, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0016, 0.0017, 0.0014, 0.0018, 0.9899, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 101, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1967e-06, 9.1234e-01, 8.6200e-02, 2.3636e-07, 1.4570e-03, 1.2688e-07,
        1.2714e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.088

[Epoch: 101, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7357e-07, 1.0903e-07, 5.9788e-07, 8.9498e-09, 1.0000e+00, 5.8174e-08,
        1.6893e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 101, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5391e-01, 3.9580e-01, 3.3277e-02, 1.5500e-06, 7.9387e-02, 1.8957e-06,
        1.3762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 101, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1207e-03, 1.8024e-03, 1.6851e-03, 3.8882e-06, 3.1693e-03, 1.4442e-03,
        9.8877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 101, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0017, 0.0015, 0.0014, 0.0018, 0.9899, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 102, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0677e-06, 9.1913e-01, 7.9576e-02, 1.9409e-07, 1.2944e-03, 1.1512e-07,
        1.4538e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.095

[Epoch: 102, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1530e-07, 8.4686e-08, 5.3082e-07, 6.9760e-09, 1.0000e+00, 4.9081e-08,
        1.4593e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.490

[Epoch: 102, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5063e-01, 3.8315e-01, 4.2419e-02, 1.4102e-06, 8.0770e-02, 1.8428e-06,
        1.4303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 102, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8536e-03, 1.7843e-03, 1.6363e-03, 3.8111e-06, 3.1171e-03, 1.4246e-03,
        9.8918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 102, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0017, 0.0016, 0.0012, 0.0017, 0.9904, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 103, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0289e-06, 9.1750e-01, 8.1098e-02, 1.9986e-07, 1.4007e-03, 1.0153e-07,
        1.3302e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 103, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1679e-07, 8.1926e-08, 5.0250e-07, 6.7871e-09, 1.0000e+00, 4.4310e-08,
        1.5432e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 103, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5422e-01, 3.8912e-01, 3.5505e-02, 1.3056e-06, 7.8389e-02, 1.7468e-06,
        1.4277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 103, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1883e-03, 1.6035e-03, 1.7089e-03, 3.4342e-06, 3.0141e-03, 1.4426e-03,
        9.8904e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 103, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0018, 0.0016, 0.0014, 0.0016, 0.9901, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 104, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.9740e-07, 9.1771e-01, 8.0909e-02, 1.6815e-07, 1.3796e-03, 9.9532e-08,
        1.1609e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.096

[Epoch: 104, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6884e-07, 7.1111e-08, 4.1793e-07, 5.2221e-09, 1.0000e+00, 3.6725e-08,
        1.1242e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 104, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5332e-01, 3.8993e-01, 4.2511e-02, 1.2052e-06, 8.2123e-02, 1.4457e-06,
        1.3211e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 104, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6092e-03, 1.8691e-03, 1.6069e-03, 3.8303e-06, 2.9629e-03, 1.2903e-03,
        9.8966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 104, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0015, 0.0016, 0.0011, 0.0017, 0.9910, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 105, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0468e-06, 9.1432e-01, 8.4281e-02, 1.8653e-07, 1.3992e-03, 9.3413e-08,
        1.3204e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.088

[Epoch: 105, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.4499e-07, 8.7411e-08, 5.5625e-07, 6.4190e-09, 1.0000e+00, 4.2943e-08,
        1.4062e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 105, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5563e-01, 3.7598e-01, 3.4329e-02, 1.0919e-06, 7.7915e-02, 1.4626e-06,
        1.5614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 105, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.8823e-03, 1.7179e-03, 1.7653e-03, 3.0339e-06, 3.4317e-03, 1.7573e-03,
        9.8744e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 105, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0018, 0.0015, 0.0015, 0.0016, 0.9903, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 106, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.5786e-07, 9.2531e-01, 7.3290e-02, 1.5964e-07, 1.4019e-03, 1.0259e-07,
        1.2094e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.105

[Epoch: 106, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.5249e-07, 7.9045e-08, 4.0634e-07, 5.3717e-09, 1.0000e+00, 3.2876e-08,
        9.8371e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 106, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4733e-01, 3.9943e-01, 4.2596e-02, 1.2415e-06, 8.1292e-02, 1.5978e-06,
        1.2935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 106, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8523e-03, 1.9250e-03, 1.7522e-03, 4.1777e-06, 3.3076e-03, 1.5107e-03,
        9.8865e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 106, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0017, 0.0015, 0.0014, 0.0018, 0.9905, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 107, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4179e-06, 8.9453e-01, 1.0371e-01, 2.6889e-07, 1.7623e-03, 1.0770e-07,
        2.0492e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.057

[Epoch: 107, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.0702e-07, 5.5940e-08, 5.6552e-07, 9.8144e-09, 1.0000e+00, 6.4854e-08,
        1.8027e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 107, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7683e-01, 3.6140e-01, 3.9986e-02, 1.1774e-06, 8.4178e-02, 2.1137e-06,
        1.3760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.246

[Epoch: 107, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5674e-03, 1.7970e-03, 1.8127e-03, 4.0156e-06, 3.7949e-03, 2.1577e-03,
        9.8687e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 107, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0017, 0.0017, 0.0016, 0.0015, 0.9896, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 108, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.2900e-07, 9.5003e-01, 4.9061e-02, 1.3078e-07, 9.0488e-04, 9.7588e-08,
        5.6297e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.186

[Epoch: 108, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.4712e-07, 2.3722e-07, 5.3566e-07, 1.1294e-08, 1.0000e+00, 5.1694e-08,
        7.1773e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 108, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4558e-01, 3.9284e-01, 2.8717e-02, 1.8327e-06, 8.5855e-02, 2.8503e-06,
        1.4700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.268

[Epoch: 108, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5742e-03, 1.3397e-03, 1.6414e-03, 4.3955e-06, 2.5163e-03, 1.2879e-03,
        9.9064e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 108, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0017, 0.0012, 0.0016, 0.0019, 0.9909, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 109, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.3362e-06, 8.3887e-01, 1.5732e-01, 6.1903e-07, 3.7995e-03, 3.0285e-07,
        4.5484e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 0.030

[Epoch: 109, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7995e-07, 4.9826e-08, 3.9845e-07, 1.1519e-08, 1.0000e+00, 5.0724e-08,
        2.5987e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 109, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6446e-01, 3.7909e-01, 4.2452e-02, 1.5257e-06, 8.1259e-02, 3.2085e-06,
        1.3273e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 109, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.6365e-03, 1.3487e-03, 1.8634e-03, 6.8892e-06, 3.4127e-03, 2.6837e-03,
        9.8605e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 109, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0020, 0.0012, 0.0013, 0.0014, 0.0022, 0.9892, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 110, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2115e-06, 9.6302e-01, 3.5659e-02, 2.1076e-07, 1.3166e-03, 1.7036e-07,
        2.2046e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.207

[Epoch: 110, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.3035e-07, 2.3782e-07, 1.1615e-06, 2.6253e-08, 1.0000e+00, 1.0370e-07,
        3.3322e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 110, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5531e-01, 3.7927e-01, 3.7389e-02, 1.6544e-06, 8.9380e-02, 3.6701e-06,
        1.3864e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.257

[Epoch: 110, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5445e-03, 1.3813e-03, 1.3410e-03, 3.7424e-06, 3.8165e-03, 1.4716e-03,
        9.8844e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 110, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0028, 0.0023, 0.0013, 0.0017, 0.9890, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 111, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8484e-06, 9.0151e-01, 9.5917e-02, 4.1996e-07, 2.5697e-03, 1.8338e-07,
        3.5037e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.033

[Epoch: 111, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.6961e-07, 1.1492e-07, 8.5945e-07, 2.2732e-08, 1.0000e+00, 7.4887e-08,
        3.7218e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.511

[Epoch: 111, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5388e-01, 3.8159e-01, 3.6974e-02, 1.7232e-06, 7.1749e-02, 3.1193e-06,
        1.5580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.253

[Epoch: 111, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.2642e-03, 1.7386e-03, 1.6361e-03, 5.4610e-06, 3.3238e-03, 2.5745e-03,
        9.8646e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 111, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0025, 0.0015, 0.0016, 0.0014, 0.0021, 0.9884, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 112, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9321e-06, 9.3040e-01, 6.7242e-02, 3.6752e-07, 2.3556e-03, 2.1893e-07,
        3.8354e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.059

[Epoch: 112, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.2995e-07, 2.8199e-07, 8.7049e-07, 2.1585e-08, 1.0000e+00, 9.8633e-08,
        6.8030e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 112, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6299e-01, 3.8197e-01, 4.3286e-02, 2.4920e-06, 8.8260e-02, 5.0605e-06,
        1.2349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.264

[Epoch: 112, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.8491e-03, 1.7194e-03, 1.3501e-03, 7.4612e-06, 4.5511e-03, 1.6303e-03,
        9.8589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 112, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0017, 0.0013, 0.0016, 0.0014, 0.9908, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 113, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9310e-06, 9.0510e-01, 9.2316e-02, 5.1726e-07, 2.5850e-03, 3.7401e-07,
        4.3343e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.094

[Epoch: 113, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.8313e-07, 8.7243e-08, 5.4879e-07, 1.5667e-08, 1.0000e+00, 4.5183e-08,
        1.9679e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 113, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4181e-01, 3.8685e-01, 3.5782e-02, 1.8515e-06, 7.7990e-02, 5.0726e-06,
        1.5756e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.241

[Epoch: 113, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3798e-03, 1.5994e-03, 2.2688e-03, 4.4301e-06, 3.5495e-03, 1.7198e-03,
        9.8748e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 113, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0023, 0.0018, 0.0013, 0.0013, 0.0016, 0.9902, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 114, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.4233e-06, 9.1530e-01, 8.2138e-02, 4.6640e-07, 2.5616e-03, 2.3460e-07,
        4.0642e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.089

[Epoch: 114, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.5382e-07, 1.7783e-07, 7.1840e-07, 2.4637e-08, 1.0000e+00, 6.1979e-08,
        3.1470e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.510

[Epoch: 114, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5972e-01, 4.0870e-01, 3.6950e-02, 3.0873e-06, 7.3014e-02, 5.3954e-06,
        1.2161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 114, batch: 188/236] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8662e-03, 2.3087e-03, 1.3715e-03, 6.3438e-06, 2.5239e-03, 1.6282e-03,
        9.8930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 114, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([2.6782e-03, 2.4682e-03, 8.6698e-04, 2.3191e-03, 9.1895e-04, 9.8866e-01,
        2.0875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 115, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8176e-06, 9.3130e-01, 6.6471e-02, 3.7750e-07, 2.2264e-03, 3.9454e-07,
        7.4439e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.063

[Epoch: 115, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.5384e-07, 1.3646e-07, 4.5233e-07, 1.8749e-08, 1.0000e+00, 8.4554e-08,
        4.8406e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.469

[Epoch: 115, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2265e-01, 3.3167e-01, 6.5063e-02, 1.0608e-06, 1.0687e-01, 3.8988e-06,
        1.7374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.277

[Epoch: 115, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.9331e-03, 2.3651e-03, 1.6381e-03, 5.1751e-06, 1.9181e-03, 1.9685e-03,
        9.8517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 115, batch: 235/236] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0020, 0.0020, 0.0016, 0.0011, 0.9903, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 116, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9681e-06, 8.9585e-01, 1.0201e-01, 3.0337e-07, 2.1412e-03, 2.9964e-07,
        6.6612e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.115

[Epoch: 116, batch: 94/236] total loss per batch: 0.379
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9997e-06, 3.4666e-07, 6.5396e-07, 3.9536e-08, 1.0000e+00, 1.5319e-07,
        4.2180e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 116, batch: 141/236] total loss per batch: 0.393
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7297e-01, 3.9739e-01, 1.5713e-02, 1.7314e-06, 7.6256e-02, 3.9048e-06,
        1.3767e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.308

[Epoch: 116, batch: 188/236] total loss per batch: 0.392
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([6.8733e-03, 1.3691e-03, 1.5503e-03, 9.0949e-07, 2.0365e-03, 1.0098e-03,
        9.8716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 116, batch: 235/236] total loss per batch: 0.384
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.1744e-03, 8.2522e-03, 4.6263e-03, 5.9602e-03, 8.7865e-04, 9.7608e-01,
        3.0260e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 117, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4918e-06, 9.5911e-01, 3.9588e-02, 1.7231e-07, 1.2947e-03, 3.0505e-07,
        3.3029e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.137

[Epoch: 117, batch: 94/236] total loss per batch: 0.381
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.1517e-06, 2.5395e-07, 1.3096e-06, 1.4981e-07, 9.9999e-01, 1.0163e-06,
        1.9187e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.471

[Epoch: 117, batch: 141/236] total loss per batch: 0.397
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6014e-01, 3.6455e-01, 2.6522e-02, 1.4256e-06, 9.5308e-02, 5.6437e-06,
        1.5347e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.206

[Epoch: 117, batch: 188/236] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.0209e-03, 2.5118e-03, 8.4037e-04, 6.1771e-07, 1.7997e-03, 6.5912e-04,
        9.9217e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 117, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([7.7370e-04, 2.6387e-03, 2.5605e-03, 1.8250e-03, 9.8370e-04, 9.8885e-01,
        2.3651e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 118, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4351e-06, 9.2957e-01, 6.8766e-02, 1.7119e-07, 1.6654e-03, 3.3540e-07,
        2.7129e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.121

[Epoch: 118, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.0160e-06, 1.4403e-07, 6.9186e-07, 4.6876e-08, 1.0000e+00, 1.4758e-07,
        1.2506e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.478

[Epoch: 118, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7629e-01, 3.5019e-01, 4.2815e-02, 1.2253e-06, 8.2972e-02, 4.8879e-06,
        1.4772e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.224

[Epoch: 118, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0250e-03, 2.4580e-03, 8.4833e-04, 8.1344e-07, 2.5755e-03, 8.9797e-04,
        9.9019e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.018

[Epoch: 118, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0010, 0.0028, 0.0019, 0.0023, 0.0011, 0.9882, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 119, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4588e-06, 9.0859e-01, 8.9239e-02, 1.4131e-07, 2.1654e-03, 3.6682e-07,
        3.0324e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.084

[Epoch: 119, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.2231e-06, 1.4358e-07, 5.8232e-07, 3.7497e-08, 9.9999e-01, 1.3880e-07,
        7.9090e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 119, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6048e-01, 3.7785e-01, 4.1496e-02, 9.2645e-07, 8.0036e-02, 4.0643e-06,
        1.4013e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.224

[Epoch: 119, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.4068e-03, 2.5763e-03, 9.7193e-04, 5.6861e-07, 3.0140e-03, 1.1467e-03,
        9.8988e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 119, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0023, 0.0021, 0.0019, 0.0012, 0.9891, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 120, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3024e-06, 9.1932e-01, 7.8797e-02, 1.3444e-07, 1.8835e-03, 2.5058e-07,
        2.8241e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.099

[Epoch: 120, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.4769e-06, 7.1832e-08, 5.1317e-07, 2.5244e-08, 1.0000e+00, 9.4127e-08,
        7.2508e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 120, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5803e-01, 3.8109e-01, 3.8915e-02, 8.1545e-07, 8.1945e-02, 3.0210e-06,
        1.4002e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.238

[Epoch: 120, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7312e-03, 2.3228e-03, 9.9100e-04, 5.9755e-07, 3.1297e-03, 1.0835e-03,
        9.8974e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 120, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0021, 0.0019, 0.0019, 0.0012, 0.9895, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 121, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1800e-06, 9.1315e-01, 8.4876e-02, 1.1833e-07, 1.9737e-03, 2.2151e-07,
        2.5168e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 121, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8133e-06, 6.5232e-08, 4.2043e-07, 1.9194e-08, 1.0000e+00, 7.4119e-08,
        7.0775e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 121, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5395e-01, 3.8588e-01, 4.0248e-02, 6.7291e-07, 8.2169e-02, 2.4871e-06,
        1.3775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.244

[Epoch: 121, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.5680e-03, 2.1666e-03, 1.0813e-03, 4.8191e-07, 3.0794e-03, 1.1645e-03,
        9.8994e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 121, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0019, 0.0020, 0.0017, 0.0012, 0.9899, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 122, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0698e-06, 9.1953e-01, 7.8611e-02, 1.0569e-07, 1.8596e-03, 1.8683e-07,
        2.8628e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 122, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4198e-06, 5.5207e-08, 3.7166e-07, 1.6195e-08, 1.0000e+00, 5.5789e-08,
        6.5000e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.493

[Epoch: 122, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5471e-01, 3.8593e-01, 3.8348e-02, 5.8432e-07, 8.0809e-02, 2.0361e-06,
        1.4020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 122, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8965e-03, 1.9473e-03, 1.1730e-03, 4.8537e-07, 2.9345e-03, 1.1906e-03,
        9.8986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 122, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0018, 0.0018, 0.0017, 0.0013, 0.9901, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 123, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0422e-06, 9.1563e-01, 8.2472e-02, 1.0038e-07, 1.8955e-03, 1.8090e-07,
        2.5646e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 123, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0796e-06, 4.3511e-08, 3.0162e-07, 1.1568e-08, 1.0000e+00, 4.5476e-08,
        5.2764e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 123, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5374e-01, 3.8863e-01, 3.7666e-02, 4.9385e-07, 7.9782e-02, 1.7777e-06,
        1.4018e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 123, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0430e-03, 2.0800e-03, 1.2246e-03, 4.8079e-07, 3.2157e-03, 1.3166e-03,
        9.8912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 123, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0016, 0.0019, 0.0015, 0.0013, 0.9905, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 124, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.1945e-07, 9.2090e-01, 7.7311e-02, 7.3685e-08, 1.7924e-03, 1.3171e-07,
        2.4009e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 124, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0980e-06, 4.6171e-08, 3.1762e-07, 1.2085e-08, 1.0000e+00, 3.9717e-08,
        5.9262e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 124, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5240e-01, 3.8828e-01, 3.9418e-02, 3.9232e-07, 8.1362e-02, 1.5388e-06,
        1.3853e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 124, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2383e-03, 1.9012e-03, 1.3346e-03, 4.3534e-07, 2.9070e-03, 1.2931e-03,
        9.8933e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 124, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0017, 0.0016, 0.0016, 0.0014, 0.9903, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 125, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0177e-06, 9.0463e-01, 9.3348e-02, 9.1111e-08, 2.0218e-03, 1.6684e-07,
        2.4967e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.088

[Epoch: 125, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.9366e-07, 3.3827e-08, 2.3764e-07, 7.0496e-09, 1.0000e+00, 2.6981e-08,
        3.8061e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 125, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5723e-01, 3.8117e-01, 3.9416e-02, 4.0374e-07, 7.9714e-02, 1.5921e-06,
        1.4247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 125, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8540e-03, 2.2260e-03, 1.2513e-03, 3.8035e-07, 3.2176e-03, 1.3964e-03,
        9.8905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 125, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0016, 0.0021, 0.0017, 0.0013, 0.9901, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 126, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.1661e-07, 9.3323e-01, 6.5073e-02, 6.0876e-08, 1.6952e-03, 1.2129e-07,
        2.2292e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.105

[Epoch: 126, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1249e-06, 5.3953e-08, 3.0567e-07, 1.5067e-08, 1.0000e+00, 4.6885e-08,
        5.6645e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 126, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5046e-01, 3.9495e-01, 3.4902e-02, 5.7962e-07, 8.1331e-02, 1.9204e-06,
        1.3835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 126, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.3630e-03, 1.5746e-03, 1.6328e-03, 4.6795e-07, 2.9492e-03, 1.6267e-03,
        9.8785e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 126, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0016, 0.0013, 0.0014, 0.0016, 0.9909, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 127, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2756e-06, 8.9648e-01, 1.0150e-01, 1.5529e-07, 2.0141e-03, 2.0057e-07,
        2.7975e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.063

[Epoch: 127, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.6454e-07, 3.7540e-08, 2.7874e-07, 1.1031e-08, 1.0000e+00, 2.8662e-08,
        5.0511e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 127, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5894e-01, 3.7198e-01, 4.7325e-02, 7.7131e-07, 8.0015e-02, 2.7297e-06,
        1.4174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 127, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.3478e-03, 1.8452e-03, 1.3009e-03, 5.4702e-07, 2.8201e-03, 1.3708e-03,
        9.9131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 127, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0016, 0.0020, 0.0019, 0.0016, 0.9897, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 128, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.3010e-07, 9.4303e-01, 5.5468e-02, 8.1818e-08, 1.5030e-03, 2.0974e-07,
        1.5469e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.147

[Epoch: 128, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3934e-06, 1.6793e-07, 7.6147e-07, 3.7482e-08, 1.0000e+00, 1.0072e-07,
        5.1730e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.487

[Epoch: 128, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6041e-01, 3.8966e-01, 3.0012e-02, 1.2800e-06, 8.1664e-02, 2.5365e-06,
        1.3825e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 128, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([5.0972e-03, 1.8559e-03, 1.5984e-03, 8.6543e-07, 2.9125e-03, 1.7554e-03,
        9.8678e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 128, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0015, 0.0016, 0.0018, 0.0016, 0.9902, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 129, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5865e-06, 9.1117e-01, 8.7375e-02, 3.0797e-07, 1.4497e-03, 1.3993e-07,
        4.6014e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.072

[Epoch: 129, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.8353e-07, 2.3601e-08, 2.5703e-07, 1.2935e-08, 1.0000e+00, 3.6200e-08,
        1.8765e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 129, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4143e-01, 3.9824e-01, 4.2771e-02, 1.5408e-06, 7.0765e-02, 2.8775e-06,
        1.4679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 129, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.0237e-03, 2.0267e-03, 1.3634e-03, 1.1220e-06, 3.9007e-03, 1.4378e-03,
        9.9025e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 129, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0015, 0.0020, 0.0015, 0.0019, 0.9900, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 130, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.3149e-06, 8.9861e-01, 9.8569e-02, 2.4156e-07, 2.8138e-03, 3.0294e-07,
        2.3125e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.077

[Epoch: 130, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9010e-06, 4.0221e-07, 1.2784e-06, 8.2583e-08, 1.0000e+00, 1.9011e-07,
        1.5810e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 130, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5057e-01, 3.6231e-01, 4.3574e-02, 2.2080e-06, 1.0790e-01, 7.8081e-06,
        1.3562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 130, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.5788e-03, 2.1876e-03, 1.6389e-03, 1.5490e-06, 3.5642e-03, 1.6068e-03,
        9.8642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 130, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0020, 0.0020, 0.0016, 0.0021, 0.0014, 0.9891, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 131, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.0125e-06, 9.3615e-01, 6.2067e-02, 3.2375e-07, 1.7780e-03, 5.4078e-07,
        8.6170e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 131, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.5476e-07, 1.6693e-07, 8.0184e-07, 4.3122e-08, 1.0000e+00, 3.5754e-08,
        4.8327e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.491

[Epoch: 131, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7493e-01, 3.8373e-01, 3.7369e-02, 1.3205e-06, 6.0691e-02, 4.0970e-06,
        1.4327e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 131, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.2111e-03, 2.3506e-03, 1.7971e-03, 1.3935e-06, 3.0331e-03, 1.8286e-03,
        9.8678e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 131, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0013, 0.0018, 0.0027, 0.0017, 0.9889, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 132, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.1016e-06, 8.9551e-01, 1.0277e-01, 2.9378e-07, 1.7174e-03, 3.7139e-07,
        5.0360e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.072

[Epoch: 132, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([7.7555e-07, 1.7628e-07, 7.6021e-07, 4.1511e-08, 1.0000e+00, 7.5535e-08,
        2.7689e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.501

[Epoch: 132, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4675e-01, 3.8785e-01, 3.9268e-02, 2.0834e-06, 9.5290e-02, 5.3461e-06,
        1.3084e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.255

[Epoch: 132, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.1372e-03, 2.3612e-03, 1.5996e-03, 1.2210e-06, 3.9697e-03, 1.7985e-03,
        9.8813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 132, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0020, 0.0015, 0.0018, 0.0013, 0.9904, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 133, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7910e-06, 9.3267e-01, 6.5512e-02, 3.7935e-07, 1.8150e-03, 3.8088e-07,
        6.2345e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.129

[Epoch: 133, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.9166e-07, 1.9855e-07, 5.4383e-07, 3.5078e-08, 1.0000e+00, 3.5309e-08,
        3.8797e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 133, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4075e-01, 3.8638e-01, 3.9650e-02, 1.1333e-06, 7.8559e-02, 2.2335e-06,
        1.5466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 133, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.9693e-03, 1.8705e-03, 1.3378e-03, 1.4733e-06, 3.0030e-03, 1.6469e-03,
        9.9017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 133, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0015, 0.0014, 0.0018, 0.0013, 0.9912, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 134, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8920e-06, 8.9976e-01, 9.8538e-02, 2.5798e-07, 1.7001e-03, 2.7910e-07,
        7.5750e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.080

[Epoch: 134, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.3931e-07, 1.1739e-07, 5.6965e-07, 2.2162e-08, 1.0000e+00, 4.3360e-08,
        3.1349e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 134, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.9332e-01, 3.9428e-01, 2.9802e-02, 1.4731e-06, 7.1806e-02, 3.2567e-06,
        1.1078e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.237

[Epoch: 134, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.9352e-03, 1.6372e-03, 1.3360e-03, 9.9871e-07, 2.1341e-03, 1.1998e-03,
        9.8976e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 134, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0021, 0.0016, 0.0019, 0.0013, 0.9895, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 135, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8317e-06, 9.2461e-01, 7.3609e-02, 2.8112e-07, 1.7752e-03, 3.8951e-07,
        5.0732e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 135, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.5564e-07, 1.3390e-07, 6.3920e-07, 2.1595e-08, 1.0000e+00, 2.4716e-08,
        4.1059e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 135, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.1784e-01, 3.6701e-01, 5.2390e-02, 1.3188e-06, 8.3023e-02, 3.3314e-06,
        1.7973e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 135, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3353e-03, 1.8150e-03, 1.4009e-03, 1.0407e-06, 3.1801e-03, 1.4117e-03,
        9.8986e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 135, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0017, 0.0014, 0.0020, 0.0014, 0.9907, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 136, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.6067e-06, 9.1158e-01, 8.6312e-02, 2.1385e-07, 2.1045e-03, 2.9818e-07,
        4.2366e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.104

[Epoch: 136, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([5.3685e-07, 1.1877e-07, 4.2996e-07, 2.3302e-08, 1.0000e+00, 3.5164e-08,
        1.6153e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.507

[Epoch: 136, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.8434e-01, 3.8130e-01, 3.1443e-02, 1.4441e-06, 8.1311e-02, 2.5681e-06,
        1.2161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.255

[Epoch: 136, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2419e-03, 1.3162e-03, 1.4217e-03, 1.0864e-06, 2.4234e-03, 1.4263e-03,
        9.9017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 136, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0013, 0.0013, 0.0013, 0.0011, 0.9924, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 137, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9967e-06, 9.1416e-01, 8.3826e-02, 2.8151e-07, 2.0139e-03, 3.1463e-07,
        9.0047e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.083

[Epoch: 137, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.0753e-07, 1.2835e-07, 7.2914e-07, 3.2384e-08, 1.0000e+00, 2.8108e-08,
        2.6635e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.493

[Epoch: 137, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6487e-01, 3.9079e-01, 3.3282e-02, 1.6367e-06, 7.0579e-02, 7.6565e-06,
        1.4047e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.252

[Epoch: 137, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.4824e-03, 2.5870e-03, 1.1963e-03, 2.1830e-06, 3.4891e-03, 1.8969e-03,
        9.8835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 137, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0012, 0.0018, 0.0020, 0.0023, 0.9891, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 138, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.8263e-06, 9.2445e-01, 7.3482e-02, 2.0951e-07, 2.0607e-03, 2.4755e-07,
        8.1855e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 138, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.3052e-07, 7.7438e-08, 3.7966e-07, 1.4675e-08, 1.0000e+00, 3.5485e-08,
        3.6160e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.491

[Epoch: 138, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4209e-01, 3.7469e-01, 4.9630e-02, 2.6885e-06, 9.0943e-02, 4.0312e-06,
        1.4264e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 138, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2838e-03, 1.3126e-03, 1.5215e-03, 9.3518e-07, 2.6088e-03, 1.6571e-03,
        9.8962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 138, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0014, 0.0012, 0.0023, 0.0021, 0.9900, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 139, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5451e-06, 9.1412e-01, 8.4239e-02, 2.1334e-07, 1.6351e-03, 3.0222e-07,
        7.4000e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 139, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.4019e-07, 2.7634e-07, 7.7823e-07, 2.9143e-08, 1.0000e+00, 4.5052e-08,
        2.5289e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 139, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5696e-01, 3.9867e-01, 3.3590e-02, 1.6684e-06, 7.3869e-02, 2.7516e-06,
        1.3690e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 139, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.1119e-03, 1.6583e-03, 1.1992e-03, 1.0511e-06, 2.6484e-03, 1.5015e-03,
        9.9088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 139, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0011, 0.0023, 0.0024, 0.0021, 0.9884, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 140, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.9660e-06, 9.1046e-01, 8.7317e-02, 3.3868e-07, 2.2233e-03, 3.2659e-07,
        6.9607e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.100

[Epoch: 140, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.7024e-07, 1.1729e-07, 8.2639e-07, 2.0925e-08, 1.0000e+00, 3.6431e-08,
        4.5726e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 140, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5194e-01, 3.8178e-01, 4.3061e-02, 2.0450e-06, 8.1873e-02, 3.3486e-06,
        1.4134e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 140, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.9591e-03, 1.7788e-03, 1.5171e-03, 1.2703e-06, 3.0423e-03, 2.0932e-03,
        9.8761e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 140, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0017, 0.0019, 0.0023, 0.0022, 0.9882, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 141, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.1429e-06, 9.3146e-01, 6.7102e-02, 1.4910e-07, 1.4312e-03, 2.0161e-07,
        6.5179e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.101

[Epoch: 141, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.0449e-07, 1.6390e-07, 7.3800e-07, 4.1146e-08, 1.0000e+00, 5.3093e-08,
        1.6300e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 141, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4700e-01, 3.9185e-01, 4.0651e-02, 1.6972e-06, 8.4756e-02, 4.5180e-06,
        1.3573e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.278

[Epoch: 141, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8254e-03, 2.0987e-03, 1.5505e-03, 1.6322e-06, 3.6777e-03, 1.6018e-03,
        9.8824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 141, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0019, 0.0017, 0.0017, 0.0016, 0.9894, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 142, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.1090e-06, 9.0545e-01, 9.2504e-02, 4.4175e-07, 2.0467e-03, 3.9939e-07,
        4.0734e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.076

[Epoch: 142, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.6498e-07, 9.1013e-08, 5.0272e-07, 2.0314e-08, 1.0000e+00, 5.3963e-08,
        4.6349e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.483

[Epoch: 142, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7242e-01, 3.7310e-01, 3.4579e-02, 1.6388e-06, 7.5218e-02, 2.2769e-06,
        1.4468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.237

[Epoch: 142, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.5712e-03, 2.0309e-03, 1.6807e-03, 1.4248e-06, 2.8504e-03, 2.5166e-03,
        9.8735e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 142, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0014, 0.0016, 0.0021, 0.0015, 0.9901, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 143, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([2.6062e-06, 9.0882e-01, 8.8919e-02, 4.7774e-07, 2.2518e-03, 3.7939e-07,
        1.4999e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.114

[Epoch: 143, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.0962e-06, 2.6159e-07, 9.6697e-07, 5.7967e-08, 1.0000e+00, 5.5805e-08,
        1.6625e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.519

[Epoch: 143, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5499e-01, 3.8524e-01, 4.0004e-02, 2.1251e-06, 8.6880e-02, 2.9279e-06,
        1.3289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 143, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.6484e-03, 1.6472e-03, 1.5728e-03, 1.2529e-06, 2.5479e-03, 1.4962e-03,
        9.8909e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 143, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0015, 0.0022, 0.0018, 0.0021, 0.9896, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 144, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4131e-06, 9.3533e-01, 6.3137e-02, 3.7256e-07, 1.5314e-03, 2.2873e-07,
        7.9278e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 144, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.3410e-07, 1.1965e-07, 4.3982e-07, 1.7807e-08, 1.0000e+00, 3.4574e-08,
        5.0326e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 144, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5431e-01, 3.8405e-01, 3.4608e-02, 1.3762e-06, 7.6115e-02, 1.4752e-06,
        1.5091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 144, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0756e-03, 1.6532e-03, 1.0034e-03, 1.3483e-06, 2.9227e-03, 1.9198e-03,
        9.8942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 144, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0011, 0.0011, 0.0011, 0.0012, 0.9930, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 145, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5618e-06, 8.9946e-01, 9.8492e-02, 5.1486e-07, 2.0477e-03, 3.6873e-07,
        1.3655e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.055

[Epoch: 145, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2476e-06, 1.9983e-07, 7.6679e-07, 6.7904e-08, 1.0000e+00, 7.0361e-08,
        7.5130e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 145, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6841e-01, 3.7003e-01, 4.2284e-02, 2.2532e-06, 8.4961e-02, 2.3541e-06,
        1.3432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.283

[Epoch: 145, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([5.2144e-03, 2.7204e-03, 2.6055e-03, 2.2726e-06, 3.2925e-03, 2.8147e-03,
        9.8335e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 145, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0022, 0.0017, 0.0022, 0.0013, 0.9892, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 146, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4510e-06, 9.2297e-01, 7.4723e-02, 4.5231e-07, 2.3058e-03, 3.4960e-07,
        7.3849e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.128

[Epoch: 146, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.5866e-07, 2.3841e-08, 3.6170e-07, 1.1899e-08, 1.0000e+00, 6.7733e-09,
        5.3298e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.506

[Epoch: 146, batch: 141/236] total loss per batch: 0.392
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2303e-01, 4.0806e-01, 2.9833e-02, 1.3174e-06, 8.3290e-02, 3.7302e-06,
        1.5579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.226

[Epoch: 146, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.3248e-03, 2.1276e-03, 1.1191e-03, 1.8317e-06, 3.2217e-03, 1.8423e-03,
        9.8936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 146, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0021, 0.0016, 0.0021, 0.0017, 0.9883, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 147, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1469e-06, 9.1720e-01, 8.0525e-02, 5.3888e-07, 2.2689e-03, 2.7168e-07,
        5.1006e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.042

[Epoch: 147, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6672e-07, 5.3837e-08, 3.0875e-07, 1.8514e-08, 1.0000e+00, 9.0321e-09,
        5.9959e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 147, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6704e-01, 3.7899e-01, 4.1635e-02, 1.3629e-06, 8.3042e-02, 2.2300e-06,
        1.2929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.233

[Epoch: 147, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2999e-03, 2.1297e-03, 1.2011e-03, 1.7588e-06, 2.9997e-03, 2.2439e-03,
        9.8912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 147, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0010, 0.0014, 0.0020, 0.0015, 0.9899, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 148, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0492e-06, 9.1078e-01, 8.7674e-02, 3.9439e-07, 1.5426e-03, 2.1350e-07,
        9.7635e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.162

[Epoch: 148, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6087e-07, 7.2160e-08, 3.4286e-07, 1.4867e-08, 1.0000e+00, 1.7449e-08,
        1.9944e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.494

[Epoch: 148, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3161e-01, 3.9029e-01, 4.7161e-02, 1.7575e-06, 8.6024e-02, 3.0148e-06,
        1.4491e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.232

[Epoch: 148, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6908e-03, 1.9341e-03, 1.3802e-03, 1.2750e-06, 3.3287e-03, 2.0931e-03,
        9.8857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 148, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0019, 0.0014, 0.0020, 0.0023, 0.9886, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 149, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4797e-06, 9.0919e-01, 8.8447e-02, 5.5653e-07, 2.3626e-03, 3.2151e-07,
        9.7506e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.018

[Epoch: 149, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9971e-07, 9.1258e-08, 4.3901e-07, 2.0164e-08, 1.0000e+00, 1.9077e-08,
        3.0823e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.508

[Epoch: 149, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5469e-01, 4.0395e-01, 3.4102e-02, 1.5997e-06, 7.3490e-02, 2.6624e-06,
        1.3376e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 149, batch: 188/236] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8455e-03, 1.7042e-03, 1.7097e-03, 2.9182e-06, 5.9997e-03, 2.0302e-03,
        9.8571e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 149, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([1.8887e-03, 7.5449e-04, 7.9703e-04, 1.5317e-03, 1.6479e-03, 9.9153e-01,
        1.8500e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 150, batch: 47/236] total loss per batch: 0.380
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.1862e-07, 9.1895e-01, 7.9371e-02, 1.2743e-07, 1.6755e-03, 9.6521e-08,
        5.1828e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.120

[Epoch: 150, batch: 94/236] total loss per batch: 0.391
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1948e-07, 6.7924e-08, 2.8005e-07, 1.1102e-08, 1.0000e+00, 9.5492e-09,
        1.1168e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 150, batch: 141/236] total loss per batch: 0.394
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2544e-01, 4.0447e-01, 3.8420e-02, 1.2267e-06, 8.4657e-02, 1.3516e-06,
        1.4701e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.234

[Epoch: 150, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([5.1708e-03, 1.1234e-03, 1.0978e-03, 3.8445e-07, 3.3579e-03, 1.7051e-03,
        9.8754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 150, batch: 235/236] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([8.4546e-04, 1.2257e-03, 9.3483e-04, 2.1136e-03, 1.1554e-03, 9.9240e-01,
        1.3290e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 151, batch: 47/236] total loss per batch: 0.389
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5783e-06, 9.0229e-01, 9.6369e-02, 1.0386e-07, 1.3388e-03, 1.6909e-07,
        2.1022e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.082

[Epoch: 151, batch: 94/236] total loss per batch: 0.389
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.6586e-07, 9.6116e-08, 5.9869e-07, 1.0868e-08, 1.0000e+00, 1.0221e-07,
        8.5853e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.506

[Epoch: 151, batch: 141/236] total loss per batch: 0.399
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4503e-01, 3.6702e-01, 4.8526e-02, 6.3621e-07, 7.1166e-02, 6.2250e-07,
        1.6826e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.257

[Epoch: 151, batch: 188/236] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.7473e-03, 6.7985e-04, 1.6097e-03, 1.4608e-07, 2.5629e-03, 1.1167e-03,
        9.9228e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 151, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.0771e-04, 1.4482e-03, 9.0449e-04, 2.0149e-03, 1.6777e-03, 9.9171e-01,
        1.3356e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 152, batch: 47/236] total loss per batch: 0.381
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.8008e-07, 9.2641e-01, 7.2241e-02, 8.7129e-08, 1.3510e-03, 1.5583e-07,
        3.5465e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.041

[Epoch: 152, batch: 94/236] total loss per batch: 0.378
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.2675e-07, 5.1243e-08, 4.8332e-07, 1.1561e-08, 1.0000e+00, 5.8654e-08,
        5.9897e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 152, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5866e-01, 4.0362e-01, 3.7524e-02, 8.4238e-07, 6.7445e-02, 7.6401e-07,
        1.3275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 152, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.4805e-03, 1.1213e-03, 1.6060e-03, 2.7172e-07, 3.3682e-03, 1.5706e-03,
        9.8985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 152, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.4937e-04, 1.3970e-03, 8.9945e-04, 1.7936e-03, 2.0934e-03, 9.9123e-01,
        1.6339e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 153, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.3675e-07, 9.0939e-01, 8.9090e-02, 9.1640e-08, 1.5146e-03, 1.7769e-07,
        3.2076e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.070

[Epoch: 153, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.1425e-07, 5.4660e-08, 4.8300e-07, 1.2785e-08, 1.0000e+00, 5.7789e-08,
        8.6757e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 153, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5070e-01, 4.0058e-01, 3.7983e-02, 8.8588e-07, 7.3074e-02, 7.5944e-07,
        1.3767e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 153, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.6595e-03, 1.2518e-03, 1.5762e-03, 2.7513e-07, 3.4883e-03, 1.5721e-03,
        9.8945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 153, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.2947e-04, 1.3891e-03, 9.9988e-04, 1.8498e-03, 2.0237e-03, 9.9113e-01,
        1.6813e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 154, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.1738e-07, 9.1898e-01, 7.9591e-02, 8.3840e-08, 1.4281e-03, 1.6786e-07,
        3.3436e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.083

[Epoch: 154, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.9550e-07, 5.6807e-08, 4.8102e-07, 1.1560e-08, 1.0000e+00, 5.3467e-08,
        7.1935e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 154, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4812e-01, 3.9729e-01, 3.7650e-02, 8.5849e-07, 7.6946e-02, 7.1922e-07,
        1.3999e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 154, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8547e-03, 1.3117e-03, 1.5418e-03, 2.7269e-07, 3.4818e-03, 1.5314e-03,
        9.8928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 154, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.5747e-04, 1.3791e-03, 1.0958e-03, 1.8909e-03, 2.0427e-03, 9.9092e-01,
        1.7156e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 155, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([7.0681e-07, 9.1517e-01, 8.3379e-02, 7.7134e-08, 1.4459e-03, 1.6113e-07,
        3.1271e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.088

[Epoch: 155, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.6634e-07, 5.4541e-08, 4.5407e-07, 1.0234e-08, 1.0000e+00, 4.8975e-08,
        6.9770e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 155, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4728e-01, 3.9514e-01, 3.7851e-02, 8.0738e-07, 7.9458e-02, 6.7306e-07,
        1.4027e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 155, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9747e-03, 1.3600e-03, 1.5298e-03, 2.6406e-07, 3.5587e-03, 1.5208e-03,
        9.8906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 155, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([9.9027e-04, 1.3920e-03, 1.1705e-03, 1.9009e-03, 2.0895e-03, 9.9068e-01,
        1.7784e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 156, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.6758e-07, 9.1657e-01, 8.2009e-02, 7.0797e-08, 1.4182e-03, 1.5254e-07,
        3.1654e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 156, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.3495e-07, 4.7604e-08, 4.0691e-07, 8.8842e-09, 1.0000e+00, 4.3455e-08,
        6.4206e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 156, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4824e-01, 3.9276e-01, 3.7746e-02, 7.5117e-07, 8.0610e-02, 6.3075e-07,
        1.4065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 156, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0500e-03, 1.4035e-03, 1.5175e-03, 2.5488e-07, 3.6305e-03, 1.5077e-03,
        9.8889e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 156, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0010, 0.0014, 0.0012, 0.0019, 0.0021, 0.9905, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 157, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.4324e-07, 9.1672e-01, 8.1850e-02, 6.3321e-08, 1.4261e-03, 1.4757e-07,
        2.9828e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 157, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1373e-07, 4.4198e-08, 3.8483e-07, 7.6904e-09, 1.0000e+00, 3.9398e-08,
        6.2164e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 157, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4996e-01, 3.9103e-01, 3.7948e-02, 6.9712e-07, 8.1035e-02, 5.7537e-07,
        1.4002e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 157, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1432e-03, 1.4345e-03, 1.5233e-03, 2.3896e-07, 3.6283e-03, 1.5078e-03,
        9.8876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 157, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0014, 0.0013, 0.0019, 0.0021, 0.9904, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 158, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.1781e-07, 9.1506e-01, 8.3510e-02, 6.0403e-08, 1.4263e-03, 1.3743e-07,
        2.9606e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 158, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9401e-07, 4.0057e-08, 3.6026e-07, 6.9274e-09, 1.0000e+00, 3.5350e-08,
        5.5256e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 158, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5069e-01, 3.9021e-01, 3.8266e-02, 6.4490e-07, 8.0532e-02, 5.3768e-07,
        1.4030e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 158, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1657e-03, 1.4751e-03, 1.5142e-03, 2.3408e-07, 3.6568e-03, 1.5074e-03,
        9.8868e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 158, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0011, 0.0014, 0.0014, 0.0019, 0.0021, 0.9903, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 159, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.7832e-07, 9.1914e-01, 7.9450e-02, 5.1320e-08, 1.4111e-03, 1.2969e-07,
        2.7742e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 159, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.7111e-07, 3.5201e-08, 3.2283e-07, 5.7790e-09, 1.0000e+00, 3.1781e-08,
        5.3661e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 159, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5268e-01, 3.8829e-01, 3.8140e-02, 5.9649e-07, 8.1201e-02, 4.9860e-07,
        1.3969e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 159, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3276e-03, 1.5202e-03, 1.5247e-03, 2.1704e-07, 3.6062e-03, 1.5072e-03,
        9.8851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 159, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0014, 0.0014, 0.0019, 0.0021, 0.9902, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 160, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.6933e-07, 9.1289e-01, 8.5642e-02, 5.1684e-08, 1.4658e-03, 1.2683e-07,
        2.7820e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 160, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.5612e-07, 3.3491e-08, 3.0678e-07, 5.2881e-09, 1.0000e+00, 2.8642e-08,
        4.5230e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 160, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5092e-01, 3.9070e-01, 3.8459e-02, 5.3382e-07, 7.9186e-02, 4.4709e-07,
        1.4074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 160, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2922e-03, 1.5419e-03, 1.5090e-03, 2.1968e-07, 3.5780e-03, 1.5222e-03,
        9.8856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 160, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0014, 0.0015, 0.0019, 0.0021, 0.9902, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 161, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.2683e-07, 9.2310e-01, 7.5494e-02, 4.3526e-08, 1.4037e-03, 1.1398e-07,
        2.7852e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.091

[Epoch: 161, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4205e-07, 2.9732e-08, 2.7784e-07, 4.4899e-09, 1.0000e+00, 2.6558e-08,
        5.0542e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 161, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5530e-01, 3.8605e-01, 3.7909e-02, 5.2394e-07, 8.1577e-02, 4.2336e-07,
        1.3916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 161, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3698e-03, 1.6213e-03, 1.5581e-03, 1.9166e-07, 3.5915e-03, 1.5084e-03,
        9.8835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 161, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0015, 0.0014, 0.0018, 0.0020, 0.9902, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 162, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.5327e-07, 9.0580e-01, 9.2672e-02, 5.0556e-08, 1.5306e-03, 1.2399e-07,
        2.3834e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 162, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2590e-07, 2.7522e-08, 2.7160e-07, 4.0954e-09, 1.0000e+00, 2.2088e-08,
        3.7977e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 162, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4963e-01, 3.9128e-01, 3.9853e-02, 4.3379e-07, 7.8152e-02, 3.7818e-07,
        1.4109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 162, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3795e-03, 1.5893e-03, 1.5179e-03, 2.1510e-07, 3.5122e-03, 1.5273e-03,
        9.8847e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 162, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0015, 0.0016, 0.0019, 0.0020, 0.9898, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 163, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.3635e-07, 9.2076e-01, 7.7797e-02, 4.0792e-08, 1.4391e-03, 1.0100e-07,
        3.0275e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 163, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2028e-07, 2.2495e-08, 2.4647e-07, 3.6346e-09, 1.0000e+00, 2.2982e-08,
        4.0478e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 163, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5982e-01, 3.8459e-01, 3.7279e-02, 4.6649e-07, 8.2552e-02, 3.8963e-07,
        1.3576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 163, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3907e-03, 1.7527e-03, 1.5257e-03, 1.9338e-07, 3.5458e-03, 1.4493e-03,
        9.8834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 163, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0013, 0.0014, 0.0014, 0.0018, 0.0018, 0.9907, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 164, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.0846e-07, 9.1397e-01, 8.4495e-02, 5.1670e-08, 1.5358e-03, 1.2276e-07,
        2.2862e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.097

[Epoch: 164, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1950e-07, 2.8135e-08, 2.6505e-07, 4.4279e-09, 1.0000e+00, 1.8694e-08,
        3.7935e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 164, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4538e-01, 3.8700e-01, 4.1408e-02, 4.6218e-07, 7.8026e-02, 3.5559e-07,
        1.4819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.257

[Epoch: 164, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1076e-03, 1.5909e-03, 1.5608e-03, 2.2580e-07, 3.5893e-03, 1.6979e-03,
        9.8845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 164, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0016, 0.0015, 0.0023, 0.0021, 0.9890, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 165, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([5.8148e-07, 9.2090e-01, 7.7528e-02, 3.5910e-08, 1.5737e-03, 8.6320e-08,
        2.6300e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.086

[Epoch: 165, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1933e-07, 1.8891e-08, 2.7522e-07, 3.8146e-09, 1.0000e+00, 2.3281e-08,
        4.3261e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 165, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6607e-01, 3.8605e-01, 3.6421e-02, 5.6400e-07, 8.1810e-02, 4.7363e-07,
        1.2965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 165, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.2285e-03, 1.9936e-03, 1.5936e-03, 2.5480e-07, 3.2176e-03, 1.4166e-03,
        9.8755e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 165, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0017, 0.0016, 0.0017, 0.0019, 0.9905, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 166, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([6.3550e-07, 9.0911e-01, 8.9242e-02, 7.4193e-08, 1.6438e-03, 1.7007e-07,
        2.5652e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.101

[Epoch: 166, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4545e-07, 3.2886e-08, 3.3579e-07, 6.2910e-09, 1.0000e+00, 2.3543e-08,
        3.6789e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 166, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4350e-01, 3.8852e-01, 4.1366e-02, 7.4011e-07, 8.2059e-02, 5.7704e-07,
        1.4455e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 166, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.1451e-03, 1.4291e-03, 1.5877e-03, 3.0689e-07, 3.0397e-03, 1.8424e-03,
        9.8896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 166, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0014, 0.0014, 0.0018, 0.0022, 0.9894, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 167, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([8.7070e-07, 9.2113e-01, 7.7211e-02, 7.2678e-08, 1.6595e-03, 1.1430e-07,
        3.2078e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.089

[Epoch: 167, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.3772e-07, 2.4036e-08, 3.1879e-07, 6.8999e-09, 1.0000e+00, 1.6614e-08,
        3.2289e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 167, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5866e-01, 3.7720e-01, 3.6188e-02, 7.1918e-07, 8.1045e-02, 6.2404e-07,
        1.4691e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 167, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.1423e-03, 2.3333e-03, 1.8414e-03, 3.4892e-07, 3.8108e-03, 1.6359e-03,
        9.8624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 167, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0015, 0.0015, 0.0019, 0.0021, 0.9898, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 168, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0164e-06, 9.1524e-01, 8.2858e-02, 8.6573e-08, 1.8982e-03, 1.7186e-07,
        3.1376e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.093

[Epoch: 168, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.2209e-07, 5.0937e-08, 4.8986e-07, 8.4023e-09, 1.0000e+00, 3.5003e-08,
        4.8200e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 168, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5362e-01, 4.0923e-01, 3.8097e-02, 1.1928e-06, 7.1200e-02, 8.2406e-07,
        1.2785e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.253

[Epoch: 168, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.2965e-03, 2.0287e-03, 1.6127e-03, 4.9067e-07, 2.7547e-03, 1.7878e-03,
        9.8952e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 168, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0017, 0.0016, 0.0016, 0.0021, 0.9898, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 169, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.3684e-07, 9.1336e-01, 8.4540e-02, 1.0447e-07, 2.1029e-03, 1.7421e-07,
        2.6334e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.101

[Epoch: 169, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.7455e-07, 4.3978e-08, 3.7159e-07, 8.7577e-09, 1.0000e+00, 4.5782e-08,
        8.5984e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 169, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5119e-01, 3.5843e-01, 3.9105e-02, 9.0204e-07, 9.8424e-02, 7.8497e-07,
        1.5285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.251

[Epoch: 169, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4467e-03, 2.2561e-03, 1.7378e-03, 5.4391e-07, 3.5230e-03, 1.7042e-03,
        9.8733e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 169, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0021, 0.0023, 0.0018, 0.0023, 0.0020, 0.9880, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 170, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2421e-06, 9.1598e-01, 8.2193e-02, 1.0873e-07, 1.8234e-03, 1.7083e-07,
        4.5784e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.075

[Epoch: 170, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8737e-07, 2.6656e-08, 3.4726e-07, 7.3305e-09, 1.0000e+00, 3.4934e-08,
        6.1348e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 170, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6091e-01, 4.0176e-01, 3.7887e-02, 1.2086e-06, 6.8307e-02, 8.3660e-07,
        1.3113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.247

[Epoch: 170, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4218e-03, 2.1633e-03, 1.8970e-03, 6.1091e-07, 3.0472e-03, 1.8565e-03,
        9.8761e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 170, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0017, 0.0017, 0.0014, 0.0021, 0.9898, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 171, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.1276e-07, 9.1885e-01, 7.9457e-02, 1.1728e-07, 1.6898e-03, 1.9982e-07,
        3.0750e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.110

[Epoch: 171, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7317e-07, 7.3635e-08, 5.8888e-07, 1.1083e-08, 1.0000e+00, 4.7044e-08,
        3.9967e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.488

[Epoch: 171, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5816e-01, 3.7999e-01, 3.8500e-02, 1.1656e-06, 8.4758e-02, 6.0997e-07,
        1.3858e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 171, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2295e-03, 2.4680e-03, 1.5575e-03, 7.0962e-07, 3.4271e-03, 1.7786e-03,
        9.8754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 171, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0015, 0.0017, 0.0023, 0.0015, 0.9897, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 172, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.7449e-07, 9.1631e-01, 8.1803e-02, 1.2386e-07, 1.8857e-03, 1.7759e-07,
        3.2990e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.075

[Epoch: 172, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9493e-07, 2.6915e-08, 3.3476e-07, 6.4770e-09, 1.0000e+00, 2.9075e-08,
        6.3080e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 172, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5341e-01, 3.8894e-01, 3.5326e-02, 1.1276e-06, 7.4271e-02, 6.8218e-07,
        1.4805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 172, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4050e-03, 2.0446e-03, 1.7002e-03, 7.0770e-07, 3.9168e-03, 1.6150e-03,
        9.8732e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 172, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0020, 0.0019, 0.0014, 0.0018, 0.9897, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 173, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.4946e-07, 9.1576e-01, 8.2549e-02, 1.1178e-07, 1.6926e-03, 1.8247e-07,
        5.4711e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.103

[Epoch: 173, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.1061e-07, 1.0338e-07, 8.7431e-07, 1.4679e-08, 1.0000e+00, 4.9274e-08,
        1.2927e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 173, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4898e-01, 3.8183e-01, 4.3633e-02, 1.4452e-06, 8.8086e-02, 8.5287e-07,
        1.3747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 173, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3535e-03, 2.1788e-03, 1.4583e-03, 8.2157e-07, 3.2881e-03, 1.7408e-03,
        9.8798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 173, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0018, 0.0019, 0.0021, 0.0017, 0.9891, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 174, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1353e-06, 9.1585e-01, 8.2179e-02, 2.0353e-07, 1.9664e-03, 2.6533e-07,
        2.9095e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 174, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.2166e-07, 2.9608e-08, 3.4903e-07, 5.6414e-09, 1.0000e+00, 3.8924e-08,
        4.5983e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 174, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7109e-01, 3.8141e-01, 3.6853e-02, 1.3451e-06, 7.7415e-02, 8.5037e-07,
        1.3324e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 174, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7485e-03, 1.7585e-03, 1.8111e-03, 6.0776e-07, 3.3823e-03, 1.7032e-03,
        9.8860e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 174, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0018, 0.0013, 0.0012, 0.0018, 0.9909, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 175, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.7323e-07, 9.1862e-01, 7.9595e-02, 1.3232e-07, 1.7845e-03, 1.6987e-07,
        5.1754e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.092

[Epoch: 175, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.7974e-07, 1.1417e-07, 6.2054e-07, 1.7459e-08, 1.0000e+00, 4.1515e-08,
        7.3832e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.507

[Epoch: 175, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4263e-01, 3.9870e-01, 3.7531e-02, 1.2533e-06, 7.3387e-02, 9.7778e-07,
        1.4775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.239

[Epoch: 175, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7885e-03, 1.9560e-03, 1.4264e-03, 7.3327e-07, 2.9804e-03, 1.4286e-03,
        9.8842e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 175, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0016, 0.0018, 0.0020, 0.0023, 0.9893, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 176, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5809e-06, 9.1056e-01, 8.7471e-02, 3.3155e-07, 1.9628e-03, 2.3182e-07,
        4.7966e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.097

[Epoch: 176, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.2339e-07, 3.4234e-08, 4.4305e-07, 7.1021e-09, 1.0000e+00, 3.3350e-08,
        4.4127e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 176, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5213e-01, 3.7881e-01, 3.9716e-02, 1.9878e-06, 8.8778e-02, 1.2749e-06,
        1.4057e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.265

[Epoch: 176, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0767e-03, 1.7135e-03, 1.9902e-03, 8.7540e-07, 2.8777e-03, 1.8474e-03,
        9.8849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 176, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0016, 0.0017, 0.0015, 0.0016, 0.9899, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 177, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1600e-06, 9.1905e-01, 7.9244e-02, 1.6505e-07, 1.7083e-03, 2.1286e-07,
        5.0371e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.081

[Epoch: 177, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([6.8203e-07, 2.0562e-07, 1.6289e-06, 4.6894e-08, 1.0000e+00, 1.6069e-07,
        1.4633e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 177, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6564e-01, 3.8039e-01, 4.1403e-02, 1.4334e-06, 8.6044e-02, 1.0366e-06,
        1.2652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 177, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.9404e-03, 2.2719e-03, 1.9880e-03, 8.6706e-07, 3.2023e-03, 1.4946e-03,
        9.8710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 177, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0020, 0.0015, 0.0015, 0.0018, 0.9901, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 178, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0320e-06, 9.1084e-01, 8.7283e-02, 2.9157e-07, 1.8716e-03, 3.1161e-07,
        6.8937e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.098

[Epoch: 178, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.7639e-07, 4.7632e-08, 6.9850e-07, 8.4417e-09, 1.0000e+00, 3.4944e-08,
        5.5198e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 178, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4493e-01, 3.9501e-01, 3.5427e-02, 1.2301e-06, 7.1034e-02, 1.2784e-06,
        1.5359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.263

[Epoch: 178, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.2390e-03, 2.0545e-03, 1.8893e-03, 7.8274e-07, 3.6328e-03, 1.9401e-03,
        9.8624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 178, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0013, 0.0017, 0.0013, 0.0016, 0.9906, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 179, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3556e-06, 9.2615e-01, 7.2218e-02, 1.6489e-07, 1.6281e-03, 1.9457e-07,
        3.3212e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.096

[Epoch: 179, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([4.7043e-07, 1.4527e-07, 1.0822e-06, 2.8172e-08, 1.0000e+00, 1.3346e-07,
        1.2340e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 179, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6457e-01, 3.8473e-01, 3.8402e-02, 1.3038e-06, 8.0086e-02, 1.0083e-06,
        1.3221e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.242

[Epoch: 179, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7450e-03, 2.2374e-03, 1.7586e-03, 1.0917e-06, 3.6954e-03, 1.5677e-03,
        9.8799e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 179, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0012, 0.0023, 0.0013, 0.0013, 0.0015, 0.9910, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 180, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1787e-06, 9.0779e-01, 9.0173e-02, 2.2202e-07, 2.0328e-03, 2.8109e-07,
        6.5838e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.087

[Epoch: 180, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.0736e-07, 7.5380e-08, 9.8413e-07, 1.2096e-08, 1.0000e+00, 4.5707e-08,
        1.2143e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.502

[Epoch: 180, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4107e-01, 3.8295e-01, 3.7862e-02, 1.2747e-06, 9.4175e-02, 1.3332e-06,
        1.4394e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 180, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3581e-03, 1.7481e-03, 1.8027e-03, 7.5341e-07, 2.9490e-03, 1.7242e-03,
        9.8842e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 180, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0015, 0.0017, 0.0018, 0.0016, 0.9899, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 181, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4452e-06, 9.1498e-01, 8.3260e-02, 3.9092e-07, 1.7532e-03, 2.2447e-07,
        3.8296e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.111

[Epoch: 181, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7938e-07, 6.6552e-08, 7.7323e-07, 1.6415e-08, 1.0000e+00, 7.0698e-08,
        4.2231e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 181, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7102e-01, 3.8025e-01, 4.0684e-02, 1.4184e-06, 6.7067e-02, 1.3508e-06,
        1.4098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.245

[Epoch: 181, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9655e-03, 2.2022e-03, 1.7039e-03, 1.0158e-06, 3.9479e-03, 1.7164e-03,
        9.8746e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 181, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0017, 0.0013, 0.0015, 0.0022, 0.9901, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 182, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0431e-06, 9.2380e-01, 7.4340e-02, 2.3474e-07, 1.8593e-03, 1.8860e-07,
        4.0780e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.086

[Epoch: 182, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.4490e-07, 7.6839e-08, 1.1602e-06, 1.6129e-08, 1.0000e+00, 7.2757e-08,
        6.5502e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.499

[Epoch: 182, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5698e-01, 3.9222e-01, 3.3874e-02, 1.1882e-06, 8.1050e-02, 9.1186e-07,
        1.3588e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.259

[Epoch: 182, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7296e-03, 1.5371e-03, 1.3640e-03, 7.8402e-07, 3.5475e-03, 1.4533e-03,
        9.8837e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 182, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0016, 0.0018, 0.0016, 0.0017, 0.9902, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 183, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([9.6783e-07, 9.1168e-01, 8.6908e-02, 2.7060e-07, 1.4145e-03, 1.5749e-07,
        3.5193e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.098

[Epoch: 183, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7193e-07, 1.0094e-07, 8.1572e-07, 1.5730e-08, 1.0000e+00, 8.9949e-08,
        1.1507e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.504

[Epoch: 183, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3713e-01, 3.7942e-01, 4.7990e-02, 1.4937e-06, 8.7031e-02, 1.4714e-06,
        1.4843e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.260

[Epoch: 183, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.6005e-03, 2.2058e-03, 1.8628e-03, 5.8382e-07, 2.5837e-03, 2.1250e-03,
        9.8762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 183, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0017, 0.0012, 0.0014, 0.0014, 0.9910, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 184, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.5987e-06, 9.1006e-01, 8.7788e-02, 3.1073e-07, 2.1498e-03, 3.1653e-07,
        6.2806e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.090

[Epoch: 184, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.9836e-07, 7.0449e-08, 1.2709e-06, 2.2779e-08, 1.0000e+00, 6.0903e-08,
        9.8850e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 184, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7503e-01, 3.8125e-01, 2.9260e-02, 1.8041e-06, 7.6257e-02, 1.3874e-06,
        1.3819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.233

[Epoch: 184, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3345e-03, 1.5935e-03, 1.7513e-03, 9.9893e-07, 2.9236e-03, 1.2400e-03,
        9.8916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 184, batch: 235/236] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0023, 0.0026, 0.0018, 0.0018, 0.0027, 0.9868, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 185, batch: 47/236] total loss per batch: 0.379
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2033e-06, 9.2849e-01, 7.0208e-02, 2.8963e-07, 1.3030e-03, 2.6554e-07,
        1.1290e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.066

[Epoch: 185, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.0173e-07, 1.5894e-07, 1.3262e-06, 1.7428e-08, 1.0000e+00, 7.9699e-08,
        1.8440e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 185, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.2690e-01, 4.1014e-01, 3.9706e-02, 1.3964e-06, 8.8564e-02, 8.9416e-07,
        1.3469e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.227

[Epoch: 185, batch: 188/236] total loss per batch: 0.378
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.4992e-03, 1.6483e-03, 2.3471e-03, 1.1106e-06, 2.7157e-03, 1.7746e-03,
        9.9001e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 185, batch: 235/236] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0014, 0.0016, 0.0017, 0.0019, 0.9902, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 186, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3204e-06, 9.1920e-01, 7.9392e-02, 2.9287e-07, 1.4039e-03, 1.7425e-07,
        1.7378e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.123

[Epoch: 186, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([3.3101e-07, 1.6929e-07, 1.5148e-06, 5.0339e-08, 1.0000e+00, 1.0623e-07,
        3.2819e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 186, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5222e-01, 3.7492e-01, 3.9463e-02, 1.3229e-06, 8.5798e-02, 1.0892e-06,
        1.4760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.266

[Epoch: 186, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.7244e-03, 2.2431e-03, 2.0107e-03, 1.7997e-06, 3.3717e-03, 1.5578e-03,
        9.8809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 186, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0019, 0.0018, 0.0014, 0.0019, 0.9898, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 187, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4748e-06, 9.0068e-01, 9.7365e-02, 3.8116e-07, 1.9466e-03, 2.3903e-07,
        1.7268e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.057

[Epoch: 187, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.8331e-07, 1.5398e-07, 1.2281e-06, 2.9319e-08, 1.0000e+00, 1.0488e-07,
        1.7863e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.496

[Epoch: 187, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6348e-01, 3.7824e-01, 3.7199e-02, 1.0841e-06, 7.7844e-02, 8.9846e-07,
        1.4323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.231

[Epoch: 187, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2337e-03, 1.8341e-03, 2.0703e-03, 1.5529e-06, 2.5417e-03, 1.5513e-03,
        9.8877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 187, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0015, 0.0019, 0.0015, 0.0020, 0.9896, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 188, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3614e-06, 9.2591e-01, 7.2314e-02, 2.7685e-07, 1.7687e-03, 1.5975e-07,
        1.4996e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.107

[Epoch: 188, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.8134e-07, 6.7577e-08, 7.8480e-07, 1.7965e-08, 1.0000e+00, 4.6788e-08,
        1.3619e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 188, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4885e-01, 4.0032e-01, 3.5901e-02, 1.0626e-06, 7.9476e-02, 9.2443e-07,
        1.3545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.267

[Epoch: 188, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2965e-03, 2.0782e-03, 2.0857e-03, 1.5259e-06, 3.5721e-03, 1.8097e-03,
        9.8716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 188, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0017, 0.0018, 0.0015, 0.0018, 0.9898, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 189, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.2278e-06, 9.1383e-01, 8.4379e-02, 2.2380e-07, 1.7842e-03, 1.5734e-07,
        1.0997e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.082

[Epoch: 189, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.6659e-07, 5.9480e-08, 7.2776e-07, 1.3931e-08, 1.0000e+00, 4.3030e-08,
        1.0430e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 189, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5478e-01, 3.8185e-01, 3.9751e-02, 9.5189e-07, 8.0633e-02, 8.2877e-07,
        1.4298e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.243

[Epoch: 189, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.0014e-03, 1.7327e-03, 1.8112e-03, 1.1446e-06, 3.0036e-03, 1.5469e-03,
        9.8890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 189, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0017, 0.0018, 0.0015, 0.0020, 0.9896, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 190, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1599e-06, 9.2184e-01, 7.6206e-02, 2.0062e-07, 1.9542e-03, 1.5419e-07,
        8.4033e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.102

[Epoch: 190, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.7832e-07, 8.0330e-08, 7.9961e-07, 1.4087e-08, 1.0000e+00, 4.6027e-08,
        1.0346e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.495

[Epoch: 190, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5432e-01, 3.9091e-01, 3.7738e-02, 7.8749e-07, 7.8419e-02, 7.1889e-07,
        1.3861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.262

[Epoch: 190, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3279e-03, 1.7637e-03, 1.8442e-03, 1.1852e-06, 3.2710e-03, 1.7020e-03,
        9.8809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 190, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0017, 0.0018, 0.0014, 0.0017, 0.9901, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 191, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0110e-06, 9.1292e-01, 8.5264e-02, 2.1251e-07, 1.8142e-03, 1.5580e-07,
        1.1532e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.086

[Epoch: 191, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.2846e-07, 4.6158e-08, 5.8441e-07, 8.2529e-09, 1.0000e+00, 3.4930e-08,
        8.3423e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 191, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5599e-01, 3.8557e-01, 3.8722e-02, 7.5003e-07, 8.0562e-02, 7.0336e-07,
        1.3916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.250

[Epoch: 191, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.2362e-03, 1.7655e-03, 1.7261e-03, 9.5769e-07, 2.9665e-03, 1.7347e-03,
        9.8857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 191, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0015, 0.0019, 0.0017, 0.0018, 0.9897, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 192, batch: 47/236] total loss per batch: 0.377
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0001e-06, 9.1542e-01, 8.2786e-02, 1.8013e-07, 1.7930e-03, 1.1905e-07,
        6.4620e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.099

[Epoch: 192, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.1140e-07, 4.2700e-08, 5.4344e-07, 8.4850e-09, 1.0000e+00, 2.6485e-08,
        6.5986e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.497

[Epoch: 192, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4632e-01, 3.9084e-01, 3.9451e-02, 7.0502e-07, 8.1942e-02, 5.9856e-07,
        1.4145e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.256

[Epoch: 192, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.9942e-03, 1.7849e-03, 2.0769e-03, 9.9097e-07, 3.3702e-03, 1.6082e-03,
        9.8816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 192, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0015, 0.0017, 0.0013, 0.0018, 0.9902, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 193, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1932e-06, 9.1755e-01, 8.0436e-02, 2.1000e-07, 2.0139e-03, 1.6006e-07,
        9.3355e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.084

[Epoch: 193, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.7776e-08, 4.1749e-08, 5.3835e-07, 6.9287e-09, 1.0000e+00, 3.1918e-08,
        5.7913e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 193, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6059e-01, 3.7815e-01, 3.9626e-02, 7.0206e-07, 8.1112e-02, 6.6063e-07,
        1.4053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.254

[Epoch: 193, batch: 188/236] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.7833e-03, 1.7618e-03, 1.6590e-03, 1.1090e-06, 3.7398e-03, 1.9848e-03,
        9.8707e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 193, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0013, 0.0016, 0.0018, 0.0016, 0.9905, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 194, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.0482e-06, 9.1829e-01, 8.0083e-02, 1.8501e-07, 1.6213e-03, 1.3430e-07,
        4.2715e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.103

[Epoch: 194, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([9.9305e-08, 3.3977e-08, 4.5478e-07, 7.0875e-09, 1.0000e+00, 2.2002e-08,
        5.5939e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.498

[Epoch: 194, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.4418e-01, 3.9584e-01, 3.5718e-02, 9.5909e-07, 7.8293e-02, 5.9029e-07,
        1.4596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.239

[Epoch: 194, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.3940e-03, 1.6750e-03, 1.5093e-03, 9.9234e-07, 3.2085e-03, 1.3648e-03,
        9.8885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 194, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0018, 0.0018, 0.0018, 0.0016, 0.0020, 0.9892, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 195, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1272e-06, 9.0945e-01, 8.8179e-02, 2.4196e-07, 2.3722e-03, 1.7826e-07,
        7.4112e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.081

[Epoch: 195, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4160e-07, 7.6034e-08, 8.0327e-07, 1.2748e-08, 1.0000e+00, 2.8967e-08,
        1.1889e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 195, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 0
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.7660e-01, 3.7352e-01, 4.1037e-02, 9.9737e-07, 8.5982e-02, 9.7647e-07,
        1.2286e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.264

[Epoch: 195, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([2.8446e-03, 1.9438e-03, 1.8106e-03, 1.2740e-06, 3.0626e-03, 1.9542e-03,
        9.8838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 195, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0015, 0.0013, 0.0017, 0.0014, 0.9911, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 196, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7880e-06, 9.2219e-01, 7.5503e-02, 3.7089e-07, 2.3041e-03, 2.3310e-07,
        5.8375e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.116

[Epoch: 196, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.7539e-07, 9.2678e-08, 8.5609e-07, 1.7175e-08, 1.0000e+00, 4.4354e-08,
        6.3554e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.503

[Epoch: 196, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.3102e-01, 3.8880e-01, 4.1182e-02, 1.8134e-06, 7.9212e-02, 1.0965e-06,
        1.5978e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.266

[Epoch: 196, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([4.4797e-03, 2.0630e-03, 1.8729e-03, 2.8986e-06, 4.3840e-03, 1.7297e-03,
        9.8547e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 196, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0022, 0.0023, 0.0015, 0.0018, 0.0014, 0.9886, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 197, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.1651e-06, 9.1717e-01, 8.0687e-02, 3.4990e-07, 2.1381e-03, 1.7901e-07,
        4.2738e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.074

[Epoch: 197, batch: 94/236] total loss per batch: 0.377
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.4611e-07, 3.2401e-08, 5.6157e-07, 1.4536e-08, 1.0000e+00, 3.2482e-08,
        1.7710e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.500

[Epoch: 197, batch: 141/236] total loss per batch: 0.391
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.6318e-01, 3.9933e-01, 3.3448e-02, 1.3719e-06, 8.1427e-02, 1.1323e-06,
        1.2261e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.208

[Epoch: 197, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([1.9235e-03, 2.1352e-03, 1.3164e-03, 2.0501e-06, 3.2447e-03, 1.3092e-03,
        9.9007e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 197, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0014, 0.0018, 0.0016, 0.0022, 0.0020, 0.9881, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 198, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.4053e-06, 9.1319e-01, 8.4920e-02, 3.5199e-07, 1.8856e-03, 2.2081e-07,
        4.7795e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.094

[Epoch: 198, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.0626e-07, 7.2551e-08, 7.7713e-07, 1.5450e-08, 1.0000e+00, 4.4714e-08,
        6.6829e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.492

[Epoch: 198, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5383e-01, 3.6182e-01, 4.5833e-02, 1.6887e-06, 8.2145e-02, 1.4835e-06,
        1.5637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.249

[Epoch: 198, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.6267e-03, 1.5004e-03, 1.5780e-03, 1.4460e-06, 3.2880e-03, 1.8748e-03,
        9.8813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 198, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0016, 0.0017, 0.0021, 0.0020, 0.0019, 0.9884, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 199, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.3942e-06, 9.1751e-01, 8.0598e-02, 4.2397e-07, 1.8848e-03, 2.7836e-07,
        8.0479e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.095

[Epoch: 199, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([2.1127e-07, 1.5287e-07, 6.6539e-07, 2.2921e-08, 1.0000e+00, 7.3636e-08,
        1.2155e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.505

[Epoch: 199, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5141e-01, 4.1028e-01, 3.9858e-02, 1.4369e-06, 7.4801e-02, 8.0688e-07,
        1.2365e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.258

[Epoch: 199, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.4764e-03, 1.7732e-03, 1.4172e-03, 1.9508e-06, 2.8766e-03, 1.6947e-03,
        9.8876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 199, batch: 235/236] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0015, 0.0015, 0.0014, 0.0016, 0.0016, 0.9905, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 200, batch: 47/236] total loss per batch: 0.378
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.9167, 0.0817, 0.0000, 0.0017, 0.0000, 0.0000])
Policy pred: tensor([1.7601e-06, 9.1334e-01, 8.4534e-02, 4.6369e-07, 2.1207e-03, 2.0891e-07,
        8.1241e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.092 -0.098

[Epoch: 200, batch: 94/236] total loss per batch: 0.376
Policy (actual, predicted): 4 4
Policy data: tensor([0., 0., 0., 0., 1., 0., 0.])
Policy pred: tensor([1.9242e-07, 5.5921e-08, 6.3760e-07, 1.3118e-08, 1.0000e+00, 3.1338e-08,
        9.9732e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.500 -0.491

[Epoch: 200, batch: 141/236] total loss per batch: 0.390
Policy (actual, predicted): 1 1
Policy data: tensor([0.3533, 0.3883, 0.0383, 0.0000, 0.0800, 0.0000, 0.1400])
Policy pred: tensor([3.5717e-01, 3.6647e-01, 3.6680e-02, 1.2504e-06, 8.7537e-02, 1.2762e-06,
        1.5214e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.251 0.248

[Epoch: 200, batch: 188/236] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0017, 0.0017, 0.0000, 0.0033, 0.0017, 0.9883])
Policy pred: tensor([3.6262e-03, 1.3305e-03, 1.7250e-03, 1.5005e-06, 3.5957e-03, 1.9692e-03,
        9.8775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 200, batch: 235/236] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.9900, 0.0017])
Policy pred: tensor([0.0017, 0.0016, 0.0016, 0.0015, 0.0016, 0.9904, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

