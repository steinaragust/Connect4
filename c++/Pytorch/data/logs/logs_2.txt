Training set samples: 6975
Batch size: 32
[Epoch: 1, batch: 43/218] total loss per batch: 1.418
Policy (actual, predicted): 0 3
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([3.5134e-02, 1.3388e-01, 3.8766e-02, 7.5441e-01, 3.3833e-05, 2.2585e-02,
        1.5194e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.007

[Epoch: 1, batch: 86/218] total loss per batch: 1.324
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([4.4584e-01, 1.4107e-01, 3.7226e-02, 6.8384e-05, 2.7005e-01, 3.9561e-02,
        6.6181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 1, batch: 129/218] total loss per batch: 1.220
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1882e-01, 6.7905e-05, 6.9761e-02, 4.9850e-01, 8.9992e-05, 2.3776e-01,
        7.4998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.022

[Epoch: 1, batch: 172/218] total loss per batch: 1.184
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.5501e-03, 1.3879e-05, 9.7086e-01, 1.5161e-02, 1.2252e-05, 5.5199e-06,
        1.0397e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 -0.290

[Epoch: 1, batch: 215/218] total loss per batch: 1.270
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.0292e-02, 6.8701e-01, 1.2867e-02, 9.0713e-08, 6.5825e-06, 1.5223e-01,
        5.7594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 2, batch: 43/218] total loss per batch: 0.996
Policy (actual, predicted): 0 3
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([2.0132e-01, 9.2521e-02, 8.0679e-02, 5.8015e-01, 9.5778e-05, 2.7523e-02,
        1.7703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.005

[Epoch: 2, batch: 86/218] total loss per batch: 0.940
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([6.6216e-01, 8.1738e-02, 3.1724e-02, 3.7691e-05, 1.8846e-01, 2.0262e-02,
        1.5622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.018

[Epoch: 2, batch: 129/218] total loss per batch: 0.826
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3563e-02, 1.4074e-05, 3.5956e-02, 7.6752e-01, 1.4974e-05, 1.0916e-01,
        7.3775e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 0.001

[Epoch: 2, batch: 172/218] total loss per batch: 0.816
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4611e-03, 1.4248e-05, 9.8653e-01, 2.6413e-03, 3.7293e-06, 1.3857e-06,
        5.3484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 -0.104

[Epoch: 2, batch: 215/218] total loss per batch: 0.861
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([2.7566e-02, 8.1357e-01, 4.7551e-03, 3.1609e-08, 1.9180e-06, 9.0106e-02,
        6.4001e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.000

[Epoch: 3, batch: 43/218] total loss per batch: 0.734
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([7.5438e-01, 2.7950e-02, 1.0138e-01, 5.6973e-02, 2.0123e-05, 4.4604e-02,
        1.4693e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.006

[Epoch: 3, batch: 86/218] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.0862e-01, 4.4519e-02, 1.1777e-02, 7.1660e-06, 2.6447e-02, 7.1530e-03,
        1.4788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.024

[Epoch: 3, batch: 129/218] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.4197e-03, 9.3349e-06, 3.1392e-02, 8.7951e-01, 4.1684e-06, 5.1040e-02,
        3.5624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 0.001

[Epoch: 3, batch: 172/218] total loss per batch: 0.650
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.6810e-03, 6.6860e-05, 9.8621e-01, 5.3995e-03, 2.7755e-06, 2.9820e-06,
        4.6412e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.008

[Epoch: 3, batch: 215/218] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([4.8649e-03, 9.6173e-01, 1.0284e-03, 5.5568e-09, 1.7177e-06, 2.0947e-02,
        1.1424e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.005

[Epoch: 4, batch: 43/218] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([7.6817e-01, 6.2414e-02, 5.7818e-02, 4.1310e-02, 2.7878e-05, 5.7811e-02,
        1.2452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.015

[Epoch: 4, batch: 86/218] total loss per batch: 0.618
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([7.0694e-01, 7.7359e-02, 4.7484e-02, 4.8482e-05, 1.3325e-01, 1.3194e-02,
        2.1727e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.021

[Epoch: 4, batch: 129/218] total loss per batch: 0.596
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.5319e-03, 1.3268e-06, 7.7856e-03, 9.6882e-01, 5.0097e-06, 1.0793e-02,
        1.0067e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.007

[Epoch: 4, batch: 172/218] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([9.9798e-03, 1.0106e-04, 9.7774e-01, 5.5443e-03, 5.1789e-06, 3.5449e-06,
        6.6292e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.112

[Epoch: 4, batch: 215/218] total loss per batch: 0.624
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([5.9512e-03, 9.4210e-01, 2.7009e-03, 6.6582e-09, 7.9757e-07, 1.1079e-02,
        3.8171e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 5, batch: 43/218] total loss per batch: 0.619
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7114e-01, 3.4245e-02, 3.1045e-02, 1.7718e-02, 1.2362e-05, 3.6602e-02,
        9.2373e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.019

[Epoch: 5, batch: 86/218] total loss per batch: 0.596
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.3143e-01, 4.2912e-02, 3.5712e-02, 5.0937e-05, 4.5628e-02, 3.7880e-02,
        6.3915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.018

[Epoch: 5, batch: 129/218] total loss per batch: 0.577
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5352e-04, 5.1794e-06, 7.4877e-03, 9.6998e-01, 3.2956e-06, 1.0527e-02,
        1.1239e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 0.011

[Epoch: 5, batch: 172/218] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([1.4607e-02, 7.7106e-05, 9.4728e-01, 1.3369e-02, 1.2144e-05, 5.3957e-06,
        2.4650e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.118

[Epoch: 5, batch: 215/218] total loss per batch: 0.595
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([4.6856e-03, 9.5273e-01, 3.0432e-03, 4.7565e-08, 2.4707e-06, 1.8998e-02,
        2.0536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 6, batch: 43/218] total loss per batch: 0.600
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5444e-01, 5.9291e-02, 2.9489e-02, 2.5090e-02, 1.7911e-05, 2.4993e-02,
        6.6797e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.018

[Epoch: 6, batch: 86/218] total loss per batch: 0.576
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.9178e-01, 1.8003e-02, 1.7238e-02, 2.2209e-05, 3.9755e-02, 2.4764e-02,
        8.4347e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 6, batch: 129/218] total loss per batch: 0.557
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.0271e-03, 1.1861e-05, 5.6042e-03, 9.6742e-01, 4.7349e-06, 1.6225e-02,
        8.7094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.007

[Epoch: 6, batch: 172/218] total loss per batch: 0.571
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([1.4214e-02, 1.8473e-04, 9.5035e-01, 2.0054e-02, 1.0158e-05, 4.9768e-06,
        1.5185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.162

[Epoch: 6, batch: 215/218] total loss per batch: 0.581
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([4.1408e-03, 9.4046e-01, 3.0236e-03, 3.1735e-08, 2.0730e-06, 1.7439e-02,
        3.4935e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 7, batch: 43/218] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3373e-01, 3.7336e-02, 4.0815e-02, 3.0683e-02, 4.0502e-05, 4.9847e-02,
        7.5517e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.013

[Epoch: 7, batch: 86/218] total loss per batch: 0.566
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3319e-01, 1.4576e-02, 8.7036e-03, 2.1302e-05, 1.3662e-02, 2.1906e-02,
        7.9417e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 7, batch: 129/218] total loss per batch: 0.544
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0505e-03, 4.9537e-06, 6.3624e-03, 9.6927e-01, 1.3109e-05, 1.1417e-02,
        1.1880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.007

[Epoch: 7, batch: 172/218] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([1.4211e-02, 9.0439e-05, 9.4147e-01, 2.2783e-02, 6.1407e-06, 2.5365e-06,
        2.1442e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.361

[Epoch: 7, batch: 215/218] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([6.7980e-03, 9.2257e-01, 1.0116e-02, 4.5176e-08, 1.4926e-06, 1.7878e-02,
        4.2639e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.001

[Epoch: 8, batch: 43/218] total loss per batch: 0.578
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6307e-01, 2.9064e-02, 2.0680e-02, 4.8219e-02, 3.7561e-05, 2.7302e-02,
        1.1626e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.025

[Epoch: 8, batch: 86/218] total loss per batch: 0.562
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.8869e-01, 2.3552e-02, 9.3656e-03, 5.1180e-05, 3.6325e-02, 3.8272e-02,
        3.7394e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.016

[Epoch: 8, batch: 129/218] total loss per batch: 0.537
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.3607e-03, 3.0252e-06, 5.4602e-03, 9.6908e-01, 2.2372e-06, 1.3699e-02,
        5.3926e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.006

[Epoch: 8, batch: 172/218] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.0888e-02, 4.3461e-05, 9.2178e-01, 2.9358e-02, 5.9740e-06, 2.1694e-06,
        2.7918e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.564

[Epoch: 8, batch: 215/218] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.4288e-03, 9.4924e-01, 3.7869e-03, 7.8929e-09, 8.6822e-07, 1.9267e-02,
        1.9281e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 9, batch: 43/218] total loss per batch: 0.575
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8879e-01, 2.6922e-02, 3.0286e-02, 2.5148e-02, 1.6733e-05, 1.9753e-02,
        9.0890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.014

[Epoch: 9, batch: 86/218] total loss per batch: 0.555
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5640e-01, 6.5819e-03, 7.3727e-03, 1.3295e-05, 1.4060e-02, 1.3680e-02,
        1.8965e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 9, batch: 129/218] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.1567e-04, 9.1291e-06, 1.1901e-02, 9.4600e-01, 1.3489e-05, 1.9036e-02,
        2.2328e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.007

[Epoch: 9, batch: 172/218] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.1150e-02, 1.2791e-04, 9.0903e-01, 4.2680e-02, 6.0072e-06, 3.3393e-06,
        2.7000e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.673

[Epoch: 9, batch: 215/218] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.8579e-03, 9.2749e-01, 1.0552e-02, 2.4662e-08, 6.2732e-07, 2.9312e-02,
        2.2787e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.007

[Epoch: 10, batch: 43/218] total loss per batch: 0.573
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([7.5885e-01, 1.1483e-01, 3.6966e-02, 1.4099e-02, 1.6378e-05, 5.4499e-02,
        2.0743e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.013

[Epoch: 10, batch: 86/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.9845e-01, 3.1158e-02, 1.4824e-02, 5.8225e-05, 1.5091e-02, 3.1646e-02,
        8.7760e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.019

[Epoch: 10, batch: 129/218] total loss per batch: 0.535
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8740e-03, 9.4620e-06, 9.9675e-03, 9.6456e-01, 1.0549e-05, 1.0228e-02,
        8.3532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.014

[Epoch: 10, batch: 172/218] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.7343e-02, 1.4839e-04, 8.9143e-01, 4.5146e-02, 1.0428e-05, 4.1258e-06,
        3.5917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.613

[Epoch: 10, batch: 215/218] total loss per batch: 0.558
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.0505e-03, 9.5817e-01, 4.9068e-03, 8.1172e-09, 5.2707e-07, 1.2990e-02,
        1.5885e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.012

[Epoch: 11, batch: 43/218] total loss per batch: 0.570
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7720e-01, 1.7370e-02, 2.7367e-02, 3.1921e-02, 4.1833e-05, 3.4866e-02,
        1.1236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.025

[Epoch: 11, batch: 86/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2190e-01, 1.1980e-02, 9.4385e-03, 9.6581e-05, 3.0516e-02, 2.1768e-02,
        4.3045e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 11, batch: 129/218] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.8214e-03, 1.6348e-05, 7.9221e-03, 9.4236e-01, 1.1019e-05, 3.2565e-02,
        1.4308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.019

[Epoch: 11, batch: 172/218] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.4225e-02, 1.6456e-04, 8.9389e-01, 3.8191e-02, 4.8921e-06, 3.3676e-06,
        3.3518e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.707

[Epoch: 11, batch: 215/218] total loss per batch: 0.557
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([5.2689e-03, 9.2650e-01, 1.1492e-02, 4.3731e-08, 1.1978e-06, 2.5415e-02,
        3.1324e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 12, batch: 43/218] total loss per batch: 0.568
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7168e-01, 4.5132e-02, 2.4313e-02, 1.3927e-02, 1.7651e-05, 2.6800e-02,
        1.8127e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.006

[Epoch: 12, batch: 86/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6041e-01, 1.0624e-02, 7.1987e-03, 2.4521e-05, 7.1096e-03, 7.6167e-03,
        7.0187e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 12, batch: 129/218] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5323e-03, 7.1281e-06, 6.9288e-03, 9.5966e-01, 7.7895e-06, 1.0771e-02,
        1.8092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.027

[Epoch: 12, batch: 172/218] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.8087e-02, 2.9613e-04, 8.6378e-01, 5.6292e-02, 3.6201e-05, 7.7299e-06,
        4.1505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.640

[Epoch: 12, batch: 215/218] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([3.1361e-03, 9.7093e-01, 4.0969e-03, 2.6441e-08, 9.2728e-07, 1.1109e-02,
        1.0726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 13, batch: 43/218] total loss per batch: 0.568
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7805e-01, 2.8981e-02, 2.7020e-02, 2.5699e-02, 3.7220e-05, 2.8647e-02,
        1.1563e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.013

[Epoch: 13, batch: 86/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.3764e-01, 2.3059e-02, 1.4517e-02, 1.4993e-04, 8.1472e-02, 3.7645e-02,
        5.5194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 13, batch: 129/218] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.3026e-03, 9.3003e-06, 9.7534e-03, 9.5665e-01, 7.1166e-06, 2.1729e-02,
        9.5491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.013

[Epoch: 13, batch: 172/218] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.8853e-02, 1.1598e-04, 8.6373e-01, 5.6134e-02, 8.2013e-06, 3.3735e-06,
        5.1155e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.637

[Epoch: 13, batch: 215/218] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([5.4265e-03, 9.4475e-01, 1.2662e-02, 4.3280e-08, 3.5446e-06, 1.7051e-02,
        2.0104e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 14, batch: 43/218] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6119e-01, 3.3035e-02, 3.2101e-02, 2.9890e-02, 5.7976e-05, 2.7081e-02,
        1.6642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.005

[Epoch: 14, batch: 86/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4898e-01, 1.2054e-02, 1.2375e-02, 1.3949e-05, 5.7764e-03, 1.7722e-02,
        3.0827e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 14, batch: 129/218] total loss per batch: 0.529
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.7123e-03, 4.7770e-06, 5.5645e-03, 9.7332e-01, 4.6906e-06, 1.4579e-02,
        3.8141e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.039

[Epoch: 14, batch: 172/218] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.4174e-02, 1.2833e-04, 8.5018e-01, 6.6090e-02, 1.9143e-05, 3.3472e-06,
        4.9409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.674

[Epoch: 14, batch: 215/218] total loss per batch: 0.555
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([6.2000e-03, 9.5839e-01, 6.3903e-03, 1.0511e-07, 1.0437e-06, 1.3379e-02,
        1.5643e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 15, batch: 43/218] total loss per batch: 0.566
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.0206e-01, 2.4283e-02, 1.7376e-02, 1.7255e-02, 1.8260e-05, 3.3697e-02,
        5.3118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.001

[Epoch: 15, batch: 86/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4456e-01, 1.4189e-02, 8.1669e-03, 4.7103e-05, 1.3516e-02, 1.1406e-02,
        8.1105e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 15, batch: 129/218] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.0613e-03, 7.5704e-06, 1.1123e-02, 9.6165e-01, 1.2503e-05, 1.2496e-02,
        1.1649e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.016

[Epoch: 15, batch: 172/218] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3811e-02, 8.4342e-05, 8.1105e-01, 7.6773e-02, 7.1190e-06, 4.6448e-06,
        6.8271e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.653

[Epoch: 15, batch: 215/218] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([5.6698e-03, 9.5713e-01, 7.4922e-03, 2.9883e-08, 1.3510e-06, 1.9625e-02,
        1.0081e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.005

[Epoch: 16, batch: 43/218] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3892e-01, 2.7245e-02, 2.8004e-02, 3.4346e-02, 7.0932e-05, 4.3504e-02,
        2.7911e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.008

[Epoch: 16, batch: 86/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1823e-01, 1.5381e-02, 2.2209e-02, 3.8242e-05, 1.7618e-02, 2.2446e-02,
        4.0765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 16, batch: 129/218] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3351e-03, 4.1825e-06, 1.1499e-02, 9.5311e-01, 2.3008e-06, 2.4102e-02,
        5.9487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.034

[Epoch: 16, batch: 172/218] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4267e-02, 1.3538e-04, 8.3524e-01, 8.0848e-02, 4.5720e-05, 2.6206e-06,
        3.9459e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.644

[Epoch: 16, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.8715e-03, 9.4019e-01, 2.0663e-02, 7.7849e-08, 1.1609e-06, 1.5977e-02,
        1.3300e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 17, batch: 43/218] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4071e-01, 5.7335e-02, 2.9625e-02, 1.9234e-02, 2.2052e-05, 2.9603e-02,
        2.3469e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.013

[Epoch: 17, batch: 86/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3004e-01, 1.6617e-02, 8.7114e-03, 1.6261e-04, 2.4266e-02, 1.3850e-02,
        6.3526e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 17, batch: 129/218] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.7146e-03, 8.7544e-06, 1.9223e-02, 9.5091e-01, 1.4101e-05, 1.6709e-02,
        1.0419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.024

[Epoch: 17, batch: 172/218] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0184e-02, 8.0299e-05, 8.6310e-01, 6.2477e-02, 1.5186e-05, 3.5138e-06,
        3.4137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.711

[Epoch: 17, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0220e-02, 9.6047e-01, 4.6756e-03, 1.3416e-08, 1.2665e-06, 1.6290e-02,
        8.3436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 18, batch: 43/218] total loss per batch: 0.564
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.0269e-01, 5.0464e-02, 4.5017e-02, 4.7141e-02, 2.1645e-04, 2.3822e-02,
        3.0647e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.021

[Epoch: 18, batch: 86/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3591e-01, 1.4643e-02, 1.5925e-02, 7.4641e-05, 1.0593e-02, 1.3207e-02,
        9.6448e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 18, batch: 129/218] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.3850e-03, 2.8282e-05, 8.5573e-03, 9.6807e-01, 4.4407e-06, 1.2526e-02,
        8.4309e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.030

[Epoch: 18, batch: 172/218] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.7464e-02, 9.8478e-05, 8.6859e-01, 6.2107e-02, 1.3263e-05, 3.1195e-06,
        4.1728e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.718

[Epoch: 18, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([4.8187e-03, 9.7701e-01, 6.9697e-03, 2.2645e-08, 9.6434e-07, 7.0441e-03,
        4.1540e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.003

[Epoch: 19, batch: 43/218] total loss per batch: 0.564
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.0451e-01, 3.0393e-02, 1.8612e-02, 1.6020e-02, 1.5817e-05, 2.2194e-02,
        8.2605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.020

[Epoch: 19, batch: 86/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2743e-01, 1.8084e-02, 1.0832e-02, 1.7757e-04, 2.1695e-02, 1.9530e-02,
        2.2503e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 19, batch: 129/218] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2511e-03, 1.2080e-05, 1.4979e-02, 9.3068e-01, 1.4778e-05, 3.3520e-02,
        1.4540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.081

[Epoch: 19, batch: 172/218] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8355e-02, 1.2907e-03, 8.4851e-01, 6.5696e-02, 3.5914e-05, 4.0050e-06,
        3.6105e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.701

[Epoch: 19, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2511e-02, 9.2405e-01, 1.5687e-02, 3.4351e-08, 1.6467e-06, 2.3494e-02,
        2.4257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 20, batch: 43/218] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8574e-01, 2.7681e-02, 2.0960e-02, 2.3643e-02, 1.0026e-04, 3.4971e-02,
        6.9053e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.000

[Epoch: 20, batch: 86/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3506e-01, 1.4927e-02, 9.5137e-03, 1.6888e-04, 2.0123e-02, 1.2657e-02,
        7.5456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 20, batch: 129/218] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.1053e-03, 6.3037e-06, 6.4571e-03, 9.7227e-01, 1.0874e-05, 1.1715e-02,
        7.4389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.034

[Epoch: 20, batch: 172/218] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1791e-02, 1.6518e-04, 8.2332e-01, 7.3340e-02, 5.5114e-05, 5.4651e-06,
        6.1321e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.738

[Epoch: 20, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.2593e-03, 9.6612e-01, 5.8953e-03, 3.0961e-08, 3.3352e-07, 6.0668e-03,
        1.2655e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.006

[Epoch: 21, batch: 43/218] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5055e-01, 3.2155e-02, 3.0669e-02, 4.3692e-02, 1.5626e-04, 2.9548e-02,
        1.3226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.025

[Epoch: 21, batch: 86/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5068e-01, 7.6511e-03, 7.6647e-03, 6.3048e-05, 8.7460e-03, 1.5410e-02,
        9.7843e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 21, batch: 129/218] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5677e-03, 6.1626e-06, 8.9909e-03, 9.6768e-01, 1.2503e-05, 9.1849e-03,
        9.5601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.024

[Epoch: 21, batch: 172/218] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1431e-02, 9.6463e-05, 8.6577e-01, 5.9039e-02, 1.9380e-05, 2.7068e-06,
        3.3641e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.722

[Epoch: 21, batch: 215/218] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3798e-02, 9.3329e-01, 1.2965e-02, 3.6300e-08, 5.5470e-07, 2.1933e-02,
        1.8015e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.002

[Epoch: 22, batch: 43/218] total loss per batch: 0.562
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4071e-01, 2.8552e-02, 3.4195e-02, 4.3292e-02, 6.6481e-05, 3.5246e-02,
        1.7940e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.014

[Epoch: 22, batch: 86/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4986e-01, 1.1507e-02, 9.6339e-03, 6.1012e-05, 1.6876e-02, 8.5916e-03,
        3.4741e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 22, batch: 129/218] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.8189e-03, 2.3633e-05, 1.3662e-02, 9.5432e-01, 9.3720e-06, 1.9057e-02,
        1.0107e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.024

[Epoch: 22, batch: 172/218] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3011e-02, 4.9473e-04, 8.4001e-01, 7.1502e-02, 4.0011e-05, 4.1660e-06,
        4.4935e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.439

[Epoch: 22, batch: 215/218] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([7.3634e-03, 9.6108e-01, 6.4448e-03, 5.0034e-08, 9.7207e-07, 1.7209e-02,
        7.9037e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.001

[Epoch: 23, batch: 43/218] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5146e-01, 4.6371e-02, 3.7408e-02, 1.6913e-02, 1.0538e-04, 3.3816e-02,
        1.3931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.040

[Epoch: 23, batch: 86/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.6482e-01, 2.2888e-02, 2.7476e-02, 1.5426e-04, 1.6882e-02, 4.3103e-02,
        2.4675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 23, batch: 129/218] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6971e-03, 4.8595e-06, 5.1739e-03, 9.7171e-01, 1.7604e-05, 1.1755e-02,
        6.6452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.019

[Epoch: 23, batch: 172/218] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0127e-02, 1.1807e-04, 8.4728e-01, 6.5243e-02, 4.2065e-05, 3.6112e-06,
        3.7186e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.708

[Epoch: 23, batch: 215/218] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([5.2313e-03, 9.6646e-01, 5.2076e-03, 1.6091e-08, 3.4104e-07, 1.4805e-02,
        8.2969e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 24, batch: 43/218] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8135e-01, 3.4177e-02, 3.9374e-02, 1.3557e-02, 1.0955e-04, 1.9719e-02,
        1.1713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.022

[Epoch: 24, batch: 86/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1370e-01, 9.1952e-03, 7.1637e-03, 9.8738e-05, 4.9806e-02, 1.3439e-02,
        6.6020e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 24, batch: 129/218] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.0248e-03, 9.2051e-06, 1.3574e-02, 9.5841e-01, 5.5076e-05, 1.2650e-02,
        1.3279e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.039

[Epoch: 24, batch: 172/218] total loss per batch: 0.536
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9274e-02, 1.2740e-04, 8.3735e-01, 7.3780e-02, 2.1045e-05, 6.5180e-06,
        3.9440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.725

[Epoch: 24, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([4.2161e-03, 9.5564e-01, 9.2682e-03, 5.2083e-08, 4.7729e-07, 1.7454e-02,
        1.3422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 25, batch: 43/218] total loss per batch: 0.567
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([7.5967e-01, 5.3075e-02, 4.3878e-02, 6.5565e-02, 1.1787e-04, 3.8817e-02,
        3.8874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.014

[Epoch: 25, batch: 86/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1404e-01, 1.9707e-02, 1.2771e-02, 1.0571e-04, 2.0762e-02, 2.9563e-02,
        3.0562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 25, batch: 129/218] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.6938e-03, 1.0428e-05, 5.6282e-03, 9.6025e-01, 8.3508e-06, 2.0424e-02,
        7.9818e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.004

[Epoch: 25, batch: 172/218] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2400e-02, 6.3177e-04, 8.3270e-01, 9.0731e-02, 3.2331e-05, 9.9095e-06,
        3.3491e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.642

[Epoch: 25, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2635e-02, 9.4393e-01, 8.5008e-03, 1.9194e-08, 1.0893e-06, 2.3329e-02,
        1.1600e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 26, batch: 43/218] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.2333e-01, 2.7503e-02, 1.2953e-02, 1.7633e-02, 5.1972e-05, 9.7016e-03,
        8.8260e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 26, batch: 86/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5672e-01, 4.9556e-03, 1.0167e-02, 4.2226e-05, 1.0742e-02, 5.3206e-03,
        1.2055e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.017

[Epoch: 26, batch: 129/218] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.8848e-03, 8.2673e-06, 1.1728e-02, 9.5842e-01, 3.9924e-05, 1.0826e-02,
        1.6092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.032

[Epoch: 26, batch: 172/218] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0480e-02, 8.6918e-05, 8.5008e-01, 6.2452e-02, 1.2847e-05, 7.6211e-06,
        4.6882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.596

[Epoch: 26, batch: 215/218] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.9271e-03, 9.5845e-01, 1.2220e-02, 1.0480e-07, 1.7242e-06, 9.7093e-03,
        9.6915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 27, batch: 43/218] total loss per batch: 0.561
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6354e-01, 3.0782e-02, 3.0115e-02, 2.9027e-02, 6.5575e-05, 3.0002e-02,
        1.6472e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.038

[Epoch: 27, batch: 86/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.9810e-01, 4.4516e-02, 5.7819e-03, 5.7961e-04, 3.2889e-02, 7.2918e-03,
        1.0838e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 27, batch: 129/218] total loss per batch: 0.523
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8455e-03, 5.8035e-06, 1.0915e-02, 9.4588e-01, 3.9192e-05, 2.3090e-02,
        1.3226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.043

[Epoch: 27, batch: 172/218] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2277e-02, 5.2748e-04, 8.3951e-01, 7.3809e-02, 1.9508e-04, 1.3033e-05,
        4.3671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.631

[Epoch: 27, batch: 215/218] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4928e-02, 9.4552e-01, 1.5418e-02, 5.4015e-08, 1.4923e-06, 1.4011e-02,
        1.0120e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 28, batch: 43/218] total loss per batch: 0.559
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.2932e-01, 4.5990e-02, 2.6041e-02, 2.5759e-02, 9.5549e-05, 5.8681e-02,
        1.4109e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.020

[Epoch: 28, batch: 86/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5720e-01, 5.6668e-03, 1.2035e-02, 4.9904e-05, 3.8691e-03, 1.7004e-02,
        4.1788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 28, batch: 129/218] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.7810e-03, 1.7409e-05, 1.2200e-02, 9.5859e-01, 4.4729e-05, 9.3118e-03,
        1.6055e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.029

[Epoch: 28, batch: 172/218] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3152e-02, 7.5774e-05, 8.3183e-01, 8.4417e-02, 4.6932e-05, 1.2405e-05,
        4.0468e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.803

[Epoch: 28, batch: 215/218] total loss per batch: 0.550
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.0981e-03, 9.5511e-01, 1.4285e-02, 4.0488e-08, 9.1538e-07, 1.2642e-02,
        8.8654e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 29, batch: 43/218] total loss per batch: 0.561
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7923e-01, 3.4268e-02, 2.2196e-02, 3.9599e-02, 2.2702e-05, 1.5246e-02,
        9.4414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 29, batch: 86/218] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5998e-01, 1.0239e-02, 6.4983e-03, 4.6694e-05, 8.9007e-03, 7.8081e-03,
        6.5269e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 29, batch: 129/218] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9657e-03, 1.7738e-05, 1.1460e-02, 9.6510e-01, 1.1263e-05, 1.0359e-02,
        7.0822e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.036

[Epoch: 29, batch: 172/218] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8401e-02, 2.2398e-04, 8.2114e-01, 6.5867e-02, 5.6520e-05, 8.3928e-06,
        6.4307e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.639

[Epoch: 29, batch: 215/218] total loss per batch: 0.549
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2552e-02, 9.6050e-01, 1.2673e-02, 5.2039e-08, 2.0743e-07, 8.7737e-03,
        5.5042e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 30, batch: 43/218] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3478e-01, 2.5910e-02, 3.9642e-02, 4.4760e-02, 7.8087e-05, 3.7669e-02,
        1.7156e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 30, batch: 86/218] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3563e-01, 1.3961e-02, 1.6973e-02, 3.0246e-05, 7.4381e-03, 1.6287e-02,
        9.6854e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 30, batch: 129/218] total loss per batch: 0.520
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.5100e-03, 4.8489e-06, 9.1088e-03, 9.6783e-01, 6.4161e-06, 9.4721e-03,
        7.0677e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.027

[Epoch: 30, batch: 172/218] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4668e-02, 5.5029e-05, 8.7542e-01, 3.6621e-02, 4.7021e-05, 4.7453e-06,
        4.3190e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.770

[Epoch: 30, batch: 215/218] total loss per batch: 0.546
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.7036e-03, 9.6380e-01, 9.8659e-03, 3.6477e-08, 4.2704e-07, 1.0920e-02,
        5.7114e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 31, batch: 43/218] total loss per batch: 0.555
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.1283e-01, 2.5316e-02, 1.6214e-02, 1.2313e-02, 1.9554e-05, 2.2744e-02,
        1.0565e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.030

[Epoch: 31, batch: 86/218] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5491e-01, 1.1378e-02, 1.2983e-02, 3.4735e-05, 1.1202e-02, 6.7634e-03,
        2.7276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 31, batch: 129/218] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9607e-03, 1.5771e-05, 2.1207e-02, 9.4121e-01, 1.2809e-05, 2.1033e-02,
        1.0564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.036

[Epoch: 31, batch: 172/218] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2866e-02, 4.5638e-04, 8.3677e-01, 8.0282e-02, 2.2361e-05, 7.9172e-06,
        3.9592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.707

[Epoch: 31, batch: 215/218] total loss per batch: 0.545
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1413e-02, 9.5297e-01, 8.4150e-03, 8.0688e-08, 6.4318e-07, 1.8509e-02,
        8.6897e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 32, batch: 43/218] total loss per batch: 0.554
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.0700e-01, 4.1842e-02, 2.0285e-02, 6.9649e-03, 1.6184e-05, 1.1754e-02,
        1.2139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.038

[Epoch: 32, batch: 86/218] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3164e-01, 1.5920e-02, 1.3411e-02, 7.3246e-05, 5.6330e-03, 1.9886e-02,
        1.3440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 32, batch: 129/218] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0211e-03, 6.3578e-06, 9.0479e-03, 9.6491e-01, 1.0223e-05, 9.2392e-03,
        1.1768e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.037

[Epoch: 32, batch: 172/218] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.6613e-02, 1.2812e-04, 8.4607e-01, 6.9858e-02, 1.6186e-05, 3.2101e-06,
        4.7316e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.696

[Epoch: 32, batch: 215/218] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6678e-02, 9.4291e-01, 1.1856e-02, 1.2077e-07, 1.0682e-06, 1.7139e-02,
        1.1419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.000

[Epoch: 33, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7524e-01, 3.4050e-02, 2.9834e-02, 2.4657e-02, 3.6916e-05, 1.8984e-02,
        1.7193e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.040

[Epoch: 33, batch: 86/218] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4650e-01, 1.0792e-02, 8.4844e-03, 3.3742e-05, 2.5676e-02, 5.9818e-03,
        2.5352e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 33, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0446e-03, 7.8507e-06, 1.3421e-02, 9.4566e-01, 1.6010e-05, 2.7035e-02,
        8.8111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.045

[Epoch: 33, batch: 172/218] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8413e-02, 1.3654e-04, 8.1633e-01, 8.6200e-02, 7.0660e-05, 1.1806e-05,
        4.8836e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.785

[Epoch: 33, batch: 215/218] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.2986e-03, 9.6775e-01, 8.3839e-03, 4.2886e-08, 3.2965e-07, 9.9509e-03,
        4.6206e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 34, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6025e-01, 4.5150e-02, 1.8771e-02, 2.8838e-02, 3.0945e-05, 3.2135e-02,
        1.4830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.028

[Epoch: 34, batch: 86/218] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3888e-01, 1.0722e-02, 1.0810e-02, 2.9160e-05, 6.3195e-03, 2.4260e-02,
        8.9769e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 34, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.5338e-03, 2.8816e-06, 1.0316e-02, 9.6557e-01, 2.5970e-05, 5.4393e-03,
        1.0108e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.039

[Epoch: 34, batch: 172/218] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4100e-02, 5.5911e-05, 8.4040e-01, 7.0248e-02, 1.1861e-04, 2.0551e-06,
        3.5080e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.724

[Epoch: 34, batch: 215/218] total loss per batch: 0.576
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([3.0753e-02, 9.4250e-01, 2.5265e-03, 2.7641e-08, 8.3132e-06, 1.7491e-02,
        6.7179e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.057

[Epoch: 35, batch: 43/218] total loss per batch: 0.624
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.1615e-01, 3.0222e-02, 2.2286e-02, 7.1684e-02, 1.3965e-05, 1.0736e-02,
        4.8904e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.007

[Epoch: 35, batch: 86/218] total loss per batch: 0.646
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.3425e-01, 1.3199e-02, 6.0534e-02, 2.6741e-04, 2.7157e-02, 4.1446e-02,
        2.3149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 35, batch: 129/218] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.8505e-02, 8.2432e-07, 1.5335e-03, 9.6142e-01, 2.6671e-06, 1.5901e-02,
        2.6413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.010

[Epoch: 35, batch: 172/218] total loss per batch: 0.636
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.0226e-02, 1.5650e-04, 9.1692e-01, 3.0607e-02, 7.4505e-05, 3.6675e-06,
        2.2015e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.045

[Epoch: 35, batch: 215/218] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0814e-02, 9.4699e-01, 6.2794e-03, 7.4045e-07, 1.9899e-06, 2.6580e-02,
        9.3316e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.042

[Epoch: 36, batch: 43/218] total loss per batch: 0.623
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.1971e-01, 9.5689e-03, 1.8745e-02, 2.5354e-02, 2.9316e-05, 1.7426e-02,
        9.1686e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.017

[Epoch: 36, batch: 86/218] total loss per batch: 0.601
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6120e-01, 1.6485e-02, 6.1785e-03, 2.0816e-04, 9.7974e-03, 4.5394e-03,
        1.5922e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.022

[Epoch: 36, batch: 129/218] total loss per batch: 0.579
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.3995e-03, 2.1515e-05, 6.5000e-03, 9.6665e-01, 1.5999e-06, 9.7442e-03,
        1.0682e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.022

[Epoch: 36, batch: 172/218] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.3759e-02, 3.1052e-05, 9.0821e-01, 4.9489e-02, 1.7922e-05, 5.2075e-06,
        1.8490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.645

[Epoch: 36, batch: 215/218] total loss per batch: 0.583
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5141e-02, 9.0954e-01, 2.5878e-02, 4.0082e-07, 4.7380e-06, 3.0125e-02,
        1.9312e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 37, batch: 43/218] total loss per batch: 0.577
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3599e-01, 3.4261e-02, 5.7254e-02, 1.9495e-02, 4.7238e-05, 4.1308e-02,
        1.1645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.019

[Epoch: 37, batch: 86/218] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.8743e-01, 1.1238e-02, 1.2244e-02, 1.3092e-04, 5.8198e-02, 2.7886e-02,
        2.8730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.021

[Epoch: 37, batch: 129/218] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.6196e-03, 1.1814e-05, 1.1242e-02, 9.5216e-01, 8.2236e-06, 2.2971e-02,
        1.1990e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.043

[Epoch: 37, batch: 172/218] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2393e-02, 5.0092e-05, 8.6507e-01, 5.9400e-02, 1.5499e-04, 1.9798e-05,
        3.2917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.802

[Epoch: 37, batch: 215/218] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([7.0722e-03, 9.6545e-01, 7.2612e-03, 5.7767e-08, 2.2724e-07, 1.3085e-02,
        7.1353e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 38, batch: 43/218] total loss per batch: 0.556
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6387e-01, 3.2416e-02, 4.5801e-02, 2.5448e-02, 2.6662e-05, 1.7787e-02,
        1.4647e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.015

[Epoch: 38, batch: 86/218] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2958e-01, 1.4025e-02, 7.8394e-03, 2.2592e-04, 2.0281e-02, 2.2284e-02,
        5.7620e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.018

[Epoch: 38, batch: 129/218] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5745e-03, 6.2821e-06, 1.0508e-02, 9.6305e-01, 6.6415e-06, 1.3878e-02,
        8.9810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.038

[Epoch: 38, batch: 172/218] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0204e-02, 4.0244e-05, 8.4334e-01, 7.7682e-02, 9.8726e-05, 6.3755e-06,
        3.8630e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.742

[Epoch: 38, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([7.3750e-03, 9.5552e-01, 1.1320e-02, 9.9339e-08, 3.0832e-07, 1.7765e-02,
        8.0188e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 39, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6383e-01, 4.2608e-02, 3.4917e-02, 2.2561e-02, 1.6438e-05, 2.5928e-02,
        1.0139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.022

[Epoch: 39, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4527e-01, 1.2415e-02, 9.8425e-03, 1.0589e-04, 9.7434e-03, 1.5571e-02,
        7.0534e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 39, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5251e-03, 4.6151e-06, 1.5478e-02, 9.5276e-01, 5.5122e-06, 1.6253e-02,
        1.1969e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.042

[Epoch: 39, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7901e-02, 6.6205e-05, 8.2289e-01, 8.0100e-02, 8.5146e-05, 6.2488e-06,
        4.8948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.724

[Epoch: 39, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.0424e-03, 9.5746e-01, 1.0293e-02, 5.3758e-08, 1.4468e-07, 1.6001e-02,
        8.2021e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 40, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7463e-01, 3.7252e-02, 3.4488e-02, 1.8170e-02, 1.6929e-05, 2.3538e-02,
        1.1903e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.019

[Epoch: 40, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5062e-01, 9.3056e-03, 9.3139e-03, 8.5579e-05, 1.0725e-02, 1.5191e-02,
        4.7545e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.023

[Epoch: 40, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5764e-03, 3.7629e-06, 1.3084e-02, 9.5800e-01, 4.0760e-06, 1.4748e-02,
        9.5823e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.042

[Epoch: 40, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1722e-02, 6.8176e-05, 8.2070e-01, 7.9412e-02, 7.6739e-05, 5.0556e-06,
        4.8022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.746

[Epoch: 40, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.9651e-03, 9.5036e-01, 1.2433e-02, 6.2004e-08, 2.6634e-07, 2.0095e-02,
        8.1436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 41, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5989e-01, 3.5101e-02, 3.0995e-02, 2.3404e-02, 1.2205e-05, 3.4362e-02,
        1.6235e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.027

[Epoch: 41, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5282e-01, 9.7837e-03, 9.1728e-03, 4.1494e-05, 9.0383e-03, 1.1441e-02,
        7.7024e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 41, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.0939e-03, 3.3015e-06, 1.2428e-02, 9.6082e-01, 2.7500e-06, 1.2072e-02,
        1.0577e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.044

[Epoch: 41, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6811e-02, 5.0253e-05, 8.2773e-01, 7.3092e-02, 5.5128e-05, 4.6733e-06,
        5.2255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.713

[Epoch: 41, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([9.5396e-03, 9.6022e-01, 9.4311e-03, 3.4083e-08, 8.6474e-08, 1.3791e-02,
        7.0209e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 42, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5105e-01, 3.8827e-02, 3.5200e-02, 2.3548e-02, 1.3311e-05, 3.6186e-02,
        1.5180e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.028

[Epoch: 42, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4689e-01, 1.0050e-02, 9.7757e-03, 5.3998e-05, 1.0812e-02, 1.6863e-02,
        5.5596e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 42, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.2612e-03, 1.9802e-06, 1.2752e-02, 9.6136e-01, 2.9531e-06, 1.2079e-02,
        9.5420e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.048

[Epoch: 42, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2152e-02, 6.1618e-05, 8.1553e-01, 8.5241e-02, 5.9610e-05, 3.4446e-06,
        4.6948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.749

[Epoch: 42, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.3722e-03, 9.5785e-01, 1.1623e-02, 3.1322e-08, 1.3037e-07, 1.5735e-02,
        6.4187e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 43, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7248e-01, 3.3148e-02, 2.9743e-02, 2.4033e-02, 6.9606e-06, 2.3873e-02,
        1.6717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.029

[Epoch: 43, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4721e-01, 1.1556e-02, 1.0613e-02, 4.2823e-05, 9.8589e-03, 1.0909e-02,
        9.8164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.018

[Epoch: 43, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0039e-03, 1.9378e-06, 1.1138e-02, 9.6141e-01, 1.5382e-06, 1.1379e-02,
        1.1063e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.045

[Epoch: 43, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.5319e-02, 5.4943e-05, 8.3422e-01, 6.5992e-02, 4.7421e-05, 3.8294e-06,
        5.4358e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.698

[Epoch: 43, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2903e-02, 9.5426e-01, 1.0813e-02, 2.8139e-08, 8.4798e-08, 1.3941e-02,
        8.0825e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 44, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5536e-01, 3.5446e-02, 3.2136e-02, 2.2541e-02, 1.3278e-05, 3.7897e-02,
        1.6608e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.030

[Epoch: 44, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4637e-01, 8.9896e-03, 1.1186e-02, 6.8892e-05, 8.8751e-03, 1.8289e-02,
        6.2221e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.016

[Epoch: 44, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5291e-03, 2.0516e-06, 1.1366e-02, 9.6564e-01, 4.3686e-06, 9.5293e-03,
        9.9292e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.056

[Epoch: 44, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3031e-02, 6.2358e-05, 8.1868e-01, 8.3453e-02, 6.4992e-05, 3.7370e-06,
        4.4708e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.771

[Epoch: 44, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([7.1909e-03, 9.6406e-01, 9.3850e-03, 3.2883e-08, 1.5169e-07, 1.3916e-02,
        5.4513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 45, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7545e-01, 3.8111e-02, 2.8599e-02, 1.9274e-02, 6.4778e-06, 2.1735e-02,
        1.6825e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 45, batch: 86/218] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4823e-01, 1.4612e-02, 1.0290e-02, 5.4682e-05, 8.0103e-03, 1.0743e-02,
        8.0623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.019

[Epoch: 45, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0383e-03, 1.6825e-06, 7.7741e-03, 9.6476e-01, 1.5034e-06, 1.3626e-02,
        8.8006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.049

[Epoch: 45, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3305e-02, 2.7135e-05, 8.4672e-01, 6.7339e-02, 3.6666e-05, 3.7254e-06,
        4.2568e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 45, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6457e-02, 9.4676e-01, 1.0836e-02, 2.2077e-08, 1.0183e-07, 1.6855e-02,
        9.0961e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 46, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5879e-01, 4.0290e-02, 2.7591e-02, 2.3594e-02, 2.3567e-05, 3.0459e-02,
        1.9251e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.031

[Epoch: 46, batch: 86/218] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6004e-01, 5.7040e-03, 9.0981e-03, 5.5769e-05, 5.7642e-03, 1.3656e-02,
        5.6780e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.023

[Epoch: 46, batch: 129/218] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.3750e-03, 1.1581e-06, 1.2830e-02, 9.6381e-01, 4.7924e-06, 6.3976e-03,
        1.3578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 46, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0479e-02, 4.4295e-05, 8.2944e-01, 7.3547e-02, 7.3206e-05, 3.3717e-06,
        4.6415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.758

[Epoch: 46, batch: 215/218] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([8.7021e-03, 9.5988e-01, 1.2665e-02, 4.6474e-08, 2.2285e-07, 1.2493e-02,
        6.2608e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 47, batch: 43/218] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3145e-01, 3.0875e-02, 4.2618e-02, 2.6715e-02, 1.6175e-05, 4.7984e-02,
        2.0342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 47, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3057e-01, 2.1288e-02, 9.6755e-03, 8.8725e-05, 2.0120e-02, 1.0371e-02,
        7.8864e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 47, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.4565e-03, 2.9145e-06, 9.3536e-03, 9.6367e-01, 2.6942e-06, 1.5331e-02,
        7.1789e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.043

[Epoch: 47, batch: 172/218] total loss per batch: 0.526
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2721e-02, 4.8797e-05, 7.8769e-01, 1.0073e-01, 8.7418e-05, 1.4806e-05,
        5.8704e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.666

[Epoch: 47, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.9193e-02, 9.4738e-01, 8.8015e-03, 5.1263e-08, 2.4228e-07, 1.4785e-02,
        9.8412e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 48, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9019e-01, 3.4243e-02, 2.0754e-02, 2.1924e-02, 4.7570e-05, 1.9350e-02,
        1.3488e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.030

[Epoch: 48, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5435e-01, 7.0395e-03, 9.4361e-03, 5.0707e-05, 2.6683e-03, 1.4622e-02,
        1.1833e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.022

[Epoch: 48, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7500e-03, 5.5442e-06, 1.3188e-02, 9.6098e-01, 6.5412e-06, 8.3184e-03,
        1.2754e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.046

[Epoch: 48, batch: 172/218] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9868e-02, 2.0189e-05, 8.7168e-01, 5.0065e-02, 3.5854e-05, 2.2121e-06,
        3.8328e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.639

[Epoch: 48, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2529e-02, 9.5369e-01, 1.1947e-02, 7.0645e-08, 2.4208e-07, 1.4262e-02,
        7.5691e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 49, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9164e-01, 2.2889e-02, 2.2365e-02, 3.8957e-02, 2.5740e-05, 1.1843e-02,
        1.2276e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 49, batch: 86/218] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1951e-01, 2.1831e-02, 1.6921e-02, 1.1325e-04, 2.2857e-02, 1.1911e-02,
        6.8574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 49, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8735e-03, 1.3376e-06, 1.0384e-02, 9.6168e-01, 2.4531e-06, 1.6624e-02,
        5.4340e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.061

[Epoch: 49, batch: 172/218] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4567e-02, 1.4845e-05, 8.2077e-01, 8.4428e-02, 4.7276e-05, 6.2863e-06,
        5.0171e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.769

[Epoch: 49, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4427e-02, 9.5192e-01, 1.2510e-02, 7.8299e-08, 9.6044e-08, 1.5061e-02,
        6.0833e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 50, batch: 43/218] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7282e-01, 3.6572e-02, 2.4181e-02, 1.7668e-02, 1.1406e-04, 3.9522e-02,
        9.1188e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.020

[Epoch: 50, batch: 86/218] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4588e-01, 7.5824e-03, 1.2354e-02, 1.1181e-04, 1.0839e-02, 1.7406e-02,
        5.8315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.018

[Epoch: 50, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0854e-03, 7.8329e-06, 1.1968e-02, 9.3539e-01, 2.0031e-05, 1.1707e-02,
        3.5827e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.045

[Epoch: 50, batch: 172/218] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0520e-02, 8.2697e-05, 8.6401e-01, 5.1486e-02, 1.6487e-05, 5.0890e-06,
        3.3881e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.729

[Epoch: 50, batch: 215/218] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5830e-02, 9.3690e-01, 1.7104e-02, 1.7077e-07, 9.0242e-07, 2.0398e-02,
        9.7632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 51, batch: 43/218] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4550e-01, 3.4850e-02, 3.0086e-02, 2.7682e-02, 3.0739e-05, 3.6495e-02,
        2.5358e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 51, batch: 86/218] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5347e-01, 1.4374e-02, 5.1990e-03, 1.1368e-04, 1.0835e-02, 7.6166e-03,
        8.3959e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 51, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2920e-03, 2.2325e-06, 8.1783e-03, 9.7244e-01, 3.5088e-06, 1.0806e-02,
        3.2751e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.046

[Epoch: 51, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.1672e-02, 7.6076e-05, 7.6032e-01, 1.1305e-01, 3.5621e-05, 9.0148e-06,
        6.4835e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.673

[Epoch: 51, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0334e-02, 9.6017e-01, 1.1180e-02, 9.5025e-08, 4.7393e-07, 1.0110e-02,
        8.2025e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 52, batch: 43/218] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4910e-01, 4.9291e-02, 2.9190e-02, 3.2406e-02, 4.9270e-05, 2.6886e-02,
        1.3072e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 52, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4501e-01, 9.1411e-03, 1.0484e-02, 7.5933e-05, 8.7364e-03, 1.5315e-02,
        1.1238e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 52, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.9646e-03, 4.1591e-06, 1.1553e-02, 9.6767e-01, 3.0098e-06, 1.1003e-02,
        5.8071e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.056

[Epoch: 52, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7070e-02, 4.9553e-05, 8.4719e-01, 6.0476e-02, 1.7945e-05, 6.2126e-06,
        4.5188e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.793

[Epoch: 52, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1621e-02, 9.5533e-01, 1.2032e-02, 4.8522e-08, 1.2777e-07, 1.4471e-02,
        6.5447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 53, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9303e-01, 3.1063e-02, 2.1567e-02, 1.6925e-02, 2.2897e-05, 1.9731e-02,
        1.7659e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 53, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4051e-01, 1.0099e-02, 1.0029e-02, 1.0911e-04, 1.4523e-02, 1.4917e-02,
        9.8097e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.015

[Epoch: 53, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.5275e-03, 2.1591e-06, 1.0013e-02, 9.6761e-01, 3.5025e-06, 1.0874e-02,
        5.9667e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.053

[Epoch: 53, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6411e-02, 5.3552e-05, 8.3264e-01, 7.2230e-02, 1.1715e-05, 5.2261e-06,
        4.8653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.757

[Epoch: 53, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2726e-02, 9.5865e-01, 1.0532e-02, 3.0123e-08, 1.5345e-07, 1.0909e-02,
        7.1824e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 54, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5198e-01, 3.9960e-02, 3.4261e-02, 2.4680e-02, 2.0483e-05, 3.1051e-02,
        1.8047e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.040

[Epoch: 54, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4566e-01, 9.3500e-03, 9.6339e-03, 5.7843e-05, 1.0013e-02, 1.6143e-02,
        9.1461e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.012

[Epoch: 54, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.1499e-03, 4.0921e-06, 1.3575e-02, 9.6499e-01, 2.1862e-06, 9.8411e-03,
        6.4373e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 54, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0382e-02, 6.3103e-05, 8.1459e-01, 8.1251e-02, 1.6759e-05, 5.0452e-06,
        5.3689e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.706

[Epoch: 54, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4308e-02, 9.5129e-01, 1.1107e-02, 3.6007e-08, 1.1874e-07, 1.6279e-02,
        7.0129e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 55, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5429e-01, 3.3486e-02, 3.3084e-02, 2.9303e-02, 1.5552e-05, 3.1409e-02,
        1.8416e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 55, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5108e-01, 1.1094e-02, 8.7348e-03, 6.1010e-05, 9.6511e-03, 9.0938e-03,
        1.0282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 55, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9517e-03, 2.0768e-06, 1.1075e-02, 9.6041e-01, 3.7012e-06, 1.3200e-02,
        9.3580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.051

[Epoch: 55, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1713e-02, 4.6295e-05, 8.2935e-01, 7.0148e-02, 9.8555e-06, 4.3963e-06,
        4.8725e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.751

[Epoch: 55, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1722e-02, 9.5141e-01, 1.3143e-02, 2.6150e-08, 1.8985e-07, 1.7603e-02,
        6.1202e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 56, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7468e-01, 3.7881e-02, 2.7582e-02, 2.6321e-02, 1.5049e-05, 2.0018e-02,
        1.3499e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 56, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5186e-01, 8.2454e-03, 9.4329e-03, 4.8121e-05, 8.3993e-03, 1.3967e-02,
        8.0436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 56, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.1879e-03, 2.8777e-06, 9.7439e-03, 9.6779e-01, 1.4120e-06, 1.0641e-02,
        7.6282e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.062

[Epoch: 56, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7371e-02, 5.0455e-05, 8.3062e-01, 7.7561e-02, 1.0567e-05, 4.8817e-06,
        4.4381e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 56, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3670e-02, 9.5714e-01, 8.8820e-03, 3.5085e-08, 1.5299e-07, 1.2974e-02,
        7.3315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 57, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7985e-01, 3.2106e-02, 2.9977e-02, 2.0219e-02, 1.5765e-05, 1.9916e-02,
        1.7913e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 57, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3959e-01, 1.5191e-02, 1.2837e-02, 9.3548e-05, 8.8231e-03, 9.9502e-03,
        1.3512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 57, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8621e-03, 2.1898e-06, 8.5150e-03, 9.6725e-01, 1.4414e-06, 1.1272e-02,
        6.1007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 57, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6218e-02, 5.5605e-05, 8.1667e-01, 8.3981e-02, 1.5146e-05, 3.7637e-06,
        5.3052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.708

[Epoch: 57, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0317e-02, 9.5995e-01, 1.1185e-02, 3.6688e-08, 5.8226e-08, 1.4180e-02,
        4.3716e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 58, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5948e-01, 4.7390e-02, 2.4393e-02, 2.1057e-02, 1.0529e-05, 2.9512e-02,
        1.8154e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 58, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5222e-01, 8.2291e-03, 7.3090e-03, 4.0348e-05, 1.2251e-02, 1.3994e-02,
        5.9605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 58, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8728e-03, 3.2061e-06, 1.0142e-02, 9.6228e-01, 4.7770e-06, 1.0330e-02,
        1.2368e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.046

[Epoch: 58, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2780e-02, 4.8608e-05, 8.2036e-01, 7.6688e-02, 3.0775e-05, 6.5711e-06,
        5.0090e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.750

[Epoch: 58, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1893e-02, 9.6340e-01, 7.5936e-03, 4.4251e-08, 4.4321e-08, 1.1785e-02,
        5.3321e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 59, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7884e-01, 3.0549e-02, 2.9241e-02, 2.0370e-02, 2.7574e-05, 3.0008e-02,
        1.0966e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.040

[Epoch: 59, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4885e-01, 1.0327e-02, 1.3862e-02, 6.9398e-05, 7.8322e-03, 8.5473e-03,
        1.0516e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 59, batch: 129/218] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6697e-03, 1.4039e-05, 9.7112e-03, 9.6365e-01, 3.2667e-06, 1.1512e-02,
        1.0438e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.057

[Epoch: 59, batch: 172/218] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9362e-02, 2.6001e-05, 8.6166e-01, 5.6369e-02, 1.3282e-05, 3.9797e-06,
        4.2569e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.670

[Epoch: 59, batch: 215/218] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0428e-02, 9.6089e-01, 9.8369e-03, 3.6889e-08, 8.0362e-08, 1.1249e-02,
        7.6008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 60, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5854e-01, 3.4042e-02, 2.6560e-02, 4.0496e-02, 2.9556e-05, 2.1679e-02,
        1.8650e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.034

[Epoch: 60, batch: 86/218] total loss per batch: 0.534
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3358e-01, 1.7244e-02, 9.6548e-03, 1.5088e-04, 1.1197e-02, 2.0616e-02,
        7.5533e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.020

[Epoch: 60, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3136e-03, 3.4904e-06, 1.2227e-02, 9.6024e-01, 7.7808e-06, 1.2185e-02,
        1.0022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.058

[Epoch: 60, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6671e-02, 1.0588e-04, 7.8163e-01, 1.0928e-01, 4.0436e-05, 4.4512e-06,
        6.2268e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.730

[Epoch: 60, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5788e-02, 9.4718e-01, 1.1069e-02, 1.2440e-07, 3.1075e-07, 1.9530e-02,
        6.4280e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.004

[Epoch: 61, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7155e-01, 4.1323e-02, 2.0016e-02, 2.1701e-02, 8.5615e-05, 2.7642e-02,
        1.7684e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 61, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4961e-01, 1.1244e-02, 1.1930e-02, 1.2043e-04, 9.2580e-03, 5.8639e-03,
        1.1973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 61, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.1710e-03, 1.0874e-05, 8.2452e-03, 9.6061e-01, 9.9734e-06, 1.4342e-02,
        1.2613e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.039

[Epoch: 61, batch: 172/218] total loss per batch: 0.526
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8160e-02, 3.0656e-05, 8.7049e-01, 4.1812e-02, 1.7096e-05, 3.0789e-06,
        3.9486e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 61, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4131e-02, 9.4734e-01, 1.0792e-02, 6.0032e-08, 3.5631e-07, 2.0934e-02,
        6.8047e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 62, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3987e-01, 5.0388e-02, 3.8747e-02, 2.9293e-02, 2.9504e-05, 2.5633e-02,
        1.6039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 62, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4805e-01, 1.0973e-02, 6.7122e-03, 1.0965e-04, 9.6147e-03, 1.6935e-02,
        7.6041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 62, batch: 129/218] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6165e-03, 8.2752e-06, 1.3315e-02, 9.6452e-01, 5.8553e-06, 6.3163e-03,
        1.1216e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.051

[Epoch: 62, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.5178e-02, 6.6895e-05, 7.9945e-01, 8.2174e-02, 9.7718e-06, 4.2991e-06,
        6.3121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.769

[Epoch: 62, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([2.0273e-02, 9.4823e-01, 8.7205e-03, 1.4159e-07, 2.6210e-07, 1.3794e-02,
        8.9805e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.004

[Epoch: 63, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6153e-01, 3.2944e-02, 2.7858e-02, 2.3332e-02, 3.4535e-05, 3.2404e-02,
        2.1899e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.033

[Epoch: 63, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4921e-01, 1.1092e-02, 7.6940e-03, 4.1371e-05, 9.9712e-03, 1.3063e-02,
        8.9277e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 63, batch: 129/218] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2417e-03, 4.6180e-06, 1.1505e-02, 9.6356e-01, 4.7895e-06, 1.2716e-02,
        6.9694e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.038

[Epoch: 63, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8744e-02, 7.3265e-05, 8.3284e-01, 7.5215e-02, 1.5824e-05, 4.7345e-06,
        4.3104e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.618

[Epoch: 63, batch: 215/218] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2664e-02, 9.6110e-01, 1.0977e-02, 3.1790e-08, 2.9018e-07, 1.0144e-02,
        5.1186e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 64, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6418e-01, 4.6126e-02, 3.4139e-02, 1.8728e-02, 8.5649e-05, 2.6124e-02,
        1.0620e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.036

[Epoch: 64, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4295e-01, 9.4776e-03, 1.1738e-02, 6.9058e-05, 1.6729e-02, 8.9214e-03,
        1.0113e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 64, batch: 129/218] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3898e-03, 3.9461e-06, 1.2769e-02, 9.5780e-01, 2.5966e-06, 1.1269e-02,
        1.2764e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.056

[Epoch: 64, batch: 172/218] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4656e-02, 5.2020e-05, 8.0157e-01, 8.8104e-02, 1.5778e-05, 7.7633e-06,
        5.5590e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.772

[Epoch: 64, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2853e-02, 9.5432e-01, 1.0314e-02, 8.5117e-08, 4.0692e-07, 1.6923e-02,
        5.5887e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 65, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5198e-01, 2.7555e-02, 3.1568e-02, 4.5235e-02, 4.5745e-05, 2.4605e-02,
        1.9012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 65, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3475e-01, 1.1492e-02, 1.0442e-02, 8.3192e-05, 1.1346e-02, 1.6226e-02,
        1.5658e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 65, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.6316e-03, 2.8390e-06, 1.0552e-02, 9.6698e-01, 4.7106e-06, 1.1514e-02,
        5.3146e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.053

[Epoch: 65, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.2874e-02, 4.2142e-05, 8.6222e-01, 6.8444e-02, 9.8136e-06, 3.2611e-06,
        3.6401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.751

[Epoch: 65, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2799e-02, 9.5975e-01, 9.3581e-03, 4.9392e-08, 2.4769e-07, 1.1180e-02,
        6.9167e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.005

[Epoch: 66, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8302e-01, 3.5269e-02, 2.2464e-02, 2.2904e-02, 5.3116e-05, 2.0008e-02,
        1.6280e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 66, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5105e-01, 9.5046e-03, 9.6782e-03, 5.2708e-05, 1.3740e-02, 9.7830e-03,
        6.1958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.014

[Epoch: 66, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.1993e-03, 4.8628e-06, 1.4630e-02, 9.5782e-01, 2.6362e-06, 1.0098e-02,
        1.1243e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.052

[Epoch: 66, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.4510e-02, 2.3263e-04, 7.8723e-01, 8.2119e-02, 3.5355e-05, 8.9995e-06,
        6.5860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.680

[Epoch: 66, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4955e-02, 9.5888e-01, 7.7099e-03, 7.0334e-08, 9.9803e-08, 1.1839e-02,
        6.6119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.008

[Epoch: 67, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.1063e-01, 2.0019e-02, 1.9149e-02, 2.2900e-02, 3.3928e-05, 1.5867e-02,
        1.1405e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 67, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5022e-01, 1.0887e-02, 8.5266e-03, 9.1262e-05, 8.7109e-03, 8.7505e-03,
        1.2814e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.017

[Epoch: 67, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.0946e-03, 2.9061e-06, 1.2469e-02, 9.5575e-01, 5.6546e-06, 1.6401e-02,
        9.2751e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.059

[Epoch: 67, batch: 172/218] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.2922e-02, 6.5712e-05, 8.6406e-01, 7.3519e-02, 2.1073e-05, 3.6664e-06,
        3.9412e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.777

[Epoch: 67, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4711e-02, 9.5495e-01, 1.3713e-02, 2.5105e-08, 1.0376e-07, 1.2339e-02,
        4.2818e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 68, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7278e-01, 4.2647e-02, 2.8716e-02, 2.2737e-02, 1.1934e-05, 1.4007e-02,
        1.9104e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 68, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4912e-01, 1.2145e-02, 1.3324e-02, 4.6420e-05, 8.4310e-03, 1.3236e-02,
        3.6932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 68, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5632e-03, 2.4830e-06, 7.6294e-03, 9.6400e-01, 1.3393e-06, 1.8971e-02,
        5.8306e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 68, batch: 172/218] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.4560e-02, 4.3714e-04, 8.2063e-01, 6.3725e-02, 1.1104e-05, 4.5645e-06,
        5.0635e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.607

[Epoch: 68, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([7.6083e-03, 9.5896e-01, 1.1907e-02, 1.2017e-07, 1.0581e-07, 1.5146e-02,
        6.3833e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 69, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3014e-01, 4.9824e-02, 2.7963e-02, 4.2412e-02, 6.2627e-05, 2.7600e-02,
        2.1996e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 69, batch: 86/218] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5010e-01, 1.5853e-02, 1.0213e-02, 1.2614e-04, 9.3602e-03, 7.3491e-03,
        7.0008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.014

[Epoch: 69, batch: 129/218] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5358e-03, 3.1492e-06, 1.2660e-02, 9.5982e-01, 2.6943e-06, 1.1369e-02,
        1.2606e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.053

[Epoch: 69, batch: 172/218] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4822e-02, 2.0472e-04, 7.9778e-01, 8.7835e-02, 8.2136e-06, 8.3154e-06,
        5.9344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.808

[Epoch: 69, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2891e-02, 9.4985e-01, 1.1051e-02, 7.8271e-08, 4.0867e-07, 1.9074e-02,
        7.1287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 70, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7863e-01, 4.1140e-02, 2.6454e-02, 2.2107e-02, 1.7745e-05, 2.0126e-02,
        1.1529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 70, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.9753e-01, 1.6612e-02, 1.0577e-02, 7.4996e-05, 1.0564e-02, 2.0711e-02,
        4.3933e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.013

[Epoch: 70, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.0394e-03, 2.8324e-06, 9.2011e-03, 9.6873e-01, 9.5326e-07, 6.7717e-03,
        9.2553e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.052

[Epoch: 70, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.1241e-02, 3.7501e-05, 8.6878e-01, 7.0490e-02, 4.1431e-06, 5.2274e-06,
        2.9443e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.810

[Epoch: 70, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3360e-02, 9.5824e-01, 1.0231e-02, 5.9563e-08, 8.4608e-08, 1.2224e-02,
        5.9443e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 71, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6137e-01, 3.5312e-02, 3.2116e-02, 2.5362e-02, 3.9588e-05, 2.8814e-02,
        1.6986e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 71, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6622e-01, 8.8263e-03, 5.5368e-03, 4.5585e-05, 1.3389e-02, 4.6199e-03,
        1.3667e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 71, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.2428e-03, 5.2424e-06, 1.0410e-02, 9.6658e-01, 3.2092e-06, 1.2231e-02,
        7.5299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.063

[Epoch: 71, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6610e-02, 6.2692e-05, 7.9922e-01, 8.5954e-02, 2.4178e-05, 9.3511e-06,
        5.8124e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.758

[Epoch: 71, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8434e-02, 9.4979e-01, 1.0160e-02, 7.6190e-08, 1.0166e-07, 1.5541e-02,
        6.0744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.003

[Epoch: 72, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7615e-01, 3.0018e-02, 2.8429e-02, 2.0322e-02, 3.4385e-05, 3.3483e-02,
        1.1559e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 72, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4736e-01, 1.4431e-02, 9.4639e-03, 3.2329e-05, 1.0191e-02, 1.2701e-02,
        5.8192e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.019

[Epoch: 72, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.1698e-03, 4.9703e-06, 9.2832e-03, 9.6326e-01, 1.5829e-06, 1.1565e-02,
        1.0715e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.060

[Epoch: 72, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0974e-02, 7.9482e-05, 8.4698e-01, 6.8494e-02, 4.3271e-06, 4.2689e-06,
        4.3463e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.687

[Epoch: 72, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1692e-02, 9.5670e-01, 9.6476e-03, 7.1743e-08, 8.8377e-08, 1.5638e-02,
        6.3240e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 73, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8640e-01, 3.4117e-02, 2.6426e-02, 1.6591e-02, 1.5215e-05, 2.4188e-02,
        1.2265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 73, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6298e-01, 8.2583e-03, 8.2768e-03, 7.8709e-05, 9.4687e-03, 6.3650e-03,
        4.5731e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 73, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3456e-03, 2.1654e-06, 8.9129e-03, 9.6621e-01, 4.0383e-06, 9.6928e-03,
        1.0829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.061

[Epoch: 73, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8715e-02, 3.8232e-05, 8.2461e-01, 7.7636e-02, 4.7155e-06, 5.8902e-06,
        4.8990e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.757

[Epoch: 73, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7868e-02, 9.4928e-01, 1.0709e-02, 4.2884e-08, 1.0027e-07, 1.5837e-02,
        6.3004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.000

[Epoch: 74, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7305e-01, 3.2670e-02, 2.4537e-02, 1.9900e-02, 3.4963e-05, 3.0233e-02,
        1.9579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 74, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4848e-01, 1.1463e-02, 8.7917e-03, 3.0241e-05, 1.4200e-02, 9.5863e-03,
        7.4510e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.016

[Epoch: 74, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.4098e-03, 3.4000e-06, 1.0553e-02, 9.6758e-01, 8.6194e-07, 9.1248e-03,
        8.3287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.058

[Epoch: 74, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.5331e-02, 7.0596e-05, 8.3816e-01, 6.4801e-02, 6.0677e-06, 5.0696e-06,
        5.1626e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.719

[Epoch: 74, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1520e-02, 9.5738e-01, 8.9913e-03, 6.9482e-08, 8.5807e-08, 1.6529e-02,
        5.5845e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 75, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3576e-01, 4.2424e-02, 4.2420e-02, 2.6495e-02, 2.1566e-05, 3.2417e-02,
        2.0460e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 75, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5274e-01, 8.5696e-03, 1.0512e-02, 9.3723e-05, 9.3305e-03, 9.7685e-03,
        8.9884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 75, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8196e-03, 3.9036e-06, 1.2951e-02, 9.6073e-01, 2.6325e-06, 1.0237e-02,
        1.1251e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.065

[Epoch: 75, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.0770e-02, 4.1048e-05, 7.8561e-01, 1.0182e-01, 4.0174e-06, 7.0655e-06,
        5.1745e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.747

[Epoch: 75, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1084e-02, 9.5774e-01, 1.3038e-02, 1.1812e-07, 3.4261e-07, 1.2179e-02,
        5.9556e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 76, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8228e-01, 3.9125e-02, 2.5404e-02, 2.1245e-02, 2.2865e-05, 1.7550e-02,
        1.4376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 76, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4871e-01, 1.3583e-02, 1.1285e-02, 7.4711e-05, 7.3965e-03, 9.7014e-03,
        9.2516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.015

[Epoch: 76, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6797e-03, 2.0987e-06, 1.0610e-02, 9.6989e-01, 1.7754e-06, 8.8723e-03,
        5.9388e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.061

[Epoch: 76, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9623e-02, 8.9343e-05, 8.5557e-01, 5.1572e-02, 7.0340e-06, 4.3913e-06,
        4.3138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.702

[Epoch: 76, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2624e-02, 9.5213e-01, 1.0504e-02, 1.1989e-07, 1.5191e-07, 1.7950e-02,
        6.7932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 77, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9239e-01, 2.8317e-02, 2.6806e-02, 1.6882e-02, 2.2610e-05, 1.9816e-02,
        1.5763e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 77, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3680e-01, 1.3218e-02, 9.3531e-03, 1.8056e-04, 1.1904e-02, 1.3722e-02,
        1.4817e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 77, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.6976e-03, 9.5456e-06, 8.0330e-03, 9.6484e-01, 1.1946e-06, 8.8770e-03,
        1.1542e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 77, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2180e-02, 1.7038e-05, 8.4690e-01, 5.4539e-02, 7.1337e-06, 7.7554e-06,
        4.6346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.698

[Epoch: 77, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7473e-02, 9.5226e-01, 8.1920e-03, 1.5697e-07, 2.9256e-07, 1.5068e-02,
        7.0066e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 78, batch: 43/218] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5397e-01, 4.5435e-02, 2.8450e-02, 3.7252e-02, 2.2678e-05, 2.0804e-02,
        1.4064e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.066

[Epoch: 78, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6087e-01, 7.8212e-03, 9.9572e-03, 9.4102e-05, 8.4454e-03, 8.5753e-03,
        4.2358e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 78, batch: 129/218] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.6605e-03, 4.3667e-06, 1.3145e-02, 9.6080e-01, 7.0932e-06, 1.5156e-02,
        7.2229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.060

[Epoch: 78, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9813e-02, 2.1042e-05, 8.1599e-01, 9.6077e-02, 1.2124e-05, 3.8309e-06,
        4.8079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.709

[Epoch: 78, batch: 215/218] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2520e-02, 9.6307e-01, 6.1565e-03, 6.5437e-08, 3.4106e-07, 1.1943e-02,
        6.3114e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.001

[Epoch: 79, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9331e-01, 3.1431e-02, 2.3086e-02, 1.5943e-02, 1.5809e-05, 1.3639e-02,
        2.2580e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 79, batch: 86/218] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2826e-01, 1.0853e-02, 1.5219e-02, 1.1843e-04, 2.5577e-02, 9.7176e-03,
        1.0255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 79, batch: 129/218] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1296e-02, 1.3631e-05, 1.0290e-02, 9.5609e-01, 2.0234e-06, 1.1589e-02,
        1.0722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.068

[Epoch: 79, batch: 172/218] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3408e-02, 3.0352e-05, 8.6969e-01, 4.3145e-02, 6.1387e-06, 4.6360e-06,
        4.3720e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.763

[Epoch: 79, batch: 215/218] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5378e-02, 9.5045e-01, 1.1294e-02, 1.9098e-07, 4.1272e-07, 1.5867e-02,
        7.0141e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.003

[Epoch: 80, batch: 43/218] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8098e-01, 3.9313e-02, 1.9432e-02, 2.8735e-02, 4.2063e-05, 2.2780e-02,
        8.7202e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 80, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5365e-01, 1.4448e-02, 9.9850e-03, 2.5227e-05, 3.0994e-03, 1.1847e-02,
        6.9465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 80, batch: 129/218] total loss per batch: 0.521
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2627e-03, 5.3243e-06, 1.8595e-02, 9.3609e-01, 1.6549e-05, 1.4297e-02,
        2.5730e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 80, batch: 172/218] total loss per batch: 0.528
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7317e-02, 8.2785e-05, 8.1681e-01, 8.5072e-02, 5.0616e-06, 8.2319e-06,
        5.0702e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.740

[Epoch: 80, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2427e-02, 9.5501e-01, 1.4397e-02, 7.7453e-08, 2.1148e-07, 1.4703e-02,
        3.4598e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 81, batch: 43/218] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.2192e-01, 5.2351e-02, 3.2945e-02, 1.6165e-02, 1.3088e-05, 5.5046e-02,
        2.1557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 81, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5722e-01, 5.5832e-03, 7.7715e-03, 4.4920e-05, 9.0583e-03, 1.4433e-02,
        5.8845e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 81, batch: 129/218] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.4702e-03, 1.2823e-05, 1.1389e-02, 9.5639e-01, 4.7562e-06, 1.9141e-02,
        4.5960e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.042

[Epoch: 81, batch: 172/218] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.7451e-02, 1.3017e-05, 8.2296e-01, 6.1603e-02, 2.7114e-05, 1.3649e-05,
        5.7930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 81, batch: 215/218] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5512e-02, 9.5256e-01, 8.4588e-03, 1.5088e-07, 1.8814e-07, 1.9210e-02,
        4.2574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 82, batch: 43/218] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.0392e-01, 2.9065e-02, 2.5329e-02, 1.4362e-02, 1.9606e-05, 1.7293e-02,
        1.0009e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.065

[Epoch: 82, batch: 86/218] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5537e-01, 6.9832e-03, 1.1495e-02, 3.6478e-05, 8.0848e-03, 1.0241e-02,
        7.7878e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 82, batch: 129/218] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.7561e-03, 1.7475e-06, 5.5639e-03, 9.7503e-01, 1.2325e-06, 9.4030e-03,
        4.2459e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.046

[Epoch: 82, batch: 172/218] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9635e-02, 4.7116e-05, 8.2982e-01, 7.5503e-02, 7.1138e-06, 1.7437e-05,
        4.4973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 82, batch: 215/218] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([2.1781e-02, 9.4830e-01, 8.7981e-03, 1.3465e-07, 1.6754e-07, 1.6473e-02,
        4.6460e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 83, batch: 43/218] total loss per batch: 0.555
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([7.5206e-01, 4.7200e-02, 9.8317e-02, 5.8531e-02, 7.7076e-06, 3.1535e-02,
        1.2344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 83, batch: 86/218] total loss per batch: 0.532
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5053e-01, 7.3463e-03, 6.1966e-03, 6.2031e-05, 1.0766e-02, 1.6200e-02,
        8.9019e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 83, batch: 129/218] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.9780e-03, 3.1259e-06, 1.1823e-02, 9.6608e-01, 3.0314e-06, 1.0403e-02,
        7.7094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.067

[Epoch: 83, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3932e-02, 4.3665e-05, 8.2228e-01, 8.3420e-02, 1.4225e-05, 1.0804e-05,
        5.0302e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.714

[Epoch: 83, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7119e-02, 9.4811e-01, 1.3236e-02, 4.7759e-08, 5.4824e-08, 1.7184e-02,
        4.3516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 84, batch: 43/218] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8022e-01, 3.1456e-02, 1.4053e-02, 3.5236e-02, 5.5973e-05, 2.5957e-02,
        1.3026e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 84, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4173e-01, 1.3314e-02, 8.6366e-03, 5.1071e-05, 6.9088e-03, 1.2928e-02,
        1.6430e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 84, batch: 129/218] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.9780e-03, 1.1129e-05, 6.7198e-03, 9.7077e-01, 3.2348e-06, 9.6330e-03,
        5.8892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.035

[Epoch: 84, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7750e-02, 6.3619e-05, 8.2314e-01, 7.7046e-02, 8.0682e-06, 1.4470e-05,
        5.1977e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 84, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2743e-02, 9.5969e-01, 9.9896e-03, 7.0633e-08, 1.1669e-07, 1.3023e-02,
        4.5512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 85, batch: 43/218] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7055e-01, 3.7915e-02, 2.3255e-02, 2.5678e-02, 2.5395e-05, 2.6135e-02,
        1.6445e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 85, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5279e-01, 8.7702e-03, 8.1646e-03, 2.8878e-05, 9.3191e-03, 1.2998e-02,
        7.9337e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 85, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.5346e-03, 3.9552e-06, 1.2715e-02, 9.6090e-01, 2.7851e-06, 1.1761e-02,
        9.0803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.053

[Epoch: 85, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8257e-02, 8.4368e-05, 8.2476e-01, 7.3969e-02, 5.8090e-06, 7.2197e-06,
        5.2918e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.734

[Epoch: 85, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6680e-02, 9.5223e-01, 9.7276e-03, 4.5504e-08, 6.1629e-08, 1.5020e-02,
        6.3425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 86, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7320e-01, 3.8687e-02, 2.5833e-02, 1.9283e-02, 2.3647e-05, 2.7480e-02,
        1.5492e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 86, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4892e-01, 1.0812e-02, 1.0224e-02, 4.8141e-05, 1.1895e-02, 8.3438e-03,
        9.7571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 86, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.5966e-03, 4.3180e-06, 1.0435e-02, 9.6296e-01, 2.0865e-06, 1.1542e-02,
        8.4567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 86, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7958e-02, 3.7526e-05, 8.3074e-01, 7.4899e-02, 4.8289e-06, 6.6567e-06,
        4.6350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.691

[Epoch: 86, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5964e-02, 9.5174e-01, 1.0188e-02, 4.4275e-08, 6.2817e-08, 1.6735e-02,
        5.3707e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 87, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6681e-01, 3.7659e-02, 2.7971e-02, 1.9510e-02, 2.0275e-05, 2.9760e-02,
        1.8268e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 87, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4874e-01, 1.0483e-02, 9.2272e-03, 3.1049e-05, 9.0827e-03, 1.3871e-02,
        8.5634e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 87, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3106e-03, 3.3680e-06, 9.4530e-03, 9.6599e-01, 2.2132e-06, 1.1287e-02,
        8.9538e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.058

[Epoch: 87, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7907e-02, 3.6072e-05, 8.2210e-01, 7.7815e-02, 4.4284e-06, 4.9590e-06,
        5.2135e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.763

[Epoch: 87, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6850e-02, 9.4894e-01, 1.1111e-02, 3.9325e-08, 5.4777e-08, 1.7279e-02,
        5.8166e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 88, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7231e-01, 3.4617e-02, 2.8814e-02, 2.4887e-02, 1.8709e-05, 2.5261e-02,
        1.4089e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 88, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5156e-01, 9.3352e-03, 1.0311e-02, 2.7626e-05, 1.0768e-02, 8.5713e-03,
        9.4310e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 88, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9631e-03, 2.5601e-06, 1.1824e-02, 9.6549e-01, 1.5686e-06, 8.1877e-03,
        9.5281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.063

[Epoch: 88, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0971e-02, 2.4619e-05, 8.2528e-01, 7.4494e-02, 3.6479e-06, 5.5340e-06,
        4.9220e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.714

[Epoch: 88, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2252e-02, 9.5956e-01, 9.4232e-03, 2.6113e-08, 3.8370e-08, 1.3853e-02,
        4.9128e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 89, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7870e-01, 2.8236e-02, 2.6391e-02, 2.6205e-02, 1.2944e-05, 2.6281e-02,
        1.4174e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 89, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4304e-01, 1.1663e-02, 1.0101e-02, 2.6115e-05, 1.0603e-02, 1.4328e-02,
        1.0236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 89, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3827e-03, 1.9236e-06, 8.7096e-03, 9.6143e-01, 1.8948e-06, 1.4151e-02,
        1.0322e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.064

[Epoch: 89, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6515e-02, 4.3313e-05, 8.3075e-01, 7.3684e-02, 4.8845e-06, 5.8150e-06,
        4.8998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.720

[Epoch: 89, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4143e-02, 9.5665e-01, 8.2786e-03, 3.3030e-08, 4.5248e-08, 1.5710e-02,
        5.2136e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 90, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6696e-01, 3.4652e-02, 2.7341e-02, 2.8162e-02, 2.4762e-05, 2.6232e-02,
        1.6624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 90, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5377e-01, 9.9780e-03, 9.6753e-03, 4.2362e-05, 1.1549e-02, 7.1534e-03,
        7.8340e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 90, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8069e-03, 2.9591e-06, 1.0185e-02, 9.6542e-01, 1.9020e-06, 1.0332e-02,
        8.2495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 90, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.0186e-02, 7.6306e-05, 8.2688e-01, 6.4731e-02, 3.6066e-06, 5.8454e-06,
        4.8119e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.707

[Epoch: 90, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2215e-02, 9.5691e-01, 1.1434e-02, 4.5124e-08, 4.0385e-08, 1.3973e-02,
        5.4693e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 91, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6997e-01, 2.7793e-02, 3.0795e-02, 3.1427e-02, 1.2669e-05, 2.4609e-02,
        1.5396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 91, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5599e-01, 8.7731e-03, 7.8780e-03, 2.9602e-05, 8.0067e-03, 9.9330e-03,
        9.3946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 91, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0897e-03, 2.6072e-06, 7.7333e-03, 9.6881e-01, 2.0148e-06, 8.9754e-03,
        9.3915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.055

[Epoch: 91, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7569e-02, 3.5290e-05, 8.1049e-01, 8.7022e-02, 1.2414e-05, 6.9712e-06,
        5.4866e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.772

[Epoch: 91, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3435e-02, 9.5555e-01, 9.4363e-03, 4.5565e-08, 9.2146e-08, 1.6344e-02,
        5.2385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 92, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.1230e-01, 4.4456e-02, 3.7351e-02, 2.9691e-02, 2.9456e-05, 5.2188e-02,
        2.3981e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 92, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4792e-01, 1.0513e-02, 8.4988e-03, 3.1397e-05, 1.2795e-02, 7.1818e-03,
        1.3059e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 92, batch: 129/218] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9252e-03, 1.1620e-05, 1.4409e-02, 9.3680e-01, 7.4797e-06, 1.5480e-02,
        2.8363e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.082

[Epoch: 92, batch: 172/218] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([1.0971e-02, 3.0753e-06, 9.4481e-01, 2.9952e-02, 3.3127e-05, 9.5484e-06,
        1.4222e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.201

[Epoch: 92, batch: 215/218] total loss per batch: 0.588
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.9309e-02, 8.7647e-01, 1.1053e-02, 1.8867e-07, 7.1506e-08, 7.7571e-02,
        1.5593e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.008

[Epoch: 93, batch: 43/218] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3493e-01, 5.5348e-02, 9.3671e-03, 4.7551e-02, 1.0866e-04, 3.2090e-02,
        2.0606e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.023

[Epoch: 93, batch: 86/218] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([8.8761e-01, 5.9457e-03, 1.3737e-02, 1.5702e-05, 4.2924e-03, 1.1779e-02,
        7.6616e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.013

[Epoch: 93, batch: 129/218] total loss per batch: 0.536
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5876e-03, 1.3386e-05, 5.8998e-03, 9.7385e-01, 6.5267e-07, 6.3761e-03,
        6.2714e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.019

[Epoch: 93, batch: 172/218] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6814e-02, 2.7239e-04, 8.0115e-01, 5.9196e-02, 5.6509e-06, 1.4067e-05,
        8.2545e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.618

[Epoch: 93, batch: 215/218] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6415e-02, 9.5976e-01, 7.6690e-03, 8.6087e-08, 2.4661e-07, 9.7209e-03,
        6.4400e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.040

[Epoch: 94, batch: 43/218] total loss per batch: 0.561
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.2893e-01, 5.6637e-02, 3.5914e-02, 2.9260e-02, 6.0482e-05, 2.9501e-02,
        1.9696e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 94, batch: 86/218] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1712e-01, 2.8776e-02, 1.2625e-02, 4.4277e-05, 1.1386e-02, 2.3076e-02,
        6.9725e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.020

[Epoch: 94, batch: 129/218] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3926e-03, 1.3236e-05, 7.4868e-03, 9.7767e-01, 2.3915e-06, 5.9110e-03,
        4.5194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.032

[Epoch: 94, batch: 172/218] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.2271e-02, 2.4542e-05, 8.1430e-01, 6.5687e-02, 9.2812e-06, 2.1948e-05,
        5.7690e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.896

[Epoch: 94, batch: 215/218] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5029e-02, 9.6038e-01, 1.1005e-02, 9.2282e-08, 1.9022e-07, 6.7349e-03,
        6.8522e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 95, batch: 43/218] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6260e-01, 4.5784e-02, 3.3188e-02, 1.7118e-02, 3.7363e-05, 2.5602e-02,
        1.5671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 95, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5041e-01, 1.4895e-02, 7.3945e-03, 3.0968e-05, 1.0380e-02, 1.1175e-02,
        5.7174e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.012

[Epoch: 95, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3724e-03, 1.4560e-05, 8.5223e-03, 9.6948e-01, 1.3615e-06, 1.0667e-02,
        5.9436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.032

[Epoch: 95, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.9601e-02, 2.6733e-05, 8.0955e-01, 8.4068e-02, 5.9534e-06, 1.5991e-05,
        4.6727e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.857

[Epoch: 95, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7067e-02, 9.5873e-01, 1.0451e-02, 5.3787e-08, 5.7188e-08, 8.6495e-03,
        5.1004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 96, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9014e-01, 3.0886e-02, 2.6236e-02, 1.8006e-02, 1.6582e-05, 2.2411e-02,
        1.2302e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 96, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4944e-01, 1.2914e-02, 1.0492e-02, 2.1688e-05, 1.0881e-02, 1.0475e-02,
        5.7783e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 96, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8958e-03, 8.0201e-06, 1.0095e-02, 9.6487e-01, 8.0219e-07, 1.0946e-02,
        8.1846e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.037

[Epoch: 96, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6029e-02, 1.8496e-05, 8.2335e-01, 7.5700e-02, 5.6821e-06, 1.1040e-05,
        4.4887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.814

[Epoch: 96, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5649e-02, 9.5883e-01, 1.0005e-02, 5.6888e-08, 5.5321e-08, 9.7720e-03,
        5.7391e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 97, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7822e-01, 2.8087e-02, 3.0864e-02, 2.2664e-02, 1.7993e-05, 2.6572e-02,
        1.3577e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 97, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5312e-01, 1.1999e-02, 8.4501e-03, 2.1227e-05, 1.1862e-02, 8.1450e-03,
        6.4028e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 97, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2623e-03, 7.2975e-06, 9.1001e-03, 9.6823e-01, 6.0049e-07, 9.8405e-03,
        7.5600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.045

[Epoch: 97, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1161e-02, 1.1762e-05, 8.1407e-01, 8.3633e-02, 3.5345e-06, 1.0012e-05,
        5.1111e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.785

[Epoch: 97, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4298e-02, 9.5992e-01, 9.7013e-03, 4.0751e-08, 4.2478e-08, 1.0787e-02,
        5.2917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 98, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6911e-01, 3.1894e-02, 3.1491e-02, 2.5747e-02, 1.3856e-05, 2.6231e-02,
        1.5517e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 98, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5322e-01, 1.0969e-02, 8.9770e-03, 1.5482e-05, 1.0027e-02, 9.6633e-03,
        7.1303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 98, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9497e-03, 5.4905e-06, 9.9725e-03, 9.6657e-01, 3.9426e-07, 9.8135e-03,
        8.6837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.048

[Epoch: 98, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9728e-02, 9.4288e-06, 8.2850e-01, 7.2829e-02, 2.8132e-06, 7.8339e-06,
        4.8924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.761

[Epoch: 98, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5407e-02, 9.5575e-01, 1.0222e-02, 3.5627e-08, 3.1278e-08, 1.2847e-02,
        5.7742e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 99, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7263e-01, 3.1009e-02, 3.0483e-02, 2.4760e-02, 1.1391e-05, 2.5802e-02,
        1.5305e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 99, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4899e-01, 1.3442e-02, 9.5889e-03, 1.7336e-05, 1.1288e-02, 8.5632e-03,
        8.1095e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 99, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0628e-03, 4.9024e-06, 9.6455e-03, 9.6747e-01, 3.3074e-07, 9.9799e-03,
        7.8413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.052

[Epoch: 99, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9903e-02, 9.1801e-06, 8.1724e-01, 7.9936e-02, 2.3049e-06, 7.1233e-06,
        5.2904e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.752

[Epoch: 99, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4501e-02, 9.5830e-01, 9.5218e-03, 2.6121e-08, 2.1491e-08, 1.2425e-02,
        5.2554e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 100, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6994e-01, 3.2602e-02, 3.1649e-02, 2.5634e-02, 9.9524e-06, 2.5000e-02,
        1.5168e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.046

[Epoch: 100, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5433e-01, 9.2373e-03, 9.0986e-03, 9.9882e-06, 9.5608e-03, 9.8834e-03,
        7.8846e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 100, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5908e-03, 4.0123e-06, 9.3941e-03, 9.6718e-01, 2.8706e-07, 9.5993e-03,
        9.2307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.056

[Epoch: 100, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8712e-02, 6.5783e-06, 8.3121e-01, 7.1342e-02, 1.9555e-06, 6.4780e-06,
        4.8721e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.732

[Epoch: 100, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4254e-02, 9.5589e-01, 1.0289e-02, 2.5839e-08, 1.9234e-08, 1.3844e-02,
        5.7182e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 101, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7287e-01, 3.1714e-02, 2.9760e-02, 2.3659e-02, 8.0172e-06, 2.7556e-02,
        1.4428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 101, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4360e-01, 1.4858e-02, 1.0681e-02, 1.6501e-05, 1.1932e-02, 8.3276e-03,
        1.0580e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 101, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0809e-03, 3.3654e-06, 1.0142e-02, 9.6557e-01, 2.5249e-07, 1.0635e-02,
        8.5691e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.056

[Epoch: 101, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2109e-02, 6.9836e-06, 8.1490e-01, 7.9977e-02, 1.7878e-06, 5.5227e-06,
        5.3004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.740

[Epoch: 101, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5125e-02, 9.5585e-01, 9.8165e-03, 1.9526e-08, 1.5997e-08, 1.4006e-02,
        5.1987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 102, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6390e-01, 3.5635e-02, 3.1309e-02, 2.7098e-02, 8.2240e-06, 2.4907e-02,
        1.7137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 102, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5340e-01, 9.9267e-03, 9.3380e-03, 8.8373e-06, 9.4314e-03, 9.0121e-03,
        8.8812e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 102, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8098e-03, 2.4486e-06, 9.0243e-03, 9.6705e-01, 1.8793e-07, 9.8007e-03,
        9.3097e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.059

[Epoch: 102, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6108e-02, 5.0203e-06, 8.3960e-01, 6.6560e-02, 1.6509e-06, 5.0881e-06,
        4.7717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.730

[Epoch: 102, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4275e-02, 9.5579e-01, 1.0292e-02, 1.9324e-08, 1.2781e-08, 1.4117e-02,
        5.5256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 103, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7334e-01, 3.1418e-02, 2.9761e-02, 2.2509e-02, 7.2495e-06, 2.8476e-02,
        1.4493e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 103, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4667e-01, 1.1912e-02, 9.8226e-03, 1.2515e-05, 1.1105e-02, 1.0783e-02,
        9.6915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 103, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7838e-03, 2.5827e-06, 1.1000e-02, 9.6461e-01, 2.0173e-07, 1.0899e-02,
        8.7059e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.063

[Epoch: 103, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3708e-02, 6.8066e-06, 8.0787e-01, 8.3850e-02, 1.6948e-06, 4.7930e-06,
        5.4558e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 103, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4850e-02, 9.5610e-01, 9.8263e-03, 1.6438e-08, 1.2644e-08, 1.4234e-02,
        4.9846e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 104, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6600e-01, 3.7390e-02, 3.0866e-02, 2.4756e-02, 6.6546e-06, 2.4990e-02,
        1.5992e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 104, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5247e-01, 9.6758e-03, 1.0422e-02, 7.7598e-06, 9.0685e-03, 9.0197e-03,
        9.3348e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 104, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3045e-03, 2.2537e-06, 9.8899e-03, 9.6423e-01, 1.4878e-07, 1.0211e-02,
        1.0364e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.064

[Epoch: 104, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6449e-02, 3.9664e-06, 8.4461e-01, 6.3175e-02, 1.3731e-06, 4.1479e-06,
        4.5758e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.729

[Epoch: 104, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3714e-02, 9.5663e-01, 1.0187e-02, 1.7427e-08, 9.4344e-09, 1.3909e-02,
        5.5585e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 105, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7802e-01, 2.9098e-02, 2.8758e-02, 2.3728e-02, 6.8057e-06, 2.6238e-02,
        1.4151e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 105, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4808e-01, 1.1632e-02, 9.1836e-03, 1.1137e-05, 1.1191e-02, 1.0831e-02,
        9.0666e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 105, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5332e-03, 2.7967e-06, 1.0706e-02, 9.6575e-01, 2.3353e-07, 1.0320e-02,
        8.6857e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.063

[Epoch: 105, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3891e-02, 6.9819e-06, 8.0546e-01, 8.5777e-02, 1.3216e-06, 4.8019e-06,
        5.4862e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.724

[Epoch: 105, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5774e-02, 9.5446e-01, 9.7687e-03, 1.5720e-08, 1.2448e-08, 1.5010e-02,
        4.9875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 106, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6343e-01, 4.2046e-02, 2.9993e-02, 2.6615e-02, 7.1797e-06, 2.2952e-02,
        1.4956e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 106, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5154e-01, 9.3188e-03, 1.1069e-02, 9.1890e-06, 9.0793e-03, 9.2569e-03,
        9.7246e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 106, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2774e-03, 2.3707e-06, 1.0110e-02, 9.6126e-01, 1.7497e-07, 1.0514e-02,
        1.1841e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.068

[Epoch: 106, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3689e-02, 5.9310e-06, 8.4845e-01, 6.5133e-02, 1.7493e-06, 5.4172e-06,
        4.2712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.752

[Epoch: 106, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1905e-02, 9.6171e-01, 9.9288e-03, 3.2294e-08, 1.5354e-08, 1.0800e-02,
        5.6577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 107, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7686e-01, 2.6978e-02, 3.1200e-02, 2.2630e-02, 1.3299e-05, 2.8940e-02,
        1.3380e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 107, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5073e-01, 1.1276e-02, 8.6715e-03, 1.3392e-05, 1.1238e-02, 9.5399e-03,
        8.5309e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 107, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.7426e-03, 3.0609e-06, 1.0193e-02, 9.6576e-01, 5.0518e-07, 9.1667e-03,
        9.1333e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 107, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2681e-02, 1.0157e-05, 8.1084e-01, 8.3955e-02, 3.2026e-06, 6.5963e-06,
        5.2503e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.673

[Epoch: 107, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6393e-02, 9.5151e-01, 1.1132e-02, 3.6698e-08, 3.1604e-08, 1.5987e-02,
        4.9780e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 108, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6382e-01, 3.5220e-02, 3.1195e-02, 2.8815e-02, 1.5493e-05, 2.3046e-02,
        1.7884e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 108, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5140e-01, 9.5888e-03, 9.3184e-03, 2.2911e-05, 6.9677e-03, 1.1366e-02,
        1.1338e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 108, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.0271e-03, 4.5044e-06, 1.0387e-02, 9.6308e-01, 5.5546e-07, 1.0757e-02,
        1.2746e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 108, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7837e-02, 1.0999e-05, 8.3990e-01, 6.7023e-02, 4.0660e-06, 7.4487e-06,
        4.5217e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.790

[Epoch: 108, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4459e-02, 9.5967e-01, 9.0989e-03, 4.2803e-08, 2.2392e-08, 1.1752e-02,
        5.0195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 109, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5810e-01, 3.1876e-02, 3.8105e-02, 2.7613e-02, 2.0495e-05, 3.2335e-02,
        1.1945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 109, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3488e-01, 1.2025e-02, 1.2281e-02, 2.2729e-05, 1.9254e-02, 1.1512e-02,
        1.0029e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 109, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.4083e-03, 2.9199e-06, 1.2975e-02, 9.6025e-01, 7.4646e-07, 1.0177e-02,
        1.0181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 109, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1034e-02, 1.1920e-05, 8.1416e-01, 7.4374e-02, 4.4423e-06, 8.0570e-06,
        6.0406e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.703

[Epoch: 109, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3130e-02, 9.5273e-01, 1.2732e-02, 5.5383e-08, 1.3410e-07, 1.5840e-02,
        5.5644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 110, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8469e-01, 2.9077e-02, 2.3464e-02, 3.1423e-02, 8.0798e-06, 1.8631e-02,
        1.2708e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 110, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4738e-01, 9.9296e-03, 1.0421e-02, 4.8297e-05, 5.9986e-03, 1.4131e-02,
        1.2092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 110, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.2938e-03, 2.0218e-06, 8.3630e-03, 9.6442e-01, 7.6199e-07, 1.2672e-02,
        1.1253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 110, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8358e-02, 1.3835e-05, 8.1920e-01, 8.1153e-02, 2.5252e-06, 8.5179e-06,
        5.1260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.759

[Epoch: 110, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4817e-02, 9.5714e-01, 9.2077e-03, 6.2863e-08, 7.0706e-08, 1.3862e-02,
        4.9707e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 111, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6642e-01, 3.4571e-02, 2.9087e-02, 2.2005e-02, 1.4752e-05, 2.9335e-02,
        1.8567e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 111, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5028e-01, 1.0330e-02, 9.3989e-03, 2.5025e-05, 1.4189e-02, 8.3571e-03,
        7.4232e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 111, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.0218e-03, 1.3675e-06, 8.7996e-03, 9.6714e-01, 3.9370e-07, 8.1770e-03,
        9.8571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.068

[Epoch: 111, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0164e-02, 9.2673e-06, 8.2578e-01, 7.2889e-02, 2.5741e-06, 8.4080e-06,
        5.1149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.699

[Epoch: 111, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2862e-02, 9.6024e-01, 1.1006e-02, 4.8697e-08, 7.3663e-08, 9.7337e-03,
        6.1609e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 112, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7309e-01, 3.7831e-02, 2.9122e-02, 1.8384e-02, 1.0127e-05, 2.4734e-02,
        1.6833e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 112, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5033e-01, 1.0970e-02, 9.5456e-03, 4.6078e-05, 7.7810e-03, 1.0457e-02,
        1.0871e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 112, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.6379e-03, 2.0625e-06, 1.1388e-02, 9.5855e-01, 1.1782e-06, 1.4174e-02,
        1.2247e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 112, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1824e-02, 1.2059e-05, 8.4429e-01, 6.7308e-02, 2.1626e-06, 6.1381e-06,
        4.6555e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.794

[Epoch: 112, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7458e-02, 9.5448e-01, 1.0476e-02, 4.5494e-08, 6.3105e-08, 1.2976e-02,
        4.6067e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 113, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6706e-01, 3.8197e-02, 3.0191e-02, 2.2853e-02, 1.2080e-05, 2.8546e-02,
        1.3144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 113, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4860e-01, 9.7966e-03, 9.5728e-03, 3.0437e-05, 1.2197e-02, 1.1300e-02,
        8.5038e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 113, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0984e-03, 1.7679e-06, 1.0206e-02, 9.6745e-01, 4.7920e-07, 8.7029e-03,
        8.5444e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 113, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6599e-02, 1.2812e-05, 7.9320e-01, 9.1714e-02, 4.8251e-06, 1.2981e-05,
        5.8461e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.715

[Epoch: 113, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3173e-02, 9.6132e-01, 7.9572e-03, 5.3881e-08, 7.7701e-08, 1.1451e-02,
        6.0943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 114, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9262e-01, 2.7355e-02, 2.5358e-02, 2.2887e-02, 1.3436e-05, 1.9868e-02,
        1.1901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 114, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4504e-01, 1.1281e-02, 1.0395e-02, 3.5319e-05, 8.0384e-03, 1.1605e-02,
        1.3604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 114, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8574e-03, 1.5031e-06, 1.0778e-02, 9.6118e-01, 1.0646e-06, 1.1255e-02,
        1.0929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.072

[Epoch: 114, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4800e-02, 1.8519e-05, 8.4928e-01, 6.1804e-02, 3.5722e-06, 5.4189e-06,
        4.4088e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.723

[Epoch: 114, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3965e-02, 9.5710e-01, 1.0437e-02, 2.8518e-08, 5.3291e-08, 1.4174e-02,
        4.3201e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 115, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5544e-01, 3.3050e-02, 3.4567e-02, 3.4757e-02, 1.9553e-05, 2.9989e-02,
        1.2175e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 115, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4987e-01, 1.0972e-02, 9.1719e-03, 2.5227e-05, 1.4862e-02, 8.6816e-03,
        6.4226e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 115, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7118e-03, 2.9223e-06, 1.1775e-02, 9.6491e-01, 5.7089e-07, 1.0025e-02,
        8.5719e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.072

[Epoch: 115, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0961e-02, 1.2550e-05, 8.4870e-01, 6.8223e-02, 2.3630e-06, 8.4569e-06,
        4.2093e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.715

[Epoch: 115, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4729e-02, 9.5737e-01, 8.0646e-03, 5.0677e-08, 1.3360e-07, 1.4727e-02,
        5.1061e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 116, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7637e-01, 3.3665e-02, 2.8461e-02, 2.8686e-02, 7.6566e-06, 1.8581e-02,
        1.4226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.064

[Epoch: 116, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4778e-01, 1.0444e-02, 1.1736e-02, 2.3741e-05, 6.7997e-03, 1.4060e-02,
        9.1588e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 116, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6576e-03, 9.2362e-07, 9.0795e-03, 9.6567e-01, 4.6196e-07, 1.0896e-02,
        9.6936e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.081

[Epoch: 116, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.9481e-02, 1.6003e-05, 7.6967e-01, 9.0287e-02, 3.4277e-06, 6.8988e-06,
        7.0539e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.759

[Epoch: 116, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4358e-02, 9.5636e-01, 1.2233e-02, 3.1729e-08, 5.1245e-08, 1.2750e-02,
        4.2964e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 117, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6662e-01, 3.7121e-02, 2.8335e-02, 2.4796e-02, 1.1548e-05, 2.5575e-02,
        1.7542e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 117, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4599e-01, 1.0151e-02, 1.0869e-02, 3.6225e-05, 1.1090e-02, 1.0356e-02,
        1.1510e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 117, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.1078e-03, 1.4821e-06, 1.1242e-02, 9.6575e-01, 4.7637e-07, 7.7818e-03,
        1.0114e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.069

[Epoch: 117, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9856e-02, 7.8764e-06, 8.5624e-01, 6.4903e-02, 2.1897e-06, 7.2593e-06,
        3.8987e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.693

[Epoch: 117, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5922e-02, 9.5098e-01, 9.1717e-03, 2.9441e-08, 4.8261e-08, 1.8923e-02,
        5.0074e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 118, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5932e-01, 4.0292e-02, 3.4030e-02, 2.7272e-02, 1.2308e-05, 2.4222e-02,
        1.4854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 118, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5782e-01, 1.0632e-02, 7.8349e-03, 2.0934e-05, 6.1438e-03, 1.0894e-02,
        6.6557e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 118, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.1289e-03, 2.0871e-06, 1.0038e-02, 9.6664e-01, 6.4166e-07, 1.0396e-02,
        8.7896e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 118, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8798e-02, 1.7785e-05, 8.0953e-01, 8.3147e-02, 3.4840e-06, 9.9418e-06,
        5.8496e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.759

[Epoch: 118, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0109e-02, 9.6177e-01, 1.1071e-02, 5.8420e-08, 1.2806e-07, 1.2803e-02,
        4.2488e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 119, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5962e-01, 2.9393e-02, 3.3600e-02, 2.3676e-02, 1.4066e-05, 2.9884e-02,
        2.3812e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 119, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4557e-01, 1.0660e-02, 1.0569e-02, 3.2381e-05, 8.5900e-03, 8.7097e-03,
        1.5872e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 119, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3803e-03, 1.4176e-06, 1.1586e-02, 9.6577e-01, 1.9758e-06, 8.9194e-03,
        9.3364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 119, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7026e-02, 1.7181e-05, 8.2707e-01, 8.3394e-02, 2.6408e-06, 7.5286e-06,
        4.2479e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.731

[Epoch: 119, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7690e-02, 9.4560e-01, 1.1208e-02, 4.2441e-08, 1.3795e-07, 1.8735e-02,
        6.7681e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 120, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6506e-01, 3.6992e-02, 3.0018e-02, 2.9229e-02, 1.9201e-05, 2.7382e-02,
        1.1295e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 120, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4987e-01, 1.0994e-02, 9.2991e-03, 2.9766e-05, 9.7180e-03, 1.1864e-02,
        8.2226e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 120, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.9533e-03, 1.6191e-06, 1.0017e-02, 9.6873e-01, 8.8845e-07, 8.7308e-03,
        9.5631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 120, batch: 172/218] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7790e-02, 1.1159e-05, 8.4435e-01, 6.0090e-02, 1.6692e-06, 4.0944e-06,
        4.7753e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.689

[Epoch: 120, batch: 215/218] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([2.3012e-02, 9.3949e-01, 1.0215e-02, 7.6137e-08, 7.2504e-08, 2.0250e-02,
        7.0289e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.016

[Epoch: 121, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7393e-01, 3.3309e-02, 3.3335e-02, 2.2932e-02, 5.6706e-06, 2.1088e-02,
        1.5397e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.034

[Epoch: 121, batch: 86/218] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5723e-01, 9.3435e-03, 8.1372e-03, 2.0982e-05, 6.6719e-03, 1.1835e-02,
        6.7643e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 121, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6851e-03, 2.8905e-06, 9.0878e-03, 9.5857e-01, 6.5582e-07, 1.4694e-02,
        1.2962e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 121, batch: 172/218] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4958e-02, 3.2609e-05, 7.9628e-01, 8.8880e-02, 4.1718e-06, 1.2416e-05,
        5.9832e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.741

[Epoch: 121, batch: 215/218] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4611e-02, 9.5038e-01, 1.2167e-02, 5.1470e-08, 9.7032e-08, 1.7255e-02,
        5.5876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 122, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5542e-01, 4.7774e-02, 3.0914e-02, 1.2323e-02, 6.6339e-06, 3.3841e-02,
        1.9717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 122, batch: 86/218] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4793e-01, 1.2254e-02, 8.8272e-03, 2.2667e-05, 5.6633e-03, 1.8389e-02,
        6.9091e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 122, batch: 129/218] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2030e-03, 3.2986e-06, 1.4007e-02, 9.6103e-01, 4.6743e-07, 1.2180e-02,
        7.5761e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 122, batch: 172/218] total loss per batch: 0.526
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6817e-02, 7.1614e-05, 8.3798e-01, 6.2674e-02, 7.9042e-06, 9.0962e-06,
        4.2440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.704

[Epoch: 122, batch: 215/218] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([2.2254e-02, 9.4187e-01, 1.7608e-02, 4.8984e-08, 1.3120e-06, 1.2172e-02,
        6.0967e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 123, batch: 43/218] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9592e-01, 2.1149e-02, 2.4412e-02, 2.5723e-02, 1.0427e-05, 1.8108e-02,
        1.4675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 123, batch: 86/218] total loss per batch: 0.533
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.1680e-01, 1.2662e-02, 1.5482e-02, 4.5403e-05, 3.0289e-02, 1.4563e-02,
        1.0160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.012

[Epoch: 123, batch: 129/218] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5577e-03, 1.2688e-06, 9.3610e-03, 9.6934e-01, 1.0216e-06, 8.9622e-03,
        8.7720e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.065

[Epoch: 123, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7556e-02, 2.3020e-05, 8.2419e-01, 7.3251e-02, 2.6436e-06, 1.2094e-05,
        5.4968e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.753

[Epoch: 123, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1950e-02, 9.5951e-01, 1.0290e-02, 3.4520e-08, 7.6686e-07, 1.2333e-02,
        5.9163e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 124, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9798e-01, 2.2473e-02, 2.4253e-02, 2.4218e-02, 1.0648e-05, 2.1829e-02,
        9.2362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 124, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5782e-01, 9.9997e-03, 7.3768e-03, 2.5127e-05, 6.9302e-03, 9.3100e-03,
        8.5430e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 124, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.7085e-03, 1.5667e-06, 9.4505e-03, 9.6540e-01, 9.7345e-07, 9.9164e-03,
        9.5206e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.069

[Epoch: 124, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4156e-02, 4.3077e-05, 8.0731e-01, 8.5546e-02, 2.2852e-06, 9.6475e-06,
        5.2929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.731

[Epoch: 124, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4992e-02, 9.5488e-01, 9.1084e-03, 3.1066e-08, 4.3577e-08, 1.4752e-02,
        6.2636e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 125, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5034e-01, 3.7514e-02, 3.3247e-02, 3.6711e-02, 1.0253e-05, 2.6924e-02,
        1.5259e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 125, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5261e-01, 9.9990e-03, 8.9921e-03, 2.0271e-05, 9.5870e-03, 9.2181e-03,
        9.5709e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 125, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3402e-03, 1.4786e-06, 9.6501e-03, 9.6465e-01, 6.1897e-07, 1.0075e-02,
        1.0287e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.080

[Epoch: 125, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1272e-02, 4.0375e-05, 8.2672e-01, 7.2484e-02, 2.1370e-06, 6.8129e-06,
        4.9473e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.727

[Epoch: 125, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8244e-02, 9.4577e-01, 1.1878e-02, 4.9641e-08, 4.9155e-07, 1.6318e-02,
        7.7896e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.037

[Epoch: 126, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8544e-01, 3.1309e-02, 2.7261e-02, 2.4341e-02, 5.6476e-06, 2.0249e-02,
        1.1396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.061

[Epoch: 126, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4807e-01, 1.0765e-02, 9.8269e-03, 1.9975e-05, 9.4053e-03, 1.0843e-02,
        1.1071e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 126, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2639e-03, 1.8786e-06, 9.5412e-03, 9.6458e-01, 5.4169e-07, 1.0885e-02,
        9.7256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 126, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2347e-02, 3.7998e-05, 8.2203e-01, 7.7294e-02, 2.1750e-06, 8.2578e-06,
        4.8284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.711

[Epoch: 126, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2045e-02, 9.5838e-01, 1.0167e-02, 2.8291e-08, 1.3824e-07, 1.3899e-02,
        5.5075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 127, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7303e-01, 3.4032e-02, 2.9738e-02, 2.4753e-02, 8.2174e-06, 2.4104e-02,
        1.4339e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 127, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4850e-01, 9.4541e-03, 1.0162e-02, 1.8931e-05, 1.0988e-02, 1.1424e-02,
        9.4549e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 127, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.5332e-03, 1.7312e-06, 9.6108e-03, 9.6454e-01, 3.6304e-07, 1.0404e-02,
        9.9132e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 127, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6869e-02, 2.7952e-05, 8.3487e-01, 7.1793e-02, 1.4281e-06, 6.2199e-06,
        4.6432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.741

[Epoch: 127, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2758e-02, 9.5764e-01, 1.0139e-02, 2.4280e-08, 1.1177e-07, 1.4030e-02,
        5.4297e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 128, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6867e-01, 3.3358e-02, 3.4387e-02, 2.5691e-02, 6.7607e-06, 2.4140e-02,
        1.3745e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 128, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5435e-01, 9.5162e-03, 8.8228e-03, 1.3673e-05, 8.7934e-03, 8.7446e-03,
        9.7631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 128, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3367e-03, 1.3602e-06, 1.0161e-02, 9.6236e-01, 3.4413e-07, 1.1624e-02,
        1.0519e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 128, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8600e-02, 2.4928e-05, 8.2656e-01, 7.5476e-02, 9.9097e-07, 5.2922e-06,
        4.9330e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.713

[Epoch: 128, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3044e-02, 9.5769e-01, 9.4749e-03, 1.7100e-08, 8.4234e-08, 1.4298e-02,
        5.4957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 129, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5882e-01, 3.6536e-02, 3.0966e-02, 3.0521e-02, 5.6485e-06, 2.5679e-02,
        1.7476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 129, batch: 86/218] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4725e-01, 1.0063e-02, 1.0197e-02, 1.6778e-05, 1.0442e-02, 1.2321e-02,
        9.7097e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 129, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.4119e-03, 1.3661e-06, 1.0061e-02, 9.6472e-01, 3.0603e-07, 1.0609e-02,
        9.2004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 129, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9270e-02, 1.7694e-05, 8.3355e-01, 7.1845e-02, 9.6892e-07, 4.7355e-06,
        4.5308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.744

[Epoch: 129, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3709e-02, 9.5608e-01, 1.0421e-02, 1.2386e-08, 6.1973e-08, 1.4344e-02,
        5.4485e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 130, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7266e-01, 3.3133e-02, 3.1635e-02, 2.3207e-02, 5.2147e-06, 2.4204e-02,
        1.5153e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 130, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4668e-01, 1.0512e-02, 1.0811e-02, 1.5500e-05, 1.0315e-02, 9.8949e-03,
        1.1774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 130, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6396e-03, 8.4111e-07, 8.7644e-03, 9.6809e-01, 2.2887e-07, 8.6006e-03,
        9.9023e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.083

[Epoch: 130, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0161e-02, 2.4784e-05, 8.1906e-01, 8.0870e-02, 7.8107e-07, 4.6817e-06,
        4.9874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.706

[Epoch: 130, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3082e-02, 9.5676e-01, 9.7479e-03, 1.0564e-08, 5.4938e-08, 1.5146e-02,
        5.2671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 131, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5763e-01, 3.7109e-02, 3.1394e-02, 2.8487e-02, 4.0168e-06, 2.7331e-02,
        1.8041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 131, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4863e-01, 9.6699e-03, 1.1001e-02, 1.6185e-05, 1.0573e-02, 1.1527e-02,
        8.5832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 131, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.7041e-03, 8.9791e-07, 1.0109e-02, 9.6265e-01, 2.4359e-07, 1.1417e-02,
        1.0122e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 131, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8308e-02, 1.1311e-05, 8.3464e-01, 6.8953e-02, 1.3092e-06, 4.6839e-06,
        4.8079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.775

[Epoch: 131, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2507e-02, 9.6030e-01, 9.5086e-03, 1.0917e-08, 7.5859e-08, 1.3157e-02,
        4.5286e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 132, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8158e-01, 3.0479e-02, 2.9985e-02, 2.1462e-02, 6.7740e-06, 2.3419e-02,
        1.3069e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 132, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4528e-01, 1.0411e-02, 1.0869e-02, 1.6428e-05, 1.0873e-02, 1.0935e-02,
        1.1609e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 132, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.7387e-03, 9.9226e-07, 9.3779e-03, 9.6850e-01, 3.9419e-07, 9.6045e-03,
        8.7763e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.082

[Epoch: 132, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.6130e-02, 4.5819e-05, 7.6771e-01, 1.0258e-01, 1.3123e-06, 6.9219e-06,
        6.3528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.664

[Epoch: 132, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1471e-02, 9.6153e-01, 8.4212e-03, 1.2365e-08, 4.5193e-08, 1.3205e-02,
        5.3771e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 133, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9516e-01, 2.8612e-02, 2.2151e-02, 1.7629e-02, 4.2283e-06, 2.1107e-02,
        1.5338e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 133, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3583e-01, 1.0951e-02, 1.4779e-02, 2.7928e-05, 1.4385e-02, 1.2673e-02,
        1.1355e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.014

[Epoch: 133, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.9888e-03, 2.4916e-06, 1.0496e-02, 9.5776e-01, 3.5758e-07, 1.0389e-02,
        1.2359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.070

[Epoch: 133, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([2.8528e-02, 2.6172e-05, 9.0118e-01, 4.1429e-02, 1.4537e-06, 3.7268e-06,
        2.8830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.794

[Epoch: 133, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3251e-02, 9.5258e-01, 1.1386e-02, 2.4256e-08, 1.0082e-07, 1.6626e-02,
        6.1563e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 134, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5953e-01, 3.6058e-02, 3.2571e-02, 3.4581e-02, 9.8010e-06, 2.0816e-02,
        1.6430e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.068

[Epoch: 134, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5873e-01, 1.0044e-02, 7.3766e-03, 1.8924e-05, 8.7840e-03, 7.9445e-03,
        7.1023e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 134, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.1351e-03, 1.9295e-06, 1.2301e-02, 9.6144e-01, 8.4836e-07, 1.2030e-02,
        1.0091e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.066

[Epoch: 134, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0279e-02, 3.3897e-05, 8.0666e-01, 9.0227e-02, 1.2651e-06, 7.9087e-06,
        5.2791e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.678

[Epoch: 134, batch: 215/218] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8610e-02, 9.5240e-01, 1.0832e-02, 4.7729e-08, 1.9026e-07, 1.2435e-02,
        5.7239e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 135, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4994e-01, 3.7530e-02, 3.5281e-02, 2.9554e-02, 1.1245e-05, 3.0981e-02,
        1.6700e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.033

[Epoch: 135, batch: 86/218] total loss per batch: 0.529
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5031e-01, 1.0429e-02, 1.1953e-02, 1.6430e-05, 7.7603e-03, 7.9242e-03,
        1.1604e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 135, batch: 129/218] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.2029e-03, 1.7174e-06, 8.3244e-03, 9.6533e-01, 1.0359e-06, 1.2287e-02,
        1.0858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 135, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4461e-02, 3.9082e-05, 8.7038e-01, 4.9764e-02, 8.7562e-06, 6.9040e-06,
        3.5339e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.791

[Epoch: 135, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4363e-02, 9.4693e-01, 1.2981e-02, 5.2534e-08, 2.0781e-07, 1.9117e-02,
        6.6111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 136, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9004e-01, 3.2027e-02, 2.5318e-02, 2.0951e-02, 3.2316e-05, 1.8260e-02,
        1.3376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 136, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5066e-01, 1.2348e-02, 9.9111e-03, 5.6575e-05, 9.0850e-03, 1.0839e-02,
        7.1027e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 136, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.6490e-03, 1.1416e-06, 7.3473e-03, 9.7024e-01, 7.7393e-07, 6.6458e-03,
        1.2118e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.067

[Epoch: 136, batch: 172/218] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.6645e-02, 3.2282e-05, 7.5109e-01, 1.1061e-01, 5.0798e-06, 1.1317e-05,
        7.1601e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.674

[Epoch: 136, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4265e-02, 9.4839e-01, 1.4554e-02, 4.4477e-08, 2.4352e-07, 1.7261e-02,
        5.5255e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 137, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5185e-01, 4.0423e-02, 3.2654e-02, 2.4127e-02, 2.2071e-05, 3.1208e-02,
        1.9711e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 137, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2429e-01, 1.3751e-02, 1.3754e-02, 5.5386e-05, 1.3655e-02, 1.5381e-02,
        1.9116e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 137, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8283e-03, 6.2790e-07, 9.3519e-03, 9.6589e-01, 4.0000e-07, 1.0903e-02,
        8.0239e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 137, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.0548e-02, 2.7297e-05, 8.7168e-01, 4.7000e-02, 6.1174e-06, 7.3029e-06,
        4.0736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.780

[Epoch: 137, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2198e-02, 9.5719e-01, 1.0628e-02, 3.2524e-08, 1.0985e-07, 1.5169e-02,
        4.8143e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 138, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7700e-01, 3.0210e-02, 2.8367e-02, 2.5641e-02, 1.3738e-05, 2.1562e-02,
        1.7209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.072

[Epoch: 138, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6113e-01, 7.4815e-03, 9.7778e-03, 3.9343e-05, 7.3670e-03, 7.1903e-03,
        7.0116e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 138, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7087e-03, 2.0640e-06, 1.2433e-02, 9.5875e-01, 5.3445e-07, 9.4183e-03,
        1.4688e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 138, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1630e-02, 1.9610e-05, 8.1268e-01, 8.2048e-02, 1.2175e-06, 5.3258e-06,
        5.3611e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.734

[Epoch: 138, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5486e-02, 9.5373e-01, 1.1664e-02, 2.4959e-08, 1.2671e-07, 1.3057e-02,
        6.0599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 139, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7460e-01, 3.1259e-02, 2.8694e-02, 2.2449e-02, 1.3765e-05, 2.8482e-02,
        1.4503e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 139, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4283e-01, 1.1672e-02, 1.0574e-02, 3.9639e-05, 1.2128e-02, 1.1912e-02,
        1.0843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 139, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5745e-03, 1.3582e-06, 1.2218e-02, 9.5647e-01, 6.2523e-07, 1.3312e-02,
        1.0428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.076

[Epoch: 139, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7985e-02, 6.4300e-05, 8.4219e-01, 6.5045e-02, 2.5600e-06, 4.9647e-06,
        4.4712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.741

[Epoch: 139, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3453e-02, 9.5510e-01, 9.7623e-03, 2.6296e-08, 1.3006e-07, 1.5638e-02,
        6.0457e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 140, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9606e-01, 3.0811e-02, 2.3105e-02, 2.1936e-02, 7.0196e-06, 1.7260e-02,
        1.0819e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 140, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4714e-01, 1.1353e-02, 1.3880e-02, 3.9660e-05, 8.3609e-03, 8.5074e-03,
        1.0719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 140, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.4559e-03, 2.5029e-06, 1.0707e-02, 9.6009e-01, 1.0723e-06, 1.2904e-02,
        1.0843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 140, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.8258e-02, 3.0790e-05, 7.8676e-01, 8.9375e-02, 4.4190e-06, 6.2443e-06,
        6.5568e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.719

[Epoch: 140, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7255e-02, 9.5310e-01, 1.0833e-02, 2.5564e-08, 8.2032e-08, 1.3529e-02,
        5.2843e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 141, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8537e-01, 3.6592e-02, 2.6142e-02, 2.5254e-02, 6.2010e-06, 1.4295e-02,
        1.2340e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 141, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5264e-01, 8.5676e-03, 6.0761e-03, 2.7695e-05, 1.0764e-02, 1.2673e-02,
        9.2507e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 141, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9002e-03, 1.7405e-06, 1.1291e-02, 9.6367e-01, 4.6348e-07, 1.0486e-02,
        9.6460e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 141, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8808e-02, 4.4988e-05, 8.4227e-01, 6.8002e-02, 2.2571e-06, 7.0663e-06,
        4.0867e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.699

[Epoch: 141, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1002e-02, 9.5783e-01, 8.3925e-03, 1.4129e-08, 1.2318e-07, 1.7281e-02,
        5.4920e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 142, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4770e-01, 3.9425e-02, 3.8605e-02, 3.1795e-02, 1.8631e-05, 2.6901e-02,
        1.5559e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 142, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5216e-01, 9.9000e-03, 1.0847e-02, 3.9559e-05, 9.9006e-03, 8.8041e-03,
        8.3455e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 142, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.1177e-03, 1.3996e-06, 8.3912e-03, 9.6455e-01, 5.6274e-07, 9.9169e-03,
        1.1022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 142, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2202e-02, 2.2154e-05, 8.2829e-01, 7.4227e-02, 3.5046e-06, 7.4471e-06,
        5.5252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.764

[Epoch: 142, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7599e-02, 9.5034e-01, 1.1017e-02, 3.5644e-08, 7.4144e-08, 1.4365e-02,
        6.6800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 143, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7442e-01, 3.4875e-02, 2.8266e-02, 2.7546e-02, 6.3226e-06, 1.9170e-02,
        1.5715e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 143, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5703e-01, 8.5266e-03, 7.5307e-03, 2.2269e-05, 8.1163e-03, 8.7690e-03,
        1.0006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 143, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0543e-03, 1.1944e-06, 1.0429e-02, 9.6328e-01, 6.9987e-07, 1.1380e-02,
        9.8523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.069

[Epoch: 143, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.8088e-02, 2.4534e-05, 8.5165e-01, 6.9493e-02, 1.8199e-06, 4.8260e-06,
        4.0739e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.730

[Epoch: 143, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2918e-02, 9.5393e-01, 7.9332e-03, 3.4105e-08, 1.2102e-07, 2.0321e-02,
        4.8980e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 144, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6878e-01, 3.3690e-02, 2.7648e-02, 2.3778e-02, 1.5570e-05, 3.1845e-02,
        1.4241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 144, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5055e-01, 1.3838e-02, 8.8519e-03, 2.5587e-05, 9.7092e-03, 8.5541e-03,
        8.4765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 144, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.9133e-03, 1.0493e-06, 8.4519e-03, 9.6947e-01, 8.5247e-07, 9.5208e-03,
        9.6429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.082

[Epoch: 144, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1141e-02, 9.1554e-06, 8.4211e-01, 7.6614e-02, 2.3243e-06, 4.9168e-06,
        4.0115e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.749

[Epoch: 144, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2609e-02, 9.6173e-01, 8.6431e-03, 2.5390e-08, 9.7058e-08, 1.2106e-02,
        4.9120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 145, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4423e-01, 4.4544e-02, 3.4534e-02, 2.8273e-02, 1.5604e-05, 2.9335e-02,
        1.9067e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 145, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3912e-01, 9.8501e-03, 1.5290e-02, 3.7515e-05, 1.4912e-02, 1.1279e-02,
        9.5115e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.014

[Epoch: 145, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6072e-03, 3.0987e-06, 8.8161e-03, 9.6858e-01, 1.3753e-06, 8.8082e-03,
        9.1824e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.072

[Epoch: 145, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.9620e-02, 2.2225e-05, 7.8599e-01, 7.6525e-02, 4.0094e-06, 1.1429e-05,
        6.7831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.698

[Epoch: 145, batch: 215/218] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0371e-02, 9.6173e-01, 8.3741e-03, 2.6804e-08, 1.0138e-07, 1.5168e-02,
        4.3611e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 146, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7470e-01, 3.4452e-02, 2.6310e-02, 2.4685e-02, 1.4136e-05, 2.6543e-02,
        1.3292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 146, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3802e-01, 1.7640e-02, 9.1303e-03, 2.9256e-05, 1.1203e-02, 1.1310e-02,
        1.2667e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 146, batch: 129/218] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2475e-03, 5.0019e-06, 1.3049e-02, 9.5226e-01, 8.9278e-07, 1.4640e-02,
        1.3793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 146, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4294e-02, 1.5456e-05, 8.4515e-01, 6.8977e-02, 3.7787e-06, 6.9721e-06,
        4.1554e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.748

[Epoch: 146, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2906e-02, 9.5367e-01, 1.2704e-02, 2.4163e-08, 2.8664e-07, 1.5031e-02,
        5.6908e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.005

[Epoch: 147, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([9.0127e-01, 3.2126e-02, 2.0253e-02, 1.3417e-02, 1.0432e-05, 1.9873e-02,
        1.3048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 147, batch: 86/218] total loss per batch: 0.528
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5190e-01, 1.0706e-02, 1.1272e-02, 2.1741e-05, 8.9368e-03, 1.0338e-02,
        6.8263e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 147, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.5606e-03, 3.6925e-06, 1.1259e-02, 9.6426e-01, 1.1982e-06, 8.9181e-03,
        8.9958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 147, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.7082e-02, 4.5455e-05, 7.9329e-01, 9.5348e-02, 4.5179e-06, 8.0280e-06,
        5.4225e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.720

[Epoch: 147, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.9343e-02, 9.4688e-01, 9.8317e-03, 3.6078e-08, 1.5568e-07, 1.6662e-02,
        7.2841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 148, batch: 43/218] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6806e-01, 3.4529e-02, 3.2525e-02, 2.8741e-02, 1.2623e-05, 2.1223e-02,
        1.4913e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.038

[Epoch: 148, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5647e-01, 1.0963e-02, 5.2499e-03, 2.3754e-05, 7.2365e-03, 9.6768e-03,
        1.0376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 148, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2271e-03, 2.3390e-06, 8.7499e-03, 9.6847e-01, 6.1489e-07, 8.2268e-03,
        8.3249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.069

[Epoch: 148, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8189e-02, 1.2018e-05, 8.3403e-01, 6.7797e-02, 4.6039e-06, 1.1433e-05,
        4.9955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.738

[Epoch: 148, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6974e-02, 9.5121e-01, 1.1682e-02, 3.1231e-08, 1.5849e-07, 1.4245e-02,
        5.8906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 149, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6147e-01, 3.4463e-02, 3.6484e-02, 2.6078e-02, 1.7782e-05, 2.5054e-02,
        1.6428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 149, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2880e-01, 1.2697e-02, 1.2950e-02, 3.1780e-05, 1.4672e-02, 1.5494e-02,
        1.5355e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 149, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.6436e-03, 1.3233e-06, 8.0233e-03, 9.7127e-01, 7.0211e-07, 8.8115e-03,
        8.2502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.083

[Epoch: 149, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2288e-02, 2.0569e-05, 8.4059e-01, 7.2664e-02, 1.7817e-06, 7.5186e-06,
        4.4428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 149, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4233e-02, 9.5587e-01, 8.3570e-03, 3.8461e-08, 8.9472e-08, 1.5776e-02,
        5.7666e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 150, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4906e-01, 3.3375e-02, 3.1255e-02, 3.2050e-02, 1.9125e-05, 3.2570e-02,
        2.1675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.065

[Epoch: 150, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5772e-01, 8.5992e-03, 8.7499e-03, 1.8098e-05, 7.7439e-03, 9.7394e-03,
        7.4260e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 150, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.7045e-03, 1.4405e-06, 1.2212e-02, 9.5849e-01, 7.9293e-07, 1.2553e-02,
        1.1039e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 150, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6269e-02, 2.2876e-05, 8.2362e-01, 6.7984e-02, 1.9878e-06, 5.5703e-06,
        5.2098e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.741

[Epoch: 150, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5215e-02, 9.5630e-01, 9.2149e-03, 2.2627e-08, 1.1934e-07, 1.3517e-02,
        5.7546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 151, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6711e-01, 3.7598e-02, 2.7348e-02, 2.1533e-02, 1.4163e-05, 2.9460e-02,
        1.6938e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 151, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4550e-01, 1.1967e-02, 1.0235e-02, 1.9370e-05, 1.0900e-02, 1.0225e-02,
        1.1157e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 151, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.5415e-03, 3.0353e-06, 1.1022e-02, 9.6339e-01, 9.5201e-07, 8.5469e-03,
        1.1492e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 151, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0649e-02, 1.8168e-05, 8.2293e-01, 7.5927e-02, 1.6585e-06, 5.3312e-06,
        5.0469e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.688

[Epoch: 151, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2436e-02, 9.6021e-01, 8.7084e-03, 2.6373e-08, 7.8045e-08, 1.3503e-02,
        5.1416e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 152, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7250e-01, 3.6452e-02, 2.9036e-02, 2.6504e-02, 1.5312e-05, 2.2909e-02,
        1.2579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 152, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5505e-01, 9.0268e-03, 9.0118e-03, 1.8334e-05, 8.3504e-03, 9.4859e-03,
        9.0617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 152, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.1542e-03, 1.3018e-06, 1.0558e-02, 9.6209e-01, 7.1542e-07, 1.2361e-02,
        9.8391e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.084

[Epoch: 152, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3295e-02, 2.8852e-05, 8.1214e-01, 7.9812e-02, 1.6429e-06, 6.1851e-06,
        5.4719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.763

[Epoch: 152, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5830e-02, 9.5325e-01, 1.0757e-02, 1.5988e-08, 1.1959e-07, 1.5241e-02,
        4.9184e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 153, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7948e-01, 3.3104e-02, 3.1778e-02, 2.4120e-02, 9.6698e-06, 2.0026e-02,
        1.1483e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 153, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5434e-01, 8.6279e-03, 8.4085e-03, 1.7107e-05, 9.3942e-03, 9.3612e-03,
        9.8467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 153, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9495e-03, 1.2141e-06, 1.0255e-02, 9.6775e-01, 6.1756e-07, 7.9438e-03,
        9.1048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.085

[Epoch: 153, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8291e-02, 2.2343e-05, 8.2860e-01, 7.2013e-02, 1.7173e-06, 5.1669e-06,
        5.1064e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.706

[Epoch: 153, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4640e-02, 9.5484e-01, 9.8088e-03, 1.9805e-08, 9.6638e-08, 1.5124e-02,
        5.5927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 154, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7705e-01, 3.1947e-02, 3.0136e-02, 2.6779e-02, 1.0323e-05, 2.0276e-02,
        1.3803e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 154, batch: 86/218] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5172e-01, 9.6973e-03, 9.0070e-03, 1.1880e-05, 8.9714e-03, 1.0322e-02,
        1.0269e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 154, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9243e-03, 7.3306e-07, 9.7770e-03, 9.6446e-01, 3.9888e-07, 1.0703e-02,
        1.0134e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.081

[Epoch: 154, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6801e-02, 1.7078e-05, 8.3634e-01, 7.0068e-02, 8.1888e-07, 3.3652e-06,
        4.6774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.748

[Epoch: 154, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4080e-02, 9.5766e-01, 9.1449e-03, 1.4230e-08, 6.6952e-08, 1.4458e-02,
        4.6543e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 155, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6781e-01, 3.5984e-02, 3.0218e-02, 2.6699e-02, 7.4145e-06, 2.5843e-02,
        1.3440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 155, batch: 86/218] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4548e-01, 1.1364e-02, 1.1127e-02, 1.5786e-05, 1.1010e-02, 1.0417e-02,
        1.0583e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 155, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.6514e-03, 8.9477e-07, 1.1139e-02, 9.6326e-01, 4.0789e-07, 9.9819e-03,
        9.9695e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.082

[Epoch: 155, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1735e-02, 1.2332e-05, 8.2643e-01, 7.3049e-02, 8.4819e-07, 3.1647e-06,
        4.8765e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 155, batch: 215/218] total loss per batch: 0.534
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4145e-02, 9.5679e-01, 9.5122e-03, 1.0197e-08, 4.0348e-08, 1.4364e-02,
        5.1930e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 156, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6083e-01, 4.1069e-02, 3.0143e-02, 2.5328e-02, 8.3814e-06, 2.7151e-02,
        1.5471e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 156, batch: 86/218] total loss per batch: 0.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4496e-01, 1.1147e-02, 1.0155e-02, 1.2577e-05, 1.1751e-02, 1.0871e-02,
        1.1099e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 156, batch: 129/218] total loss per batch: 0.505
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0340e-03, 1.1640e-06, 1.0875e-02, 9.6172e-01, 3.9586e-07, 1.0872e-02,
        1.1501e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.085

[Epoch: 156, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.0281e-02, 1.8804e-05, 8.1171e-01, 8.4568e-02, 7.7393e-07, 3.0554e-06,
        5.3423e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.727

[Epoch: 156, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4144e-02, 9.6056e-01, 9.3869e-03, 8.9838e-09, 5.4685e-08, 1.2013e-02,
        3.9008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 157, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7608e-01, 3.5779e-02, 3.0525e-02, 1.9590e-02, 5.3208e-06, 2.5083e-02,
        1.2942e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 157, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4918e-01, 9.8099e-03, 1.0660e-02, 1.4471e-05, 8.5724e-03, 1.1702e-02,
        1.0064e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 157, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.6485e-03, 1.1728e-06, 1.0609e-02, 9.6287e-01, 3.1777e-07, 1.1178e-02,
        8.6979e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 157, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9855e-02, 1.9775e-05, 8.3990e-01, 6.4187e-02, 6.9601e-07, 3.2615e-06,
        4.6033e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.717

[Epoch: 157, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5355e-02, 9.5284e-01, 1.0467e-02, 1.0587e-08, 3.8259e-08, 1.5968e-02,
        5.3671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 158, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7060e-01, 3.7334e-02, 2.9813e-02, 2.2989e-02, 8.9218e-06, 2.4837e-02,
        1.4415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 158, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5792e-01, 8.3072e-03, 7.3204e-03, 1.1703e-05, 1.0266e-02, 7.0462e-03,
        9.1276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 158, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.7773e-03, 1.5171e-06, 9.9667e-03, 9.6769e-01, 5.9257e-07, 7.8488e-03,
        1.0714e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 158, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9235e-02, 2.3571e-05, 7.9692e-01, 9.7909e-02, 1.2517e-06, 4.2186e-06,
        5.5905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 158, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8436e-02, 9.4939e-01, 1.1508e-02, 1.0441e-08, 8.8150e-08, 1.4819e-02,
        5.8438e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 159, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6397e-01, 3.3605e-02, 3.0945e-02, 3.0073e-02, 6.9662e-06, 2.2486e-02,
        1.8917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 159, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4990e-01, 1.0186e-02, 1.0525e-02, 1.1653e-05, 7.0702e-03, 1.1117e-02,
        1.1190e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 159, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3682e-03, 6.1296e-07, 7.7574e-03, 9.6987e-01, 4.2814e-07, 1.0743e-02,
        7.2644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.072

[Epoch: 159, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2908e-02, 1.1011e-05, 8.5989e-01, 5.4495e-02, 9.8825e-07, 4.7695e-06,
        4.2692e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.738

[Epoch: 159, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4812e-02, 9.5368e-01, 9.2798e-03, 3.1278e-08, 1.2534e-07, 1.6026e-02,
        6.2061e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 160, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6511e-01, 3.3085e-02, 3.3595e-02, 3.1529e-02, 1.4188e-05, 2.2152e-02,
        1.4510e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 160, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4987e-01, 1.1209e-02, 1.0138e-02, 2.1285e-05, 1.0901e-02, 9.5742e-03,
        8.2867e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 160, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.2856e-03, 1.1114e-06, 8.8185e-03, 9.6756e-01, 6.7960e-07, 7.6317e-03,
        1.1705e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 160, batch: 172/218] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2018e-02, 7.2151e-06, 8.2784e-01, 7.2180e-02, 1.5402e-06, 3.6954e-06,
        4.7949e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.725

[Epoch: 160, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6116e-02, 9.5346e-01, 1.1659e-02, 3.0751e-08, 6.5818e-08, 1.3483e-02,
        5.2856e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 161, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6303e-01, 3.4940e-02, 3.0615e-02, 2.7215e-02, 8.1519e-06, 2.6534e-02,
        1.7655e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 161, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4916e-01, 9.9488e-03, 9.3698e-03, 2.1951e-05, 1.0964e-02, 1.1363e-02,
        9.1684e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 161, batch: 129/218] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.1091e-03, 1.7753e-06, 1.0284e-02, 9.5712e-01, 5.5272e-07, 1.6668e-02,
        1.0818e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.081

[Epoch: 161, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3537e-02, 2.6458e-05, 7.9987e-01, 8.9212e-02, 2.1737e-06, 5.5837e-06,
        5.7351e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 161, batch: 215/218] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3179e-02, 9.5708e-01, 1.1795e-02, 3.5324e-08, 1.3701e-07, 1.3723e-02,
        4.2181e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 162, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8444e-01, 3.3098e-02, 2.2924e-02, 2.0871e-02, 1.3613e-05, 2.6546e-02,
        1.2109e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 162, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5069e-01, 9.3814e-03, 1.1840e-02, 2.1451e-05, 9.0902e-03, 8.5918e-03,
        1.0390e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 162, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.1781e-03, 2.3895e-06, 1.1826e-02, 9.6117e-01, 5.2473e-07, 8.5397e-03,
        1.2286e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.073

[Epoch: 162, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1039e-02, 2.1398e-05, 8.3701e-01, 6.3908e-02, 1.0467e-06, 4.4703e-06,
        4.8017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.726

[Epoch: 162, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4584e-02, 9.5295e-01, 1.0345e-02, 3.4538e-08, 9.4492e-08, 1.6333e-02,
        5.7840e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 163, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8968e-01, 3.2011e-02, 2.5837e-02, 2.0132e-02, 7.1818e-06, 2.0107e-02,
        1.2229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.038

[Epoch: 163, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4539e-01, 1.0181e-02, 9.6993e-03, 2.7754e-05, 1.2087e-02, 1.2089e-02,
        1.0526e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 163, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8990e-03, 1.7044e-06, 1.0454e-02, 9.6300e-01, 6.3323e-07, 1.1503e-02,
        1.0145e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 163, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9110e-02, 2.9089e-05, 8.0632e-01, 8.8666e-02, 1.2210e-06, 6.2415e-06,
        5.5870e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.718

[Epoch: 163, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6573e-02, 9.5350e-01, 1.0073e-02, 2.4575e-08, 7.8473e-08, 1.4695e-02,
        5.1627e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 164, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8464e-01, 3.1205e-02, 3.0093e-02, 1.7169e-02, 8.6541e-06, 2.3792e-02,
        1.3093e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 164, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5273e-01, 9.5005e-03, 8.0419e-03, 1.6398e-05, 8.8794e-03, 1.0057e-02,
        1.0777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 164, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.6349e-03, 1.3501e-06, 8.8216e-03, 9.6973e-01, 3.4572e-07, 7.6348e-03,
        8.1737e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 164, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9479e-02, 1.2508e-05, 8.5372e-01, 6.5112e-02, 8.0823e-07, 4.3557e-06,
        4.1672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.737

[Epoch: 164, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4477e-02, 9.5454e-01, 9.7219e-03, 2.2216e-08, 6.1358e-08, 1.5963e-02,
        5.3007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 165, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4269e-01, 4.1150e-02, 3.6060e-02, 3.2403e-02, 1.0813e-05, 2.9540e-02,
        1.8150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 165, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4181e-01, 1.1207e-02, 1.2146e-02, 1.4912e-05, 1.0530e-02, 1.3483e-02,
        1.0807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 165, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.3978e-03, 1.4647e-06, 1.1092e-02, 9.6007e-01, 4.2145e-07, 1.1073e-02,
        1.2367e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 165, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.6363e-02, 8.4213e-06, 8.1614e-01, 7.9302e-02, 8.2503e-07, 3.8951e-06,
        4.8185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 165, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.1830e-02, 9.6343e-01, 8.5119e-03, 1.2670e-08, 3.6234e-08, 1.1632e-02,
        4.5986e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 166, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6269e-01, 3.8100e-02, 3.0507e-02, 2.4695e-02, 1.3620e-05, 2.8116e-02,
        1.5882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.061

[Epoch: 166, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4560e-01, 1.1325e-02, 9.5164e-03, 2.2527e-05, 1.1558e-02, 9.3846e-03,
        1.2596e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 166, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9407e-03, 1.4417e-06, 9.7147e-03, 9.6468e-01, 3.9357e-07, 1.1432e-02,
        9.2353e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 166, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9200e-02, 1.1206e-05, 8.3069e-01, 7.0865e-02, 6.9286e-07, 3.8674e-06,
        4.9228e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 166, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3626e-02, 9.5745e-01, 9.6686e-03, 1.9881e-08, 4.7647e-08, 1.3435e-02,
        5.8186e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 167, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5933e-01, 4.0848e-02, 3.2838e-02, 2.5839e-02, 8.5910e-06, 2.8008e-02,
        1.3132e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 167, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5568e-01, 8.7749e-03, 9.1241e-03, 1.3352e-05, 8.3700e-03, 1.1142e-02,
        6.8916e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 167, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9305e-03, 1.4594e-06, 1.1008e-02, 9.6572e-01, 4.8364e-07, 8.9421e-03,
        9.4018e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 167, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4604e-02, 1.2187e-05, 8.1077e-01, 8.1244e-02, 9.1841e-07, 4.0429e-06,
        5.3367e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.735

[Epoch: 167, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6984e-02, 9.5033e-01, 1.1040e-02, 2.2038e-08, 5.8214e-08, 1.6163e-02,
        5.4821e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 168, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8192e-01, 2.9485e-02, 3.0834e-02, 2.4272e-02, 9.1582e-06, 2.0118e-02,
        1.3365e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 168, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5085e-01, 9.2519e-03, 9.7617e-03, 2.2752e-05, 9.7564e-03, 8.6720e-03,
        1.1690e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 168, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.3064e-03, 1.3649e-06, 8.5558e-03, 9.7060e-01, 2.7403e-07, 8.0544e-03,
        8.4803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 168, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8815e-02, 1.0706e-05, 8.2162e-01, 7.8815e-02, 1.0615e-06, 4.6278e-06,
        5.0734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.742

[Epoch: 168, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7421e-02, 9.4650e-01, 1.2034e-02, 1.9192e-08, 9.0542e-08, 1.8160e-02,
        5.8871e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 169, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4163e-01, 3.7832e-02, 3.7058e-02, 3.4739e-02, 1.0042e-05, 2.8518e-02,
        2.0216e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 169, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4998e-01, 8.7491e-03, 8.9763e-03, 1.5997e-05, 9.7145e-03, 1.2931e-02,
        9.6303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 169, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.4024e-03, 9.0560e-07, 8.5253e-03, 9.6852e-01, 3.5838e-07, 9.3416e-03,
        9.2115e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 169, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.6298e-02, 1.0511e-05, 8.4130e-01, 6.7819e-02, 7.7091e-07, 4.0971e-06,
        4.4564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.702

[Epoch: 169, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7977e-02, 9.5190e-01, 9.1189e-03, 2.9874e-08, 5.7683e-08, 1.5733e-02,
        5.2727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 170, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9067e-01, 3.0709e-02, 2.3415e-02, 2.1979e-02, 9.2858e-06, 1.9855e-02,
        1.3364e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 170, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.2868e-01, 1.6374e-02, 1.2960e-02, 2.9277e-05, 1.6943e-02, 1.0661e-02,
        1.4350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 170, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8801e-03, 1.5159e-06, 1.2783e-02, 9.5339e-01, 4.3418e-07, 1.4369e-02,
        1.3578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 170, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4799e-02, 1.6375e-05, 8.4466e-01, 6.5134e-02, 1.0637e-06, 3.0408e-06,
        4.5389e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.738

[Epoch: 170, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4448e-02, 9.5375e-01, 1.1407e-02, 1.8598e-08, 4.0049e-08, 1.6031e-02,
        4.3592e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 171, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7443e-01, 3.2586e-02, 2.9021e-02, 2.5816e-02, 5.8732e-06, 2.3818e-02,
        1.4325e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 171, batch: 86/218] total loss per batch: 0.527
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5324e-01, 8.1611e-03, 1.0902e-02, 1.8545e-05, 9.0993e-03, 1.0741e-02,
        7.8414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 171, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.4917e-03, 8.3784e-07, 1.0390e-02, 9.6535e-01, 3.2274e-07, 8.7987e-03,
        8.9665e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 171, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.0052e-02, 1.2989e-05, 7.9184e-01, 8.5359e-02, 9.1342e-07, 3.6763e-06,
        6.2734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.713

[Epoch: 171, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6242e-02, 9.5870e-01, 9.0418e-03, 4.1611e-08, 4.3055e-08, 1.0927e-02,
        5.0905e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 172, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7588e-01, 3.8883e-02, 2.6762e-02, 2.0237e-02, 7.0583e-06, 2.5317e-02,
        1.2916e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 172, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5895e-01, 8.9681e-03, 8.7586e-03, 1.9981e-05, 7.6316e-03, 7.0459e-03,
        8.6305e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 172, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9153e-03, 3.3209e-06, 9.9183e-03, 9.5887e-01, 5.6050e-07, 1.6308e-02,
        9.9809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 172, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1112e-02, 2.6537e-05, 8.4857e-01, 6.4626e-02, 1.2291e-06, 4.9220e-06,
        4.5657e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.717

[Epoch: 172, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.0819e-02, 9.6039e-01, 1.0407e-02, 2.9920e-08, 8.1280e-08, 1.4171e-02,
        4.2150e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 173, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7091e-01, 3.2933e-02, 2.9573e-02, 2.8176e-02, 8.4898e-06, 2.4155e-02,
        1.4242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 173, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5132e-01, 9.0278e-03, 1.0809e-02, 1.8725e-05, 9.4543e-03, 1.0117e-02,
        9.2518e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 173, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5210e-03, 1.3336e-06, 9.1414e-03, 9.7034e-01, 5.6229e-07, 6.3119e-03,
        9.6799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.069

[Epoch: 173, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.1318e-02, 1.4892e-05, 7.9794e-01, 8.5182e-02, 8.8845e-07, 4.5043e-06,
        5.5543e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.716

[Epoch: 173, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6817e-02, 9.5471e-01, 9.1729e-03, 2.2351e-08, 3.9626e-08, 1.3920e-02,
        5.3760e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 174, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8592e-01, 3.0136e-02, 2.6295e-02, 2.0926e-02, 5.0119e-06, 2.3675e-02,
        1.3038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 174, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4540e-01, 1.0771e-02, 1.0367e-02, 2.0744e-05, 1.0620e-02, 1.0558e-02,
        1.2262e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 174, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7916e-03, 1.1439e-06, 9.3387e-03, 9.6831e-01, 2.5272e-07, 9.1842e-03,
        8.3753e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.071

[Epoch: 174, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9511e-02, 1.1817e-05, 8.4366e-01, 7.0712e-02, 7.1561e-07, 3.4267e-06,
        4.6099e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.748

[Epoch: 174, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4837e-02, 9.5335e-01, 1.0527e-02, 2.3306e-08, 5.9493e-08, 1.5931e-02,
        5.3571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 175, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6446e-01, 3.3891e-02, 3.3638e-02, 2.8077e-02, 8.4988e-06, 2.4016e-02,
        1.5907e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 175, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4798e-01, 9.6678e-03, 1.1087e-02, 2.1424e-05, 1.0147e-02, 1.0126e-02,
        1.0972e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 175, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2880e-03, 7.5147e-07, 1.0393e-02, 9.6352e-01, 3.8813e-07, 9.7102e-03,
        1.1083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 175, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1813e-02, 8.5265e-06, 8.3519e-01, 6.7706e-02, 7.9476e-07, 3.4413e-06,
        4.5275e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.728

[Epoch: 175, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3147e-02, 9.5947e-01, 8.6213e-03, 2.2068e-08, 2.4161e-08, 1.3826e-02,
        4.9345e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 176, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.3910e-01, 4.3486e-02, 3.5949e-02, 2.9020e-02, 3.8208e-06, 3.6464e-02,
        1.5974e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 176, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4929e-01, 1.1117e-02, 9.0711e-03, 2.3223e-05, 1.0729e-02, 9.8642e-03,
        9.9053e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 176, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2954e-03, 1.4514e-06, 1.0633e-02, 9.6433e-01, 3.4692e-07, 1.0007e-02,
        9.7283e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 176, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.7618e-02, 1.5072e-05, 7.9923e-01, 8.4063e-02, 7.2810e-07, 3.3077e-06,
        5.9073e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.739

[Epoch: 176, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5200e-02, 9.5621e-01, 1.0477e-02, 2.1351e-08, 7.2953e-08, 1.3235e-02,
        4.8818e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 177, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8024e-01, 3.3429e-02, 2.5898e-02, 2.0316e-02, 7.8170e-06, 2.3333e-02,
        1.6778e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 177, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4897e-01, 8.9927e-03, 1.1556e-02, 1.8986e-05, 9.7198e-03, 1.0580e-02,
        1.0159e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 177, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9584e-03, 2.3358e-06, 1.0991e-02, 9.6281e-01, 3.8452e-07, 1.0431e-02,
        1.0803e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 177, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.5402e-02, 1.4435e-05, 8.3993e-01, 6.8926e-02, 5.7543e-07, 3.4634e-06,
        4.5722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.709

[Epoch: 177, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4421e-02, 9.5205e-01, 1.1367e-02, 2.7908e-08, 3.3902e-08, 1.7141e-02,
        5.0211e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 178, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8447e-01, 3.3096e-02, 3.0289e-02, 1.9431e-02, 4.2339e-06, 2.0665e-02,
        1.2042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 178, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5853e-01, 8.6375e-03, 7.8801e-03, 2.2427e-05, 9.0196e-03, 7.4864e-03,
        8.4256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 178, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0186e-03, 1.2601e-06, 8.2419e-03, 9.6811e-01, 2.5130e-07, 8.6178e-03,
        1.0006e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 178, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1077e-02, 2.1170e-05, 8.2239e-01, 7.6283e-02, 6.8004e-07, 2.7087e-06,
        5.0226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.736

[Epoch: 178, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8309e-02, 9.4858e-01, 1.1011e-02, 1.9730e-08, 7.0039e-08, 1.6279e-02,
        5.8243e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 179, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.5933e-01, 3.4625e-02, 3.4619e-02, 3.1688e-02, 6.7914e-06, 2.2063e-02,
        1.7671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 179, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4299e-01, 1.1415e-02, 9.9760e-03, 1.5268e-05, 1.0520e-02, 1.2864e-02,
        1.2223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 179, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.5459e-03, 9.5571e-07, 9.5087e-03, 9.7018e-01, 3.1994e-07, 7.9334e-03,
        8.8343e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 179, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9913e-02, 7.3734e-06, 8.2178e-01, 7.8535e-02, 2.0268e-06, 4.3475e-06,
        4.9759e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 179, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4507e-02, 9.5538e-01, 1.0239e-02, 3.2698e-08, 5.5330e-08, 1.4931e-02,
        4.9414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 180, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6711e-01, 3.5787e-02, 2.9684e-02, 2.2864e-02, 8.8775e-06, 2.8809e-02,
        1.5740e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 180, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4210e-01, 1.4052e-02, 1.0580e-02, 2.1560e-05, 1.0434e-02, 1.1121e-02,
        1.1690e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 180, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.8050e-03, 9.5095e-07, 9.6914e-03, 9.6128e-01, 3.2856e-07, 1.2072e-02,
        1.1147e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 180, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.7914e-02, 6.3341e-06, 8.3751e-01, 6.6620e-02, 6.8674e-07, 3.6205e-06,
        4.7944e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.705

[Epoch: 180, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4669e-02, 9.5591e-01, 1.0506e-02, 2.9441e-08, 4.7894e-08, 1.3717e-02,
        5.2024e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 181, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8030e-01, 2.8169e-02, 2.4356e-02, 2.6550e-02, 6.2965e-06, 2.5013e-02,
        1.5612e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 181, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5068e-01, 9.4644e-03, 1.1301e-02, 1.9683e-05, 9.5479e-03, 9.6609e-03,
        9.3262e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.014

[Epoch: 181, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.1695e-03, 1.1721e-06, 1.2938e-02, 9.6226e-01, 2.7874e-07, 8.1506e-03,
        1.0476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 181, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2170e-02, 8.4658e-06, 8.4834e-01, 6.6043e-02, 9.4944e-07, 3.6269e-06,
        4.3437e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.743

[Epoch: 181, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2307e-02, 9.5813e-01, 8.7299e-03, 2.1266e-08, 5.0807e-08, 1.6260e-02,
        4.5749e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 182, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8245e-01, 3.6467e-02, 2.7636e-02, 1.8607e-02, 4.0171e-06, 2.2068e-02,
        1.2763e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 182, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5073e-01, 9.9993e-03, 8.9793e-03, 2.5685e-05, 9.8851e-03, 1.0951e-02,
        9.4342e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 182, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8943e-03, 2.7209e-06, 1.0345e-02, 9.5920e-01, 4.8556e-07, 1.3177e-02,
        1.2382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.086

[Epoch: 182, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.8776e-02, 1.0903e-05, 7.8102e-01, 9.3715e-02, 1.5163e-06, 4.1732e-06,
        6.6474e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.712

[Epoch: 182, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.5107e-02, 9.5503e-01, 9.6940e-03, 3.6779e-08, 6.0159e-08, 1.5166e-02,
        5.0012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 183, batch: 43/218] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6512e-01, 3.4700e-02, 2.9808e-02, 2.8694e-02, 1.1608e-05, 2.6791e-02,
        1.4878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 183, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5841e-01, 8.3924e-03, 1.0099e-02, 1.4123e-05, 8.6217e-03, 6.4207e-03,
        8.0446e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 183, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.4712e-03, 1.4621e-06, 1.1241e-02, 9.6374e-01, 3.6275e-07, 1.1306e-02,
        8.2384e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.071

[Epoch: 183, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.4244e-02, 1.1868e-05, 8.4505e-01, 6.8306e-02, 8.5159e-07, 4.1560e-06,
        4.2379e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.733

[Epoch: 183, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3877e-02, 9.5327e-01, 1.1798e-02, 2.4397e-08, 1.1657e-07, 1.5822e-02,
        5.2352e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 184, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8238e-01, 3.2883e-02, 3.2666e-02, 2.3576e-02, 3.9766e-06, 1.6176e-02,
        1.2313e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 184, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4455e-01, 1.0493e-02, 1.0197e-02, 2.6207e-05, 9.7704e-03, 1.2049e-02,
        1.2918e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 184, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9075e-03, 1.1302e-06, 8.9415e-03, 9.6690e-01, 2.1834e-07, 8.1620e-03,
        1.0085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.071

[Epoch: 184, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9898e-02, 1.0794e-05, 8.1774e-01, 8.2937e-02, 1.1202e-06, 4.8444e-06,
        4.9405e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.756

[Epoch: 184, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6075e-02, 9.5428e-01, 8.8882e-03, 2.8026e-08, 6.8014e-08, 1.6234e-02,
        4.5268e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 185, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4478e-01, 3.9234e-02, 3.4322e-02, 3.2687e-02, 8.4435e-06, 3.1140e-02,
        1.7830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 185, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4673e-01, 9.5505e-03, 9.0772e-03, 1.6594e-05, 1.0272e-02, 1.2652e-02,
        1.1703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 185, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.4756e-03, 1.6766e-06, 9.1583e-03, 9.6843e-01, 3.8801e-07, 8.6613e-03,
        8.2682e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 185, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.2133e-02, 6.0590e-06, 8.6017e-01, 5.8825e-02, 7.9712e-07, 2.7507e-06,
        3.8865e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.689

[Epoch: 185, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2333e-02, 9.6000e-01, 9.8674e-03, 2.2711e-08, 3.3005e-08, 1.2822e-02,
        4.9815e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 186, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7759e-01, 3.6028e-02, 2.7153e-02, 1.8019e-02, 4.4022e-06, 2.4364e-02,
        1.6840e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 186, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4832e-01, 1.1053e-02, 1.2229e-02, 2.5221e-05, 1.0158e-02, 9.6611e-03,
        8.5559e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 186, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.4500e-03, 1.6734e-06, 1.1794e-02, 9.5962e-01, 3.5716e-07, 1.0838e-02,
        1.2298e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 186, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.1226e-02, 8.1853e-06, 7.7949e-01, 9.5252e-02, 1.5361e-06, 4.2415e-06,
        6.4023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.761

[Epoch: 186, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2625e-02, 9.5714e-01, 9.5788e-03, 1.8602e-08, 4.5646e-08, 1.5821e-02,
        4.8377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 187, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6326e-01, 3.5034e-02, 2.9234e-02, 2.7690e-02, 6.9680e-06, 2.7327e-02,
        1.7445e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 187, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4143e-01, 1.1841e-02, 1.1272e-02, 3.0497e-05, 1.1535e-02, 1.1816e-02,
        1.2077e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 187, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.6928e-03, 2.1154e-06, 8.7105e-03, 9.6809e-01, 3.6606e-07, 8.4778e-03,
        1.0031e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.074

[Epoch: 187, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.9609e-02, 1.1391e-05, 8.6298e-01, 5.8738e-02, 1.1030e-06, 3.8363e-06,
        3.8658e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 187, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4271e-02, 9.5634e-01, 9.5317e-03, 2.4137e-08, 4.1138e-08, 1.4833e-02,
        5.0258e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 188, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6453e-01, 3.7830e-02, 3.1684e-02, 2.3442e-02, 7.7850e-06, 2.8271e-02,
        1.4236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 188, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.6097e-01, 8.1691e-03, 7.0048e-03, 1.9027e-05, 8.8416e-03, 7.4198e-03,
        7.5804e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 188, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.8729e-03, 1.6119e-06, 1.0003e-02, 9.6310e-01, 3.2112e-07, 1.1519e-02,
        1.0498e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.081

[Epoch: 188, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([6.3801e-02, 1.2645e-05, 7.8342e-01, 8.9718e-02, 2.4783e-06, 4.0029e-06,
        6.3038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.729

[Epoch: 188, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7756e-02, 9.4752e-01, 1.2382e-02, 2.3365e-08, 4.0147e-08, 1.6566e-02,
        5.7786e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 189, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6128e-01, 3.7545e-02, 3.6846e-02, 2.3213e-02, 4.7731e-06, 2.5273e-02,
        1.5836e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 189, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5294e-01, 9.2963e-03, 1.0234e-02, 2.5671e-05, 8.4874e-03, 8.6234e-03,
        1.0390e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 189, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.9872e-03, 1.0968e-06, 8.7156e-03, 9.7139e-01, 2.3255e-07, 7.7858e-03,
        8.1173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 189, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([3.8791e-02, 8.7517e-06, 8.5080e-01, 6.6657e-02, 1.6767e-06, 3.8130e-06,
        4.3743e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.756

[Epoch: 189, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7343e-02, 9.4947e-01, 1.1375e-02, 3.2010e-08, 9.0198e-08, 1.6643e-02,
        5.1732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 190, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6754e-01, 3.3145e-02, 2.9641e-02, 2.9261e-02, 8.0849e-06, 2.4769e-02,
        1.5638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 190, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4426e-01, 1.0275e-02, 1.0727e-02, 2.2658e-05, 1.0706e-02, 1.1616e-02,
        1.2396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 190, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.4959e-03, 8.7196e-07, 8.3911e-03, 9.6545e-01, 2.5650e-07, 1.2865e-02,
        8.7959e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 190, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.3287e-02, 6.5170e-06, 8.1412e-01, 7.7141e-02, 1.7260e-06, 3.0379e-06,
        5.5437e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.707

[Epoch: 190, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7135e-02, 9.5296e-01, 9.6410e-03, 2.4674e-08, 3.2126e-08, 1.4964e-02,
        5.2982e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 191, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6685e-01, 3.5288e-02, 2.9708e-02, 2.7234e-02, 6.2306e-06, 2.1634e-02,
        1.9284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 191, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4995e-01, 1.1337e-02, 1.0277e-02, 2.1653e-05, 9.2645e-03, 1.0017e-02,
        9.1317e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 191, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9440e-03, 1.5774e-06, 1.1036e-02, 9.6229e-01, 2.9422e-07, 8.8358e-03,
        1.1895e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 191, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3481e-02, 7.5113e-06, 8.4920e-01, 6.1651e-02, 1.1979e-06, 2.7980e-06,
        4.5659e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.742

[Epoch: 191, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2987e-02, 9.5614e-01, 1.0711e-02, 2.6300e-08, 4.4796e-08, 1.4921e-02,
        5.2445e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 192, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.8254e-01, 3.3089e-02, 2.8810e-02, 1.9831e-02, 5.9623e-06, 2.1236e-02,
        1.4491e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 192, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4023e-01, 9.9931e-03, 1.1613e-02, 2.0460e-05, 1.9808e-02, 1.0054e-02,
        8.2774e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 192, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8689e-03, 1.4754e-06, 1.3538e-02, 9.5403e-01, 4.5208e-07, 1.2883e-02,
        1.2681e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.080

[Epoch: 192, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.8364e-02, 9.0661e-06, 7.9633e-01, 9.3828e-02, 2.3424e-06, 4.1105e-06,
        5.1467e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.721

[Epoch: 192, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.2611e-02, 9.6104e-01, 8.2285e-03, 2.4020e-08, 4.1105e-08, 1.4245e-02,
        3.8732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 193, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7617e-01, 3.2514e-02, 2.7011e-02, 2.8999e-02, 3.7562e-06, 2.3292e-02,
        1.2010e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 193, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5497e-01, 9.1611e-03, 8.4413e-03, 1.4168e-05, 5.9748e-03, 9.9135e-03,
        1.1525e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 193, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.2126e-03, 1.8315e-06, 1.1031e-02, 9.6301e-01, 4.5873e-07, 1.0987e-02,
        9.7604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.077

[Epoch: 193, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.3110e-02, 1.0416e-05, 8.4791e-01, 6.1324e-02, 1.7063e-06, 3.8328e-06,
        4.7641e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.726

[Epoch: 193, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.6250e-02, 9.5347e-01, 1.0380e-02, 3.1144e-08, 4.0456e-08, 1.4878e-02,
        5.0240e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 194, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.9310e-01, 2.9940e-02, 2.5218e-02, 1.7584e-02, 5.5993e-06, 2.2787e-02,
        1.1362e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 194, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5475e-01, 8.7065e-03, 9.1201e-03, 1.5883e-05, 8.5192e-03, 9.2897e-03,
        9.6012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 194, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.4757e-03, 1.0039e-06, 1.0081e-02, 9.6318e-01, 3.4857e-07, 9.2624e-03,
        9.9998e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.084

[Epoch: 194, batch: 172/218] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.9908e-02, 1.4395e-05, 8.1532e-01, 8.4251e-02, 2.2497e-06, 3.8993e-06,
        5.0505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.706

[Epoch: 194, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4080e-02, 9.5833e-01, 8.7437e-03, 1.8490e-08, 5.0353e-08, 1.4340e-02,
        4.5045e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 195, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.4496e-01, 4.1441e-02, 3.4713e-02, 2.8503e-02, 7.9992e-06, 3.0102e-02,
        2.0275e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 195, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5389e-01, 9.4812e-03, 9.3573e-03, 1.6744e-05, 7.6848e-03, 9.6397e-03,
        9.9302e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 195, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.1834e-03, 1.2402e-06, 1.0585e-02, 9.6685e-01, 3.3385e-07, 8.7158e-03,
        9.6619e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.078

[Epoch: 195, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.8431e-02, 6.0172e-06, 8.4128e-01, 6.5271e-02, 8.5429e-07, 3.4600e-06,
        4.5007e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.742

[Epoch: 195, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3970e-02, 9.5891e-01, 8.8875e-03, 2.6871e-08, 4.5458e-08, 1.3371e-02,
        4.8573e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 196, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6449e-01, 3.2933e-02, 3.1188e-02, 2.9906e-02, 5.8900e-06, 2.6255e-02,
        1.5221e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 196, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4418e-01, 1.1333e-02, 1.0818e-02, 1.8453e-05, 1.1905e-02, 1.0337e-02,
        1.1406e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 196, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.5195e-03, 1.4095e-06, 9.2188e-03, 9.6690e-01, 3.1423e-07, 9.8991e-03,
        9.4570e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.082

[Epoch: 196, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.4901e-02, 3.3499e-06, 8.0770e-01, 8.3934e-02, 8.6718e-07, 3.9676e-06,
        5.3453e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.714

[Epoch: 196, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.3834e-02, 9.5869e-01, 8.7008e-03, 2.4365e-08, 6.1163e-08, 1.3784e-02,
        4.9889e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 197, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7372e-01, 3.8631e-02, 2.9777e-02, 2.0417e-02, 4.2899e-06, 2.1228e-02,
        1.6227e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 197, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.4569e-01, 1.0467e-02, 1.1601e-02, 1.6922e-05, 1.0943e-02, 1.2485e-02,
        8.7952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 197, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.9679e-03, 1.1973e-06, 9.5002e-03, 9.6619e-01, 2.6311e-07, 9.5984e-03,
        9.7470e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.075

[Epoch: 197, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.2273e-02, 5.6682e-06, 8.3710e-01, 6.2116e-02, 8.5789e-07, 3.5618e-06,
        4.8502e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.754

[Epoch: 197, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4417e-02, 9.5470e-01, 1.1169e-02, 3.9440e-08, 7.4931e-08, 1.4738e-02,
        4.9755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 198, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.6014e-01, 4.1069e-02, 2.9821e-02, 2.6272e-02, 6.7538e-06, 2.7375e-02,
        1.5315e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 198, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5188e-01, 9.3152e-03, 9.6462e-03, 3.5257e-05, 9.4859e-03, 8.5923e-03,
        1.1045e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 198, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9346e-03, 1.6608e-06, 1.1079e-02, 9.6456e-01, 3.7174e-07, 8.0385e-03,
        1.0387e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.080

[Epoch: 198, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.1272e-02, 7.2493e-06, 8.1692e-01, 8.2591e-02, 1.6346e-06, 4.3590e-06,
        4.9206e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.702

[Epoch: 198, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.7346e-02, 9.4628e-01, 1.2440e-02, 2.5895e-08, 6.5179e-08, 1.8345e-02,
        5.5910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 199, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7019e-01, 3.9564e-02, 2.9913e-02, 1.8950e-02, 4.8001e-06, 2.6126e-02,
        1.5257e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 199, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.5621e-01, 9.1255e-03, 7.5830e-03, 1.5897e-05, 7.5906e-03, 1.0379e-02,
        9.0975e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 199, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([3.7293e-03, 1.0478e-06, 9.2286e-03, 9.6688e-01, 3.3229e-07, 1.1289e-02,
        8.8703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.079

[Epoch: 199, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([4.1614e-02, 5.9295e-06, 8.2838e-01, 7.7444e-02, 1.2942e-06, 3.3491e-06,
        5.2546e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.741

[Epoch: 199, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.8112e-02, 9.4884e-01, 1.2419e-02, 2.7742e-08, 6.7858e-08, 1.4929e-02,
        5.6993e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 200, batch: 43/218] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.8700, 0.0350, 0.0300, 0.0250, 0.0000, 0.0250, 0.0150])
Policy pred: tensor([8.7354e-01, 2.7788e-02, 2.9208e-02, 3.3160e-02, 4.0272e-06, 2.1525e-02,
        1.4777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 200, batch: 86/218] total loss per batch: 0.526
Policy (actual, predicted): 0 0
Policy data: tensor([0.9500, 0.0100, 0.0100, 0.0000, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([9.3852e-01, 1.1965e-02, 1.7042e-02, 2.9688e-05, 1.3946e-02, 9.6676e-03,
        8.8299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 200, batch: 129/218] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0000, 0.0100, 0.9650, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.0799e-03, 7.1744e-07, 9.4873e-03, 9.6567e-01, 3.0026e-07, 9.4626e-03,
        1.0296e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.082 -0.083

[Epoch: 200, batch: 172/218] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0500, 0.0000, 0.8250, 0.0750, 0.0000, 0.0000, 0.0500])
Policy pred: tensor([5.7466e-02, 4.9836e-06, 8.1830e-01, 7.6514e-02, 1.0949e-06, 3.0364e-06,
        4.7707e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.728 0.720

[Epoch: 200, batch: 215/218] total loss per batch: 0.535
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.9550, 0.0100, 0.0000, 0.0000, 0.0150, 0.0050])
Policy pred: tensor([1.4734e-02, 9.5507e-01, 9.3827e-03, 3.2948e-08, 4.6773e-08, 1.5365e-02,
        5.4479e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

