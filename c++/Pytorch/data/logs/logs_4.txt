Training set samples: 6993
Batch size: 32
[Epoch: 1, batch: 43/219] total loss per batch: 1.193
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.2576e-02, 4.6241e-01, 9.6209e-03, 1.2470e-01, 7.2248e-06, 1.2806e-02,
        3.6788e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.018

[Epoch: 1, batch: 86/219] total loss per batch: 1.162
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0217, 0.0123, 0.0219, 0.2588, 0.0218, 0.1165, 0.5470],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 1, batch: 129/219] total loss per batch: 1.192
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.2549, 0.0147, 0.0213, 0.0955, 0.2617, 0.1384, 0.2134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.029

[Epoch: 1, batch: 172/219] total loss per batch: 1.089
Policy (actual, predicted): 2 1
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.9154e-08, 8.4887e-01, 1.5113e-01, 1.5415e-06, 3.1856e-07, 8.8434e-10,
        2.4967e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.697

[Epoch: 1, batch: 215/219] total loss per batch: 1.061
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1468, 0.0208, 0.1815, 0.0404, 0.4059, 0.1950],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 2, batch: 43/219] total loss per batch: 0.836
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.2690e-03, 6.0701e-01, 5.0737e-03, 1.3067e-01, 2.0833e-06, 3.2464e-03,
        2.4473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.018

[Epoch: 2, batch: 86/219] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0034, 0.0024, 0.0059, 0.3505, 0.0046, 0.0477, 0.5856],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.057

[Epoch: 2, batch: 129/219] total loss per batch: 0.835
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1640, 0.0080, 0.0131, 0.0553, 0.4901, 0.1483, 0.1213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.035

[Epoch: 2, batch: 172/219] total loss per batch: 0.781
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3685e-08, 4.3201e-02, 9.5680e-01, 9.5883e-08, 3.8004e-08, 4.7921e-10,
        2.5858e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.802

[Epoch: 2, batch: 215/219] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1627, 0.0227, 0.1715, 0.0268, 0.4101, 0.1960],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 3, batch: 43/219] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.5113e-03, 7.0280e-01, 3.4939e-03, 1.2576e-01, 1.2465e-06, 2.1286e-03,
        1.5930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.022

[Epoch: 3, batch: 86/219] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0012, 0.0019, 0.0029, 0.1145, 0.0034, 0.0135, 0.8627],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 3, batch: 129/219] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.2087, 0.0118, 0.0068, 0.0272, 0.3456, 0.3138, 0.0862],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.039

[Epoch: 3, batch: 172/219] total loss per batch: 0.671
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.9830e-09, 3.5452e-02, 9.6455e-01, 9.5136e-08, 1.2094e-07, 4.7956e-10,
        9.8184e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.851

[Epoch: 3, batch: 215/219] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1678, 0.0215, 0.1646, 0.0273, 0.4037, 0.2043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.029

[Epoch: 4, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.1683e-03, 7.7837e-01, 3.1890e-03, 1.0622e-01, 3.9029e-07, 2.2618e-03,
        1.0379e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.015

[Epoch: 4, batch: 86/219] total loss per batch: 0.648
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0009, 0.0017, 0.0028, 0.3196, 0.0050, 0.0143, 0.6558],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 4, batch: 129/219] total loss per batch: 0.664
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0416, 0.0100, 0.0056, 0.0247, 0.7124, 0.1671, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.049

[Epoch: 4, batch: 172/219] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4893e-09, 6.7018e-03, 9.9330e-01, 2.5104e-08, 2.5930e-08, 1.6203e-10,
        1.2436e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.953

[Epoch: 4, batch: 215/219] total loss per batch: 0.638
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1751, 0.0234, 0.1749, 0.0280, 0.3846, 0.2036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 5, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5562e-03, 8.4506e-01, 4.8325e-03, 9.1825e-02, 1.0889e-06, 3.2765e-03,
        4.7450e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.030

[Epoch: 5, batch: 86/219] total loss per batch: 0.633
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0029, 0.0021, 0.1994, 0.0060, 0.0304, 0.7547],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 5, batch: 129/219] total loss per batch: 0.649
Policy (actual, predicted): 4 5
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.3018, 0.0065, 0.0064, 0.0170, 0.2582, 0.3586, 0.0514],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.050

[Epoch: 5, batch: 172/219] total loss per batch: 0.627
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1331e-09, 2.6780e-02, 9.7322e-01, 2.4337e-07, 1.8946e-07, 2.0996e-09,
        7.1719e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.931

[Epoch: 5, batch: 215/219] total loss per batch: 0.618
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1754, 0.0223, 0.1653, 0.0275, 0.4004, 0.1988],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 6, batch: 43/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.9983e-03, 8.5878e-01, 4.8779e-03, 7.3516e-02, 1.5101e-06, 2.4613e-03,
        5.3361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.014

[Epoch: 6, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0011, 0.0018, 0.0034, 0.1392, 0.0056, 0.0115, 0.8374],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 6, batch: 129/219] total loss per batch: 0.636
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0730, 0.0105, 0.0072, 0.0223, 0.7121, 0.1276, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.043

[Epoch: 6, batch: 172/219] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7369e-09, 9.1467e-03, 9.9085e-01, 9.1442e-08, 3.8348e-08, 5.0192e-10,
        7.8119e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.959

[Epoch: 6, batch: 215/219] total loss per batch: 0.604
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1639, 0.0230, 0.1703, 0.0301, 0.4110, 0.1922],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 7, batch: 43/219] total loss per batch: 0.599
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.0299e-03, 8.5292e-01, 7.2322e-03, 8.5023e-02, 2.2934e-06, 3.5209e-03,
        4.5272e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.012

[Epoch: 7, batch: 86/219] total loss per batch: 0.603
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0026, 0.0011, 0.0016, 0.1063, 0.0036, 0.0116, 0.8732],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 7, batch: 129/219] total loss per batch: 0.628
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.2007, 0.0093, 0.0067, 0.0126, 0.5815, 0.1480, 0.0412],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.040

[Epoch: 7, batch: 172/219] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4476e-09, 9.0309e-03, 9.9097e-01, 5.9583e-08, 2.9646e-07, 6.5699e-10,
        5.9055e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.924

[Epoch: 7, batch: 215/219] total loss per batch: 0.594
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1707, 0.0245, 0.1692, 0.0284, 0.3918, 0.2044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 8, batch: 43/219] total loss per batch: 0.589
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.9615e-03, 8.7214e-01, 5.1877e-03, 7.6009e-02, 9.0257e-07, 3.5473e-03,
        3.6155e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.022

[Epoch: 8, batch: 86/219] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0026, 0.0011, 0.0049, 0.0647, 0.0035, 0.0117, 0.9115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.027

[Epoch: 8, batch: 129/219] total loss per batch: 0.621
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0692, 0.0074, 0.0081, 0.0186, 0.5083, 0.3474, 0.0409],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.038

[Epoch: 8, batch: 172/219] total loss per batch: 0.587
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0555e-09, 6.4581e-03, 9.9354e-01, 6.9177e-08, 8.8089e-08, 7.4752e-10,
        4.9828e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 8, batch: 215/219] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1638, 0.0263, 0.1657, 0.0290, 0.3990, 0.2061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 9, batch: 43/219] total loss per batch: 0.583
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.4418e-03, 8.5497e-01, 7.6616e-03, 9.5647e-02, 1.7237e-06, 4.1659e-03,
        3.0112e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.025

[Epoch: 9, batch: 86/219] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0015, 0.0022, 0.2292, 0.0051, 0.0195, 0.7392],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.052

[Epoch: 9, batch: 129/219] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1787, 0.0092, 0.0070, 0.0113, 0.5101, 0.2517, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.037

[Epoch: 9, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9126e-09, 9.0270e-03, 9.9097e-01, 8.1068e-08, 1.5608e-07, 1.3072e-09,
        2.1129e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.970

[Epoch: 9, batch: 215/219] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1683, 0.0242, 0.1765, 0.0279, 0.3907, 0.2014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 10, batch: 43/219] total loss per batch: 0.582
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8530e-03, 8.5148e-01, 8.8090e-03, 1.0049e-01, 4.1640e-06, 4.5719e-03,
        2.7787e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 10, batch: 86/219] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0057, 0.0011, 0.0044, 0.3767, 0.0068, 0.0133, 0.5920],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.027

[Epoch: 10, batch: 129/219] total loss per batch: 0.609
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1365, 0.0123, 0.0071, 0.0189, 0.6648, 0.1368, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.039

[Epoch: 10, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1927e-09, 1.1897e-02, 9.8810e-01, 1.1409e-07, 1.6287e-07, 1.8943e-09,
        7.5675e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 10, batch: 215/219] total loss per batch: 0.584
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1623, 0.0246, 0.1760, 0.0293, 0.4009, 0.1965],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 11, batch: 43/219] total loss per batch: 0.580
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.9038e-03, 8.6761e-01, 7.5493e-03, 8.9517e-02, 4.5108e-06, 7.3398e-03,
        2.1076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.021

[Epoch: 11, batch: 86/219] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0019, 0.0059, 0.2578, 0.0073, 0.0149, 0.7090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 11, batch: 129/219] total loss per batch: 0.608
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1058, 0.0066, 0.0059, 0.0149, 0.5860, 0.2557, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.033

[Epoch: 11, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.9949e-09, 1.1526e-02, 9.8847e-01, 1.2912e-07, 3.8295e-07, 2.0266e-09,
        5.7449e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.961

[Epoch: 11, batch: 215/219] total loss per batch: 0.583
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1754, 0.0243, 0.1715, 0.0314, 0.3868, 0.2001],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 12, batch: 43/219] total loss per batch: 0.577
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0569e-02, 8.2450e-01, 1.0400e-02, 1.2442e-01, 6.7413e-06, 6.4482e-03,
        2.3660e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 12, batch: 86/219] total loss per batch: 0.589
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0077, 0.0016, 0.0038, 0.1925, 0.0065, 0.0099, 0.7780],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.023

[Epoch: 12, batch: 129/219] total loss per batch: 0.605
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1611, 0.0085, 0.0059, 0.0180, 0.5369, 0.2434, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.035

[Epoch: 12, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7858e-09, 6.9821e-03, 9.9302e-01, 1.6766e-07, 2.3622e-07, 4.5262e-09,
        1.7234e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 12, batch: 215/219] total loss per batch: 0.582
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1630, 0.0247, 0.1745, 0.0286, 0.4022, 0.1965],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 13, batch: 43/219] total loss per batch: 0.577
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.6920e-03, 8.7158e-01, 7.4366e-03, 8.6694e-02, 7.0789e-06, 8.9935e-03,
        1.7601e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.002

[Epoch: 13, batch: 86/219] total loss per batch: 0.588
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0020, 0.0011, 0.0041, 0.1801, 0.0036, 0.0102, 0.7989],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 13, batch: 129/219] total loss per batch: 0.605
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1282, 0.0075, 0.0072, 0.0106, 0.4961, 0.3247, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.039

[Epoch: 13, batch: 172/219] total loss per batch: 0.578
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5960e-09, 6.0452e-03, 9.9395e-01, 3.8077e-07, 2.2215e-07, 4.3236e-09,
        1.1671e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.976

[Epoch: 13, batch: 215/219] total loss per batch: 0.581
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1648, 0.0248, 0.1696, 0.0295, 0.3993, 0.2012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 14, batch: 43/219] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.0685e-03, 8.5294e-01, 1.6135e-02, 9.3465e-02, 1.1396e-05, 6.7441e-03,
        2.1639e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.012

[Epoch: 14, batch: 86/219] total loss per batch: 0.586
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0029, 0.0010, 0.0055, 0.1778, 0.0053, 0.0096, 0.7979],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.025

[Epoch: 14, batch: 129/219] total loss per batch: 0.605
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1362, 0.0110, 0.0085, 0.0187, 0.6258, 0.1798, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 14, batch: 172/219] total loss per batch: 0.576
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8197e-08, 7.3712e-03, 9.9263e-01, 5.1263e-07, 3.1570e-07, 1.2858e-08,
        7.9330e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 14, batch: 215/219] total loss per batch: 0.580
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0093, 0.1762, 0.0236, 0.1758, 0.0258, 0.3980, 0.1913],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 15, batch: 43/219] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.5383e-03, 8.7422e-01, 7.1625e-03, 8.5482e-02, 1.0539e-05, 6.2546e-03,
        1.8331e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 15, batch: 86/219] total loss per batch: 0.586
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0063, 0.0014, 0.0056, 0.2004, 0.0055, 0.0110, 0.7698],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 15, batch: 129/219] total loss per batch: 0.608
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1292, 0.0087, 0.0064, 0.0121, 0.6208, 0.1932, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.020

[Epoch: 15, batch: 172/219] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1940e-08, 8.9116e-03, 9.9108e-01, 4.0843e-07, 3.4426e-06, 3.3361e-08,
        4.1376e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 15, batch: 215/219] total loss per batch: 0.581
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0119, 0.1648, 0.0250, 0.1793, 0.0286, 0.3874, 0.2031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 16, batch: 43/219] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.5396e-02, 8.4683e-01, 1.4389e-02, 9.9436e-02, 1.0109e-05, 7.9529e-03,
        1.5982e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.000

[Epoch: 16, batch: 86/219] total loss per batch: 0.587
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0040, 0.0013, 0.0045, 0.1626, 0.0057, 0.0092, 0.8127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.001

[Epoch: 16, batch: 129/219] total loss per batch: 0.607
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1159, 0.0098, 0.0087, 0.0188, 0.6044, 0.2198, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.006

[Epoch: 16, batch: 172/219] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2887e-08, 8.0239e-03, 9.9197e-01, 7.7580e-07, 9.7712e-07, 2.8829e-08,
        7.2481e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.983

[Epoch: 16, batch: 215/219] total loss per batch: 0.580
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1737, 0.0235, 0.1680, 0.0275, 0.4001, 0.1975],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 17, batch: 43/219] total loss per batch: 0.576
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2763e-03, 8.5928e-01, 1.1645e-02, 9.6912e-02, 1.1104e-05, 8.5333e-03,
        1.7346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 17, batch: 86/219] total loss per batch: 0.587
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0022, 0.0059, 0.2297, 0.0045, 0.0058, 0.7472],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 17, batch: 129/219] total loss per batch: 0.604
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1437, 0.0113, 0.0060, 0.0118, 0.5218, 0.2884, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.026

[Epoch: 17, batch: 172/219] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7079e-08, 6.8314e-03, 9.9317e-01, 9.4054e-07, 6.5580e-07, 1.3758e-08,
        1.0876e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 17, batch: 215/219] total loss per batch: 0.580
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0112, 0.1652, 0.0248, 0.1774, 0.0302, 0.3956, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 18, batch: 43/219] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0056e-02, 8.3120e-01, 1.5862e-02, 1.1313e-01, 4.1854e-05, 8.5077e-03,
        2.1202e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 18, batch: 86/219] total loss per batch: 0.585
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0032, 0.0012, 0.0036, 0.1978, 0.0078, 0.0111, 0.7754],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.029

[Epoch: 18, batch: 129/219] total loss per batch: 0.604
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1348, 0.0123, 0.0081, 0.0180, 0.5899, 0.2142, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 18, batch: 172/219] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1418e-08, 3.4001e-03, 9.9660e-01, 2.7893e-07, 2.4268e-07, 6.6731e-09,
        2.8799e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.982

[Epoch: 18, batch: 215/219] total loss per batch: 0.577
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1664, 0.0241, 0.1740, 0.0284, 0.4003, 0.1964],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 19, batch: 43/219] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.9736e-03, 8.7522e-01, 1.0801e-02, 8.5101e-02, 2.0822e-05, 7.6536e-03,
        1.3232e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 19, batch: 86/219] total loss per batch: 0.583
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0024, 0.0012, 0.0046, 0.1425, 0.0042, 0.0041, 0.8410],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 19, batch: 129/219] total loss per batch: 0.603
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1235, 0.0098, 0.0097, 0.0156, 0.6297, 0.1945, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 19, batch: 172/219] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4540e-09, 4.1231e-03, 9.9588e-01, 2.0481e-07, 1.9949e-07, 9.9355e-09,
        2.7040e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 19, batch: 215/219] total loss per batch: 0.576
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1591, 0.0250, 0.1650, 0.0304, 0.4156, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 20, batch: 43/219] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.6670e-03, 8.4298e-01, 1.5298e-02, 1.0195e-01, 2.2886e-05, 1.0707e-02,
        2.0376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.015

[Epoch: 20, batch: 86/219] total loss per batch: 0.582
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0034, 0.0011, 0.0065, 0.0921, 0.0050, 0.0055, 0.8863],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.021

[Epoch: 20, batch: 129/219] total loss per batch: 0.606
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.2045, 0.0120, 0.0076, 0.0222, 0.5576, 0.1777, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.020

[Epoch: 20, batch: 172/219] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0575e-08, 6.8144e-03, 9.9318e-01, 3.6756e-07, 5.7212e-07, 1.8834e-08,
        1.1379e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.979

[Epoch: 20, batch: 215/219] total loss per batch: 0.576
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1574, 0.0237, 0.1682, 0.0289, 0.4113, 0.1996],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 21, batch: 43/219] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.6477e-03, 8.5704e-01, 1.6338e-02, 9.5705e-02, 1.9735e-05, 8.9826e-03,
        1.2265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 21, batch: 86/219] total loss per batch: 0.585
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0015, 0.0100, 0.2287, 0.0086, 0.0109, 0.7358],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 21, batch: 129/219] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1197, 0.0066, 0.0081, 0.0114, 0.6625, 0.1810, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 21, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1551e-08, 4.4366e-03, 9.9556e-01, 1.7318e-07, 7.3146e-08, 2.4235e-09,
        2.3420e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 21, batch: 215/219] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1620, 0.0287, 0.1652, 0.0316, 0.4050, 0.1967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.026

[Epoch: 22, batch: 43/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2044e-02, 7.3949e-01, 2.4442e-02, 1.7799e-01, 7.1240e-05, 1.7796e-02,
        2.8168e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.007

[Epoch: 22, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0010, 0.0039, 0.0028, 0.1891, 0.0028, 0.0047, 0.7956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.029

[Epoch: 22, batch: 129/219] total loss per batch: 0.633
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1002, 0.0197, 0.0041, 0.0321, 0.5471, 0.2620, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 22, batch: 172/219] total loss per batch: 0.612
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.1898e-08, 3.5515e-03, 9.9645e-01, 1.2366e-07, 3.9070e-07, 3.4318e-08,
        3.9936e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.937

[Epoch: 22, batch: 215/219] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0119, 0.1748, 0.0265, 0.1836, 0.0289, 0.3885, 0.1859],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 23, batch: 43/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.4344e-03, 8.3227e-01, 1.2717e-02, 1.1291e-01, 3.2102e-05, 8.4149e-03,
        2.6226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.011

[Epoch: 23, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0071, 0.0009, 0.0077, 0.2701, 0.0085, 0.0338, 0.6718],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 23, batch: 129/219] total loss per batch: 0.637
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1164, 0.0057, 0.0051, 0.0156, 0.4192, 0.4102, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 23, batch: 172/219] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2299e-07, 3.2987e-02, 9.6701e-01, 1.2153e-06, 1.2774e-06, 1.1363e-07,
        4.5344e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.948

[Epoch: 23, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1698, 0.0224, 0.1776, 0.0297, 0.3882, 0.2018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 24, batch: 43/219] total loss per batch: 0.598
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.9333e-03, 9.2680e-01, 6.0803e-03, 3.1410e-02, 2.5683e-05, 4.8609e-03,
        2.4888e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.024

[Epoch: 24, batch: 86/219] total loss per batch: 0.605
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0020, 0.0023, 0.0043, 0.2847, 0.0105, 0.0125, 0.6837],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 24, batch: 129/219] total loss per batch: 0.627
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1589, 0.0044, 0.0072, 0.0338, 0.4237, 0.3514, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 24, batch: 172/219] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7052e-08, 2.3720e-03, 9.9763e-01, 8.9283e-08, 3.7259e-08, 2.4235e-09,
        1.5325e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.973

[Epoch: 24, batch: 215/219] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0119, 0.1699, 0.0227, 0.1736, 0.0289, 0.3974, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 25, batch: 43/219] total loss per batch: 0.579
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.6142e-03, 9.0994e-01, 6.6891e-03, 5.5875e-02, 1.8150e-05, 5.2101e-03,
        1.5653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.023

[Epoch: 25, batch: 86/219] total loss per batch: 0.589
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0029, 0.0019, 0.0075, 0.2398, 0.0047, 0.0291, 0.7139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 25, batch: 129/219] total loss per batch: 0.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1722, 0.0095, 0.0085, 0.0235, 0.6733, 0.0945, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.021

[Epoch: 25, batch: 172/219] total loss per batch: 0.578
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3793e-07, 8.9987e-03, 9.9100e-01, 4.7877e-07, 4.7017e-07, 1.4358e-08,
        4.0189e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.954

[Epoch: 25, batch: 215/219] total loss per batch: 0.577
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1718, 0.0235, 0.1752, 0.0293, 0.3908, 0.1986],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 26, batch: 43/219] total loss per batch: 0.570
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([4.7796e-03, 8.8510e-01, 9.9158e-03, 7.8606e-02, 2.0180e-05, 6.2868e-03,
        1.5288e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.008

[Epoch: 26, batch: 86/219] total loss per batch: 0.581
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0018, 0.0023, 0.0062, 0.1447, 0.0058, 0.0167, 0.8225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 26, batch: 129/219] total loss per batch: 0.598
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0936, 0.0087, 0.0057, 0.0216, 0.6186, 0.2377, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.009

[Epoch: 26, batch: 172/219] total loss per batch: 0.570
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1827e-08, 6.3657e-03, 9.9363e-01, 2.7049e-07, 8.3964e-08, 5.5860e-09,
        3.2465e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.978

[Epoch: 26, batch: 215/219] total loss per batch: 0.572
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0114, 0.1600, 0.0219, 0.1757, 0.0274, 0.4044, 0.1992],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 27, batch: 43/219] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.7251e-03, 8.8601e-01, 1.0296e-02, 7.4331e-02, 4.4802e-05, 7.9118e-03,
        1.4685e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.010

[Epoch: 27, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0019, 0.0060, 0.2023, 0.0045, 0.0130, 0.7690],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 27, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1476, 0.0102, 0.0061, 0.0243, 0.5676, 0.2307, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.016

[Epoch: 27, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3088e-07, 6.5015e-03, 9.9350e-01, 4.9072e-07, 1.6904e-07, 4.2514e-09,
        3.7401e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.982

[Epoch: 27, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0114, 0.1652, 0.0230, 0.1785, 0.0281, 0.3986, 0.1951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 28, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.0951e-03, 8.5745e-01, 1.5804e-02, 9.1619e-02, 3.0256e-05, 9.5644e-03,
        1.7436e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.009

[Epoch: 28, batch: 86/219] total loss per batch: 0.575
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0025, 0.0021, 0.0075, 0.1956, 0.0045, 0.0103, 0.7776],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 28, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1194, 0.0095, 0.0064, 0.0207, 0.5497, 0.2818, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.016

[Epoch: 28, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1129e-07, 4.6459e-03, 9.9535e-01, 2.2932e-07, 8.9379e-08, 6.0380e-09,
        2.4529e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.984

[Epoch: 28, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1626, 0.0239, 0.1771, 0.0286, 0.3983, 0.1986],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 29, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5841e-03, 8.7244e-01, 1.3299e-02, 8.4966e-02, 3.1090e-05, 8.3504e-03,
        1.3327e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.008

[Epoch: 29, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0022, 0.0057, 0.1887, 0.0047, 0.0094, 0.7847],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 29, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.2284, 0.0099, 0.0074, 0.0169, 0.5005, 0.2271, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 29, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4518e-08, 5.5798e-03, 9.9442e-01, 1.7457e-07, 1.0306e-07, 4.2959e-09,
        3.7259e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.989

[Epoch: 29, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0112, 0.1617, 0.0242, 0.1735, 0.0294, 0.4019, 0.1979],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 30, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.8607e-03, 8.4270e-01, 1.4739e-02, 1.0685e-01, 2.1832e-05, 1.0542e-02,
        1.5280e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.006

[Epoch: 30, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0021, 0.0058, 0.1829, 0.0038, 0.0074, 0.7947],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 30, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0929, 0.0074, 0.0063, 0.0169, 0.6696, 0.1973, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.008

[Epoch: 30, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2377e-07, 4.3847e-03, 9.9561e-01, 3.3003e-07, 1.1775e-07, 6.8841e-09,
        2.1212e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 30, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0116, 0.1642, 0.0241, 0.1741, 0.0280, 0.4007, 0.1973],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 31, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.4545e-03, 8.7716e-01, 1.2833e-02, 8.1203e-02, 2.2069e-05, 8.7081e-03,
        1.2622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.003

[Epoch: 31, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0047, 0.0021, 0.0058, 0.2343, 0.0047, 0.0086, 0.7397],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 31, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0999, 0.0093, 0.0080, 0.0159, 0.6293, 0.2291, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 31, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1188e-07, 3.8730e-03, 9.9613e-01, 1.5226e-07, 8.8018e-08, 5.3905e-09,
        3.5899e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.988

[Epoch: 31, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1642, 0.0242, 0.1747, 0.0295, 0.3985, 0.1978],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 32, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4998e-03, 8.5117e-01, 1.7729e-02, 1.0171e-01, 2.4351e-05, 7.8663e-03,
        1.2001e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.001

[Epoch: 32, batch: 86/219] total loss per batch: 0.575
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0040, 0.0018, 0.0060, 0.1616, 0.0042, 0.0063, 0.8160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 32, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1469, 0.0075, 0.0061, 0.0173, 0.5677, 0.2455, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.010

[Epoch: 32, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0119e-08, 7.2645e-03, 9.9274e-01, 2.5386e-07, 1.3547e-07, 7.6911e-09,
        5.2872e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.988

[Epoch: 32, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0112, 0.1616, 0.0248, 0.1727, 0.0295, 0.4104, 0.1899],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 33, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.2689e-03, 8.7538e-01, 1.2467e-02, 8.0506e-02, 1.9389e-05, 1.0336e-02,
        1.3023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.001

[Epoch: 33, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0039, 0.0025, 0.0048, 0.2595, 0.0038, 0.0067, 0.7188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 33, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1397, 0.0106, 0.0104, 0.0159, 0.5556, 0.2590, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 33, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9601e-08, 3.1754e-03, 9.9682e-01, 2.8515e-07, 1.1384e-07, 3.8992e-09,
        2.6321e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.989

[Epoch: 33, batch: 215/219] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0111, 0.1619, 0.0234, 0.1662, 0.0301, 0.4074, 0.1999],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 34, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3473e-02, 8.1971e-01, 2.2909e-02, 1.2411e-01, 2.5331e-05, 8.2783e-03,
        1.1494e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 34, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0066, 0.0020, 0.0053, 0.2041, 0.0056, 0.0085, 0.7678],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 34, batch: 129/219] total loss per batch: 0.595
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1169, 0.0097, 0.0090, 0.0160, 0.6374, 0.2046, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 34, batch: 172/219] total loss per batch: 0.567
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4175e-07, 4.8710e-03, 9.9513e-01, 2.9678e-07, 1.6124e-07, 7.1111e-09,
        1.0457e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 34, batch: 215/219] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0112, 0.1632, 0.0257, 0.1727, 0.0305, 0.4049, 0.1918],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 35, batch: 43/219] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3409e-02, 8.1634e-01, 1.4254e-02, 1.2719e-01, 5.3336e-05, 1.0472e-02,
        1.8285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.002

[Epoch: 35, batch: 86/219] total loss per batch: 0.578
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0023, 0.0059, 0.1823, 0.0056, 0.0059, 0.7947],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 35, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1578, 0.0097, 0.0080, 0.0148, 0.5746, 0.2231, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.014

[Epoch: 35, batch: 172/219] total loss per batch: 0.569
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0343e-07, 3.3779e-03, 9.9662e-01, 9.3814e-08, 2.5089e-07, 3.5941e-09,
        2.1380e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.991

[Epoch: 35, batch: 215/219] total loss per batch: 0.572
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0112, 0.1689, 0.0230, 0.1738, 0.0291, 0.4020, 0.1920],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 36, batch: 43/219] total loss per batch: 0.568
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.3675e-03, 8.6157e-01, 1.6075e-02, 9.3973e-02, 5.1121e-05, 1.1133e-02,
        8.8275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.000

[Epoch: 36, batch: 86/219] total loss per batch: 0.578
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0047, 0.0024, 0.0062, 0.1845, 0.0065, 0.0081, 0.7875],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 36, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1876, 0.0105, 0.0069, 0.0171, 0.4633, 0.3045, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 36, batch: 172/219] total loss per batch: 0.572
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6297e-08, 2.4382e-03, 9.9756e-01, 3.7492e-07, 3.8077e-07, 3.3135e-08,
        1.0257e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.977

[Epoch: 36, batch: 215/219] total loss per batch: 0.575
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1684, 0.0245, 0.1678, 0.0291, 0.4037, 0.1966],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 37, batch: 43/219] total loss per batch: 0.569
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7357e-03, 8.7928e-01, 1.5498e-02, 7.6589e-02, 3.6194e-05, 8.8542e-03,
        1.0002e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.005

[Epoch: 37, batch: 86/219] total loss per batch: 0.580
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0037, 0.0014, 0.0054, 0.2351, 0.0072, 0.0045, 0.7426],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 37, batch: 129/219] total loss per batch: 0.599
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1211, 0.0124, 0.0106, 0.0157, 0.5366, 0.2895, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.007

[Epoch: 37, batch: 172/219] total loss per batch: 0.578
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3863e-07, 1.0146e-02, 9.8985e-01, 4.3017e-07, 5.1332e-07, 3.8972e-08,
        2.3638e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 37, batch: 215/219] total loss per batch: 0.579
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0120, 0.1660, 0.0259, 0.1776, 0.0324, 0.3871, 0.1989],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 38, batch: 43/219] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.9612e-03, 8.4340e-01, 1.5249e-02, 1.1197e-01, 5.3072e-05, 7.9571e-03,
        1.2416e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 38, batch: 86/219] total loss per batch: 0.584
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0030, 0.0025, 0.0036, 0.1574, 0.0034, 0.0033, 0.8268],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 38, batch: 129/219] total loss per batch: 0.605
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1739, 0.0099, 0.0051, 0.0114, 0.5241, 0.2674, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 38, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7700e-07, 2.0952e-01, 7.9048e-01, 3.3916e-07, 3.6563e-06, 6.9967e-08,
        1.1412e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 38, batch: 215/219] total loss per batch: 0.602
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0094, 0.1613, 0.0229, 0.1702, 0.0295, 0.4104, 0.1962],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.042

[Epoch: 39, batch: 43/219] total loss per batch: 0.643
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.8062e-02, 8.7945e-01, 8.4711e-03, 7.8980e-02, 1.6160e-05, 3.0069e-03,
        1.2012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 39, batch: 86/219] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0066, 0.0078, 0.1468, 0.0085, 0.0125, 0.8131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 39, batch: 129/219] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0733, 0.0105, 0.0074, 0.0227, 0.4633, 0.3708, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.039

[Epoch: 39, batch: 172/219] total loss per batch: 0.651
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7483e-08, 7.5844e-04, 9.9924e-01, 5.2413e-07, 2.4863e-06, 3.1476e-09,
        9.9026e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.832

[Epoch: 39, batch: 215/219] total loss per batch: 0.644
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0116, 0.1605, 0.0240, 0.1750, 0.0305, 0.4041, 0.1942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 40, batch: 43/219] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([2.3821e-03, 9.0661e-01, 6.0413e-03, 5.5923e-02, 1.7512e-05, 7.6918e-03,
        2.1331e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.008

[Epoch: 40, batch: 86/219] total loss per batch: 0.637
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0025, 0.0052, 0.2086, 0.0045, 0.0415, 0.7325],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 40, batch: 129/219] total loss per batch: 0.646
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1642, 0.0163, 0.0083, 0.0062, 0.6148, 0.1747, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 40, batch: 172/219] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8724e-07, 6.1343e-04, 9.9938e-01, 1.2431e-07, 1.7495e-06, 5.9006e-09,
        2.5737e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.945

[Epoch: 40, batch: 215/219] total loss per batch: 0.603
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0114, 0.1655, 0.0244, 0.1709, 0.0300, 0.3967, 0.2012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 41, batch: 43/219] total loss per batch: 0.595
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([5.6639e-03, 8.4771e-01, 1.6802e-02, 1.0110e-01, 1.3709e-05, 6.9892e-03,
        2.1726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 41, batch: 86/219] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0031, 0.0025, 0.0077, 0.2346, 0.0053, 0.0103, 0.7366],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.031

[Epoch: 41, batch: 129/219] total loss per batch: 0.609
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1148, 0.0082, 0.0055, 0.0061, 0.7133, 0.1386, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.025

[Epoch: 41, batch: 172/219] total loss per batch: 0.579
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6030e-07, 6.8172e-04, 9.9932e-01, 4.5501e-07, 1.1075e-07, 2.7121e-08,
        3.0718e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.977

[Epoch: 41, batch: 215/219] total loss per batch: 0.577
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1629, 0.0216, 0.1731, 0.0285, 0.4123, 0.1905],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 42, batch: 43/219] total loss per batch: 0.574
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.7686e-03, 8.5313e-01, 2.0119e-02, 9.1873e-02, 1.1287e-05, 9.6328e-03,
        1.6468e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 42, batch: 86/219] total loss per batch: 0.581
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0037, 0.0017, 0.0053, 0.2271, 0.0035, 0.0097, 0.7490],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.033

[Epoch: 42, batch: 129/219] total loss per batch: 0.600
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1979, 0.0086, 0.0070, 0.0077, 0.5662, 0.1990, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 42, batch: 172/219] total loss per batch: 0.568
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5935e-07, 1.0481e-03, 9.9895e-01, 2.7288e-07, 2.2978e-07, 2.0538e-08,
        3.6252e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.978

[Epoch: 42, batch: 215/219] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1673, 0.0251, 0.1736, 0.0280, 0.4004, 0.1950],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 43, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5333e-03, 8.5557e-01, 2.1175e-02, 9.1374e-02, 7.9132e-06, 1.0006e-02,
        1.4337e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.004

[Epoch: 43, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0040, 0.0023, 0.0062, 0.1758, 0.0040, 0.0088, 0.7990],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.025

[Epoch: 43, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1181, 0.0087, 0.0060, 0.0107, 0.6331, 0.2134, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 43, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6733e-07, 1.5346e-03, 9.9846e-01, 2.2274e-07, 3.5433e-07, 1.0095e-08,
        6.2749e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.984

[Epoch: 43, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1689, 0.0234, 0.1738, 0.0287, 0.4000, 0.1946],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 44, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.2921e-03, 8.4344e-01, 1.9909e-02, 1.0539e-01, 7.5430e-06, 9.6210e-03,
        1.3342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 44, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0047, 0.0023, 0.0075, 0.1949, 0.0042, 0.0075, 0.7790],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.020

[Epoch: 44, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1517, 0.0080, 0.0063, 0.0100, 0.5745, 0.2399, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 44, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2696e-07, 1.7846e-03, 9.9821e-01, 1.6404e-07, 2.6909e-07, 6.3447e-09,
        6.7387e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.982

[Epoch: 44, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1657, 0.0238, 0.1715, 0.0285, 0.4034, 0.1968],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 45, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.9982e-03, 8.5146e-01, 1.7879e-02, 9.9665e-02, 5.9795e-06, 9.7684e-03,
        1.3224e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 45, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0024, 0.0067, 0.2012, 0.0035, 0.0077, 0.7738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 45, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1462, 0.0087, 0.0061, 0.0134, 0.5985, 0.2184, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 45, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.8167e-08, 2.2730e-03, 9.9773e-01, 1.2375e-07, 2.9106e-07, 5.3887e-09,
        3.3682e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.983

[Epoch: 45, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1654, 0.0239, 0.1744, 0.0291, 0.4015, 0.1951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 46, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.1331e-03, 8.4034e-01, 1.9641e-02, 1.1011e-01, 5.4751e-06, 1.0170e-02,
        1.0593e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 46, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0057, 0.0025, 0.0067, 0.1998, 0.0040, 0.0077, 0.7735],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 46, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1267, 0.0086, 0.0078, 0.0133, 0.5920, 0.2432, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 46, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.7151e-08, 1.8926e-03, 9.9811e-01, 1.2632e-07, 1.9331e-07, 4.0573e-09,
        5.5961e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.985

[Epoch: 46, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1651, 0.0241, 0.1738, 0.0282, 0.4035, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 47, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.6867e-03, 8.5437e-01, 1.7985e-02, 9.7562e-02, 5.7439e-06, 9.3098e-03,
        1.2083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 47, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0048, 0.0028, 0.0059, 0.2221, 0.0043, 0.0069, 0.7532],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 47, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1414, 0.0088, 0.0064, 0.0152, 0.6111, 0.2090, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 47, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0910e-08, 2.8330e-03, 9.9717e-01, 9.6817e-08, 2.8087e-07, 4.2089e-09,
        3.2797e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.984

[Epoch: 47, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1670, 0.0238, 0.1719, 0.0295, 0.4023, 0.1948],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 48, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.4145e-03, 8.5650e-01, 1.8761e-02, 9.6897e-02, 4.3512e-06, 9.8911e-03,
        9.5366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 48, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0025, 0.0053, 0.1861, 0.0034, 0.0071, 0.7896],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 48, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1445, 0.0090, 0.0078, 0.0147, 0.5862, 0.2304, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 48, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0147e-08, 2.5339e-03, 9.9747e-01, 1.0520e-07, 1.4914e-07, 3.4201e-09,
        4.9242e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 48, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1603, 0.0250, 0.1755, 0.0280, 0.4051, 0.1961],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 49, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.9693e-03, 8.6024e-01, 1.7555e-02, 9.3018e-02, 6.7475e-06, 8.6723e-03,
        1.0543e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.003

[Epoch: 49, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0031, 0.0063, 0.1936, 0.0055, 0.0056, 0.7810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 49, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1150, 0.0101, 0.0074, 0.0167, 0.5898, 0.2522, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 49, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7382e-08, 3.3100e-03, 9.9669e-01, 1.0930e-07, 2.8823e-07, 6.0262e-09,
        6.7545e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.987

[Epoch: 49, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1688, 0.0228, 0.1675, 0.0290, 0.4096, 0.1919],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 50, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.3011e-03, 8.4841e-01, 1.7631e-02, 1.0506e-01, 7.1197e-06, 9.1407e-03,
        1.1449e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 50, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0065, 0.0032, 0.0062, 0.1835, 0.0033, 0.0082, 0.7891],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 50, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1795, 0.0107, 0.0079, 0.0162, 0.5885, 0.1890, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 50, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2370e-08, 2.8134e-03, 9.9719e-01, 9.3889e-08, 3.0965e-07, 3.6409e-09,
        3.7641e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.987

[Epoch: 50, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1594, 0.0249, 0.1748, 0.0281, 0.4042, 0.1978],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 51, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1029e-02, 8.4512e-01, 2.0177e-02, 1.0511e-01, 1.2644e-05, 8.8721e-03,
        9.6725e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.002

[Epoch: 51, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0039, 0.0034, 0.0050, 0.2241, 0.0047, 0.0068, 0.7522],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 51, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1070, 0.0099, 0.0090, 0.0140, 0.5057, 0.3463, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 51, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1833e-07, 3.1900e-03, 9.9681e-01, 1.7304e-07, 2.1030e-07, 4.3656e-09,
        1.3977e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 51, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1653, 0.0242, 0.1747, 0.0293, 0.4016, 0.1951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 52, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0751e-02, 8.4189e-01, 1.8156e-02, 1.0772e-01, 1.0909e-05, 1.0070e-02,
        1.1409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 52, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0064, 0.0030, 0.0055, 0.1798, 0.0050, 0.0058, 0.7946],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 52, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1165, 0.0100, 0.0089, 0.0169, 0.6543, 0.1856, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 52, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0845e-08, 3.0746e-03, 9.9692e-01, 9.3623e-08, 2.4351e-07, 4.4812e-09,
        8.4044e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 52, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1656, 0.0244, 0.1734, 0.0299, 0.4012, 0.1958],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 53, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4683e-03, 8.6419e-01, 1.7841e-02, 9.1924e-02, 1.1244e-05, 8.9407e-03,
        7.6276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 53, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0031, 0.0054, 0.2026, 0.0042, 0.0067, 0.7734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 53, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1620, 0.0125, 0.0091, 0.0157, 0.5546, 0.2384, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 53, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1700e-08, 3.7009e-03, 9.9630e-01, 1.0664e-07, 2.3122e-07, 4.2502e-09,
        1.8192e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.984

[Epoch: 53, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1654, 0.0248, 0.1728, 0.0299, 0.3989, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 54, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2032e-02, 8.3789e-01, 1.8851e-02, 1.0861e-01, 6.2083e-06, 9.4573e-03,
        1.3150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.001

[Epoch: 54, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0030, 0.0057, 0.2011, 0.0044, 0.0058, 0.7746],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 54, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1489, 0.0090, 0.0071, 0.0171, 0.5810, 0.2301, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 54, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4087e-08, 3.4628e-03, 9.9654e-01, 9.5903e-08, 2.2165e-07, 4.2383e-09,
        1.2719e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.989

[Epoch: 54, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1651, 0.0252, 0.1740, 0.0298, 0.4030, 0.1923],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 55, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.7061e-03, 8.7276e-01, 1.9054e-02, 8.3831e-02, 9.3432e-06, 8.3893e-03,
        7.2542e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 55, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0040, 0.0057, 0.2121, 0.0047, 0.0064, 0.7620],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 55, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1129, 0.0124, 0.0091, 0.0143, 0.6030, 0.2411, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 55, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8180e-08, 3.4630e-03, 9.9654e-01, 9.0457e-08, 2.6495e-07, 5.1309e-09,
        1.0910e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 55, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1673, 0.0247, 0.1718, 0.0293, 0.4007, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 56, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2208e-02, 8.1249e-01, 2.0953e-02, 1.3111e-01, 1.1181e-05, 9.8992e-03,
        1.3325e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.001

[Epoch: 56, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0033, 0.0042, 0.1853, 0.0045, 0.0060, 0.7913],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 56, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1637, 0.0091, 0.0094, 0.0173, 0.5998, 0.1938, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 56, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4134e-08, 4.3962e-03, 9.9560e-01, 7.9140e-08, 2.0464e-07, 3.6615e-09,
        2.0339e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 56, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1620, 0.0254, 0.1741, 0.0308, 0.4032, 0.1939],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 57, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.8535e-03, 8.6691e-01, 2.0474e-02, 8.2321e-02, 9.8357e-06, 1.0081e-02,
        1.0346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 57, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0062, 0.0034, 0.0054, 0.1903, 0.0039, 0.0049, 0.7860],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 57, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1537, 0.0135, 0.0101, 0.0138, 0.5589, 0.2414, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 57, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3497e-07, 3.8826e-03, 9.9612e-01, 1.5986e-07, 3.3179e-07, 5.7419e-09,
        1.6128e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.979

[Epoch: 57, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1625, 0.0234, 0.1716, 0.0289, 0.4062, 0.1973],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 58, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0616e-02, 8.7544e-01, 1.5689e-02, 8.2302e-02, 1.1547e-05, 7.0542e-03,
        8.8832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.003

[Epoch: 58, batch: 86/219] total loss per batch: 0.575
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0027, 0.0032, 0.0033, 0.2504, 0.0038, 0.0069, 0.7298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 58, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0963, 0.0077, 0.0083, 0.0162, 0.6242, 0.2394, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 58, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.9396e-08, 2.6463e-03, 9.9735e-01, 5.6095e-08, 1.9283e-07, 5.9529e-09,
        1.0160e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 58, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1655, 0.0270, 0.1759, 0.0305, 0.3985, 0.1924],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 59, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.5086e-03, 8.2677e-01, 2.5390e-02, 1.1487e-01, 9.5477e-06, 1.1653e-02,
        1.2795e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 59, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0064, 0.0049, 0.0078, 0.1710, 0.0057, 0.0052, 0.7989],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 59, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1839, 0.0108, 0.0097, 0.0124, 0.5284, 0.2462, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 59, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3326e-07, 5.5023e-03, 9.9450e-01, 8.6726e-08, 2.0411e-07, 3.7762e-09,
        5.0839e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.987

[Epoch: 59, batch: 215/219] total loss per batch: 0.570
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1581, 0.0220, 0.1711, 0.0282, 0.4166, 0.1940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 60, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2530e-02, 8.8273e-01, 1.9237e-02, 6.8788e-02, 1.4159e-05, 7.1569e-03,
        9.5435e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.003

[Epoch: 60, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0084, 0.0036, 0.0046, 0.2847, 0.0061, 0.0116, 0.6810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 60, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1391, 0.0078, 0.0107, 0.0189, 0.6039, 0.2135, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 60, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1701e-07, 3.4728e-03, 9.9653e-01, 2.5736e-07, 1.2803e-07, 7.8059e-09,
        7.6674e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.985

[Epoch: 60, batch: 215/219] total loss per batch: 0.570
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1699, 0.0264, 0.1767, 0.0305, 0.3921, 0.1939],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 61, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1121e-02, 7.4521e-01, 2.2941e-02, 2.0182e-01, 2.4180e-05, 8.9989e-03,
        9.8864e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.002

[Epoch: 61, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0036, 0.0044, 0.0049, 0.1677, 0.0051, 0.0082, 0.8061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 61, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1420, 0.0084, 0.0097, 0.0132, 0.5685, 0.2511, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 61, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2942e-07, 3.3613e-03, 9.9664e-01, 9.3402e-08, 2.6636e-07, 7.4037e-09,
        2.3028e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 61, batch: 215/219] total loss per batch: 0.570
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0111, 0.1668, 0.0260, 0.1756, 0.0302, 0.3961, 0.1943],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 62, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0838e-02, 8.9385e-01, 1.3359e-02, 5.9686e-02, 2.7956e-05, 7.8702e-03,
        1.4371e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 62, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0065, 0.0039, 0.0063, 0.2260, 0.0046, 0.0075, 0.7453],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 62, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1700, 0.0120, 0.0075, 0.0151, 0.4997, 0.2877, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 62, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1520e-07, 5.2595e-03, 9.9474e-01, 1.2965e-07, 4.9049e-07, 4.7706e-09,
        1.0462e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.985

[Epoch: 62, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1597, 0.0234, 0.1729, 0.0291, 0.4091, 0.1957],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 63, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1625e-02, 8.5357e-01, 2.2381e-02, 9.4538e-02, 3.1546e-05, 9.1953e-03,
        8.6575e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 63, batch: 86/219] total loss per batch: 0.575
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0060, 0.0038, 0.1919, 0.0051, 0.0067, 0.7823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 63, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1525, 0.0129, 0.0145, 0.0150, 0.5564, 0.2384, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 63, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6809e-07, 2.4686e-03, 9.9753e-01, 1.2898e-07, 1.8778e-07, 5.6927e-09,
        2.3895e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.989

[Epoch: 63, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1644, 0.0254, 0.1776, 0.0295, 0.3950, 0.1974],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 64, batch: 43/219] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.8610e-03, 8.8358e-01, 1.7638e-02, 7.1132e-02, 2.8722e-05, 7.4561e-03,
        1.0301e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 64, batch: 86/219] total loss per batch: 0.575
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0034, 0.0049, 0.0059, 0.2116, 0.0048, 0.0074, 0.7620],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 64, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1119, 0.0119, 0.0097, 0.0096, 0.6660, 0.1831, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 64, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7579e-08, 4.7158e-03, 9.9528e-01, 1.4813e-07, 8.8701e-07, 3.7915e-09,
        1.6481e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 64, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1654, 0.0241, 0.1743, 0.0280, 0.4033, 0.1942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 65, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.7790e-03, 8.7690e-01, 1.8952e-02, 7.5143e-02, 2.7784e-05, 1.0036e-02,
        1.0160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 65, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0050, 0.0042, 0.0059, 0.1852, 0.0044, 0.0059, 0.7894],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 65, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1095, 0.0127, 0.0083, 0.0185, 0.6545, 0.1893, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 65, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0594e-07, 3.2354e-03, 9.9676e-01, 1.3977e-07, 3.0292e-07, 4.4552e-09,
        5.1629e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 65, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1611, 0.0260, 0.1793, 0.0307, 0.3992, 0.1935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 66, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3813e-02, 8.1933e-01, 2.2484e-02, 1.2287e-01, 2.1520e-05, 9.9367e-03,
        1.1545e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 66, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0048, 0.0043, 0.0055, 0.1952, 0.0051, 0.0062, 0.7790],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 66, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1432, 0.0119, 0.0076, 0.0112, 0.6126, 0.2058, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 66, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.5150e-08, 3.3532e-03, 9.9665e-01, 1.5909e-07, 6.1677e-07, 4.2045e-09,
        1.4490e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.987

[Epoch: 66, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1643, 0.0251, 0.1741, 0.0284, 0.4044, 0.1933],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 67, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.9678e-03, 8.7400e-01, 1.6680e-02, 8.1692e-02, 1.8259e-05, 6.6685e-03,
        1.0972e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.001

[Epoch: 67, batch: 86/219] total loss per batch: 0.579
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0035, 0.0031, 0.0027, 0.1963, 0.0044, 0.0052, 0.7848],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 67, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1089, 0.0060, 0.0078, 0.0083, 0.5694, 0.2912, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 67, batch: 172/219] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2647e-08, 4.4700e-03, 9.9553e-01, 1.8635e-07, 2.8536e-07, 1.8645e-09,
        2.4858e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 67, batch: 215/219] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0093, 0.1626, 0.0230, 0.1789, 0.0286, 0.3933, 0.2042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 68, batch: 43/219] total loss per batch: 0.594
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.2797e-03, 8.1366e-01, 1.9754e-02, 1.4810e-01, 1.5976e-05, 6.6951e-03,
        4.5018e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 68, batch: 86/219] total loss per batch: 0.605
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0011, 0.0028, 0.0035, 0.1721, 0.0054, 0.0026, 0.8124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 68, batch: 129/219] total loss per batch: 0.614
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0704, 0.0175, 0.0051, 0.0160, 0.6323, 0.2473, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.418

[Epoch: 68, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0255e-08, 9.3499e-03, 9.9064e-01, 1.8053e-07, 8.7398e-06, 2.3515e-08,
        1.0583e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 68, batch: 215/219] total loss per batch: 0.583
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0129, 0.1669, 0.0261, 0.1749, 0.0312, 0.3982, 0.1897],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 69, batch: 43/219] total loss per batch: 0.576
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2637e-02, 6.9460e-01, 1.9234e-02, 2.5861e-01, 4.5806e-06, 5.5085e-03,
        9.4078e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 69, batch: 86/219] total loss per batch: 0.589
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0016, 0.0021, 0.0042, 0.2123, 0.0020, 0.0058, 0.7721],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 69, batch: 129/219] total loss per batch: 0.600
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1368, 0.0148, 0.0088, 0.0163, 0.6517, 0.1572, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.023

[Epoch: 69, batch: 172/219] total loss per batch: 0.571
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9053e-08, 3.2441e-03, 9.9675e-01, 8.5665e-08, 1.2736e-06, 8.8492e-09,
        8.1229e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 69, batch: 215/219] total loss per batch: 0.572
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1640, 0.0265, 0.1751, 0.0296, 0.4050, 0.1894],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 70, batch: 43/219] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.5467e-02, 8.9574e-01, 1.0529e-02, 5.5558e-02, 1.2809e-05, 6.8850e-03,
        1.5805e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 70, batch: 86/219] total loss per batch: 0.578
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0032, 0.0033, 0.0103, 0.2389, 0.0062, 0.0052, 0.7330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 70, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1638, 0.0120, 0.0066, 0.0120, 0.5601, 0.2329, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.019

[Epoch: 70, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9543e-08, 2.9788e-03, 9.9702e-01, 4.1720e-08, 3.4568e-07, 4.3334e-09,
        1.7179e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 70, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1657, 0.0253, 0.1735, 0.0299, 0.4045, 0.1909],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 71, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.8732e-03, 8.9197e-01, 1.1606e-02, 6.8567e-02, 9.6909e-06, 8.0173e-03,
        1.0961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 71, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0022, 0.0030, 0.0059, 0.2065, 0.0038, 0.0047, 0.7738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 71, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1381, 0.0103, 0.0078, 0.0117, 0.5616, 0.2585, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.020

[Epoch: 71, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5440e-08, 3.0505e-03, 9.9695e-01, 3.9565e-08, 4.0045e-07, 2.9334e-09,
        2.3447e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 71, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1611, 0.0246, 0.1765, 0.0298, 0.4036, 0.1943],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 72, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1586e-02, 8.6002e-01, 1.6105e-02, 9.1684e-02, 7.8434e-06, 9.2905e-03,
        1.1311e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 72, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0025, 0.0030, 0.0067, 0.1941, 0.0042, 0.0044, 0.7851],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 72, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1488, 0.0103, 0.0088, 0.0108, 0.5806, 0.2308, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.018

[Epoch: 72, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.8726e-08, 3.4674e-03, 9.9653e-01, 3.0914e-08, 3.0089e-07, 2.1263e-09,
        2.3135e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 72, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1612, 0.0249, 0.1754, 0.0299, 0.4072, 0.1912],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 73, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0560e-02, 8.6624e-01, 1.5784e-02, 8.6971e-02, 5.9309e-06, 9.8923e-03,
        1.0547e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 73, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0025, 0.0031, 0.0057, 0.1932, 0.0046, 0.0043, 0.7864],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 73, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1289, 0.0094, 0.0080, 0.0122, 0.6048, 0.2275, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.014

[Epoch: 73, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4681e-08, 3.6317e-03, 9.9637e-01, 3.6637e-08, 3.3791e-07, 1.8928e-09,
        2.1987e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.991

[Epoch: 73, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1608, 0.0245, 0.1752, 0.0300, 0.4062, 0.1929],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 74, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0537e-02, 8.5334e-01, 1.8911e-02, 9.7189e-02, 5.3725e-06, 9.9381e-03,
        1.0076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.020

[Epoch: 74, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0032, 0.0033, 0.0059, 0.2074, 0.0045, 0.0044, 0.7712],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 74, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1492, 0.0103, 0.0078, 0.0114, 0.5851, 0.2267, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 74, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2153e-08, 3.7960e-03, 9.9620e-01, 2.5797e-08, 2.8419e-07, 1.5584e-09,
        1.8587e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 74, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1610, 0.0246, 0.1758, 0.0299, 0.4058, 0.1927],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 75, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0944e-02, 8.4372e-01, 1.8844e-02, 1.0702e-01, 4.4081e-06, 9.7050e-03,
        9.7605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 75, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0038, 0.0055, 0.2222, 0.0047, 0.0045, 0.7560],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 75, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1383, 0.0097, 0.0085, 0.0120, 0.5779, 0.2454, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.012

[Epoch: 75, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7439e-08, 4.2592e-03, 9.9574e-01, 3.0266e-08, 2.8223e-07, 1.4821e-09,
        1.9847e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 75, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1624, 0.0240, 0.1734, 0.0296, 0.4088, 0.1915],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 76, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1288e-02, 8.5920e-01, 1.8969e-02, 9.1131e-02, 3.7925e-06, 9.8657e-03,
        9.5377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 76, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0034, 0.0039, 0.0057, 0.1923, 0.0049, 0.0047, 0.7851],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 76, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1460, 0.0090, 0.0075, 0.0119, 0.5711, 0.2468, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.014

[Epoch: 76, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4810e-08, 4.4067e-03, 9.9559e-01, 2.3584e-08, 2.4635e-07, 1.5139e-09,
        1.3429e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.991

[Epoch: 76, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1593, 0.0249, 0.1747, 0.0298, 0.4068, 0.1937],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 77, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0450e-02, 8.4393e-01, 1.9251e-02, 1.0613e-01, 4.2994e-06, 9.9509e-03,
        1.0284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 77, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0042, 0.0039, 0.0060, 0.1789, 0.0053, 0.0044, 0.7972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 77, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1199, 0.0098, 0.0078, 0.0124, 0.6314, 0.2110, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.007

[Epoch: 77, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7077e-08, 4.1288e-03, 9.9587e-01, 2.8542e-08, 3.2215e-07, 1.4232e-09,
        1.5093e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 77, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1640, 0.0240, 0.1709, 0.0297, 0.4090, 0.1923],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 78, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1846e-02, 8.6580e-01, 2.0145e-02, 8.1878e-02, 5.1581e-06, 1.0572e-02,
        9.7547e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 78, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0037, 0.0039, 0.0060, 0.1677, 0.0060, 0.0048, 0.8081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 78, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1180, 0.0099, 0.0073, 0.0132, 0.6799, 0.1653, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 78, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4639e-08, 4.5462e-03, 9.9545e-01, 2.6853e-08, 2.1078e-07, 2.0030e-09,
        1.5650e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.992

[Epoch: 78, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1636, 0.0253, 0.1752, 0.0294, 0.4019, 0.1937],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 79, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1334e-02, 8.4340e-01, 1.6790e-02, 1.0601e-01, 8.6487e-06, 1.1605e-02,
        1.0858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.021

[Epoch: 79, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0041, 0.0036, 0.0059, 0.2317, 0.0056, 0.0050, 0.7441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 79, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1309, 0.0093, 0.0088, 0.0108, 0.5344, 0.2981, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 79, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0933e-08, 5.4474e-03, 9.9455e-01, 3.1232e-08, 4.2975e-07, 2.2835e-09,
        2.3434e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.986

[Epoch: 79, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1597, 0.0243, 0.1735, 0.0297, 0.4097, 0.1935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 80, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0356e-02, 8.4765e-01, 1.9815e-02, 1.0564e-01, 6.1955e-06, 7.7739e-03,
        8.7603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 80, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0060, 0.0040, 0.0045, 0.1929, 0.0053, 0.0055, 0.7819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.020

[Epoch: 80, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1924, 0.0105, 0.0091, 0.0167, 0.4766, 0.2886, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 80, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4524e-08, 5.6339e-03, 9.9437e-01, 8.3669e-08, 4.6785e-07, 4.6079e-09,
        1.6439e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 80, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1631, 0.0245, 0.1731, 0.0295, 0.4040, 0.1958],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 81, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3213e-02, 8.4299e-01, 1.7197e-02, 1.0241e-01, 1.1010e-05, 1.2073e-02,
        1.2111e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 81, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0040, 0.0054, 0.2002, 0.0059, 0.0051, 0.7747],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 81, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1571, 0.0104, 0.0099, 0.0117, 0.5613, 0.2410, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.006

[Epoch: 81, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5341e-08, 3.3353e-03, 9.9666e-01, 6.1688e-08, 5.3597e-07, 2.4779e-09,
        1.0036e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.989

[Epoch: 81, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1614, 0.0247, 0.1797, 0.0289, 0.4038, 0.1913],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 82, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0473e-02, 8.6362e-01, 1.8815e-02, 8.5704e-02, 9.1208e-06, 1.1339e-02,
        1.0044e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 82, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0048, 0.0054, 0.0047, 0.2129, 0.0041, 0.0055, 0.7625],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 82, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1070, 0.0100, 0.0081, 0.0138, 0.6644, 0.1897, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 82, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8959e-08, 6.1471e-03, 9.9385e-01, 7.2573e-08, 4.9404e-07, 3.0526e-09,
        2.5421e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 82, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1652, 0.0250, 0.1762, 0.0317, 0.3953, 0.1957],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 83, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0192e-02, 8.4793e-01, 1.7112e-02, 1.0464e-01, 7.6079e-06, 9.5679e-03,
        1.0554e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 83, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0043, 0.0057, 0.1942, 0.0057, 0.0047, 0.7806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 83, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1526, 0.0077, 0.0089, 0.0147, 0.5725, 0.2369, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 83, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2988e-08, 3.6204e-03, 9.9638e-01, 9.3884e-08, 7.0743e-07, 4.5507e-09,
        1.1285e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 83, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0093, 0.1598, 0.0256, 0.1722, 0.0294, 0.4078, 0.1959],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 84, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3176e-02, 8.6161e-01, 1.6113e-02, 9.1142e-02, 7.6836e-06, 8.6066e-03,
        9.3442e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.005

[Epoch: 84, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0055, 0.0045, 0.0058, 0.1834, 0.0064, 0.0045, 0.7898],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 84, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1430, 0.0101, 0.0080, 0.0123, 0.6360, 0.1835, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 84, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2820e-08, 5.8877e-03, 9.9411e-01, 1.3358e-07, 4.1222e-07, 3.6794e-09,
        1.2251e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 84, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1686, 0.0245, 0.1758, 0.0304, 0.3924, 0.1975],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 85, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.1748e-03, 8.7176e-01, 2.0569e-02, 8.1551e-02, 1.8250e-05, 8.1076e-03,
        9.8208e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 85, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0062, 0.0042, 0.0046, 0.1927, 0.0036, 0.0042, 0.7845],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 85, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1440, 0.0074, 0.0078, 0.0147, 0.5455, 0.2738, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 85, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0725e-08, 3.8564e-03, 9.9614e-01, 8.5208e-08, 1.3120e-06, 4.5271e-09,
        1.2003e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 85, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1665, 0.0261, 0.1691, 0.0304, 0.4015, 0.1967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 86, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2540e-02, 7.9716e-01, 2.3041e-02, 1.4596e-01, 1.7538e-05, 8.8640e-03,
        1.2413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 86, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0040, 0.0043, 0.2075, 0.0055, 0.0052, 0.7685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 86, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1239, 0.0099, 0.0071, 0.0105, 0.5671, 0.2762, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 86, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.8541e-08, 5.4009e-03, 9.9460e-01, 1.1415e-07, 4.4717e-07, 2.1278e-09,
        2.9657e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 86, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0113, 0.1681, 0.0249, 0.1748, 0.0305, 0.3929, 0.1976],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 87, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4419e-03, 8.4760e-01, 1.9819e-02, 1.0157e-01, 1.6244e-05, 1.1207e-02,
        1.0342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 87, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0060, 0.0057, 0.2281, 0.0054, 0.0052, 0.7437],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 87, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1626, 0.0112, 0.0119, 0.0188, 0.5810, 0.2067, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 87, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3154e-07, 4.5643e-03, 9.9543e-01, 1.1193e-07, 6.2258e-07, 4.5510e-09,
        1.8733e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 87, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1619, 0.0257, 0.1737, 0.0303, 0.4004, 0.1981],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 88, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1599e-02, 8.5446e-01, 2.1419e-02, 9.4180e-02, 1.8362e-05, 9.3631e-03,
        8.9563e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.009

[Epoch: 88, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0046, 0.0053, 0.2015, 0.0054, 0.0042, 0.7745],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 88, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1416, 0.0105, 0.0070, 0.0138, 0.5807, 0.2414, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 88, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5152e-08, 3.5903e-03, 9.9641e-01, 7.2270e-08, 3.6789e-07, 2.0213e-09,
        1.7075e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 88, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1636, 0.0249, 0.1741, 0.0292, 0.4028, 0.1952],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 89, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.1231e-03, 8.9583e-01, 1.5635e-02, 5.8019e-02, 1.1978e-05, 8.9582e-03,
        1.2425e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 89, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0037, 0.0052, 0.0062, 0.1891, 0.0046, 0.0056, 0.7856],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 89, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1173, 0.0094, 0.0072, 0.0141, 0.6528, 0.1935, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 89, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1467e-07, 8.0844e-03, 9.9191e-01, 1.4914e-07, 9.2744e-07, 2.4874e-09,
        1.3669e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 89, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1635, 0.0255, 0.1756, 0.0296, 0.3948, 0.2006],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 90, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5622e-03, 8.5111e-01, 2.1985e-02, 1.0208e-01, 8.7775e-06, 8.8251e-03,
        8.4296e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 90, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0069, 0.0037, 0.0046, 0.2029, 0.0052, 0.0046, 0.7721],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 90, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1442, 0.0103, 0.0115, 0.0162, 0.5840, 0.2270, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 90, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3621e-07, 3.3777e-03, 9.9662e-01, 6.8618e-08, 4.1298e-07, 6.8850e-09,
        3.1142e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 90, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1614, 0.0228, 0.1724, 0.0295, 0.4107, 0.1936],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 91, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.6864e-03, 8.6538e-01, 1.7652e-02, 8.5814e-02, 4.7749e-06, 8.9262e-03,
        1.2538e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 91, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0058, 0.0053, 0.0069, 0.1574, 0.0055, 0.0048, 0.8143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 91, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1410, 0.0090, 0.0060, 0.0124, 0.6095, 0.2173, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 91, batch: 172/219] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2937e-08, 5.0067e-03, 9.9499e-01, 1.1764e-07, 9.9155e-07, 6.7953e-10,
        1.7371e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 91, batch: 215/219] total loss per batch: 0.570
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1679, 0.0247, 0.1758, 0.0309, 0.3924, 0.1974],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 92, batch: 43/219] total loss per batch: 0.573
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0403e-02, 7.8721e-01, 1.4859e-02, 1.6731e-01, 2.3180e-05, 8.9817e-03,
        1.1216e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 92, batch: 86/219] total loss per batch: 0.578
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0030, 0.0048, 0.0052, 0.2069, 0.0051, 0.0055, 0.7695],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 92, batch: 129/219] total loss per batch: 0.598
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1569, 0.0092, 0.0180, 0.0119, 0.5034, 0.2955, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 92, batch: 172/219] total loss per batch: 0.567
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5945e-08, 6.3644e-03, 9.9363e-01, 9.8553e-08, 3.1827e-06, 9.8748e-10,
        1.3888e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 92, batch: 215/219] total loss per batch: 0.574
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1688, 0.0240, 0.1769, 0.0291, 0.4050, 0.1859],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.024

[Epoch: 93, batch: 43/219] total loss per batch: 0.579
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.8501e-03, 9.0469e-01, 1.0238e-02, 6.0951e-02, 2.2073e-05, 7.2399e-03,
        1.0012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.009

[Epoch: 93, batch: 86/219] total loss per batch: 0.582
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0072, 0.0030, 0.0045, 0.1830, 0.0013, 0.0075, 0.7935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 93, batch: 129/219] total loss per batch: 0.606
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1282, 0.0090, 0.0160, 0.0117, 0.5860, 0.2404, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 93, batch: 172/219] total loss per batch: 0.571
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0479e-07, 8.3511e-03, 9.9164e-01, 1.5308e-07, 6.4698e-06, 2.3682e-09,
        4.3319e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 93, batch: 215/219] total loss per batch: 0.574
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1634, 0.0232, 0.1741, 0.0286, 0.3972, 0.2035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 94, batch: 43/219] total loss per batch: 0.569
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.9670e-03, 8.3023e-01, 2.5530e-02, 1.1405e-01, 8.5501e-06, 1.3828e-02,
        7.3881e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.019

[Epoch: 94, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0054, 0.0076, 0.1888, 0.0059, 0.0078, 0.7786],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 94, batch: 129/219] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1416, 0.0091, 0.0141, 0.0124, 0.5973, 0.2164, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 94, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8347e-07, 9.8505e-03, 9.9015e-01, 1.0122e-07, 2.1113e-06, 1.2465e-09,
        7.7616e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.985

[Epoch: 94, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1645, 0.0240, 0.1673, 0.0291, 0.4107, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 95, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0152e-02, 8.3899e-01, 2.1433e-02, 1.1035e-01, 8.3643e-06, 8.8173e-03,
        1.0252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 95, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0051, 0.0060, 0.1933, 0.0061, 0.0068, 0.7784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 95, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1453, 0.0105, 0.0105, 0.0109, 0.5912, 0.2247, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 95, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1949e-07, 8.3431e-03, 9.9166e-01, 5.7829e-08, 1.5003e-06, 4.8598e-10,
        3.2444e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 95, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1651, 0.0237, 0.1703, 0.0298, 0.4051, 0.1960],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 96, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1114e-02, 8.2003e-01, 2.4252e-02, 1.2553e-01, 5.4014e-06, 1.1171e-02,
        7.9015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 96, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0058, 0.0048, 0.0060, 0.2024, 0.0049, 0.0057, 0.7704],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 96, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1610, 0.0103, 0.0093, 0.0155, 0.5377, 0.2593, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 96, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1521e-07, 5.4647e-03, 9.9453e-01, 5.9235e-08, 1.6675e-06, 4.0929e-10,
        2.3499e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 96, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1657, 0.0235, 0.1712, 0.0299, 0.4041, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 97, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0508e-02, 8.5089e-01, 1.9465e-02, 1.0113e-01, 5.9315e-06, 1.0057e-02,
        7.9342e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 97, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0051, 0.0061, 0.2146, 0.0054, 0.0054, 0.7590],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 97, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1325, 0.0091, 0.0083, 0.0151, 0.5856, 0.2430, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 97, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0689e-08, 5.4292e-03, 9.9457e-01, 3.9768e-08, 8.4881e-07, 3.2001e-10,
        1.8133e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 97, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1657, 0.0238, 0.1720, 0.0300, 0.4032, 0.1953],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 98, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0084e-02, 8.5192e-01, 2.1257e-02, 9.9849e-02, 4.7511e-06, 9.4523e-03,
        7.4288e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 98, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0054, 0.0054, 0.0058, 0.1928, 0.0052, 0.0047, 0.7806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 98, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1395, 0.0100, 0.0086, 0.0162, 0.5919, 0.2273, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 98, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6152e-08, 4.7043e-03, 9.9529e-01, 3.5517e-08, 9.1493e-07, 2.8942e-10,
        1.5406e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 98, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1650, 0.0237, 0.1718, 0.0297, 0.4042, 0.1955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 99, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0852e-02, 8.4614e-01, 2.0046e-02, 1.0462e-01, 3.4926e-06, 9.7240e-03,
        8.6185e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 99, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0049, 0.0059, 0.1993, 0.0054, 0.0045, 0.7751],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 99, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1374, 0.0089, 0.0090, 0.0146, 0.5888, 0.2353, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 99, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.9351e-08, 4.6000e-03, 9.9540e-01, 2.5437e-08, 6.2405e-07, 1.9448e-10,
        1.0491e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 99, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1665, 0.0244, 0.1714, 0.0307, 0.4021, 0.1947],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 100, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0435e-02, 8.5260e-01, 2.0384e-02, 9.8437e-02, 3.1641e-06, 9.5403e-03,
        8.6026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 100, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0051, 0.0055, 0.1882, 0.0048, 0.0048, 0.7865],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 100, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1336, 0.0104, 0.0086, 0.0163, 0.6019, 0.2229, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 100, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5703e-08, 5.1254e-03, 9.9487e-01, 2.4174e-08, 5.8399e-07, 2.3341e-10,
        1.3395e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 100, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1660, 0.0240, 0.1733, 0.0304, 0.3999, 0.1961],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 101, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0764e-02, 8.4729e-01, 1.9623e-02, 1.0437e-01, 2.9073e-06, 9.4689e-03,
        8.4799e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 101, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0048, 0.0055, 0.1844, 0.0052, 0.0044, 0.7909],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 101, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1407, 0.0097, 0.0098, 0.0154, 0.5817, 0.2371, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 101, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7306e-08, 5.0853e-03, 9.9491e-01, 3.1990e-08, 9.1187e-07, 3.6123e-10,
        7.2771e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 101, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1661, 0.0245, 0.1723, 0.0300, 0.4003, 0.1964],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 102, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4506e-03, 8.5547e-01, 1.9324e-02, 9.7587e-02, 2.7771e-06, 9.3427e-03,
        8.8241e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 102, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0050, 0.0057, 0.2033, 0.0055, 0.0052, 0.7700],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 102, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1449, 0.0096, 0.0078, 0.0139, 0.5699, 0.2482, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 102, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7670e-08, 5.0922e-03, 9.9491e-01, 2.2889e-08, 4.4859e-07, 1.7702e-10,
        1.0983e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 102, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1654, 0.0245, 0.1702, 0.0303, 0.4045, 0.1948],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 103, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1436e-02, 8.5289e-01, 2.1136e-02, 9.6972e-02, 3.0471e-06, 9.4859e-03,
        8.0739e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 103, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0049, 0.0050, 0.2129, 0.0048, 0.0045, 0.7626],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 103, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1438, 0.0118, 0.0104, 0.0185, 0.5783, 0.2313, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 103, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.9395e-08, 5.1515e-03, 9.9485e-01, 2.7410e-08, 6.6485e-07, 3.6550e-10,
        5.9325e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 103, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1663, 0.0239, 0.1733, 0.0306, 0.3965, 0.1989],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 104, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.5017e-03, 8.4475e-01, 1.9813e-02, 1.0633e-01, 2.2757e-06, 1.0053e-02,
        9.5505e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 104, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0050, 0.0058, 0.2019, 0.0053, 0.0047, 0.7722],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 104, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1287, 0.0088, 0.0079, 0.0143, 0.6165, 0.2184, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 104, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9015e-08, 5.1751e-03, 9.9482e-01, 2.6975e-08, 4.4696e-07, 3.3457e-10,
        1.2258e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 104, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1628, 0.0247, 0.1723, 0.0305, 0.4046, 0.1947],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 105, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1281e-02, 8.3959e-01, 2.0067e-02, 1.1188e-01, 2.7770e-06, 9.2274e-03,
        7.9490e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 105, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0054, 0.0049, 0.0045, 0.1966, 0.0049, 0.0049, 0.7787],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 105, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1381, 0.0107, 0.0094, 0.0148, 0.5765, 0.2442, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 105, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.9423e-08, 5.6981e-03, 9.9430e-01, 3.2015e-08, 9.1575e-07, 3.8105e-10,
        4.9517e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 105, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1668, 0.0239, 0.1705, 0.0307, 0.3994, 0.1980],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 106, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.0974e-03, 8.5898e-01, 1.7836e-02, 9.5341e-02, 2.1275e-06, 8.9316e-03,
        9.8073e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 106, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0048, 0.0060, 0.2146, 0.0050, 0.0043, 0.7608],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 106, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1445, 0.0096, 0.0093, 0.0149, 0.5397, 0.2753, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 106, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3506e-08, 4.6512e-03, 9.9535e-01, 4.4236e-08, 6.2466e-07, 8.0089e-10,
        1.5086e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 106, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1636, 0.0243, 0.1721, 0.0312, 0.4011, 0.1976],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 107, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0920e-02, 8.6904e-01, 2.0471e-02, 8.3697e-02, 3.6936e-06, 7.9797e-03,
        7.8896e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 107, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0057, 0.0058, 0.0046, 0.2002, 0.0046, 0.0061, 0.7729],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 107, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1292, 0.0111, 0.0087, 0.0160, 0.6232, 0.2059, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 107, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6767e-08, 4.2979e-03, 9.9570e-01, 4.6634e-08, 5.3754e-07, 8.5570e-10,
        1.4502e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 107, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0107, 0.1665, 0.0247, 0.1758, 0.0311, 0.3959, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 108, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3208e-02, 8.2864e-01, 1.9636e-02, 1.1626e-01, 4.0762e-06, 1.0625e-02,
        1.1632e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 108, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0032, 0.0042, 0.0046, 0.2167, 0.0050, 0.0040, 0.7623],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 108, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1128, 0.0089, 0.0088, 0.0124, 0.6124, 0.2396, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 108, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5226e-07, 6.1143e-03, 9.9388e-01, 1.3865e-07, 6.9625e-07, 5.2965e-10,
        1.6425e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 108, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1704, 0.0235, 0.1737, 0.0293, 0.3982, 0.1947],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 109, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1523e-02, 8.4610e-01, 2.1219e-02, 1.0376e-01, 7.4795e-06, 8.0279e-03,
        9.3646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 109, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0047, 0.0037, 0.1685, 0.0057, 0.0058, 0.8063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 109, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1775, 0.0123, 0.0110, 0.0141, 0.5569, 0.2232, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 109, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4819e-08, 4.7199e-03, 9.9528e-01, 4.0138e-08, 1.4628e-07, 6.3129e-09,
        9.3616e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 109, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1596, 0.0246, 0.1753, 0.0310, 0.4026, 0.1959],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 110, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0194e-02, 8.5665e-01, 1.8796e-02, 9.2451e-02, 9.1785e-06, 9.9309e-03,
        1.1967e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.004

[Epoch: 110, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0165, 0.0074, 0.0064, 0.1965, 0.0040, 0.0114, 0.7579],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.000

[Epoch: 110, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1296, 0.0095, 0.0064, 0.0175, 0.5983, 0.2331, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.020

[Epoch: 110, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6550e-07, 2.9377e-03, 9.9706e-01, 1.1745e-07, 1.1661e-06, 2.2034e-09,
        1.9963e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.991

[Epoch: 110, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1706, 0.0245, 0.1752, 0.0290, 0.3984, 0.1925],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 111, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.7554e-03, 8.6993e-01, 1.8820e-02, 8.6962e-02, 3.0177e-06, 9.4224e-03,
        7.1099e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 111, batch: 86/219] total loss per batch: 0.576
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0018, 0.0050, 0.0057, 0.1723, 0.0063, 0.0055, 0.8034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 111, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1473, 0.0093, 0.0079, 0.0188, 0.5656, 0.2466, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 111, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4187e-08, 4.6981e-03, 9.9530e-01, 2.9408e-08, 1.3303e-07, 3.1061e-09,
        3.7525e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 111, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1619, 0.0243, 0.1737, 0.0306, 0.4066, 0.1925],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 112, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.3060e-03, 8.6945e-01, 1.8916e-02, 8.4939e-02, 5.5049e-06, 8.7545e-03,
        8.6289e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.022

[Epoch: 112, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0043, 0.0063, 0.1818, 0.0038, 0.0059, 0.7933],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 112, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1530, 0.0093, 0.0080, 0.0150, 0.5500, 0.2596, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 112, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2383e-07, 3.2862e-03, 9.9671e-01, 4.5760e-08, 1.1297e-07, 1.6850e-09,
        5.1669e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.984

[Epoch: 112, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1660, 0.0252, 0.1752, 0.0308, 0.3982, 0.1940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 113, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0983e-02, 8.4835e-01, 2.1913e-02, 1.0088e-01, 4.3406e-06, 9.6043e-03,
        8.2591e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 113, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0036, 0.0042, 0.0064, 0.1642, 0.0039, 0.0041, 0.8136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 113, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1446, 0.0096, 0.0078, 0.0166, 0.6010, 0.2144, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 113, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.6398e-08, 3.5493e-03, 9.9645e-01, 2.7709e-08, 8.6962e-08, 1.7449e-09,
        6.1629e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.988

[Epoch: 113, batch: 215/219] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1645, 0.0240, 0.1745, 0.0308, 0.3989, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 114, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.9288e-03, 8.3209e-01, 2.3161e-02, 1.1771e-01, 7.5243e-06, 7.7375e-03,
        9.3670e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.030

[Epoch: 114, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0042, 0.0045, 0.0060, 0.2259, 0.0038, 0.0049, 0.7506],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 114, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1449, 0.0100, 0.0089, 0.0154, 0.6320, 0.1828, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 114, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0255e-07, 7.3693e-04, 9.9926e-01, 3.0470e-08, 1.0673e-08, 3.8089e-10,
        4.9148e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 114, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1606, 0.0244, 0.1738, 0.0298, 0.4071, 0.1937],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 115, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0492e-02, 8.5779e-01, 2.0961e-02, 9.2460e-02, 4.3105e-06, 8.8455e-03,
        9.4513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.021

[Epoch: 115, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0038, 0.0047, 0.0057, 0.2204, 0.0034, 0.0039, 0.7580],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 115, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1321, 0.0080, 0.0080, 0.0129, 0.6155, 0.2179, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 115, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4325e-08, 1.4792e-03, 9.9852e-01, 3.3545e-08, 1.2213e-08, 4.8572e-10,
        2.5508e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 115, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1624, 0.0248, 0.1723, 0.0299, 0.4057, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 116, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1303e-02, 8.4530e-01, 2.1437e-02, 1.0287e-01, 4.5822e-06, 8.8187e-03,
        1.0267e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.021

[Epoch: 116, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0051, 0.0054, 0.2021, 0.0042, 0.0044, 0.7737],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 116, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1314, 0.0085, 0.0085, 0.0146, 0.5621, 0.2692, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 116, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4071e-08, 1.9067e-03, 9.9809e-01, 1.6936e-08, 1.0868e-08, 2.3683e-10,
        1.9212e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 116, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1648, 0.0245, 0.1720, 0.0306, 0.4036, 0.1945],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 117, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4587e-03, 8.5943e-01, 2.1069e-02, 9.3502e-02, 4.5127e-06, 8.0796e-03,
        8.4516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.023

[Epoch: 117, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0047, 0.0049, 0.0050, 0.1962, 0.0040, 0.0043, 0.7809],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 117, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1498, 0.0094, 0.0096, 0.0158, 0.5705, 0.2392, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 117, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4053e-08, 2.5399e-03, 9.9746e-01, 2.2004e-08, 1.0299e-08, 3.3492e-10,
        1.9047e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 117, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1616, 0.0245, 0.1743, 0.0302, 0.4032, 0.1957],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 118, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.9973e-03, 8.4999e-01, 2.0833e-02, 1.0088e-01, 4.1852e-06, 8.6763e-03,
        9.6122e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.019

[Epoch: 118, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0049, 0.0048, 0.2091, 0.0043, 0.0043, 0.7681],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 118, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1329, 0.0093, 0.0081, 0.0157, 0.6209, 0.2075, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 118, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7523e-08, 2.9777e-03, 9.9702e-01, 1.6706e-08, 9.9613e-09, 2.5402e-10,
        1.5540e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 118, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1626, 0.0247, 0.1759, 0.0302, 0.4033, 0.1933],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 119, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0056e-02, 8.5119e-01, 2.0595e-02, 9.9795e-02, 3.6214e-06, 9.0624e-03,
        9.2938e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 119, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0047, 0.0052, 0.2020, 0.0039, 0.0041, 0.7750],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 119, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1379, 0.0100, 0.0101, 0.0163, 0.5631, 0.2566, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 119, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6441e-08, 3.5852e-03, 9.9641e-01, 1.9152e-08, 9.1053e-09, 3.3117e-10,
        1.8702e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 119, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1634, 0.0246, 0.1746, 0.0305, 0.4027, 0.1940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 120, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0245e-02, 8.5173e-01, 2.0049e-02, 9.9723e-02, 3.0179e-06, 9.2511e-03,
        8.9962e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 120, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0041, 0.0048, 0.0047, 0.1939, 0.0045, 0.0042, 0.7838],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 120, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1422, 0.0093, 0.0087, 0.0143, 0.5642, 0.2562, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 120, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5062e-08, 3.9726e-03, 9.9603e-01, 1.5635e-08, 1.2125e-08, 2.7722e-10,
        1.7582e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 120, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1633, 0.0251, 0.1761, 0.0302, 0.4006, 0.1942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 121, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0278e-02, 8.5462e-01, 2.1895e-02, 9.4176e-02, 2.6976e-06, 9.4166e-03,
        9.6130e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 121, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0044, 0.0048, 0.0047, 0.2012, 0.0040, 0.0045, 0.7764],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 121, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1435, 0.0103, 0.0098, 0.0164, 0.5980, 0.2158, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 121, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9776e-08, 3.7546e-03, 9.9625e-01, 1.3214e-08, 6.4251e-09, 1.9576e-10,
        2.1284e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 121, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1640, 0.0242, 0.1748, 0.0308, 0.4018, 0.1944],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 122, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0730e-02, 8.4407e-01, 2.1393e-02, 1.0298e-01, 3.0670e-06, 1.0382e-02,
        1.0444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.019

[Epoch: 122, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0047, 0.0051, 0.0054, 0.1994, 0.0047, 0.0041, 0.7766],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 122, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1346, 0.0091, 0.0085, 0.0144, 0.5975, 0.2309, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 122, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1856e-08, 4.5003e-03, 9.9550e-01, 1.1192e-08, 1.6034e-08, 2.7843e-10,
        1.7552e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 122, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1659, 0.0247, 0.1751, 0.0299, 0.3992, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 123, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2541e-02, 8.4906e-01, 2.0763e-02, 9.6620e-02, 2.7507e-06, 1.1399e-02,
        9.6097e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 123, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0046, 0.0052, 0.1940, 0.0046, 0.0044, 0.7830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 123, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1441, 0.0108, 0.0114, 0.0168, 0.5578, 0.2526, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 123, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4328e-08, 3.4940e-03, 9.9651e-01, 1.0175e-08, 6.8644e-09, 2.6464e-10,
        3.0324e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 123, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0110, 0.1650, 0.0253, 0.1730, 0.0324, 0.3974, 0.1960],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 124, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1681e-02, 7.9090e-01, 2.4586e-02, 1.4959e-01, 4.7113e-06, 1.1098e-02,
        1.2138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 124, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0042, 0.0060, 0.0049, 0.1841, 0.0047, 0.0050, 0.7911],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 124, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1290, 0.0107, 0.0083, 0.0139, 0.6160, 0.2169, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 124, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8427e-08, 4.8988e-03, 9.9510e-01, 1.7715e-08, 1.6942e-08, 1.1658e-09,
        3.9535e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 124, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1690, 0.0247, 0.1739, 0.0307, 0.3995, 0.1920],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 125, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2578e-02, 8.6733e-01, 1.7083e-02, 8.5482e-02, 3.2897e-06, 7.7562e-03,
        9.7707e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 125, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0060, 0.0042, 0.0051, 0.2127, 0.0068, 0.0045, 0.7608],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 125, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1420, 0.0119, 0.0090, 0.0149, 0.6090, 0.2061, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 125, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4159e-08, 3.8546e-03, 9.9615e-01, 3.9593e-08, 4.7113e-08, 7.2657e-10,
        2.8649e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 125, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0108, 0.1640, 0.0249, 0.1707, 0.0314, 0.3996, 0.1986],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 126, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.3328e-02, 8.5750e-01, 2.1597e-02, 8.6476e-02, 3.2787e-06, 1.0362e-02,
        1.0735e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 126, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0056, 0.0054, 0.0051, 0.2140, 0.0054, 0.0051, 0.7592],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 126, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1463, 0.0103, 0.0090, 0.0153, 0.5226, 0.2922, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 126, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.5711e-08, 5.1262e-03, 9.9487e-01, 3.1665e-08, 3.9996e-08, 1.6722e-09,
        5.8982e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 126, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0106, 0.1626, 0.0243, 0.1755, 0.0297, 0.4010, 0.1963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 127, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1696e-02, 8.5906e-01, 1.7613e-02, 9.3194e-02, 4.2102e-06, 8.4060e-03,
        1.0026e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 127, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0033, 0.0049, 0.0039, 0.2452, 0.0043, 0.0066, 0.7318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 127, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1515, 0.0118, 0.0080, 0.0167, 0.6035, 0.2028, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 127, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.3820e-08, 3.5865e-03, 9.9641e-01, 2.8735e-08, 3.0609e-08, 1.4555e-09,
        3.1746e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 127, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0109, 0.1656, 0.0249, 0.1729, 0.0316, 0.3995, 0.1945],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 128, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0345e-02, 8.5296e-01, 2.0201e-02, 9.8274e-02, 2.9640e-06, 9.4949e-03,
        8.7264e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 128, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0039, 0.0038, 0.2119, 0.0036, 0.0050, 0.7667],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 128, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1475, 0.0107, 0.0103, 0.0150, 0.5721, 0.2398, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 128, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1234e-08, 4.6977e-03, 9.9530e-01, 3.7828e-08, 4.1043e-08, 1.2112e-09,
        2.9557e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 128, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1612, 0.0232, 0.1747, 0.0302, 0.4041, 0.1967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 129, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.3092e-03, 8.2860e-01, 2.1648e-02, 1.2509e-01, 3.5083e-06, 9.2850e-03,
        7.0631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 129, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0038, 0.0038, 0.0041, 0.1751, 0.0041, 0.0040, 0.8051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 129, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1333, 0.0116, 0.0118, 0.0177, 0.5878, 0.2313, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 129, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3542e-08, 4.5808e-03, 9.9542e-01, 1.9695e-08, 2.7633e-08, 9.8260e-10,
        6.3995e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 129, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1591, 0.0243, 0.1753, 0.0306, 0.4071, 0.1935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 130, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.0354e-03, 8.5802e-01, 1.8371e-02, 9.9404e-02, 6.3527e-06, 7.6784e-03,
        8.4875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.021

[Epoch: 130, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0060, 0.0044, 0.0050, 0.1816, 0.0045, 0.0052, 0.7933],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 130, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1395, 0.0114, 0.0095, 0.0180, 0.5825, 0.2337, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 130, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4984e-08, 3.9069e-03, 9.9609e-01, 3.5922e-08, 4.3361e-08, 1.2312e-09,
        1.5140e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.991

[Epoch: 130, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1655, 0.0243, 0.1774, 0.0301, 0.3966, 0.1959],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 131, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0648e-02, 8.7508e-01, 1.7323e-02, 8.0543e-02, 3.4218e-06, 7.3596e-03,
        9.0380e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.020

[Epoch: 131, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0048, 0.0047, 0.0056, 0.2116, 0.0043, 0.0047, 0.7644],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 131, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1264, 0.0080, 0.0086, 0.0136, 0.5806, 0.2578, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 131, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0936e-08, 5.2836e-03, 9.9472e-01, 1.5219e-08, 3.1100e-08, 1.1069e-09,
        9.1190e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 131, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1610, 0.0227, 0.1728, 0.0298, 0.4064, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 132, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7507e-03, 8.5952e-01, 2.2193e-02, 8.9256e-02, 4.5933e-06, 8.0223e-03,
        1.1250e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 132, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0056, 0.0047, 0.0060, 0.2016, 0.0062, 0.0056, 0.7703],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 132, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1616, 0.0113, 0.0113, 0.0143, 0.5579, 0.2382, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 132, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1449e-08, 5.7312e-03, 9.9427e-01, 3.7921e-08, 5.7674e-08, 8.6387e-10,
        3.9064e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 132, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1604, 0.0243, 0.1737, 0.0304, 0.4059, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 133, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1119e-02, 8.4152e-01, 2.1838e-02, 1.0760e-01, 6.9417e-06, 8.9106e-03,
        9.0098e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 133, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0084, 0.0052, 0.0048, 0.2153, 0.0060, 0.0054, 0.7548],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 133, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1183, 0.0108, 0.0089, 0.0152, 0.6291, 0.2124, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 133, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3131e-08, 4.4737e-03, 9.9553e-01, 4.4085e-08, 5.7024e-08, 1.5660e-09,
        5.6690e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.988

[Epoch: 133, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1653, 0.0232, 0.1712, 0.0291, 0.4040, 0.1975],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 134, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.0963e-03, 8.5079e-01, 2.2362e-02, 9.8642e-02, 6.0517e-06, 8.3727e-03,
        1.1734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 134, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0052, 0.0046, 0.2225, 0.0058, 0.0051, 0.7524],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 134, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.0996, 0.0077, 0.0096, 0.0154, 0.6985, 0.1624, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 134, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2212e-08, 4.5526e-03, 9.9545e-01, 2.6719e-08, 3.7282e-08, 2.5679e-10,
        1.9193e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 134, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1659, 0.0242, 0.1749, 0.0310, 0.3976, 0.1959],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 135, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1241e-02, 8.4568e-01, 1.6893e-02, 1.0480e-01, 5.0358e-06, 1.1346e-02,
        1.0030e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 135, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0035, 0.0044, 0.0063, 0.1992, 0.0039, 0.0044, 0.7783],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 135, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1478, 0.0107, 0.0132, 0.0160, 0.5330, 0.2653, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 135, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2622e-08, 6.0830e-03, 9.9392e-01, 8.2680e-08, 7.6285e-08, 7.9815e-10,
        4.1840e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 135, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0111, 0.1670, 0.0253, 0.1762, 0.0314, 0.3947, 0.1943],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 136, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.7470e-03, 8.5950e-01, 2.2293e-02, 8.9208e-02, 1.0142e-05, 9.6983e-03,
        1.0542e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 136, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0070, 0.0044, 0.0046, 0.1645, 0.0055, 0.0049, 0.8092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 136, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1459, 0.0105, 0.0101, 0.0163, 0.5211, 0.2882, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 136, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4854e-08, 4.5882e-03, 9.9541e-01, 2.4367e-08, 2.1845e-08, 1.6391e-10,
        4.5689e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 136, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1635, 0.0247, 0.1759, 0.0312, 0.3973, 0.1970],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 137, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7332e-03, 8.4961e-01, 1.7440e-02, 1.0161e-01, 6.8478e-06, 1.0723e-02,
        1.0870e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 137, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0054, 0.0046, 0.0054, 0.2124, 0.0047, 0.0043, 0.7632],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 137, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1605, 0.0115, 0.0144, 0.0104, 0.5427, 0.2526, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 137, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2438e-08, 4.8425e-03, 9.9516e-01, 3.5991e-08, 3.9519e-08, 2.4008e-10,
        3.4258e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 137, batch: 215/219] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0104, 0.1668, 0.0245, 0.1755, 0.0305, 0.3968, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 138, batch: 43/219] total loss per batch: 0.563
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.9908e-03, 8.6567e-01, 1.5871e-02, 8.9254e-02, 1.0133e-05, 9.2303e-03,
        1.1975e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 138, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0045, 0.0050, 0.0043, 0.1974, 0.0042, 0.0047, 0.7798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 138, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1453, 0.0096, 0.0101, 0.0138, 0.5946, 0.2212, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 138, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8354e-08, 3.7480e-03, 9.9625e-01, 6.1199e-08, 3.9679e-07, 7.2530e-10,
        3.9344e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 138, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1655, 0.0246, 0.1738, 0.0316, 0.3976, 0.1969],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 139, batch: 43/219] total loss per batch: 0.569
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.5022e-02, 8.3870e-01, 2.9812e-02, 9.3069e-02, 8.1494e-06, 9.5962e-03,
        1.3789e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.022

[Epoch: 139, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0066, 0.0043, 0.0057, 0.2326, 0.0057, 0.0084, 0.7368],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 139, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1486, 0.0095, 0.0072, 0.0109, 0.6153, 0.1996, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 139, batch: 172/219] total loss per batch: 0.570
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4048e-08, 2.4452e-03, 9.9755e-01, 5.7868e-08, 3.6536e-07, 3.8021e-10,
        2.4542e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.990

[Epoch: 139, batch: 215/219] total loss per batch: 0.574
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1670, 0.0249, 0.1713, 0.0293, 0.4013, 0.1961],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 140, batch: 43/219] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.5749e-03, 8.2494e-01, 1.9917e-02, 1.2740e-01, 7.4316e-06, 9.7308e-03,
        8.4314e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.020

[Epoch: 140, batch: 86/219] total loss per batch: 0.577
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0062, 0.0049, 0.1844, 0.0054, 0.0060, 0.7883],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 140, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1264, 0.0100, 0.0072, 0.0237, 0.5835, 0.2430, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 140, batch: 172/219] total loss per batch: 0.567
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3302e-08, 3.4930e-03, 9.9651e-01, 2.6040e-08, 6.0257e-08, 4.6077e-10,
        4.8515e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 140, batch: 215/219] total loss per batch: 0.569
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0094, 0.1622, 0.0240, 0.1724, 0.0303, 0.4018, 0.1997],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 141, batch: 43/219] total loss per batch: 0.565
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.6787e-03, 8.6125e-01, 1.9208e-02, 9.6268e-02, 9.6064e-06, 7.5525e-03,
        7.0301e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.029

[Epoch: 141, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0056, 0.0069, 0.0053, 0.1763, 0.0062, 0.0046, 0.7952],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 141, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1742, 0.0135, 0.0084, 0.0125, 0.5054, 0.2764, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.006

[Epoch: 141, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.3834e-08, 3.5059e-03, 9.9649e-01, 1.5097e-07, 5.6310e-08, 4.7559e-10,
        7.7906e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 141, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1631, 0.0239, 0.1721, 0.0302, 0.4020, 0.1991],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 142, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7132e-03, 8.4821e-01, 2.2061e-02, 1.0316e-01, 1.2419e-05, 8.3940e-03,
        8.4517e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 142, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0055, 0.0060, 0.0056, 0.1953, 0.0055, 0.0048, 0.7774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 142, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1450, 0.0130, 0.0089, 0.0148, 0.5691, 0.2420, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 142, batch: 172/219] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7492e-08, 3.3811e-03, 9.9662e-01, 1.6922e-08, 1.9718e-08, 2.7215e-10,
        9.5263e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 142, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1622, 0.0242, 0.1735, 0.0305, 0.4021, 0.1976],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 143, batch: 43/219] total loss per batch: 0.562
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.2085e-03, 8.6210e-01, 1.8974e-02, 9.4987e-02, 3.9418e-06, 8.6257e-03,
        6.1004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.024

[Epoch: 143, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0054, 0.0054, 0.2014, 0.0054, 0.0046, 0.7728],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 143, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1335, 0.0117, 0.0073, 0.0143, 0.5876, 0.2374, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 143, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3808e-08, 4.3084e-03, 9.9569e-01, 4.0542e-08, 3.7924e-08, 3.1328e-10,
        2.4884e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 143, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1615, 0.0239, 0.1726, 0.0303, 0.4048, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 144, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7848e-03, 8.5443e-01, 2.1139e-02, 9.9038e-02, 5.6898e-06, 8.3403e-03,
        7.2602e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.022

[Epoch: 144, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0048, 0.0044, 0.1953, 0.0051, 0.0043, 0.7811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 144, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1528, 0.0116, 0.0094, 0.0153, 0.5603, 0.2431, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 144, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4193e-08, 4.7223e-03, 9.9528e-01, 4.9642e-08, 3.6508e-08, 3.9326e-10,
        1.8378e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 144, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1616, 0.0237, 0.1725, 0.0304, 0.4055, 0.1964],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 145, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7723e-03, 8.5360e-01, 1.9803e-02, 1.0006e-01, 4.7566e-06, 8.7038e-03,
        8.0615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.021

[Epoch: 145, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0055, 0.0048, 0.0046, 0.1930, 0.0052, 0.0044, 0.7826],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 145, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1441, 0.0114, 0.0092, 0.0155, 0.5774, 0.2358, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 145, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7388e-08, 4.6791e-03, 9.9532e-01, 4.0251e-08, 2.7828e-08, 3.3119e-10,
        1.5356e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 145, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1621, 0.0236, 0.1735, 0.0303, 0.4046, 0.1961],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 146, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0239e-02, 8.4982e-01, 2.0617e-02, 1.0237e-01, 4.3227e-06, 8.8663e-03,
        8.0843e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.020

[Epoch: 146, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0057, 0.0048, 0.0049, 0.1954, 0.0052, 0.0045, 0.7794],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 146, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1433, 0.0111, 0.0094, 0.0158, 0.5803, 0.2337, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 146, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3859e-08, 4.7641e-03, 9.9524e-01, 3.4813e-08, 2.6295e-08, 3.0286e-10,
        1.4885e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 146, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1628, 0.0237, 0.1732, 0.0304, 0.4046, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 147, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0505e-02, 8.5004e-01, 2.0596e-02, 1.0175e-01, 4.2307e-06, 8.8910e-03,
        8.2080e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.019

[Epoch: 147, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0057, 0.0049, 0.0050, 0.1971, 0.0052, 0.0046, 0.7774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 147, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1415, 0.0109, 0.0094, 0.0156, 0.5819, 0.2346, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 147, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9546e-08, 4.8385e-03, 9.9516e-01, 2.6765e-08, 1.9219e-08, 2.2848e-10,
        1.2352e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 147, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1622, 0.0237, 0.1737, 0.0303, 0.4051, 0.1952],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 148, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0268e-02, 8.5407e-01, 2.0288e-02, 9.8271e-02, 3.1811e-06, 8.8626e-03,
        8.2370e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.018

[Epoch: 148, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0049, 0.0052, 0.1989, 0.0055, 0.0047, 0.7750],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 148, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1409, 0.0107, 0.0098, 0.0158, 0.5820, 0.2350, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 148, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6733e-08, 4.7593e-03, 9.9524e-01, 2.3955e-08, 1.7715e-08, 2.0470e-10,
        1.1461e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 148, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1625, 0.0238, 0.1741, 0.0302, 0.4041, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 149, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0702e-02, 8.4580e-01, 2.0537e-02, 1.0517e-01, 3.3641e-06, 9.1256e-03,
        8.6556e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 149, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0058, 0.0049, 0.0052, 0.1995, 0.0055, 0.0048, 0.7743],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 149, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1387, 0.0105, 0.0095, 0.0152, 0.5915, 0.2288, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 149, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4153e-08, 4.5518e-03, 9.9545e-01, 1.9310e-08, 1.2913e-08, 1.5531e-10,
        9.2439e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 149, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1619, 0.0235, 0.1732, 0.0302, 0.4062, 0.1953],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 150, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0453e-02, 8.5487e-01, 2.0682e-02, 9.6280e-02, 2.5880e-06, 9.0754e-03,
        8.6348e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 150, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0062, 0.0048, 0.0053, 0.1978, 0.0055, 0.0048, 0.7755],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 150, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1402, 0.0104, 0.0104, 0.0160, 0.5774, 0.2400, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 150, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2515e-08, 4.6877e-03, 9.9531e-01, 1.8326e-08, 1.2770e-08, 1.6896e-10,
        8.6569e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 150, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1625, 0.0238, 0.1747, 0.0305, 0.4037, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 151, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0531e-02, 8.4527e-01, 1.9867e-02, 1.0616e-01, 2.8446e-06, 9.3676e-03,
        8.7958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 151, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0048, 0.0051, 0.2028, 0.0055, 0.0051, 0.7715],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 151, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1442, 0.0104, 0.0093, 0.0148, 0.5846, 0.2312, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 151, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0808e-08, 4.5070e-03, 9.9549e-01, 1.4919e-08, 1.0151e-08, 1.1639e-10,
        8.2121e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 151, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1610, 0.0235, 0.1741, 0.0297, 0.4077, 0.1944],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 152, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0679e-02, 8.4979e-01, 2.1137e-02, 9.9810e-02, 2.8227e-06, 9.3591e-03,
        9.2250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 152, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0061, 0.0049, 0.0053, 0.1958, 0.0056, 0.0051, 0.7771],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 152, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1345, 0.0103, 0.0106, 0.0157, 0.5844, 0.2393, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 152, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0473e-08, 4.5159e-03, 9.9548e-01, 1.4757e-08, 1.0925e-08, 1.4667e-10,
        8.9299e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 152, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1625, 0.0236, 0.1743, 0.0301, 0.4054, 0.1945],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 153, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0801e-02, 8.5024e-01, 1.9213e-02, 1.0140e-01, 2.3152e-06, 9.3511e-03,
        8.9941e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 153, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0047, 0.0050, 0.1975, 0.0053, 0.0051, 0.7770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 153, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1383, 0.0097, 0.0090, 0.0142, 0.6000, 0.2235, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 153, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1164e-08, 4.8488e-03, 9.9515e-01, 1.3065e-08, 1.2374e-08, 1.2744e-10,
        9.0547e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 153, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1608, 0.0236, 0.1756, 0.0297, 0.4060, 0.1944],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 154, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0199e-02, 8.5180e-01, 2.1221e-02, 9.7869e-02, 3.0814e-06, 9.5756e-03,
        9.3346e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 154, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0048, 0.0054, 0.1978, 0.0058, 0.0053, 0.7750],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 154, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1381, 0.0102, 0.0102, 0.0154, 0.5827, 0.2383, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 154, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0186e-09, 4.7706e-03, 9.9523e-01, 1.2944e-08, 8.6389e-09, 1.5563e-10,
        5.5550e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 154, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1623, 0.0231, 0.1728, 0.0297, 0.4081, 0.1945],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 155, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1198e-02, 8.4518e-01, 1.8978e-02, 1.0524e-01, 2.7594e-06, 9.9493e-03,
        9.4548e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 155, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0061, 0.0045, 0.0049, 0.2028, 0.0054, 0.0050, 0.7712],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 155, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1333, 0.0101, 0.0097, 0.0143, 0.5912, 0.2362, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 155, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4275e-08, 5.3193e-03, 9.9468e-01, 1.5435e-08, 1.9877e-08, 1.5112e-10,
        1.3452e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 155, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1601, 0.0240, 0.1761, 0.0297, 0.4071, 0.1933],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 156, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0016e-02, 8.5415e-01, 2.0402e-02, 9.6100e-02, 3.3883e-06, 9.8821e-03,
        9.4513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 156, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0050, 0.0057, 0.1952, 0.0051, 0.0057, 0.7782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 156, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1448, 0.0100, 0.0093, 0.0142, 0.5806, 0.2357, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 156, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1577e-08, 4.3794e-03, 9.9562e-01, 1.8974e-08, 1.0511e-08, 3.1566e-10,
        6.0359e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 156, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0093, 0.1639, 0.0233, 0.1724, 0.0296, 0.4075, 0.1941],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 157, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1418e-02, 8.4044e-01, 1.9860e-02, 1.0829e-01, 4.6143e-06, 9.9715e-03,
        1.0016e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 157, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0058, 0.0045, 0.0044, 0.1876, 0.0050, 0.0051, 0.7875],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 157, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1227, 0.0105, 0.0088, 0.0146, 0.5875, 0.2505, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 157, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3977e-08, 4.6500e-03, 9.9535e-01, 1.3714e-08, 1.6911e-08, 1.1543e-10,
        1.9433e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 157, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1617, 0.0247, 0.1744, 0.0310, 0.4027, 0.1952],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 158, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.3053e-03, 8.5951e-01, 1.9340e-02, 9.3845e-02, 4.5636e-06, 9.9600e-03,
        8.0354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 158, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0056, 0.0063, 0.2054, 0.0049, 0.0055, 0.7663],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 158, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1483, 0.0109, 0.0099, 0.0153, 0.5818, 0.2284, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 158, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7128e-08, 4.6295e-03, 9.9537e-01, 3.4199e-08, 2.3816e-08, 6.7231e-10,
        9.7464e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 158, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1670, 0.0237, 0.1733, 0.0301, 0.4018, 0.1946],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 159, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0873e-02, 8.4022e-01, 1.9086e-02, 1.1023e-01, 5.1098e-06, 9.6531e-03,
        9.9319e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 159, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0042, 0.0045, 0.2018, 0.0052, 0.0049, 0.7745],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 159, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1324, 0.0107, 0.0097, 0.0141, 0.5899, 0.2380, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 159, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0286e-08, 4.8150e-03, 9.9518e-01, 8.9793e-09, 8.6956e-09, 1.1017e-10,
        1.2601e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 159, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1615, 0.0251, 0.1778, 0.0312, 0.4004, 0.1936],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 160, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0725e-02, 8.4866e-01, 2.0220e-02, 1.0168e-01, 5.5204e-06, 1.0040e-02,
        8.6719e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 160, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0055, 0.0049, 0.0052, 0.2132, 0.0047, 0.0048, 0.7616],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 160, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1339, 0.0105, 0.0099, 0.0148, 0.6016, 0.2238, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 160, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5110e-08, 4.9659e-03, 9.9503e-01, 2.9852e-08, 4.7618e-08, 7.1952e-10,
        1.8567e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 160, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1667, 0.0242, 0.1742, 0.0304, 0.3989, 0.1955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 161, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0771e-02, 8.5456e-01, 1.8166e-02, 9.7075e-02, 4.3214e-06, 9.9045e-03,
        9.5204e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 161, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0049, 0.0053, 0.1890, 0.0042, 0.0047, 0.7871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 161, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1388, 0.0091, 0.0098, 0.0116, 0.5695, 0.2569, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 161, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0447e-08, 5.5125e-03, 9.9449e-01, 2.8517e-08, 1.5362e-08, 2.4990e-10,
        1.1081e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 161, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1635, 0.0248, 0.1793, 0.0312, 0.3976, 0.1937],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 162, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1768e-02, 8.3903e-01, 2.3258e-02, 1.0525e-01, 1.1363e-05, 1.0773e-02,
        9.9110e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.003

[Epoch: 162, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0060, 0.0043, 0.0049, 0.1973, 0.0050, 0.0054, 0.7772],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 162, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1394, 0.0111, 0.0103, 0.0154, 0.5999, 0.2182, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 162, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0584e-08, 4.6661e-03, 9.9533e-01, 1.2613e-08, 1.9850e-08, 3.3470e-10,
        1.5167e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 162, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1640, 0.0249, 0.1736, 0.0310, 0.3994, 0.1970],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 163, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0456e-02, 8.6092e-01, 1.8321e-02, 9.0101e-02, 4.4122e-06, 1.0444e-02,
        9.7567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 163, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0066, 0.0048, 0.0046, 0.1934, 0.0052, 0.0050, 0.7803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 163, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1478, 0.0097, 0.0094, 0.0139, 0.5672, 0.2478, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 163, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7622e-08, 6.3625e-03, 9.9364e-01, 3.3651e-08, 1.6374e-08, 4.2960e-10,
        1.9193e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 163, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1641, 0.0238, 0.1768, 0.0298, 0.4021, 0.1934],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 164, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0342e-02, 8.4938e-01, 1.9631e-02, 1.0101e-01, 4.6068e-06, 1.0478e-02,
        9.1594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 164, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0054, 0.0051, 0.0051, 0.1973, 0.0062, 0.0051, 0.7757],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 164, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1363, 0.0112, 0.0107, 0.0141, 0.6186, 0.2039, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 164, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4473e-08, 4.2193e-03, 9.9578e-01, 1.7040e-08, 2.1500e-08, 4.4246e-10,
        6.7020e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 164, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1624, 0.0244, 0.1770, 0.0305, 0.4009, 0.1948],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 165, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.3589e-03, 8.6066e-01, 1.7974e-02, 9.5248e-02, 4.9870e-06, 8.2309e-03,
        8.5187e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 165, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0061, 0.0050, 0.0049, 0.2070, 0.0045, 0.0057, 0.7668],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 165, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1417, 0.0110, 0.0097, 0.0167, 0.5682, 0.2474, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 165, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1327e-08, 5.3178e-03, 9.9468e-01, 3.6342e-08, 1.9476e-08, 3.6999e-10,
        1.8814e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 165, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1647, 0.0243, 0.1776, 0.0313, 0.3983, 0.1937],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 166, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.0701e-03, 8.6722e-01, 1.7237e-02, 8.9560e-02, 3.5899e-06, 8.7128e-03,
        8.2008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 166, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0047, 0.0045, 0.2094, 0.0051, 0.0047, 0.7669],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 166, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1409, 0.0110, 0.0101, 0.0178, 0.5626, 0.2516, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 166, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3170e-08, 4.5025e-03, 9.9550e-01, 1.0607e-08, 1.8055e-08, 2.8134e-10,
        1.1708e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 166, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1663, 0.0248, 0.1765, 0.0303, 0.3972, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 167, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.0582e-03, 8.5556e-01, 1.8010e-02, 9.9230e-02, 4.4808e-06, 9.5118e-03,
        8.6297e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.001

[Epoch: 167, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0038, 0.0053, 0.0047, 0.2119, 0.0049, 0.0044, 0.7651],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 167, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1366, 0.0108, 0.0111, 0.0143, 0.5942, 0.2262, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 167, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5883e-08, 4.6279e-03, 9.9537e-01, 1.6445e-08, 1.0688e-08, 3.6522e-10,
        8.9181e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 167, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1654, 0.0250, 0.1726, 0.0317, 0.4001, 0.1951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 168, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0999e-02, 8.2504e-01, 2.0903e-02, 1.2367e-01, 3.4140e-06, 9.4029e-03,
        9.9825e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 168, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0063, 0.0044, 0.0046, 0.2049, 0.0052, 0.0047, 0.7699],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 168, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1562, 0.0101, 0.0097, 0.0152, 0.5518, 0.2520, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 168, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1955e-08, 5.1871e-03, 9.9481e-01, 2.4354e-08, 4.9617e-08, 5.5193e-10,
        3.9053e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 168, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1635, 0.0236, 0.1717, 0.0292, 0.4044, 0.1978],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 169, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1111e-02, 8.4732e-01, 2.1216e-02, 9.8072e-02, 4.5435e-06, 1.1126e-02,
        1.1149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 169, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0056, 0.0049, 0.0067, 0.1906, 0.0057, 0.0054, 0.7811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 169, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1509, 0.0102, 0.0113, 0.0140, 0.5690, 0.2388, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 169, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7743e-08, 4.0389e-03, 9.9596e-01, 1.9842e-08, 3.1507e-08, 3.9818e-10,
        5.7121e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 169, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1640, 0.0234, 0.1711, 0.0286, 0.4114, 0.1917],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 170, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.4056e-02, 8.2186e-01, 2.3300e-02, 1.1789e-01, 7.0650e-06, 1.1178e-02,
        1.1703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 170, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0061, 0.0060, 0.0047, 0.1950, 0.0055, 0.0050, 0.7777],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 170, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1194, 0.0118, 0.0086, 0.0137, 0.6359, 0.2049, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 170, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9727e-08, 4.7925e-03, 9.9521e-01, 2.4615e-08, 3.8031e-08, 5.8884e-10,
        8.3474e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 170, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1625, 0.0234, 0.1732, 0.0295, 0.4045, 0.1973],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 171, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.8408e-03, 8.7058e-01, 1.7669e-02, 8.3826e-02, 4.7525e-06, 9.9750e-03,
        9.1064e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.016

[Epoch: 171, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0053, 0.0044, 0.0056, 0.2115, 0.0055, 0.0059, 0.7618],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 171, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1428, 0.0108, 0.0107, 0.0149, 0.5905, 0.2253, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 171, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9709e-08, 4.6219e-03, 9.9538e-01, 2.0796e-08, 2.3839e-08, 3.1060e-10,
        1.3953e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 171, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1632, 0.0238, 0.1717, 0.0299, 0.4078, 0.1938],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 172, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.8687e-03, 8.6641e-01, 1.5742e-02, 9.1563e-02, 4.4744e-06, 9.0412e-03,
        8.3682e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.009

[Epoch: 172, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0050, 0.0045, 0.2040, 0.0051, 0.0043, 0.7722],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 172, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1320, 0.0102, 0.0113, 0.0170, 0.5511, 0.2733, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 172, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7063e-08, 5.2875e-03, 9.9471e-01, 1.7234e-08, 2.7552e-08, 2.4771e-10,
        1.4942e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 172, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1626, 0.0250, 0.1769, 0.0307, 0.3995, 0.1953],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 173, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0181e-02, 8.3821e-01, 2.5098e-02, 1.0817e-01, 5.9482e-06, 8.8865e-03,
        9.4467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 173, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0044, 0.0045, 0.0048, 0.1839, 0.0045, 0.0040, 0.7939],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 173, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1433, 0.0107, 0.0099, 0.0162, 0.5875, 0.2270, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 173, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4451e-08, 3.8738e-03, 9.9613e-01, 1.8827e-08, 2.1361e-08, 1.9730e-10,
        1.6999e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.994

[Epoch: 173, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1621, 0.0250, 0.1735, 0.0312, 0.4042, 0.1942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 174, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0923e-02, 8.5482e-01, 1.8935e-02, 9.2422e-02, 4.9902e-06, 1.1263e-02,
        1.1628e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 174, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0058, 0.0047, 0.0041, 0.1867, 0.0047, 0.0043, 0.7895],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 174, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1349, 0.0082, 0.0069, 0.0123, 0.6129, 0.2207, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 174, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2589e-08, 4.1088e-03, 9.9589e-01, 1.7544e-08, 1.9154e-08, 1.8828e-10,
        7.7472e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 174, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1637, 0.0247, 0.1725, 0.0301, 0.4046, 0.1944],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 175, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1194e-02, 8.3471e-01, 1.9276e-02, 1.1430e-01, 6.4785e-06, 9.5581e-03,
        1.0953e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 175, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0064, 0.0055, 0.0065, 0.1812, 0.0063, 0.0055, 0.7885],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 175, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1332, 0.0099, 0.0091, 0.0135, 0.6124, 0.2173, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 175, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4413e-08, 4.5337e-03, 9.9547e-01, 1.2000e-08, 2.7560e-08, 3.1147e-10,
        5.0759e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 175, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1590, 0.0238, 0.1779, 0.0310, 0.4027, 0.1958],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 176, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0589e-02, 8.4455e-01, 1.9916e-02, 1.0618e-01, 6.3093e-06, 9.3490e-03,
        9.4112e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.020

[Epoch: 176, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0047, 0.0047, 0.2089, 0.0050, 0.0047, 0.7674],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 176, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1414, 0.0113, 0.0101, 0.0158, 0.5672, 0.2493, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 176, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5489e-08, 5.0832e-03, 9.9492e-01, 2.9225e-08, 4.3451e-08, 2.8278e-10,
        1.6589e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 176, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1655, 0.0249, 0.1756, 0.0302, 0.3998, 0.1940],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 177, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.8523e-03, 8.6717e-01, 1.6977e-02, 8.9951e-02, 4.3652e-06, 8.9160e-03,
        8.1275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.017

[Epoch: 177, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0049, 0.0047, 0.2129, 0.0044, 0.0049, 0.7637],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 177, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1553, 0.0109, 0.0104, 0.0165, 0.5479, 0.2531, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 177, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4537e-08, 6.3074e-03, 9.9369e-01, 1.2951e-08, 1.7840e-08, 3.6135e-10,
        9.5651e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 177, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1635, 0.0244, 0.1750, 0.0309, 0.4023, 0.1942],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 178, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.7071e-03, 8.8070e-01, 1.6212e-02, 7.4855e-02, 5.2232e-06, 1.0312e-02,
        9.2046e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 178, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0054, 0.0045, 0.0046, 0.2251, 0.0067, 0.0043, 0.7493],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 178, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1386, 0.0109, 0.0111, 0.0172, 0.5833, 0.2331, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 178, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8267e-08, 5.6454e-03, 9.9435e-01, 4.5860e-08, 5.5283e-08, 4.5359e-10,
        1.8124e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.998

[Epoch: 178, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0105, 0.1691, 0.0256, 0.1794, 0.0314, 0.3885, 0.1955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 179, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0334e-02, 8.3918e-01, 2.1346e-02, 1.0817e-01, 3.1410e-06, 1.0551e-02,
        1.0422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.014

[Epoch: 179, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0039, 0.0045, 0.0044, 0.1858, 0.0039, 0.0050, 0.7924],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 179, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1324, 0.0099, 0.0096, 0.0145, 0.6050, 0.2234, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 179, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9197e-08, 4.7255e-03, 9.9527e-01, 1.8543e-08, 2.4228e-08, 5.2644e-10,
        1.7671e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 179, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1643, 0.0257, 0.1750, 0.0307, 0.3981, 0.1962],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 180, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0910e-02, 8.4414e-01, 2.0791e-02, 1.0547e-01, 5.6633e-06, 1.0608e-02,
        8.0704e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 180, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0045, 0.0049, 0.1953, 0.0051, 0.0053, 0.7797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 180, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1431, 0.0106, 0.0095, 0.0138, 0.5862, 0.2317, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 180, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1578e-08, 4.5440e-03, 9.9546e-01, 1.5894e-08, 2.4510e-08, 2.8336e-10,
        1.5597e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 180, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1651, 0.0249, 0.1733, 0.0307, 0.3990, 0.1970],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 181, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([8.8571e-03, 8.3957e-01, 1.9748e-02, 1.1262e-01, 3.8949e-06, 8.9688e-03,
        1.0232e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.011

[Epoch: 181, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0061, 0.0048, 0.0048, 0.1930, 0.0061, 0.0053, 0.7799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 181, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1446, 0.0101, 0.0084, 0.0126, 0.5993, 0.2201, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 181, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2033e-08, 4.4569e-03, 9.9554e-01, 2.3358e-08, 2.2575e-08, 4.5388e-10,
        8.1600e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 181, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0101, 0.1657, 0.0245, 0.1743, 0.0304, 0.3974, 0.1975],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 182, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0870e-02, 8.5832e-01, 1.9686e-02, 9.3171e-02, 3.7117e-06, 9.5603e-03,
        8.3849e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.008

[Epoch: 182, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0044, 0.0058, 0.0055, 0.2289, 0.0049, 0.0060, 0.7445],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 182, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1378, 0.0109, 0.0133, 0.0170, 0.5875, 0.2272, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 182, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4805e-08, 4.9990e-03, 9.9500e-01, 3.1402e-08, 3.1921e-08, 2.7397e-10,
        1.2747e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 182, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1614, 0.0240, 0.1766, 0.0303, 0.4020, 0.1958],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 183, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0364e-02, 8.4449e-01, 2.1785e-02, 1.0198e-01, 4.3835e-06, 1.0192e-02,
        1.1181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 183, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0042, 0.0044, 0.0053, 0.2006, 0.0050, 0.0042, 0.7764],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 183, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1563, 0.0117, 0.0114, 0.0160, 0.5332, 0.2652, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 183, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6522e-08, 5.6722e-03, 9.9433e-01, 2.6125e-08, 2.4714e-08, 3.3028e-10,
        1.8972e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 183, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1642, 0.0242, 0.1748, 0.0298, 0.4022, 0.1949],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 184, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.2832e-03, 8.5846e-01, 1.8907e-02, 9.5641e-02, 3.4914e-06, 9.4351e-03,
        8.2686e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.009

[Epoch: 184, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0053, 0.0048, 0.1941, 0.0045, 0.0053, 0.7800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 184, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1473, 0.0097, 0.0079, 0.0153, 0.5756, 0.2391, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 184, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3046e-08, 5.0597e-03, 9.9494e-01, 2.5115e-08, 1.8687e-08, 3.3004e-10,
        1.4939e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 184, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0094, 0.1631, 0.0240, 0.1748, 0.0296, 0.4025, 0.1966],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 185, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.2857e-02, 8.3533e-01, 2.0704e-02, 1.0778e-01, 4.0296e-06, 1.0533e-02,
        1.2793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 185, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0061, 0.0060, 0.2109, 0.0062, 0.0052, 0.7605],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 185, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1183, 0.0102, 0.0112, 0.0146, 0.6205, 0.2196, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 185, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8057e-08, 4.6800e-03, 9.9532e-01, 3.9986e-08, 4.5269e-08, 4.4756e-10,
        6.4367e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 185, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0094, 0.1643, 0.0235, 0.1731, 0.0296, 0.4076, 0.1924],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 186, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.1944e-03, 8.4912e-01, 1.7729e-02, 1.0709e-01, 4.9811e-06, 8.8530e-03,
        8.0031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 186, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0052, 0.0056, 0.0052, 0.1856, 0.0052, 0.0050, 0.7881],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 186, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1288, 0.0097, 0.0093, 0.0153, 0.5935, 0.2389, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 186, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1668e-08, 4.8606e-03, 9.9514e-01, 1.8904e-08, 1.3576e-08, 1.7273e-10,
        2.3244e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 186, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0098, 0.1618, 0.0249, 0.1756, 0.0298, 0.4027, 0.1953],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 187, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1208e-02, 8.5129e-01, 2.1589e-02, 9.7320e-02, 4.6807e-06, 8.9242e-03,
        9.6632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 187, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0059, 0.0046, 0.0052, 0.1952, 0.0056, 0.0043, 0.7792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 187, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1287, 0.0097, 0.0093, 0.0129, 0.5936, 0.2403, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 187, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4403e-08, 4.3408e-03, 9.9566e-01, 1.3343e-08, 1.2240e-08, 1.9447e-10,
        6.0857e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 187, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0100, 0.1649, 0.0243, 0.1730, 0.0301, 0.4041, 0.1935],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 188, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.7578e-03, 8.5576e-01, 2.0032e-02, 9.5560e-02, 3.0878e-06, 1.0075e-02,
        8.8085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.013

[Epoch: 188, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0051, 0.0049, 0.0055, 0.2047, 0.0051, 0.0050, 0.7697],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 188, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1516, 0.0097, 0.0113, 0.0145, 0.5844, 0.2229, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 188, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0044e-09, 3.9819e-03, 9.9602e-01, 1.0868e-08, 1.9751e-08, 9.6982e-11,
        1.4870e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 188, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0103, 0.1631, 0.0252, 0.1760, 0.0307, 0.3989, 0.1958],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 189, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0885e-02, 8.3952e-01, 2.2129e-02, 1.0425e-01, 6.0457e-06, 1.0887e-02,
        1.2327e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.010

[Epoch: 189, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0044, 0.0058, 0.0041, 0.1897, 0.0046, 0.0042, 0.7873],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 189, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1423, 0.0099, 0.0088, 0.0128, 0.5752, 0.2470, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 189, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4093e-08, 6.2255e-03, 9.9377e-01, 1.1977e-08, 1.3268e-08, 1.9146e-10,
        1.9176e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.993

[Epoch: 189, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0102, 0.1620, 0.0249, 0.1758, 0.0307, 0.3997, 0.1967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 190, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.1580e-02, 8.1427e-01, 2.3454e-02, 1.2716e-01, 5.7813e-06, 1.1631e-02,
        1.1898e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.006

[Epoch: 190, batch: 86/219] total loss per batch: 0.573
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0066, 0.0047, 0.0046, 0.2247, 0.0025, 0.0038, 0.7530],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.007

[Epoch: 190, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1158, 0.0091, 0.0095, 0.0192, 0.6433, 0.1951, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 190, batch: 172/219] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2312e-09, 5.3328e-03, 9.9467e-01, 2.8849e-09, 7.1147e-09, 1.8490e-10,
        3.4035e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.997

[Epoch: 190, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1666, 0.0252, 0.1721, 0.0305, 0.4002, 0.1955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 191, batch: 43/219] total loss per batch: 0.566
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([6.2865e-03, 8.6693e-01, 1.9907e-02, 9.0602e-02, 9.5145e-07, 7.2237e-03,
        9.0532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 0.006

[Epoch: 191, batch: 86/219] total loss per batch: 0.574
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0051, 0.0063, 0.1874, 0.0088, 0.0068, 0.7809],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.006

[Epoch: 191, batch: 129/219] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1074, 0.0119, 0.0072, 0.0131, 0.6484, 0.2069, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 191, batch: 172/219] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2487e-09, 3.7383e-03, 9.9626e-01, 2.0602e-09, 1.0745e-09, 2.5359e-11,
        7.0756e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 191, batch: 215/219] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0097, 0.1644, 0.0242, 0.1776, 0.0302, 0.3964, 0.1976],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 192, batch: 43/219] total loss per batch: 0.561
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.5915e-03, 8.6173e-01, 2.4205e-02, 8.6923e-02, 7.8374e-07, 9.2338e-03,
        1.0317e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.007

[Epoch: 192, batch: 86/219] total loss per batch: 0.572
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0043, 0.0049, 0.0046, 0.1986, 0.0054, 0.0077, 0.7745],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 192, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1609, 0.0084, 0.0094, 0.0140, 0.5753, 0.2260, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 192, batch: 172/219] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9883e-09, 6.9195e-03, 9.9308e-01, 2.5285e-09, 4.7542e-09, 8.4343e-11,
        1.5083e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 192, batch: 215/219] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1643, 0.0241, 0.1767, 0.0309, 0.3964, 0.1980],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 193, batch: 43/219] total loss per batch: 0.560
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([7.9732e-03, 8.6351e-01, 2.0443e-02, 8.8693e-02, 6.6942e-07, 1.0484e-02,
        8.9000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.015

[Epoch: 193, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0037, 0.0056, 0.0051, 0.2042, 0.0051, 0.0060, 0.7702],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.003

[Epoch: 193, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1505, 0.0078, 0.0089, 0.0169, 0.5675, 0.2424, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 193, batch: 172/219] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8122e-09, 5.0806e-03, 9.9492e-01, 4.4438e-09, 4.7995e-09, 1.7096e-10,
        1.0104e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.996

[Epoch: 193, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0099, 0.1654, 0.0241, 0.1736, 0.0308, 0.3999, 0.1963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 194, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.1445e-03, 8.6082e-01, 1.9336e-02, 9.0829e-02, 6.3422e-07, 1.0716e-02,
        9.1514e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 194, batch: 86/219] total loss per batch: 0.571
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0040, 0.0056, 0.0054, 0.1983, 0.0051, 0.0065, 0.7749],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.003

[Epoch: 194, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1528, 0.0084, 0.0096, 0.0155, 0.5682, 0.2396, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 194, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5029e-09, 5.7363e-03, 9.9426e-01, 4.8464e-09, 5.3510e-09, 1.6659e-10,
        1.3115e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 194, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1654, 0.0241, 0.1741, 0.0308, 0.3996, 0.1963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 195, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.2768e-03, 8.5710e-01, 1.9753e-02, 9.3972e-02, 6.4629e-07, 1.0595e-02,
        9.3009e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 195, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0042, 0.0053, 0.0053, 0.2004, 0.0051, 0.0061, 0.7737],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.001

[Epoch: 195, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1474, 0.0088, 0.0096, 0.0152, 0.5781, 0.2348, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 195, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0485e-09, 5.4793e-03, 9.9452e-01, 5.1040e-09, 5.1293e-09, 1.6095e-10,
        1.2432e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 195, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0096, 0.1651, 0.0241, 0.1739, 0.0308, 0.4002, 0.1963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 196, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.4533e-03, 8.5529e-01, 1.9991e-02, 9.5481e-02, 6.4633e-07, 1.0449e-02,
        9.3357e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 196, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0044, 0.0052, 0.0052, 0.2019, 0.0050, 0.0058, 0.7725],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 196, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1439, 0.0090, 0.0097, 0.0149, 0.5824, 0.2344, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 196, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7645e-09, 5.3234e-03, 9.9468e-01, 4.8227e-09, 4.9160e-09, 1.5194e-10,
        1.1643e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 196, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1648, 0.0241, 0.1740, 0.0307, 0.4006, 0.1962],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 197, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.6127e-03, 8.5429e-01, 1.9931e-02, 9.6549e-02, 6.3880e-07, 1.0278e-02,
        9.3390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 197, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0046, 0.0052, 0.0052, 0.2015, 0.0050, 0.0057, 0.7729],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 197, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1419, 0.0091, 0.0097, 0.0146, 0.5839, 0.2349, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 197, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4816e-09, 5.1540e-03, 9.9485e-01, 4.6047e-09, 4.6025e-09, 1.4632e-10,
        1.0530e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 197, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1647, 0.0241, 0.1738, 0.0307, 0.4007, 0.1965],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 198, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.8172e-03, 8.5282e-01, 2.0116e-02, 9.7590e-02, 6.3376e-07, 1.0237e-02,
        9.4235e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 198, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0048, 0.0051, 0.0051, 0.2020, 0.0050, 0.0055, 0.7724],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 198, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1409, 0.0092, 0.0097, 0.0145, 0.5868, 0.2333, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 198, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1371e-09, 5.0979e-03, 9.9490e-01, 4.1301e-09, 4.3828e-09, 1.3449e-10,
        9.7438e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 198, batch: 215/219] total loss per batch: 0.565
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1645, 0.0241, 0.1739, 0.0306, 0.4011, 0.1963],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 199, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([9.9098e-03, 8.5270e-01, 1.9956e-02, 9.7903e-02, 6.1064e-07, 1.0072e-02,
        9.4592e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 199, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0049, 0.0051, 0.0051, 0.2007, 0.0050, 0.0054, 0.7738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 199, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1397, 0.0093, 0.0097, 0.0145, 0.5854, 0.2359, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 199, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8758e-09, 4.9599e-03, 9.9504e-01, 3.8887e-09, 4.0450e-09, 1.2855e-10,
        8.6909e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 199, batch: 215/219] total loss per batch: 0.564
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1645, 0.0241, 0.1736, 0.0306, 0.4011, 0.1967],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 200, batch: 43/219] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.8500, 0.0200, 0.1000, 0.0000, 0.0100, 0.0100])
Policy pred: tensor([1.0101e-02, 8.5182e-01, 2.0112e-02, 9.8354e-02, 5.8384e-07, 1.0073e-02,
        9.5395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.013 -0.012

[Epoch: 200, batch: 86/219] total loss per batch: 0.570
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.2000, 0.0050, 0.0050, 0.7750])
Policy pred: tensor([0.0050, 0.0051, 0.0051, 0.2011, 0.0050, 0.0053, 0.7734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 200, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.1400, 0.0100, 0.0100, 0.0150, 0.5850, 0.2350, 0.0050])
Policy pred: tensor([0.1401, 0.0093, 0.0096, 0.0144, 0.5887, 0.2325, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 200, batch: 172/219] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0000, 0.0050, 0.9950, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5081e-09, 4.9814e-03, 9.9502e-01, 3.4116e-09, 3.7989e-09, 1.1631e-10,
        7.8512e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -0.996 -0.995

[Epoch: 200, batch: 215/219] total loss per batch: 0.564
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.1650, 0.0250, 0.1750, 0.0300, 0.4000, 0.1950])
Policy pred: tensor([0.0095, 0.1643, 0.0241, 0.1734, 0.0305, 0.4013, 0.1969],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

