Training set samples: 6517
Batch size: 32
[Epoch: 1, batch: 40/204] total loss per batch: 1.875
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.1367, 0.1783, 0.1053, 0.1504, 0.2268, 0.0882, 0.1144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.011

[Epoch: 1, batch: 80/204] total loss per batch: 1.770
Policy (actual, predicted): 1 2
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.0339e-01, 1.5274e-01, 2.8123e-01, 2.4524e-01, 1.2674e-04, 1.2329e-01,
        9.3990e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.052

[Epoch: 1, batch: 120/204] total loss per batch: 1.686
Policy (actual, predicted): 3 4
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0158, 0.1049, 0.2494, 0.1991, 0.3934, 0.0250, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.056

[Epoch: 1, batch: 160/204] total loss per batch: 1.681
Policy (actual, predicted): 0 6
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([0.2214, 0.0008, 0.1360, 0.0977, 0.0886, 0.1703, 0.2851],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 1, batch: 200/204] total loss per batch: 1.643
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0567, 0.1728, 0.2355, 0.1716, 0.2722, 0.0484, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 2, batch: 40/204] total loss per batch: 1.479
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.1784, 0.1117, 0.2947, 0.1588, 0.1536, 0.0477, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 2, batch: 80/204] total loss per batch: 1.492
Policy (actual, predicted): 1 3
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1652e-01, 2.5488e-01, 1.4746e-01, 3.0647e-01, 3.0284e-05, 9.3758e-02,
        8.0868e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.047

[Epoch: 2, batch: 120/204] total loss per batch: 1.412
Policy (actual, predicted): 3 2
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0231, 0.0721, 0.3170, 0.2242, 0.2209, 0.1182, 0.0244],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.048

[Epoch: 2, batch: 160/204] total loss per batch: 1.406
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([0.3347, 0.0004, 0.1045, 0.0509, 0.2018, 0.1025, 0.2053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.001

[Epoch: 2, batch: 200/204] total loss per batch: 1.400
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0409, 0.1944, 0.1622, 0.2465, 0.2974, 0.0295, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.011

[Epoch: 3, batch: 40/204] total loss per batch: 1.238
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0737, 0.0123, 0.5531, 0.1790, 0.1235, 0.0338, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 3, batch: 80/204] total loss per batch: 1.224
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([6.8645e-02, 5.4565e-01, 8.5489e-02, 1.9424e-01, 3.5727e-06, 8.4968e-02,
        2.0997e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.029

[Epoch: 3, batch: 120/204] total loss per batch: 1.149
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0166, 0.0316, 0.2407, 0.3792, 0.1711, 0.1496, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.044

[Epoch: 3, batch: 160/204] total loss per batch: 1.137
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5212e-01, 3.7599e-05, 4.4060e-02, 1.1544e-02, 1.1541e-01, 4.2549e-02,
        1.3428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.005

[Epoch: 3, batch: 200/204] total loss per batch: 1.141
Policy (actual, predicted): 3 4
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0262, 0.1655, 0.1412, 0.3106, 0.3159, 0.0208, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 4, batch: 40/204] total loss per batch: 1.065
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0230, 0.0023, 0.7747, 0.0404, 0.1430, 0.0092, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 4, batch: 80/204] total loss per batch: 1.063
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([4.3741e-02, 6.5080e-01, 3.1521e-02, 2.2008e-01, 1.1494e-06, 2.9050e-02,
        2.4801e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.014

[Epoch: 4, batch: 120/204] total loss per batch: 1.019
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0195, 0.0170, 0.1600, 0.4983, 0.0536, 0.2362, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.023

[Epoch: 4, batch: 160/204] total loss per batch: 1.041
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.7190e-01, 1.3238e-04, 5.3712e-02, 1.3825e-02, 1.5024e-01, 1.5929e-01,
        5.0906e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 4, batch: 200/204] total loss per batch: 1.025
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0276, 0.0546, 0.8205, 0.0681, 0.0131, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 5, batch: 40/204] total loss per batch: 1.000
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0297, 0.0034, 0.2551, 0.0468, 0.6405, 0.0045, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 5, batch: 80/204] total loss per batch: 1.021
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7363e-02, 6.6785e-01, 2.4535e-02, 2.3491e-01, 2.3009e-06, 3.6545e-02,
        1.8790e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.006

[Epoch: 5, batch: 120/204] total loss per batch: 0.987
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0188, 0.0063, 0.3361, 0.4155, 0.0358, 0.1712, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.039

[Epoch: 5, batch: 160/204] total loss per batch: 1.008
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.0624e-01, 6.4225e-05, 9.0802e-03, 2.7118e-02, 1.4068e-01, 1.0406e-01,
        1.1275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 5, batch: 200/204] total loss per batch: 1.001
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0168, 0.2450, 0.0410, 0.6430, 0.0304, 0.0101, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.009

[Epoch: 6, batch: 40/204] total loss per batch: 0.978
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0178, 0.0022, 0.5902, 0.0291, 0.3425, 0.0114, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.002

[Epoch: 6, batch: 80/204] total loss per batch: 1.007
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1189e-02, 7.9087e-01, 9.6030e-03, 1.6293e-01, 3.1881e-07, 1.0508e-02,
        4.8963e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.009

[Epoch: 6, batch: 120/204] total loss per batch: 0.956
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0206, 0.0223, 0.1342, 0.5802, 0.0384, 0.1801, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.040

[Epoch: 6, batch: 160/204] total loss per batch: 0.983
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([3.3230e-01, 5.1424e-05, 8.4371e-03, 1.8452e-02, 2.3568e-01, 2.4371e-01,
        1.6138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 6, batch: 200/204] total loss per batch: 0.969
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0821, 0.0409, 0.8340, 0.0165, 0.0064, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 7, batch: 40/204] total loss per batch: 0.951
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0345, 0.0059, 0.5645, 0.0383, 0.3139, 0.0084, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 7, batch: 80/204] total loss per batch: 0.980
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9186e-02, 7.6162e-01, 3.7796e-02, 1.0954e-01, 3.1459e-07, 1.2847e-02,
        5.9004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.011

[Epoch: 7, batch: 120/204] total loss per batch: 0.930
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0090, 0.0050, 0.3002, 0.5836, 0.0189, 0.0684, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.035

[Epoch: 7, batch: 160/204] total loss per batch: 0.965
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.2917e-01, 3.3939e-05, 5.8581e-03, 5.4550e-03, 1.6650e-02, 1.3051e-01,
        1.2322e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 7, batch: 200/204] total loss per batch: 0.946
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0472, 0.2433, 0.0171, 0.5378, 0.0711, 0.0658, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 8, batch: 40/204] total loss per batch: 0.935
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0294, 0.0059, 0.2089, 0.0241, 0.6951, 0.0159, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.007

[Epoch: 8, batch: 80/204] total loss per batch: 0.959
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1543e-02, 7.1052e-01, 1.0835e-02, 2.1955e-01, 2.9893e-07, 1.9189e-02,
        1.8368e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.004

[Epoch: 8, batch: 120/204] total loss per batch: 0.914
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0123, 0.0141, 0.1358, 0.5747, 0.0210, 0.2174, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.031

[Epoch: 8, batch: 160/204] total loss per batch: 0.949
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.3671e-01, 9.4645e-05, 9.8394e-03, 2.7330e-02, 1.9942e-01, 2.8482e-01,
        4.1782e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 8, batch: 200/204] total loss per batch: 0.929
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.1409, 0.0255, 0.8062, 0.0131, 0.0031, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 9, batch: 40/204] total loss per batch: 0.916
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0171, 0.0036, 0.4912, 0.0365, 0.4327, 0.0057, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.004

[Epoch: 9, batch: 80/204] total loss per batch: 0.947
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8778e-02, 4.7652e-01, 1.6093e-02, 4.4602e-01, 3.9803e-07, 1.1944e-02,
        3.0645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.000

[Epoch: 9, batch: 120/204] total loss per batch: 0.897
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0278, 0.0091, 0.2802, 0.3404, 0.0442, 0.2836, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.000

[Epoch: 9, batch: 160/204] total loss per batch: 0.938
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.1799e-01, 4.1652e-05, 1.1457e-02, 3.4931e-03, 5.5880e-02, 5.6792e-02,
        5.4344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.000

[Epoch: 9, batch: 200/204] total loss per batch: 0.920
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0232, 0.1602, 0.0200, 0.7559, 0.0186, 0.0091, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 10, batch: 40/204] total loss per batch: 0.908
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0517, 0.0085, 0.2164, 0.0271, 0.6402, 0.0236, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.004

[Epoch: 10, batch: 80/204] total loss per batch: 0.942
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.4086e-02, 5.7202e-01, 1.1122e-02, 3.3159e-01, 3.2337e-07, 2.8558e-02,
        3.2617e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 10, batch: 120/204] total loss per batch: 0.897
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0141, 0.0137, 0.1970, 0.4612, 0.0251, 0.2687, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 10, batch: 160/204] total loss per batch: 0.928
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.2502e-01, 9.6269e-05, 1.5181e-02, 1.5638e-02, 6.5274e-02, 3.5514e-01,
        2.3651e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 10, batch: 200/204] total loss per batch: 0.918
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0977, 0.0387, 0.7152, 0.1115, 0.0060, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 11, batch: 40/204] total loss per batch: 0.903
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0147, 0.0048, 0.6508, 0.0221, 0.2950, 0.0052, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 11, batch: 80/204] total loss per batch: 0.936
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6669e-02, 5.9777e-01, 1.2648e-02, 3.2544e-01, 3.5963e-07, 1.9394e-02,
        2.8078e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 11, batch: 120/204] total loss per batch: 0.893
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0125, 0.0066, 0.2105, 0.5657, 0.0140, 0.1784, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 11, batch: 160/204] total loss per batch: 0.925
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.0033e-01, 5.0744e-05, 1.0462e-02, 6.7320e-03, 4.3376e-02, 8.7064e-02,
        5.1986e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 11, batch: 200/204] total loss per batch: 0.912
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.2154, 0.0171, 0.7461, 0.0060, 0.0027, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 12, batch: 40/204] total loss per batch: 0.905
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0339, 0.0069, 0.1836, 0.0171, 0.7269, 0.0101, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.002

[Epoch: 12, batch: 80/204] total loss per batch: 0.931
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2035e-02, 6.6363e-01, 4.0057e-03, 2.8153e-01, 1.4924e-07, 1.3771e-02,
        2.5024e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 12, batch: 120/204] total loss per batch: 0.890
Policy (actual, predicted): 3 2
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0075, 0.3370, 0.3320, 0.0844, 0.2085, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 12, batch: 160/204] total loss per batch: 0.922
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.9723e-01, 9.9536e-05, 1.0219e-02, 9.6434e-03, 4.9881e-02, 4.2310e-01,
        9.8263e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 12, batch: 200/204] total loss per batch: 0.902
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2452, 0.0122, 0.7162, 0.0081, 0.0039, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 13, batch: 40/204] total loss per batch: 0.898
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0355, 0.0066, 0.5621, 0.0211, 0.3535, 0.0095, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 13, batch: 80/204] total loss per batch: 0.927
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.3353e-02, 4.7041e-01, 1.0116e-02, 4.5307e-01, 7.3678e-07, 1.8093e-02,
        2.4963e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.001

[Epoch: 13, batch: 120/204] total loss per batch: 0.890
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0144, 0.0053, 0.2596, 0.5810, 0.0137, 0.1175, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 13, batch: 160/204] total loss per batch: 0.918
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.4475e-01, 7.0182e-05, 5.6641e-03, 6.5209e-03, 1.8955e-02, 1.0230e-01,
        2.1748e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 13, batch: 200/204] total loss per batch: 0.899
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2265, 0.0185, 0.7047, 0.0302, 0.0053, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 14, batch: 40/204] total loss per batch: 0.897
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0156, 0.0063, 0.2345, 0.0144, 0.7097, 0.0107, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.004

[Epoch: 14, batch: 80/204] total loss per batch: 0.922
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1467e-02, 6.7645e-01, 5.0157e-03, 2.6880e-01, 2.2942e-07, 1.0917e-02,
        1.7356e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 14, batch: 120/204] total loss per batch: 0.881
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0099, 0.0081, 0.1533, 0.4010, 0.0457, 0.3524, 0.0297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 14, batch: 160/204] total loss per batch: 0.914
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.0696e-01, 1.3630e-04, 9.4718e-03, 2.4770e-02, 4.6263e-02, 4.5103e-01,
        6.1376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.008

[Epoch: 14, batch: 200/204] total loss per batch: 0.899
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0114, 0.1791, 0.0097, 0.7800, 0.0122, 0.0025, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 15, batch: 40/204] total loss per batch: 0.901
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0449, 0.0071, 0.5943, 0.0203, 0.3117, 0.0108, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 15, batch: 80/204] total loss per batch: 0.922
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7101e-02, 7.4177e-01, 4.1931e-03, 2.0632e-01, 5.2154e-07, 1.1468e-02,
        1.9152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.000

[Epoch: 15, batch: 120/204] total loss per batch: 0.877
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0092, 0.0041, 0.2202, 0.6606, 0.0223, 0.0764, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 15, batch: 160/204] total loss per batch: 0.915
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([9.0816e-01, 7.2692e-05, 4.4073e-03, 5.1361e-03, 3.2978e-02, 4.0352e-02,
        8.8932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.000

[Epoch: 15, batch: 200/204] total loss per batch: 0.897
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.2816, 0.0162, 0.6646, 0.0127, 0.0063, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 16, batch: 40/204] total loss per batch: 0.896
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0145, 0.0038, 0.1783, 0.0122, 0.7787, 0.0054, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.000

[Epoch: 16, batch: 80/204] total loss per batch: 0.924
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4067e-02, 5.4559e-01, 3.9576e-03, 3.9710e-01, 2.8934e-07, 1.2656e-02,
        2.6624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.003

[Epoch: 16, batch: 120/204] total loss per batch: 0.877
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0128, 0.0080, 0.3110, 0.3745, 0.0275, 0.2288, 0.0374],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 16, batch: 160/204] total loss per batch: 0.914
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.6659e-01, 6.1849e-05, 1.0837e-02, 1.7604e-02, 1.9737e-02, 3.6285e-01,
        2.2311e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 16, batch: 200/204] total loss per batch: 0.898
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0152, 0.1350, 0.0095, 0.7895, 0.0432, 0.0023, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 17, batch: 40/204] total loss per batch: 0.895
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0119, 0.0034, 0.5976, 0.0126, 0.3514, 0.0106, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.004

[Epoch: 17, batch: 80/204] total loss per batch: 0.927
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5881e-02, 6.0426e-01, 7.9107e-03, 3.0813e-01, 1.0761e-06, 1.9367e-02,
        4.4448e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.003

[Epoch: 17, batch: 120/204] total loss per batch: 0.876
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0058, 0.0964, 0.6856, 0.0373, 0.1541, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 17, batch: 160/204] total loss per batch: 0.908
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5103e-01, 8.5331e-05, 5.8642e-03, 1.2991e-02, 6.6387e-02, 1.4914e-01,
        1.4498e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.000

[Epoch: 17, batch: 200/204] total loss per batch: 0.897
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.1163, 0.0149, 0.8288, 0.0137, 0.0040, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 18, batch: 40/204] total loss per batch: 0.890
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0314, 0.0045, 0.3575, 0.0297, 0.5523, 0.0133, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.015

[Epoch: 18, batch: 80/204] total loss per batch: 0.924
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.0029e-02, 6.4108e-01, 1.0250e-02, 2.8993e-01, 1.6300e-06, 1.8216e-02,
        2.0486e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.004

[Epoch: 18, batch: 120/204] total loss per batch: 0.873
Policy (actual, predicted): 3 2
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0038, 0.3980, 0.3314, 0.0276, 0.2071, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 18, batch: 160/204] total loss per batch: 0.906
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6307e-01, 1.2629e-04, 6.4486e-03, 1.2905e-02, 2.1012e-02, 2.6854e-01,
        2.7895e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 18, batch: 200/204] total loss per batch: 0.894
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.3198, 0.0119, 0.6343, 0.0174, 0.0042, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 19, batch: 40/204] total loss per batch: 0.885
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0113, 0.0053, 0.3720, 0.0106, 0.5695, 0.0122, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.002

[Epoch: 19, batch: 80/204] total loss per batch: 0.921
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4688e-02, 5.4691e-01, 6.3352e-03, 3.8729e-01, 1.1260e-06, 8.9368e-03,
        3.5835e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 19, batch: 120/204] total loss per batch: 0.872
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0143, 0.0043, 0.1970, 0.4945, 0.0302, 0.2412, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 19, batch: 160/204] total loss per batch: 0.910
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5773e-01, 1.3802e-04, 9.0247e-03, 1.9691e-02, 4.1633e-02, 2.5335e-01,
        1.8435e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 19, batch: 200/204] total loss per batch: 0.891
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0125, 0.1621, 0.0151, 0.7765, 0.0182, 0.0055, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 20, batch: 40/204] total loss per batch: 0.884
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0263, 0.0053, 0.3445, 0.0189, 0.5711, 0.0148, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 20, batch: 80/204] total loss per batch: 0.918
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([9.9130e-03, 4.8033e-01, 5.7041e-03, 4.7313e-01, 7.1281e-07, 1.3284e-02,
        1.7638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.004

[Epoch: 20, batch: 120/204] total loss per batch: 0.873
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0062, 0.0027, 0.2142, 0.5152, 0.0151, 0.2249, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.001

[Epoch: 20, batch: 160/204] total loss per batch: 0.909
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2764e-01, 8.1423e-05, 4.8263e-03, 1.1128e-02, 1.5083e-02, 2.2549e-01,
        1.5750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 20, batch: 200/204] total loss per batch: 0.891
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.2158, 0.0143, 0.7275, 0.0219, 0.0039, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 21, batch: 40/204] total loss per batch: 0.885
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0083, 0.0024, 0.4080, 0.0071, 0.5562, 0.0124, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 21, batch: 80/204] total loss per batch: 0.916
Policy (actual, predicted): 1 3
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9627e-02, 4.5984e-01, 6.9244e-03, 4.7374e-01, 9.9070e-07, 1.6478e-02,
        2.3393e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 21, batch: 120/204] total loss per batch: 0.866
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0142, 0.0048, 0.2551, 0.3936, 0.0639, 0.2278, 0.0407],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 21, batch: 160/204] total loss per batch: 0.907
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.7888e-01, 8.9077e-05, 7.2973e-03, 1.6804e-02, 2.6961e-02, 3.5103e-01,
        1.8937e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.002

[Epoch: 21, batch: 200/204] total loss per batch: 0.890
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.2698, 0.0093, 0.6908, 0.0078, 0.0039, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 22, batch: 40/204] total loss per batch: 0.884
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0217, 0.0050, 0.3585, 0.0077, 0.5841, 0.0121, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 22, batch: 80/204] total loss per batch: 0.915
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4711e-02, 7.9401e-01, 7.7288e-03, 1.3630e-01, 8.3845e-07, 1.1862e-02,
        3.5384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 22, batch: 120/204] total loss per batch: 0.865
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0062, 0.0019, 0.2437, 0.5176, 0.0241, 0.1863, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.003

[Epoch: 22, batch: 160/204] total loss per batch: 0.909
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.9553e-01, 2.0827e-04, 7.7401e-03, 5.5783e-03, 1.3467e-02, 6.6185e-02,
        1.1291e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.000

[Epoch: 22, batch: 200/204] total loss per batch: 0.886
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.1081, 0.0083, 0.8479, 0.0224, 0.0038, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 23, batch: 40/204] total loss per batch: 0.885
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0143, 0.0049, 0.4698, 0.0185, 0.4667, 0.0094, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 23, batch: 80/204] total loss per batch: 0.916
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6923e-02, 6.2143e-01, 1.6470e-02, 3.1120e-01, 1.3492e-06, 2.0917e-02,
        1.3057e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 23, batch: 120/204] total loss per batch: 0.866
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0108, 0.0066, 0.2050, 0.5099, 0.0201, 0.2336, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 23, batch: 160/204] total loss per batch: 0.903
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.1994e-01, 2.0840e-04, 1.0596e-02, 4.6961e-02, 2.7723e-02, 4.5845e-01,
        3.6122e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.002

[Epoch: 23, batch: 200/204] total loss per batch: 0.891
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0151, 0.3380, 0.0081, 0.6051, 0.0191, 0.0063, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.002

[Epoch: 24, batch: 40/204] total loss per batch: 0.887
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0123, 0.0074, 0.3217, 0.0123, 0.6316, 0.0057, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.005

[Epoch: 24, batch: 80/204] total loss per batch: 0.915
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1098e-02, 5.4226e-01, 5.1864e-03, 3.8938e-01, 1.6164e-06, 1.8525e-02,
        2.3548e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.000

[Epoch: 24, batch: 120/204] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0087, 0.0073, 0.1687, 0.5254, 0.0246, 0.2503, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.001

[Epoch: 24, batch: 160/204] total loss per batch: 0.903
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.8148e-01, 1.1435e-04, 5.9109e-03, 5.8464e-03, 5.0827e-02, 1.4347e-01,
        1.2350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 24, batch: 200/204] total loss per batch: 0.889
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0028, 0.1214, 0.0074, 0.8563, 0.0074, 0.0022, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 25, batch: 40/204] total loss per batch: 0.884
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0164, 0.0076, 0.4787, 0.0127, 0.4664, 0.0086, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 25, batch: 80/204] total loss per batch: 0.913
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4210e-02, 7.0108e-01, 6.0292e-03, 2.5088e-01, 4.0979e-07, 6.7318e-03,
        2.1075e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 25, batch: 120/204] total loss per batch: 0.865
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0092, 0.0032, 0.1711, 0.5973, 0.0196, 0.1828, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.001

[Epoch: 25, batch: 160/204] total loss per batch: 0.903
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.9342e-01, 2.4722e-04, 5.8564e-03, 1.5918e-02, 3.2308e-02, 1.3388e-01,
        1.8365e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 25, batch: 200/204] total loss per batch: 0.885
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0119, 0.2347, 0.0153, 0.7108, 0.0159, 0.0054, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 26, batch: 40/204] total loss per batch: 0.882
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0195, 0.0059, 0.3136, 0.0148, 0.6147, 0.0153, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 26, batch: 80/204] total loss per batch: 0.910
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7414e-02, 4.8692e-01, 1.4944e-02, 4.4200e-01, 2.8716e-06, 1.6384e-02,
        2.2339e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.003

[Epoch: 26, batch: 120/204] total loss per batch: 0.863
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0090, 0.0048, 0.2592, 0.4493, 0.0458, 0.2119, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 26, batch: 160/204] total loss per batch: 0.900
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.0139e-01, 7.8159e-05, 3.8746e-03, 1.9308e-02, 2.3117e-02, 5.3244e-01,
        1.9791e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 26, batch: 200/204] total loss per batch: 0.883
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.2847, 0.0104, 0.6739, 0.0179, 0.0025, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 27, batch: 40/204] total loss per batch: 0.877
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0183, 0.0108, 0.5321, 0.0209, 0.3927, 0.0116, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.010

[Epoch: 27, batch: 80/204] total loss per batch: 0.909
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2348e-02, 7.1837e-01, 4.9479e-03, 2.2778e-01, 5.3445e-07, 1.0268e-02,
        2.6279e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.002

[Epoch: 27, batch: 120/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0112, 0.0036, 0.2427, 0.5290, 0.0212, 0.1600, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 27, batch: 160/204] total loss per batch: 0.896
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.9533e-01, 3.5672e-05, 2.6995e-03, 7.2042e-03, 1.5688e-02, 6.1597e-02,
        1.7450e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.002

[Epoch: 27, batch: 200/204] total loss per batch: 0.880
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.1378, 0.0128, 0.7934, 0.0276, 0.0098, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 28, batch: 40/204] total loss per batch: 0.874
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0135, 0.0055, 0.2690, 0.0094, 0.6716, 0.0119, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 28, batch: 80/204] total loss per batch: 0.909
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.3203e-02, 6.4604e-01, 1.2342e-02, 2.7092e-01, 1.7576e-06, 1.3676e-02,
        3.3813e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.002

[Epoch: 28, batch: 120/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0082, 0.0035, 0.1779, 0.5336, 0.0261, 0.2390, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 28, batch: 160/204] total loss per batch: 0.898
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.1896e-01, 8.6376e-05, 4.2863e-03, 1.4638e-02, 4.6595e-02, 4.9199e-01,
        2.3452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.001

[Epoch: 28, batch: 200/204] total loss per batch: 0.880
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.1574, 0.0108, 0.8001, 0.0165, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 29, batch: 40/204] total loss per batch: 0.875
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0108, 0.0091, 0.3044, 0.0180, 0.6332, 0.0119, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.001

[Epoch: 29, batch: 80/204] total loss per batch: 0.907
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5151e-02, 4.9695e-01, 7.8975e-03, 4.4714e-01, 7.3651e-07, 1.0548e-02,
        2.2311e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.001

[Epoch: 29, batch: 120/204] total loss per batch: 0.861
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0077, 0.2150, 0.4208, 0.0504, 0.2695, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 29, batch: 160/204] total loss per batch: 0.895
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([9.2280e-01, 4.7848e-05, 1.6770e-03, 8.6625e-03, 1.3038e-02, 4.3566e-02,
        1.0214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.001

[Epoch: 29, batch: 200/204] total loss per batch: 0.878
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2033, 0.0052, 0.7549, 0.0231, 0.0047, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 30, batch: 40/204] total loss per batch: 0.874
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0160, 0.0070, 0.5720, 0.0121, 0.3557, 0.0167, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.000

[Epoch: 30, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4574e-02, 4.6517e-01, 6.7812e-03, 4.6290e-01, 8.1302e-07, 9.4332e-03,
        4.1137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 30, batch: 120/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0148, 0.0036, 0.1975, 0.5966, 0.0225, 0.1455, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 30, batch: 160/204] total loss per batch: 0.894
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.5815e-01, 1.3814e-04, 1.1637e-02, 2.8458e-02, 1.2241e-01, 2.4900e-01,
        3.0201e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.001

[Epoch: 30, batch: 200/204] total loss per batch: 0.881
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.2137, 0.0124, 0.7234, 0.0327, 0.0058, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 31, batch: 40/204] total loss per batch: 0.875
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0112, 0.0103, 0.2923, 0.0156, 0.6430, 0.0123, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 31, batch: 80/204] total loss per batch: 0.907
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([9.8766e-03, 6.1024e-01, 7.1140e-03, 3.4726e-01, 1.0470e-06, 7.4577e-03,
        1.8049e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.000

[Epoch: 31, batch: 120/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0042, 0.3716, 0.3854, 0.0282, 0.1800, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 31, batch: 160/204] total loss per batch: 0.894
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.4098e-01, 6.6581e-05, 3.7902e-03, 1.1619e-02, 1.0822e-02, 3.2012e-01,
        1.2609e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.000

[Epoch: 31, batch: 200/204] total loss per batch: 0.881
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.1652, 0.0089, 0.7994, 0.0115, 0.0043, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 32, batch: 40/204] total loss per batch: 0.876
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0092, 0.0071, 0.4208, 0.0081, 0.5384, 0.0065, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 32, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8162e-02, 6.1922e-01, 7.3312e-03, 3.2203e-01, 1.3211e-06, 8.0734e-03,
        2.5181e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.000

[Epoch: 32, batch: 120/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0032, 0.2369, 0.4406, 0.0293, 0.2412, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 32, batch: 160/204] total loss per batch: 0.893
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.6164e-01, 7.2357e-05, 3.1899e-03, 1.0440e-02, 4.0809e-02, 7.2646e-02,
        1.1208e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.000

[Epoch: 32, batch: 200/204] total loss per batch: 0.877
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.2307, 0.0129, 0.7296, 0.0131, 0.0046, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 33, batch: 40/204] total loss per batch: 0.874
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0079, 0.0086, 0.3939, 0.0108, 0.5561, 0.0089, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.000

[Epoch: 33, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1713e-02, 6.5925e-01, 5.9318e-03, 2.7812e-01, 1.0201e-06, 7.3147e-03,
        2.7661e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 33, batch: 120/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0190, 0.0057, 0.1974, 0.4544, 0.0349, 0.2539, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 33, batch: 160/204] total loss per batch: 0.892
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.3867e-01, 2.3473e-04, 6.5935e-03, 1.4852e-02, 1.6747e-02, 5.0787e-01,
        1.5038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 33, batch: 200/204] total loss per batch: 0.877
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0140, 0.3302, 0.0064, 0.6210, 0.0140, 0.0081, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 34, batch: 40/204] total loss per batch: 0.873
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0202, 0.0098, 0.4515, 0.0123, 0.4855, 0.0079, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.001

[Epoch: 34, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.8303e-02, 7.2085e-01, 1.2034e-02, 1.7815e-01, 1.6314e-06, 1.1828e-02,
        4.8829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 34, batch: 120/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0046, 0.1870, 0.6046, 0.0226, 0.1442, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 34, batch: 160/204] total loss per batch: 0.893
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.3526e-01, 1.1606e-04, 3.4985e-03, 1.0081e-02, 2.2282e-02, 7.4713e-02,
        5.4054e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.000

[Epoch: 34, batch: 200/204] total loss per batch: 0.875
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0084, 0.0820, 0.0100, 0.8824, 0.0121, 0.0023, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 35, batch: 40/204] total loss per batch: 0.873
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0071, 0.2914, 0.0111, 0.6588, 0.0058, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.000

[Epoch: 35, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 3
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1907e-02, 4.1457e-01, 8.7897e-03, 5.4673e-01, 8.3409e-07, 7.1151e-03,
        1.0886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.001

[Epoch: 35, batch: 120/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0165, 0.0044, 0.2178, 0.4586, 0.0269, 0.2529, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 35, batch: 160/204] total loss per batch: 0.893
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.9951e-01, 9.3518e-05, 6.9460e-03, 1.7795e-02, 8.7064e-03, 3.5429e-01,
        1.2659e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 35, batch: 200/204] total loss per batch: 0.877
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.3755, 0.0081, 0.5952, 0.0071, 0.0036, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.002

[Epoch: 36, batch: 40/204] total loss per batch: 0.872
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0120, 0.0059, 0.4668, 0.0191, 0.4677, 0.0114, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 36, batch: 80/204] total loss per batch: 0.904
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8123e-02, 6.3347e-01, 5.8854e-03, 3.0340e-01, 2.0160e-06, 1.6483e-02,
        2.2637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 36, batch: 120/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0029, 0.2404, 0.4679, 0.0281, 0.2286, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.004

[Epoch: 36, batch: 160/204] total loss per batch: 0.893
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1562e-01, 1.6730e-04, 5.0841e-03, 1.1614e-02, 2.3140e-02, 2.3446e-01,
        9.9081e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.000

[Epoch: 36, batch: 200/204] total loss per batch: 0.876
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.1319, 0.0112, 0.8167, 0.0155, 0.0064, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 37, batch: 40/204] total loss per batch: 0.871
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0074, 0.0052, 0.4348, 0.0114, 0.5167, 0.0084, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.002

[Epoch: 37, batch: 80/204] total loss per batch: 0.905
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6160e-02, 6.3758e-01, 1.2899e-02, 2.8518e-01, 3.9631e-06, 9.6966e-03,
        3.8472e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 37, batch: 120/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0115, 0.0027, 0.1752, 0.5844, 0.0258, 0.1818, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 37, batch: 160/204] total loss per batch: 0.892
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.6459e-01, 2.5921e-04, 4.9132e-03, 1.2472e-02, 1.4993e-02, 1.7953e-01,
        2.3247e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.007

[Epoch: 37, batch: 200/204] total loss per batch: 0.876
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.1788, 0.0084, 0.7889, 0.0090, 0.0026, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 38, batch: 40/204] total loss per batch: 0.872
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0060, 0.3092, 0.0050, 0.6598, 0.0046, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.000

[Epoch: 38, batch: 80/204] total loss per batch: 0.904
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6032e-02, 5.8856e-01, 8.3765e-03, 3.5725e-01, 2.6017e-06, 7.8839e-03,
        2.1894e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 38, batch: 120/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0136, 0.0034, 0.2087, 0.4732, 0.0213, 0.2464, 0.0334],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 38, batch: 160/204] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.1104e-01, 7.7550e-05, 3.8772e-03, 1.3666e-02, 1.6410e-02, 4.4097e-01,
        1.3968e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 38, batch: 200/204] total loss per batch: 0.876
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0110, 0.2419, 0.0097, 0.7046, 0.0178, 0.0050, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 39, batch: 40/204] total loss per batch: 0.871
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0075, 0.4896, 0.0099, 0.4526, 0.0118, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 39, batch: 80/204] total loss per batch: 0.903
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.4358e-02, 7.5211e-01, 6.0426e-03, 1.6323e-01, 2.6369e-06, 8.5831e-03,
        4.5669e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 39, batch: 120/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0079, 0.0059, 0.2972, 0.4892, 0.0167, 0.1716, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 39, batch: 160/204] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.5359e-01, 1.0612e-04, 1.9198e-03, 1.0167e-02, 1.8794e-02, 1.0207e-01,
        1.3353e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.006

[Epoch: 39, batch: 200/204] total loss per batch: 0.874
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.2004, 0.0053, 0.7745, 0.0057, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 40, batch: 40/204] total loss per batch: 0.871
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0099, 0.0097, 0.3819, 0.0092, 0.5701, 0.0070, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 40, batch: 80/204] total loss per batch: 0.902
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3849e-02, 4.8121e-01, 1.2059e-02, 4.5470e-01, 2.5965e-06, 1.2713e-02,
        2.5462e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.000

[Epoch: 40, batch: 120/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0116, 0.0032, 0.1494, 0.5508, 0.0215, 0.2411, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 40, batch: 160/204] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.5042e-01, 1.7197e-04, 6.5531e-03, 2.7341e-02, 1.6064e-02, 3.8492e-01,
        1.4528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.002

[Epoch: 40, batch: 200/204] total loss per batch: 0.874
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.1980, 0.0068, 0.7659, 0.0118, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 41, batch: 40/204] total loss per batch: 0.868
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0066, 0.0068, 0.3721, 0.0093, 0.5736, 0.0113, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.002

[Epoch: 41, batch: 80/204] total loss per batch: 0.901
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1665e-02, 5.5382e-01, 3.3336e-03, 4.0800e-01, 1.2108e-06, 9.8230e-03,
        1.3357e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.001

[Epoch: 41, batch: 120/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0058, 0.0046, 0.3773, 0.4004, 0.0181, 0.1815, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 41, batch: 160/204] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.2778e-01, 1.4578e-04, 4.2836e-03, 8.9310e-03, 2.9868e-02, 8.7583e-02,
        4.1412e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 41, batch: 200/204] total loss per batch: 0.872
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2232, 0.0083, 0.7428, 0.0103, 0.0046, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.004

[Epoch: 42, batch: 40/204] total loss per batch: 0.867
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0111, 0.0077, 0.4675, 0.0191, 0.4698, 0.0078, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.020

[Epoch: 42, batch: 80/204] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.0999e-02, 5.6061e-01, 6.8196e-03, 3.7562e-01, 2.1826e-06, 1.1075e-02,
        2.4873e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 42, batch: 120/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0152, 0.0059, 0.1961, 0.5158, 0.0319, 0.2075, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.010

[Epoch: 42, batch: 160/204] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.4617e-01, 1.0931e-04, 4.4448e-03, 1.1768e-02, 1.8576e-02, 3.9820e-01,
        2.0727e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.020

[Epoch: 42, batch: 200/204] total loss per batch: 0.872
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.1856, 0.0098, 0.7791, 0.0089, 0.0055, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 43, batch: 40/204] total loss per batch: 0.868
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0074, 0.0066, 0.2742, 0.0069, 0.6874, 0.0080, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 43, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4380e-02, 7.4480e-01, 8.2360e-03, 1.9033e-01, 1.5999e-06, 7.3734e-03,
        3.4876e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 43, batch: 120/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0039, 0.2144, 0.4640, 0.0340, 0.2563, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 43, batch: 160/204] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.1045e-01, 1.3036e-04, 3.4936e-03, 1.0662e-02, 7.6018e-03, 1.5916e-01,
        8.5095e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.004

[Epoch: 43, batch: 200/204] total loss per batch: 0.872
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2384, 0.0089, 0.7223, 0.0086, 0.0070, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.026

[Epoch: 44, batch: 40/204] total loss per batch: 0.868
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0061, 0.0064, 0.4836, 0.0092, 0.4774, 0.0045, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 44, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4657e-02, 6.5411e-01, 7.3065e-03, 2.8311e-01, 2.4501e-06, 1.5954e-02,
        2.4860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.002

[Epoch: 44, batch: 120/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0142, 0.0069, 0.2734, 0.4527, 0.0203, 0.2044, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 44, batch: 160/204] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7300e-01, 2.5088e-04, 1.0758e-02, 1.7806e-02, 2.1577e-02, 2.6363e-01,
        1.2973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.017

[Epoch: 44, batch: 200/204] total loss per batch: 0.873
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1901, 0.0117, 0.7695, 0.0093, 0.0051, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 45, batch: 40/204] total loss per batch: 0.867
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0101, 0.0059, 0.4120, 0.0061, 0.5360, 0.0072, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 45, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([7.3492e-03, 6.0675e-01, 8.1350e-03, 3.4455e-01, 1.5764e-06, 9.1624e-03,
        2.4058e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 45, batch: 120/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0098, 0.0041, 0.2348, 0.4437, 0.0252, 0.2663, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 45, batch: 160/204] total loss per batch: 0.886
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.8081e-01, 1.0765e-04, 4.4047e-03, 1.5286e-02, 1.3100e-02, 3.7241e-01,
        1.3880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 45, batch: 200/204] total loss per batch: 0.873
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0124, 0.2023, 0.0119, 0.7388, 0.0125, 0.0097, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 46, batch: 40/204] total loss per batch: 0.868
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0111, 0.0063, 0.3403, 0.0098, 0.6100, 0.0076, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.019

[Epoch: 46, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3694e-02, 6.4466e-01, 1.2278e-02, 2.8434e-01, 4.1095e-06, 1.0770e-02,
        3.4256e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 0.007

[Epoch: 46, batch: 120/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0144, 0.0067, 0.1927, 0.5289, 0.0223, 0.2163, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 46, batch: 160/204] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.4232e-01, 2.4492e-04, 2.3340e-03, 1.2936e-02, 1.0729e-02, 1.2279e-01,
        8.6425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.003

[Epoch: 46, batch: 200/204] total loss per batch: 0.874
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.2110, 0.0069, 0.7482, 0.0164, 0.0066, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 47, batch: 40/204] total loss per batch: 0.866
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0064, 0.3217, 0.0050, 0.6404, 0.0069, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 47, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1592e-02, 6.9219e-01, 5.6024e-03, 2.6110e-01, 1.6210e-06, 5.6295e-03,
        2.3890e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 47, batch: 120/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0084, 0.0023, 0.2573, 0.4183, 0.0397, 0.2465, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 47, batch: 160/204] total loss per batch: 0.888
Policy (actual, predicted): 0 5
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([4.0653e-01, 3.1663e-04, 1.1979e-02, 1.7474e-02, 3.5966e-02, 5.0592e-01,
        2.1807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.005

[Epoch: 47, batch: 200/204] total loss per batch: 0.871
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.1800, 0.0083, 0.7936, 0.0049, 0.0032, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 48, batch: 40/204] total loss per batch: 0.865
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0078, 0.0091, 0.5560, 0.0075, 0.3863, 0.0082, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.016

[Epoch: 48, batch: 80/204] total loss per batch: 0.899
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.0576e-02, 5.9823e-01, 6.8931e-03, 3.2355e-01, 4.5264e-06, 1.6115e-02,
        3.4633e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 48, batch: 120/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0052, 0.0038, 0.2448, 0.5755, 0.0114, 0.1457, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 48, batch: 160/204] total loss per batch: 0.885
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([9.4488e-01, 7.9948e-05, 1.7230e-03, 5.4125e-03, 4.9876e-03, 3.6897e-02,
        6.0179e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.009

[Epoch: 48, batch: 200/204] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2697, 0.0158, 0.6712, 0.0284, 0.0065, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 49, batch: 40/204] total loss per batch: 0.863
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0087, 0.3237, 0.0086, 0.6314, 0.0089, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 49, batch: 80/204] total loss per batch: 0.897
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7359e-02, 5.4067e-01, 5.4214e-03, 4.1542e-01, 1.5434e-06, 7.6407e-03,
        1.3493e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 49, batch: 120/204] total loss per batch: 0.848
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0121, 0.0045, 0.2204, 0.4476, 0.0279, 0.2562, 0.0312],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 49, batch: 160/204] total loss per batch: 0.884
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.0536e-01, 2.2809e-04, 3.8109e-03, 3.2560e-02, 2.1654e-02, 3.2576e-01,
        1.0625e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.002

[Epoch: 49, batch: 200/204] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.1969, 0.0057, 0.7727, 0.0099, 0.0040, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 50, batch: 40/204] total loss per batch: 0.862
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0073, 0.4796, 0.0068, 0.4749, 0.0076, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.014

[Epoch: 50, batch: 80/204] total loss per batch: 0.896
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8186e-02, 5.3972e-01, 6.9838e-03, 4.0351e-01, 6.4008e-06, 8.8004e-03,
        2.2799e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 50, batch: 120/204] total loss per batch: 0.848
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0081, 0.0042, 0.2060, 0.4932, 0.0204, 0.2456, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 50, batch: 160/204] total loss per batch: 0.888
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0043e-01, 9.8933e-05, 5.3402e-03, 1.8684e-02, 1.1460e-02, 2.4739e-01,
        1.6594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.025

[Epoch: 50, batch: 200/204] total loss per batch: 0.868
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.1728, 0.0166, 0.7710, 0.0116, 0.0096, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 51, batch: 40/204] total loss per batch: 0.863
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0062, 0.0076, 0.3915, 0.0130, 0.5576, 0.0069, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.014

[Epoch: 51, batch: 80/204] total loss per batch: 0.894
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1932e-02, 6.3895e-01, 5.0358e-03, 3.1265e-01, 1.8794e-06, 1.1383e-02,
        2.0042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 51, batch: 120/204] total loss per batch: 0.846
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0125, 0.0031, 0.2421, 0.5278, 0.0238, 0.1662, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 51, batch: 160/204] total loss per batch: 0.886
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5769e-01, 1.8028e-04, 6.6730e-03, 2.8435e-02, 1.3466e-02, 2.7971e-01,
        1.3843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.005

[Epoch: 51, batch: 200/204] total loss per batch: 0.866
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0084, 0.2424, 0.0058, 0.7182, 0.0155, 0.0053, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 52, batch: 40/204] total loss per batch: 0.857
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0063, 0.0084, 0.3098, 0.0118, 0.6368, 0.0078, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 52, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7928e-02, 7.1832e-01, 5.7629e-03, 2.2110e-01, 2.6507e-06, 8.5990e-03,
        2.8292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 52, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0054, 0.2320, 0.4690, 0.0273, 0.2402, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 52, batch: 160/204] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0810e-01, 2.0985e-04, 4.6299e-03, 2.0966e-02, 1.4423e-02, 2.3595e-01,
        1.5722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.015

[Epoch: 52, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.1709, 0.0056, 0.7991, 0.0083, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 53, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0077, 0.0108, 0.4319, 0.0096, 0.5158, 0.0069, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.049

[Epoch: 53, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3910e-02, 6.1645e-01, 5.9102e-03, 3.3248e-01, 1.8641e-06, 7.6227e-03,
        2.3621e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.002

[Epoch: 53, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0182, 0.0046, 0.2299, 0.4927, 0.0269, 0.2054, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 53, batch: 160/204] total loss per batch: 0.873
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1296e-01, 2.0264e-04, 4.5322e-03, 2.4225e-02, 1.1321e-02, 2.3084e-01,
        1.5915e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.005

[Epoch: 53, batch: 200/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.2121, 0.0064, 0.7579, 0.0060, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 54, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0062, 0.4089, 0.0074, 0.5508, 0.0063, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 54, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5785e-02, 6.3013e-01, 5.7546e-03, 3.1548e-01, 1.5139e-06, 9.3183e-03,
        2.3532e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 54, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0071, 0.0050, 0.2617, 0.4826, 0.0169, 0.2101, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 54, batch: 160/204] total loss per batch: 0.871
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9189e-01, 9.2939e-05, 3.9033e-03, 1.6248e-02, 9.2156e-03, 2.6713e-01,
        1.1526e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.013

[Epoch: 54, batch: 200/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.1784, 0.0054, 0.7957, 0.0078, 0.0037, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 55, batch: 40/204] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0056, 0.0071, 0.3915, 0.0086, 0.5649, 0.0058, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 55, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2656e-02, 5.8021e-01, 7.6400e-03, 3.7106e-01, 1.8338e-06, 6.6462e-03,
        2.1786e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 55, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0041, 0.2026, 0.5009, 0.0271, 0.2319, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 55, batch: 160/204] total loss per batch: 0.871
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5359e-01, 1.2967e-04, 5.9115e-03, 1.8889e-02, 8.9774e-03, 3.0325e-01,
        9.2537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.008

[Epoch: 55, batch: 200/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2366, 0.0079, 0.7239, 0.0102, 0.0073, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 56, batch: 40/204] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0072, 0.0086, 0.4205, 0.0074, 0.5314, 0.0072, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 56, batch: 80/204] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1836e-02, 6.3334e-01, 6.4762e-03, 3.2228e-01, 1.4630e-06, 6.1864e-03,
        1.9882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 56, batch: 120/204] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0093, 0.0037, 0.2592, 0.5011, 0.0221, 0.1871, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 56, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.8981e-01, 1.0934e-04, 5.1294e-03, 2.1679e-02, 9.6876e-03, 1.6479e-01,
        8.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.001

[Epoch: 56, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.1688, 0.0037, 0.8115, 0.0055, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 57, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0097, 0.0087, 0.3259, 0.0076, 0.6287, 0.0042, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.057

[Epoch: 57, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([8.8557e-03, 6.4251e-01, 5.6123e-03, 3.2114e-01, 2.1279e-06, 5.2103e-03,
        1.6671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 57, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0151, 0.0042, 0.2044, 0.4861, 0.0291, 0.2393, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 57, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.6840e-01, 1.7999e-04, 6.2260e-03, 2.7628e-02, 8.0036e-03, 3.7888e-01,
        1.0675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 -0.007

[Epoch: 57, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.2327, 0.0084, 0.7346, 0.0074, 0.0051, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 58, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0085, 0.0104, 0.3990, 0.0076, 0.5381, 0.0086, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 58, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4511e-02, 5.9212e-01, 1.1618e-02, 3.4201e-01, 1.9885e-06, 8.1215e-03,
        3.1614e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 58, batch: 120/204] total loss per batch: 0.839
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0080, 0.0036, 0.1986, 0.5569, 0.0205, 0.1949, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 58, batch: 160/204] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.6134e-01, 4.6458e-05, 2.2322e-03, 7.9190e-03, 5.9959e-03, 1.1781e-01,
        4.6583e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.009

[Epoch: 58, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2615, 0.0055, 0.7097, 0.0082, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 59, batch: 40/204] total loss per batch: 0.855
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0099, 0.0113, 0.4183, 0.0187, 0.5125, 0.0052, 0.0241],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 59, batch: 80/204] total loss per batch: 0.889
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5740e-02, 7.1324e-01, 4.8009e-03, 2.3585e-01, 2.1901e-06, 6.1517e-03,
        2.4220e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 59, batch: 120/204] total loss per batch: 0.841
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0136, 0.0054, 0.2872, 0.4553, 0.0290, 0.1807, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 59, batch: 160/204] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.1586e-01, 8.8630e-05, 8.3534e-03, 2.0610e-02, 9.9304e-03, 4.1963e-01,
        2.5524e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.001

[Epoch: 59, batch: 200/204] total loss per batch: 0.861
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.1595, 0.0094, 0.8075, 0.0069, 0.0057, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 60, batch: 40/204] total loss per batch: 0.858
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0081, 0.0056, 0.3587, 0.0050, 0.5926, 0.0073, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.046

[Epoch: 60, batch: 80/204] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([7.2618e-03, 6.7579e-01, 5.4110e-03, 2.8462e-01, 2.3571e-06, 4.7842e-03,
        2.2127e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 60, batch: 120/204] total loss per batch: 0.842
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0108, 0.0041, 0.1958, 0.4854, 0.0265, 0.2512, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 60, batch: 160/204] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.1253e-01, 6.6626e-05, 2.4787e-03, 1.4479e-02, 8.8006e-03, 1.5754e-01,
        4.0999e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.008

[Epoch: 60, batch: 200/204] total loss per batch: 0.862
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.2410, 0.0050, 0.7241, 0.0074, 0.0073, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.024

[Epoch: 61, batch: 40/204] total loss per batch: 0.861
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0068, 0.4262, 0.0122, 0.5306, 0.0036, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 61, batch: 80/204] total loss per batch: 0.893
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2686e-02, 5.7909e-01, 9.3586e-03, 3.7292e-01, 1.8303e-06, 1.1913e-02,
        1.4037e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.003

[Epoch: 61, batch: 120/204] total loss per batch: 0.843
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0039, 0.1592, 0.5763, 0.0297, 0.2027, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 61, batch: 160/204] total loss per batch: 0.879
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.1575e-01, 1.0012e-04, 7.3801e-03, 2.3105e-02, 9.9411e-03, 3.3427e-01,
        9.4508e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.022

[Epoch: 61, batch: 200/204] total loss per batch: 0.862
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.1653, 0.0078, 0.8050, 0.0091, 0.0057, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 62, batch: 40/204] total loss per batch: 0.859
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0083, 0.0085, 0.3991, 0.0091, 0.5481, 0.0099, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.008

[Epoch: 62, batch: 80/204] total loss per batch: 0.893
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4261e-02, 5.7816e-01, 5.5626e-03, 3.7529e-01, 4.2996e-06, 4.5741e-03,
        2.2148e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 62, batch: 120/204] total loss per batch: 0.842
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0073, 0.0026, 0.2858, 0.4254, 0.0207, 0.2330, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 62, batch: 160/204] total loss per batch: 0.878
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.7229e-01, 5.0697e-05, 1.9820e-03, 1.8840e-02, 1.5360e-02, 1.8000e-01,
        1.1482e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 62, batch: 200/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2297, 0.0068, 0.7319, 0.0102, 0.0076, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.035

[Epoch: 63, batch: 40/204] total loss per batch: 0.857
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0094, 0.0132, 0.3772, 0.0081, 0.5702, 0.0049, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 63, batch: 80/204] total loss per batch: 0.891
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7036e-02, 5.5257e-01, 6.1633e-03, 4.0285e-01, 2.0705e-06, 6.0148e-03,
        1.5369e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.004

[Epoch: 63, batch: 120/204] total loss per batch: 0.840
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0159, 0.0080, 0.2123, 0.5008, 0.0281, 0.2122, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 63, batch: 160/204] total loss per batch: 0.877
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5761e-01, 1.3398e-04, 5.2233e-03, 2.1428e-02, 1.4533e-02, 2.8974e-01,
        1.1327e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.003

[Epoch: 63, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.1800, 0.0073, 0.7804, 0.0130, 0.0045, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 64, batch: 40/204] total loss per batch: 0.855
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0070, 0.4574, 0.0097, 0.4935, 0.0073, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.013

[Epoch: 64, batch: 80/204] total loss per batch: 0.888
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7947e-02, 6.4124e-01, 9.8082e-03, 2.9752e-01, 4.3022e-06, 8.3293e-03,
        2.5151e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 64, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0086, 0.0029, 0.2033, 0.5389, 0.0253, 0.2004, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 64, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9977e-01, 1.3801e-04, 3.8683e-03, 1.7036e-02, 9.0080e-03, 2.6185e-01,
        8.3253e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 64, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.2650, 0.0073, 0.7042, 0.0084, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 65, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0095, 0.0072, 0.3388, 0.0096, 0.6118, 0.0064, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 65, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1277e-02, 6.4396e-01, 5.9345e-03, 3.0936e-01, 2.1038e-06, 6.0654e-03,
        2.3409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 65, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0127, 0.0063, 0.2353, 0.4305, 0.0292, 0.2633, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 65, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8488e-01, 1.1445e-04, 7.3391e-03, 2.1989e-02, 1.2306e-02, 2.6664e-01,
        6.7377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.014

[Epoch: 65, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.2289, 0.0064, 0.7368, 0.0094, 0.0060, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 66, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0102, 0.0107, 0.4027, 0.0067, 0.5447, 0.0070, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.027

[Epoch: 66, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8723e-02, 5.8037e-01, 5.9225e-03, 3.6696e-01, 3.5172e-06, 8.0249e-03,
        1.9999e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 66, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0107, 0.0045, 0.2348, 0.5169, 0.0232, 0.1920, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.020

[Epoch: 66, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3716e-01, 8.4445e-05, 4.0324e-03, 1.1147e-02, 4.5975e-03, 2.3656e-01,
        6.4230e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.015

[Epoch: 66, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.1302, 0.0053, 0.8406, 0.0069, 0.0062, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.022

[Epoch: 67, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0086, 0.3917, 0.0078, 0.5641, 0.0065, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.026

[Epoch: 67, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.2370e-02, 5.4591e-01, 1.4565e-02, 3.7676e-01, 6.0086e-06, 9.5434e-03,
        3.0847e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 67, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0099, 0.0052, 0.2308, 0.4753, 0.0293, 0.2217, 0.0279],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 67, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1991e-01, 9.2456e-05, 5.1664e-03, 1.4058e-02, 1.0281e-02, 2.4269e-01,
        7.7969e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.017

[Epoch: 67, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.2442, 0.0045, 0.7344, 0.0063, 0.0040, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 68, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0076, 0.0079, 0.3814, 0.0084, 0.5620, 0.0071, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 68, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8581e-02, 6.7790e-01, 3.8712e-03, 2.6924e-01, 1.3972e-06, 6.4653e-03,
        2.3941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 68, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0021, 0.2208, 0.5218, 0.0282, 0.2019, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 68, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.9136e-01, 1.3068e-04, 6.2204e-03, 1.6618e-02, 9.2738e-03, 3.6333e-01,
        1.3073e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 68, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.1664, 0.0073, 0.7976, 0.0101, 0.0061, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 69, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0060, 0.4620, 0.0056, 0.4993, 0.0059, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 69, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.4894e-02, 6.6492e-01, 9.8623e-03, 2.5469e-01, 8.7511e-06, 9.4437e-03,
        3.6185e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 69, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0111, 0.0046, 0.2571, 0.4158, 0.0337, 0.2450, 0.0327],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 69, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.0232e-01, 1.3197e-04, 4.7980e-03, 2.4316e-02, 6.3491e-03, 1.5277e-01,
        9.3096e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.024

[Epoch: 69, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2367, 0.0032, 0.7388, 0.0065, 0.0057, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.042

[Epoch: 70, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0069, 0.2954, 0.0062, 0.6581, 0.0054, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.019

[Epoch: 70, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2225e-02, 5.3733e-01, 3.8512e-03, 4.2168e-01, 2.8934e-06, 4.9682e-03,
        1.9945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 70, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0048, 0.1952, 0.5559, 0.0269, 0.1912, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.030

[Epoch: 70, batch: 160/204] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.7042e-01, 1.2185e-04, 6.3053e-03, 1.7727e-02, 5.8732e-03, 3.9304e-01,
        6.5137e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.017

[Epoch: 70, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.1644, 0.0060, 0.8084, 0.0080, 0.0029, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 71, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0079, 0.0076, 0.5129, 0.0057, 0.4334, 0.0070, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 71, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4021e-02, 6.3177e-01, 6.7921e-03, 3.2143e-01, 3.5227e-06, 5.2021e-03,
        2.0785e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 71, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0042, 0.2647, 0.4430, 0.0313, 0.2268, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 71, batch: 160/204] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.8805e-01, 1.7604e-04, 7.1756e-03, 1.9564e-02, 5.9515e-03, 1.7086e-01,
        8.2273e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 71, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.2694, 0.0049, 0.7033, 0.0081, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 72, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0039, 0.0076, 0.3181, 0.0050, 0.6451, 0.0051, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.044

[Epoch: 72, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3299e-02, 6.3173e-01, 6.4302e-03, 3.1820e-01, 2.5330e-06, 7.4615e-03,
        2.2870e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 72, batch: 120/204] total loss per batch: 0.838
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0050, 0.2008, 0.5210, 0.0231, 0.2200, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 72, batch: 160/204] total loss per batch: 0.877
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.9786e-01, 1.1332e-04, 5.3119e-03, 2.2173e-02, 1.4840e-02, 3.4852e-01,
        1.1176e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.009

[Epoch: 72, batch: 200/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.1569, 0.0058, 0.8153, 0.0073, 0.0036, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 73, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0080, 0.0080, 0.3785, 0.0089, 0.5730, 0.0071, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 73, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([9.5171e-03, 5.9581e-01, 3.7149e-03, 3.6581e-01, 1.6921e-06, 5.1432e-03,
        1.9998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 73, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0132, 0.0038, 0.2408, 0.4756, 0.0229, 0.2198, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 73, batch: 160/204] total loss per batch: 0.876
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4269e-01, 1.1476e-04, 4.3740e-03, 2.4782e-02, 9.8578e-03, 2.0982e-01,
        8.3611e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.008

[Epoch: 73, batch: 200/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.2644, 0.0056, 0.7023, 0.0083, 0.0055, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.038

[Epoch: 74, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 2
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0080, 0.0075, 0.5152, 0.0055, 0.4423, 0.0066, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.017

[Epoch: 74, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.5871e-02, 6.4030e-01, 6.9585e-03, 2.8609e-01, 3.7000e-06, 8.3836e-03,
        3.2393e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 74, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0055, 0.0043, 0.2153, 0.5268, 0.0240, 0.2108, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.025

[Epoch: 74, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1549e-01, 1.3180e-04, 5.6615e-03, 1.1364e-02, 4.9759e-03, 2.5581e-01,
        6.5758e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.042

[Epoch: 74, batch: 200/204] total loss per batch: 0.860
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2140, 0.0130, 0.7444, 0.0110, 0.0047, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 75, batch: 40/204] total loss per batch: 0.855
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0120, 0.0068, 0.2783, 0.0111, 0.6569, 0.0081, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 75, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5511e-02, 5.8102e-01, 7.1465e-03, 3.6451e-01, 1.8868e-06, 6.5198e-03,
        2.5286e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.005

[Epoch: 75, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0094, 0.0032, 0.2014, 0.4610, 0.0266, 0.2758, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.028

[Epoch: 75, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8557e-01, 7.8455e-05, 3.1318e-03, 1.3025e-02, 3.4508e-03, 2.8830e-01,
        6.4359e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.023

[Epoch: 75, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.1853, 0.0048, 0.7901, 0.0060, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.042

[Epoch: 76, batch: 40/204] total loss per batch: 0.855
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0063, 0.0056, 0.4169, 0.0035, 0.5400, 0.0046, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.014

[Epoch: 76, batch: 80/204] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5670e-02, 6.2548e-01, 9.7474e-03, 3.1828e-01, 2.9447e-06, 6.6473e-03,
        2.4168e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 76, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0109, 0.0057, 0.2298, 0.5315, 0.0287, 0.1719, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.026

[Epoch: 76, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2001e-01, 1.7857e-04, 4.0991e-03, 2.2178e-02, 4.5270e-03, 2.4234e-01,
        6.6630e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.027

[Epoch: 76, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.2340, 0.0061, 0.7376, 0.0078, 0.0040, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 77, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0073, 0.0067, 0.3720, 0.0077, 0.5840, 0.0055, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.018

[Epoch: 77, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6668e-02, 5.7303e-01, 1.0413e-02, 3.6462e-01, 3.0554e-06, 8.1915e-03,
        2.7072e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 77, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0094, 0.0033, 0.2297, 0.4905, 0.0231, 0.2256, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 77, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5194e-01, 1.0271e-04, 6.7291e-03, 2.3427e-02, 6.4875e-03, 3.0565e-01,
        5.6645e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.019

[Epoch: 77, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2190, 0.0051, 0.7518, 0.0093, 0.0054, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.040

[Epoch: 78, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0056, 0.4455, 0.0066, 0.5122, 0.0039, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 78, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1449e-02, 6.4730e-01, 4.6877e-03, 3.0817e-01, 5.2936e-06, 3.8656e-03,
        2.4521e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 78, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0108, 0.0056, 0.1978, 0.5536, 0.0276, 0.1856, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 78, batch: 160/204] total loss per batch: 0.875
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.6488e-01, 7.0585e-05, 2.6661e-03, 1.2722e-02, 2.1359e-03, 2.0797e-01,
        9.5559e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 78, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.2204, 0.0086, 0.7424, 0.0088, 0.0050, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.026

[Epoch: 79, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0049, 0.3493, 0.0025, 0.6190, 0.0041, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.007

[Epoch: 79, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1995e-02, 6.7183e-01, 5.2525e-03, 2.8188e-01, 1.7549e-06, 5.2934e-03,
        2.3750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 79, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0111, 0.0040, 0.2457, 0.4396, 0.0280, 0.2473, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 79, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.8027e-01, 1.2749e-04, 7.1785e-03, 2.6066e-02, 2.1546e-02, 3.5977e-01,
        5.0433e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.016

[Epoch: 79, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.1996, 0.0076, 0.7631, 0.0099, 0.0064, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 80, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0043, 0.0059, 0.4642, 0.0058, 0.4895, 0.0052, 0.0251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 80, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1943e-02, 6.5146e-01, 4.8551e-03, 3.0349e-01, 2.3937e-06, 4.7097e-03,
        2.3540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 80, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0098, 0.0040, 0.2242, 0.4956, 0.0300, 0.2196, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 80, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.3056e-01, 3.0498e-05, 3.2520e-03, 7.1941e-03, 1.1382e-02, 1.3050e-01,
        1.7084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 80, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.1923, 0.0110, 0.7692, 0.0079, 0.0050, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.028

[Epoch: 81, batch: 40/204] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0061, 0.0057, 0.3580, 0.0062, 0.5989, 0.0055, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 81, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1529e-02, 5.3296e-01, 3.7306e-03, 4.2186e-01, 3.1596e-06, 6.4342e-03,
        2.3477e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 81, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0107, 0.0040, 0.2071, 0.4975, 0.0245, 0.2364, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 81, batch: 160/204] total loss per batch: 0.873
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.4282e-01, 9.8531e-05, 6.2108e-03, 2.8265e-02, 4.8047e-02, 3.5901e-01,
        1.5552e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 81, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.2213, 0.0068, 0.7479, 0.0076, 0.0046, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 82, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0142, 0.0085, 0.4267, 0.0089, 0.5049, 0.0093, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.026

[Epoch: 82, batch: 80/204] total loss per batch: 0.886
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8293e-02, 5.1016e-01, 8.8122e-03, 4.2716e-01, 6.2631e-06, 7.6644e-03,
        2.7901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 82, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0096, 0.0055, 0.1951, 0.5590, 0.0230, 0.1901, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 82, batch: 160/204] total loss per batch: 0.873
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0115e-01, 3.9732e-05, 3.7151e-03, 1.6102e-02, 5.1810e-03, 2.6541e-01,
        8.4109e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 82, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.2000, 0.0067, 0.7697, 0.0077, 0.0054, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 83, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0062, 0.3262, 0.0083, 0.6240, 0.0051, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.026

[Epoch: 83, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8067e-02, 5.7266e-01, 6.5563e-03, 3.6051e-01, 6.2955e-06, 6.5368e-03,
        3.5662e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 83, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0115, 0.0062, 0.2873, 0.4326, 0.0220, 0.2211, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 83, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2302e-01, 4.5066e-05, 4.0586e-03, 1.1786e-02, 5.7344e-03, 2.4850e-01,
        6.8588e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.016

[Epoch: 83, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.1963, 0.0070, 0.7745, 0.0069, 0.0039, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 84, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0061, 0.0067, 0.4706, 0.0055, 0.4858, 0.0051, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 84, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9309e-02, 6.9454e-01, 6.5098e-03, 2.5014e-01, 3.9617e-06, 6.0022e-03,
        2.3492e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 84, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0051, 0.2174, 0.4772, 0.0258, 0.2450, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 84, batch: 160/204] total loss per batch: 0.873
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.1835e-01, 6.7249e-05, 8.0739e-03, 2.9152e-02, 8.2870e-03, 3.2741e-01,
        8.6582e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.019

[Epoch: 84, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.2014, 0.0056, 0.7700, 0.0073, 0.0055, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.025

[Epoch: 85, batch: 40/204] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0068, 0.0074, 0.2962, 0.0074, 0.6587, 0.0049, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 85, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2238e-02, 7.1078e-01, 6.1896e-03, 2.3810e-01, 4.8262e-06, 6.2679e-03,
        2.6422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.008

[Epoch: 85, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0077, 0.0061, 0.2168, 0.5192, 0.0253, 0.2050, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 85, batch: 160/204] total loss per batch: 0.871
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3787e-01, 4.8431e-05, 4.4422e-03, 1.3380e-02, 4.6405e-03, 2.3234e-01,
        7.2783e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 85, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.1971, 0.0040, 0.7777, 0.0059, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.037

[Epoch: 86, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0040, 0.4160, 0.0060, 0.5507, 0.0042, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 86, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6159e-02, 6.0071e-01, 5.0183e-03, 3.4802e-01, 3.2487e-06, 5.9260e-03,
        2.4164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 86, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0025, 0.2338, 0.4725, 0.0270, 0.2383, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.023

[Epoch: 86, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0201e-01, 4.2404e-05, 6.0895e-03, 2.1430e-02, 5.0135e-03, 2.5693e-01,
        8.4769e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.025

[Epoch: 86, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2262, 0.0047, 0.7530, 0.0039, 0.0042, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 87, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0051, 0.0053, 0.4438, 0.0078, 0.5034, 0.0055, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 87, batch: 80/204] total loss per batch: 0.885
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5299e-02, 5.2243e-01, 5.9893e-03, 4.2656e-01, 3.7647e-06, 6.4053e-03,
        2.3305e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 87, batch: 120/204] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0072, 0.0049, 0.2329, 0.5158, 0.0283, 0.1942, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 87, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6985e-01, 2.1445e-05, 5.2584e-03, 1.2016e-02, 4.9211e-03, 3.0356e-01,
        4.3755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.021

[Epoch: 87, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.1855, 0.0039, 0.7876, 0.0078, 0.0053, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.028

[Epoch: 88, batch: 40/204] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0050, 0.3507, 0.0077, 0.6084, 0.0065, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.038

[Epoch: 88, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6763e-02, 6.1751e-01, 7.7491e-03, 3.2205e-01, 4.0606e-06, 5.3946e-03,
        3.0532e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.010

[Epoch: 88, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0114, 0.0039, 0.2177, 0.5082, 0.0306, 0.2090, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 88, batch: 160/204] total loss per batch: 0.873
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4274e-01, 4.2616e-05, 3.5064e-03, 2.2100e-02, 5.4710e-03, 2.1929e-01,
        6.8505e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.021

[Epoch: 88, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2624, 0.0069, 0.7100, 0.0074, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 89, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0078, 0.0063, 0.4437, 0.0059, 0.5134, 0.0056, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 89, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6911e-02, 6.5912e-01, 3.9688e-03, 2.8930e-01, 3.3606e-06, 7.3120e-03,
        2.3384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.006

[Epoch: 89, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0105, 0.0056, 0.2189, 0.4903, 0.0266, 0.2269, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 89, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7316e-01, 6.5784e-05, 9.1174e-03, 2.8931e-02, 8.6012e-03, 2.7010e-01,
        1.0023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 89, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.2018, 0.0067, 0.7551, 0.0142, 0.0074, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.025

[Epoch: 90, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0065, 0.3991, 0.0072, 0.5583, 0.0045, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.040

[Epoch: 90, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8701e-02, 6.7877e-01, 4.4040e-03, 2.6896e-01, 3.2839e-06, 6.0830e-03,
        2.3079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 90, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0134, 0.0047, 0.2327, 0.5036, 0.0279, 0.1965, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.022

[Epoch: 90, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0477e-01, 4.8013e-05, 4.3352e-03, 1.8895e-02, 6.6144e-03, 2.5926e-01,
        6.0753e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 90, batch: 200/204] total loss per batch: 0.857
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.2088, 0.0056, 0.7649, 0.0068, 0.0036, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 91, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0095, 0.0094, 0.3830, 0.0082, 0.5564, 0.0088, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.048

[Epoch: 91, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 3
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1058e-02, 4.4661e-01, 4.9223e-03, 4.8636e-01, 5.9977e-06, 8.4287e-03,
        3.2614e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.008

[Epoch: 91, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0072, 0.0038, 0.2007, 0.5174, 0.0257, 0.2288, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 91, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3505e-01, 7.5642e-05, 3.1571e-03, 1.4717e-02, 7.3444e-03, 2.3249e-01,
        7.1649e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.021

[Epoch: 91, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.1873, 0.0052, 0.7861, 0.0079, 0.0045, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 92, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0120, 0.0117, 0.4007, 0.0049, 0.5444, 0.0048, 0.0214],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.019

[Epoch: 92, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1178e-02, 6.2988e-01, 8.0695e-03, 3.0201e-01, 4.9680e-06, 9.6374e-03,
        2.9226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.007

[Epoch: 92, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0087, 0.0041, 0.2638, 0.4481, 0.0247, 0.2350, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 92, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9110e-01, 2.4821e-05, 2.9519e-03, 1.7258e-02, 4.0801e-03, 2.8058e-01,
        4.0040e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 92, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.2117, 0.0052, 0.7673, 0.0053, 0.0030, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 93, batch: 40/204] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0038, 0.0055, 0.3390, 0.0063, 0.6252, 0.0052, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 93, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7654e-02, 6.5162e-01, 4.4688e-03, 2.9671e-01, 4.7613e-06, 5.3569e-03,
        2.4187e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.009

[Epoch: 93, batch: 120/204] total loss per batch: 0.837
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0082, 0.0043, 0.1951, 0.5476, 0.0314, 0.1810, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 93, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7026e-01, 6.7997e-05, 1.0335e-02, 2.6497e-02, 1.1631e-02, 2.6880e-01,
        1.2404e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 93, batch: 200/204] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.2389, 0.0063, 0.7347, 0.0081, 0.0059, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 94, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0061, 0.4227, 0.0050, 0.5427, 0.0039, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.037

[Epoch: 94, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([9.4340e-03, 6.6442e-01, 4.1570e-03, 2.9883e-01, 1.7887e-06, 3.4807e-03,
        1.9674e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.008

[Epoch: 94, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0103, 0.0057, 0.2449, 0.4924, 0.0215, 0.2029, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 94, batch: 160/204] total loss per batch: 0.874
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8018e-01, 6.3833e-05, 6.8187e-03, 2.3412e-02, 8.7875e-03, 2.7595e-01,
        4.7924e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.031

[Epoch: 94, batch: 200/204] total loss per batch: 0.858
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.1679, 0.0080, 0.7987, 0.0086, 0.0061, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 95, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0067, 0.3684, 0.0045, 0.5899, 0.0035, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 95, batch: 80/204] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.0912e-02, 6.2557e-01, 3.3826e-03, 3.3853e-01, 2.3511e-06, 3.2094e-03,
        1.8388e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.008

[Epoch: 95, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0037, 0.2499, 0.5118, 0.0236, 0.1811, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 95, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1818e-01, 7.2927e-05, 5.3332e-03, 1.3421e-02, 5.1473e-03, 2.4311e-01,
        1.4734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.026

[Epoch: 95, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0081, 0.2450, 0.0075, 0.7201, 0.0058, 0.0067, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 96, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0093, 0.3907, 0.0106, 0.5537, 0.0059, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.048

[Epoch: 96, batch: 80/204] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5593e-02, 6.4339e-01, 6.5392e-03, 3.0151e-01, 1.9869e-06, 4.4080e-03,
        2.8557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.010

[Epoch: 96, batch: 120/204] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0079, 0.0036, 0.1916, 0.4984, 0.0254, 0.2533, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 96, batch: 160/204] total loss per batch: 0.871
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6996e-01, 5.6107e-05, 6.7613e-03, 1.9402e-02, 6.1572e-03, 2.9044e-01,
        7.2200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 96, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.2209, 0.0111, 0.7310, 0.0163, 0.0065, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 97, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0083, 0.0071, 0.4473, 0.0062, 0.5038, 0.0074, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 97, batch: 80/204] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5841e-02, 5.8938e-01, 4.0435e-03, 3.6005e-01, 4.0125e-06, 6.2925e-03,
        2.4392e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.008

[Epoch: 97, batch: 120/204] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0164, 0.0076, 0.2264, 0.4691, 0.0320, 0.2188, 0.0297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 97, batch: 160/204] total loss per batch: 0.871
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5138e-01, 5.8392e-05, 4.7170e-03, 1.7781e-02, 5.6973e-03, 2.0962e-01,
        1.0745e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 97, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.1934, 0.0047, 0.7731, 0.0046, 0.0088, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.027

[Epoch: 98, batch: 40/204] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0061, 0.3709, 0.0061, 0.5834, 0.0045, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.049

[Epoch: 98, batch: 80/204] total loss per batch: 0.883
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.4789e-02, 5.0719e-01, 1.2144e-02, 4.1932e-01, 6.6000e-06, 9.3368e-03,
        2.7213e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.009

[Epoch: 98, batch: 120/204] total loss per batch: 0.834
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0053, 0.2334, 0.5126, 0.0216, 0.1973, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 98, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.2241e-01, 4.8726e-05, 6.3890e-03, 1.9987e-02, 6.1825e-03, 3.3863e-01,
        6.3590e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.040

[Epoch: 98, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.2486, 0.0048, 0.7296, 0.0064, 0.0035, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 99, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0062, 0.0061, 0.4109, 0.0052, 0.5477, 0.0040, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 99, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2898e-02, 6.3641e-01, 5.2557e-03, 3.1095e-01, 2.4754e-06, 6.9081e-03,
        2.7582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.009

[Epoch: 99, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0108, 0.0051, 0.1922, 0.5149, 0.0208, 0.2336, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 99, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.8813e-01, 4.6841e-05, 3.4034e-03, 2.1271e-02, 5.3985e-03, 1.7732e-01,
        4.4291e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 99, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.1766, 0.0058, 0.7991, 0.0045, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.026

[Epoch: 100, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0036, 0.0044, 0.4082, 0.0064, 0.5550, 0.0033, 0.0192],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.043

[Epoch: 100, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6578e-02, 6.4637e-01, 6.0806e-03, 3.0022e-01, 3.3680e-06, 6.7868e-03,
        2.3961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.009

[Epoch: 100, batch: 120/204] total loss per batch: 0.836
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0115, 0.0033, 0.2551, 0.4541, 0.0331, 0.2242, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 100, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.4774e-01, 4.5728e-05, 7.3281e-03, 1.9474e-02, 5.9338e-03, 3.1031e-01,
        9.1666e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.024

[Epoch: 100, batch: 200/204] total loss per batch: 0.856
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.2316, 0.0057, 0.7417, 0.0086, 0.0049, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 101, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0064, 0.3625, 0.0063, 0.5981, 0.0045, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.038

[Epoch: 101, batch: 80/204] total loss per batch: 0.884
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([7.8158e-03, 6.6361e-01, 2.5871e-03, 3.1016e-01, 1.6046e-06, 2.8174e-03,
        1.3015e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.010

[Epoch: 101, batch: 120/204] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0085, 0.0039, 0.1749, 0.5610, 0.0316, 0.1990, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 101, batch: 160/204] total loss per batch: 0.872
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0127e-01, 6.8375e-05, 7.4003e-03, 2.8457e-02, 9.6429e-03, 2.4362e-01,
        9.5366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 101, batch: 200/204] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.1741, 0.0074, 0.7860, 0.0112, 0.0071, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 102, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0094, 0.0085, 0.4056, 0.0074, 0.5348, 0.0067, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.048

[Epoch: 102, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2092e-02, 6.2212e-01, 6.1416e-03, 3.2166e-01, 2.3549e-06, 7.7743e-03,
        3.0212e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 102, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0107, 0.0052, 0.2320, 0.4752, 0.0292, 0.2287, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 102, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9227e-01, 8.2595e-05, 8.9074e-03, 2.2436e-02, 7.3679e-03, 2.6157e-01,
        7.3674e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.031

[Epoch: 102, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2219, 0.0052, 0.7549, 0.0040, 0.0049, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 103, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0068, 0.0070, 0.3958, 0.0069, 0.5549, 0.0059, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.052

[Epoch: 103, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6194e-02, 5.0581e-01, 7.3092e-03, 4.3509e-01, 3.5128e-06, 8.3532e-03,
        2.7242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.010

[Epoch: 103, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0105, 0.0049, 0.2224, 0.4929, 0.0239, 0.2267, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 103, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3773e-01, 3.0062e-05, 3.8672e-03, 1.4268e-02, 4.9079e-03, 2.3360e-01,
        5.5941e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 103, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.1919, 0.0047, 0.7856, 0.0052, 0.0049, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 104, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0051, 0.0045, 0.4003, 0.0034, 0.5649, 0.0047, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 104, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4567e-02, 5.9617e-01, 6.9174e-03, 3.5169e-01, 2.1915e-06, 6.8782e-03,
        2.3781e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 104, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0096, 0.0042, 0.2343, 0.4988, 0.0246, 0.2079, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 104, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6477e-01, 2.6708e-05, 4.4396e-03, 1.7922e-02, 4.5948e-03, 3.0437e-01,
        3.8763e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 104, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.2105, 0.0050, 0.7642, 0.0053, 0.0054, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 105, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0045, 0.3923, 0.0049, 0.5705, 0.0041, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.042

[Epoch: 105, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2569e-02, 7.1196e-01, 4.5146e-03, 2.4666e-01, 1.2407e-06, 3.6143e-03,
        2.0684e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 105, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0044, 0.2181, 0.4961, 0.0290, 0.2217, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 105, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1218e-01, 3.2927e-05, 5.3390e-03, 2.2546e-02, 6.5865e-03, 2.4506e-01,
        8.2460e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 105, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2073, 0.0071, 0.7600, 0.0070, 0.0070, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 106, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0080, 0.0076, 0.3875, 0.0079, 0.5594, 0.0071, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.040

[Epoch: 106, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4485e-02, 6.3832e-01, 4.2800e-03, 3.1032e-01, 2.2343e-06, 6.6006e-03,
        2.5992e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 106, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0055, 0.2215, 0.4911, 0.0248, 0.2243, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 106, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5218e-01, 5.5275e-05, 6.2589e-03, 2.5533e-02, 8.3679e-03, 3.0206e-01,
        5.5494e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 106, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.2049, 0.0059, 0.7628, 0.0069, 0.0061, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 107, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0088, 0.0099, 0.3889, 0.0056, 0.5583, 0.0075, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.047

[Epoch: 107, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8371e-02, 5.8693e-01, 6.4776e-03, 3.5381e-01, 1.3083e-06, 8.7525e-03,
        2.5657e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 107, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0044, 0.2191, 0.5146, 0.0251, 0.2095, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 107, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.9650e-01, 2.8596e-05, 3.4827e-03, 1.3359e-02, 3.9615e-03, 1.7700e-01,
        5.6672e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 107, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.2313, 0.0054, 0.7462, 0.0039, 0.0040, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 108, batch: 40/204] total loss per batch: 0.849
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0038, 0.3955, 0.0035, 0.5682, 0.0040, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.027

[Epoch: 108, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8799e-02, 5.5015e-01, 7.5192e-03, 3.8941e-01, 2.6129e-06, 7.2134e-03,
        2.6902e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 108, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0080, 0.0035, 0.2178, 0.5130, 0.0199, 0.2213, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 108, batch: 160/204] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.2453e-01, 1.6833e-05, 4.5491e-03, 1.5493e-02, 4.4211e-03, 3.4765e-01,
        3.3380e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.048

[Epoch: 108, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0030, 0.1886, 0.0050, 0.7913, 0.0046, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 109, batch: 40/204] total loss per batch: 0.849
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0040, 0.4341, 0.0056, 0.5292, 0.0041, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 109, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2410e-02, 6.1633e-01, 4.6584e-03, 3.4592e-01, 1.4129e-06, 3.7069e-03,
        1.6974e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 109, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0047, 0.2454, 0.4785, 0.0324, 0.2033, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 109, batch: 160/204] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3827e-01, 4.8515e-05, 7.3043e-03, 3.3824e-02, 8.6507e-03, 2.0315e-01,
        8.7562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.028

[Epoch: 109, batch: 200/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.2212, 0.0074, 0.7405, 0.0082, 0.0083, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 110, batch: 40/204] total loss per batch: 0.849
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0077, 0.0075, 0.3522, 0.0063, 0.6021, 0.0050, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.042

[Epoch: 110, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2055e-02, 6.5173e-01, 4.4334e-03, 3.0454e-01, 1.7573e-06, 4.9336e-03,
        2.2307e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 110, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0120, 0.0071, 0.2357, 0.4689, 0.0249, 0.2289, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 110, batch: 160/204] total loss per batch: 0.870
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.2186e-01, 6.7138e-05, 5.5730e-03, 2.3693e-02, 7.5250e-03, 3.3552e-01,
        5.7621e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.043

[Epoch: 110, batch: 200/204] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.2085, 0.0044, 0.7682, 0.0051, 0.0053, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 111, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0061, 0.0052, 0.3934, 0.0064, 0.5627, 0.0063, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.045

[Epoch: 111, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.1121e-02, 5.9030e-01, 7.5639e-03, 3.3640e-01, 4.5179e-06, 9.2371e-03,
        3.5376e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 111, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0106, 0.0046, 0.2248, 0.5489, 0.0219, 0.1719, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 111, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.8804e-01, 3.2781e-05, 3.1023e-03, 1.1362e-02, 5.6941e-03, 1.8530e-01,
        6.4764e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.049

[Epoch: 111, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.2032, 0.0046, 0.7764, 0.0047, 0.0048, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 112, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0051, 0.0056, 0.4159, 0.0034, 0.5477, 0.0043, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 112, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4664e-02, 6.4526e-01, 5.7714e-03, 3.0352e-01, 2.3841e-06, 5.6569e-03,
        2.5121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.011

[Epoch: 112, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0042, 0.2162, 0.4793, 0.0232, 0.2483, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 112, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.9921e-01, 3.5862e-05, 5.9204e-03, 1.9937e-02, 5.6820e-03, 3.6317e-01,
        6.0469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.044

[Epoch: 112, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.2058, 0.0067, 0.7696, 0.0049, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 113, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0040, 0.0045, 0.3792, 0.0067, 0.5775, 0.0050, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 113, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4268e-02, 5.9642e-01, 5.2873e-03, 3.5833e-01, 2.6563e-06, 4.8735e-03,
        2.0820e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 113, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0122, 0.0050, 0.2403, 0.4885, 0.0300, 0.2008, 0.0233],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 113, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5221e-01, 5.3301e-05, 6.2052e-03, 2.4540e-02, 8.7919e-03, 2.0042e-01,
        7.7778e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.043

[Epoch: 113, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.1869, 0.0073, 0.7847, 0.0069, 0.0059, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 114, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0076, 0.0059, 0.3993, 0.0075, 0.5525, 0.0077, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 114, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3680e-02, 6.0745e-01, 6.3261e-03, 3.4163e-01, 3.0358e-06, 5.4126e-03,
        2.5495e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 114, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0046, 0.2196, 0.4910, 0.0271, 0.2289, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 114, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7080e-01, 7.3325e-05, 6.7246e-03, 2.4300e-02, 6.2200e-03, 2.8527e-01,
        6.6130e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.040

[Epoch: 114, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2344, 0.0052, 0.7399, 0.0060, 0.0043, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 115, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0056, 0.0061, 0.3895, 0.0060, 0.5657, 0.0052, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.042

[Epoch: 115, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.3677e-02, 5.9518e-01, 6.8825e-03, 3.2796e-01, 3.7454e-06, 9.5958e-03,
        3.6702e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 115, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0047, 0.2450, 0.4807, 0.0247, 0.2133, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 115, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2828e-01, 3.1754e-05, 3.7869e-03, 1.7763e-02, 6.9970e-03, 2.3742e-01,
        5.7184e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.047

[Epoch: 115, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.2146, 0.0065, 0.7594, 0.0050, 0.0050, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 116, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0050, 0.3940, 0.0061, 0.5563, 0.0069, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 116, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1927e-02, 5.7423e-01, 5.9759e-03, 3.8518e-01, 1.8719e-06, 4.9295e-03,
        1.7752e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 116, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0043, 0.2050, 0.5590, 0.0233, 0.1790, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 116, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1773e-01, 4.1851e-05, 4.5785e-03, 1.9172e-02, 4.5151e-03, 2.5002e-01,
        3.9407e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 116, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0032, 0.1942, 0.0030, 0.7881, 0.0046, 0.0039, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 117, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0051, 0.0049, 0.4207, 0.0038, 0.5469, 0.0047, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.036

[Epoch: 117, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5809e-02, 6.6315e-01, 5.2774e-03, 2.8225e-01, 1.8755e-06, 6.5061e-03,
        2.7009e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 117, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0095, 0.0047, 0.2378, 0.4560, 0.0269, 0.2453, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 117, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.3705e-01, 3.0817e-05, 5.9260e-03, 2.0226e-02, 6.7031e-03, 3.2410e-01,
        5.9616e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.047

[Epoch: 117, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.2546, 0.0080, 0.7139, 0.0074, 0.0059, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 118, batch: 40/204] total loss per batch: 0.850
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0043, 0.0052, 0.3497, 0.0039, 0.6088, 0.0037, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 118, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2694e-02, 6.7888e-01, 4.3099e-03, 2.7738e-01, 2.3491e-06, 6.7784e-03,
        1.9958e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 118, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0107, 0.0042, 0.2217, 0.5106, 0.0254, 0.2085, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 118, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.6151e-01, 3.9464e-05, 4.8480e-03, 1.9216e-02, 7.9313e-03, 2.0109e-01,
        5.3589e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 118, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.1739, 0.0060, 0.7928, 0.0065, 0.0068, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 119, batch: 40/204] total loss per batch: 0.852
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0075, 0.0071, 0.4672, 0.0108, 0.4728, 0.0102, 0.0244],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 119, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4556e-02, 6.5523e-01, 5.6850e-03, 2.8485e-01, 1.7285e-06, 6.0190e-03,
        3.3662e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 119, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0117, 0.0052, 0.2095, 0.5247, 0.0246, 0.2045, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 119, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.3641e-01, 4.0027e-05, 6.4060e-03, 2.4619e-02, 8.9906e-03, 3.1785e-01,
        5.6914e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.047

[Epoch: 119, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2353, 0.0063, 0.7366, 0.0050, 0.0054, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 120, batch: 40/204] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0069, 0.3467, 0.0042, 0.6175, 0.0052, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.056

[Epoch: 120, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7359e-02, 5.2945e-01, 8.4369e-03, 4.0905e-01, 4.1953e-06, 6.9198e-03,
        2.8784e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 120, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0141, 0.0060, 0.2286, 0.4983, 0.0256, 0.2076, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 120, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5598e-01, 2.7419e-05, 3.8078e-03, 1.4296e-02, 6.4511e-03, 2.1594e-01,
        3.5024e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.040

[Epoch: 120, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0080, 0.1850, 0.0038, 0.7888, 0.0045, 0.0052, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 121, batch: 40/204] total loss per batch: 0.851
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0053, 0.0041, 0.3986, 0.0052, 0.5618, 0.0035, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.014

[Epoch: 121, batch: 80/204] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6246e-02, 5.0915e-01, 9.5073e-03, 4.2876e-01, 2.0641e-06, 7.7314e-03,
        2.8607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 121, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0086, 0.0030, 0.2566, 0.4967, 0.0213, 0.1973, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 121, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5886e-01, 2.2602e-05, 4.2864e-03, 2.3306e-02, 6.3704e-03, 3.0209e-01,
        5.0722e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.047

[Epoch: 121, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.1956, 0.0075, 0.7764, 0.0060, 0.0048, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 122, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0049, 0.4250, 0.0054, 0.5333, 0.0040, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 122, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7719e-02, 6.8344e-01, 6.9889e-03, 2.5346e-01, 3.8755e-06, 6.9902e-03,
        3.1395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.012

[Epoch: 122, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0112, 0.0047, 0.1949, 0.4932, 0.0243, 0.2494, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.014

[Epoch: 122, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2171e-01, 3.5067e-05, 4.1280e-03, 2.0620e-02, 6.2067e-03, 2.4196e-01,
        5.3385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.044

[Epoch: 122, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2218, 0.0042, 0.7507, 0.0061, 0.0065, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.019

[Epoch: 123, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0035, 0.0054, 0.3577, 0.0037, 0.6123, 0.0029, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.042

[Epoch: 123, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3158e-02, 6.5954e-01, 4.1997e-03, 2.9838e-01, 1.4533e-06, 5.1383e-03,
        1.9588e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 123, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0119, 0.0045, 0.2253, 0.5121, 0.0261, 0.2029, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 123, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.4156e-01, 3.9394e-05, 6.7078e-03, 2.8846e-02, 9.5253e-03, 3.0751e-01,
        5.8118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.044

[Epoch: 123, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.2091, 0.0072, 0.7596, 0.0070, 0.0056, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 124, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0078, 0.4132, 0.0071, 0.5316, 0.0057, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 124, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3101e-02, 6.0131e-01, 6.0886e-03, 3.5014e-01, 1.5399e-06, 5.5602e-03,
        2.3798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 124, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0106, 0.0050, 0.2171, 0.5042, 0.0213, 0.2215, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 124, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5917e-01, 4.6343e-05, 5.8775e-03, 1.6371e-02, 5.2501e-03, 2.0700e-01,
        6.2914e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.045

[Epoch: 124, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.2129, 0.0047, 0.7640, 0.0035, 0.0062, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 125, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0055, 0.0065, 0.3803, 0.0061, 0.5774, 0.0049, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 125, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.2100e-02, 5.5756e-01, 5.6660e-03, 3.7331e-01, 3.4762e-06, 8.1070e-03,
        3.3258e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 125, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0107, 0.0052, 0.2327, 0.5194, 0.0241, 0.1884, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 125, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8122e-01, 2.1670e-05, 5.1363e-03, 1.6106e-02, 5.3899e-03, 2.8695e-01,
        5.1752e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.045

[Epoch: 125, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.2205, 0.0058, 0.7536, 0.0062, 0.0038, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.025

[Epoch: 126, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0050, 0.4119, 0.0039, 0.5533, 0.0035, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 126, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4631e-02, 5.8489e-01, 6.3006e-03, 3.6282e-01, 1.9759e-06, 6.9245e-03,
        2.4442e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 126, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0053, 0.2130, 0.4863, 0.0227, 0.2429, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 126, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1999e-01, 2.6617e-05, 4.8869e-03, 1.9262e-02, 5.1275e-03, 2.4601e-01,
        4.6995e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.044

[Epoch: 126, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.2018, 0.0045, 0.7752, 0.0052, 0.0050, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 127, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0040, 0.0043, 0.3705, 0.0042, 0.5964, 0.0038, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 127, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.0852e-02, 6.9723e-01, 4.2785e-03, 2.6424e-01, 1.4163e-06, 3.8889e-03,
        1.9506e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 127, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0112, 0.0047, 0.2312, 0.5184, 0.0290, 0.1835, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 127, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9205e-01, 2.4985e-05, 5.7656e-03, 1.8670e-02, 5.1825e-03, 2.7209e-01,
        6.2196e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 127, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.1965, 0.0051, 0.7825, 0.0058, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 128, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0053, 0.4289, 0.0051, 0.5314, 0.0039, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 128, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3801e-02, 6.8383e-01, 4.0005e-03, 2.6874e-01, 1.4974e-06, 6.0073e-03,
        2.3620e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 128, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0083, 0.0040, 0.2525, 0.4535, 0.0265, 0.2341, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 128, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5341e-01, 5.4350e-05, 7.0540e-03, 2.5561e-02, 8.9605e-03, 2.9762e-01,
        7.3447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 128, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0080, 0.2175, 0.0070, 0.7426, 0.0113, 0.0074, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 129, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0067, 0.3895, 0.0084, 0.5538, 0.0087, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.037

[Epoch: 129, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6937e-02, 6.0303e-01, 5.9256e-03, 3.3844e-01, 4.4972e-06, 5.5098e-03,
        3.0153e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 129, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0114, 0.0054, 0.1927, 0.5259, 0.0251, 0.2204, 0.0192],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.015

[Epoch: 129, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.7682e-01, 2.9865e-05, 4.1742e-03, 1.6010e-02, 5.9244e-03, 1.9074e-01,
        6.3018e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 129, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.2386, 0.0059, 0.7318, 0.0059, 0.0049, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 130, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0094, 0.0082, 0.3933, 0.0064, 0.5542, 0.0068, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 130, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7302e-02, 5.5709e-01, 4.6996e-03, 3.9312e-01, 3.1320e-06, 6.0928e-03,
        2.1689e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 130, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0101, 0.0061, 0.2411, 0.4949, 0.0191, 0.2102, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 130, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6087e-01, 3.8623e-05, 5.1385e-03, 1.6829e-02, 5.5880e-03, 3.0503e-01,
        6.5032e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.045

[Epoch: 130, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.1661, 0.0050, 0.8140, 0.0045, 0.0040, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 131, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0038, 0.0042, 0.3962, 0.0046, 0.5709, 0.0066, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.039

[Epoch: 131, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7982e-02, 5.9989e-01, 7.1812e-03, 3.4007e-01, 1.9271e-06, 8.3512e-03,
        2.6529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 131, batch: 120/204] total loss per batch: 0.833
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0096, 0.0030, 0.2059, 0.5153, 0.0239, 0.2213, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 131, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1681e-01, 2.0827e-05, 4.8926e-03, 1.8534e-02, 5.2616e-03, 2.5024e-01,
        4.2444e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 131, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.2814, 0.0058, 0.6900, 0.0056, 0.0054, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 132, batch: 40/204] total loss per batch: 0.848
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0046, 0.0079, 0.3840, 0.0043, 0.5742, 0.0043, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.017

[Epoch: 132, batch: 80/204] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.0860e-02, 6.0209e-01, 5.7302e-03, 3.5815e-01, 1.6853e-06, 4.2054e-03,
        1.8965e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 132, batch: 120/204] total loss per batch: 0.832
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0044, 0.2286, 0.5174, 0.0242, 0.1997, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 132, batch: 160/204] total loss per batch: 0.869
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6591e-01, 4.9159e-05, 5.7962e-03, 1.8212e-02, 5.0314e-03, 2.9527e-01,
        9.7351e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.041

[Epoch: 132, batch: 200/204] total loss per batch: 0.853
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.1378, 0.0061, 0.8298, 0.0090, 0.0071, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 133, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0040, 0.0044, 0.3903, 0.0061, 0.5678, 0.0050, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 133, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2268e-02, 6.4006e-01, 4.1048e-03, 3.1589e-01, 1.9353e-06, 5.6960e-03,
        2.1980e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 133, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0050, 0.2435, 0.4732, 0.0318, 0.2142, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 133, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0558e-01, 6.3683e-05, 6.2419e-03, 2.8887e-02, 8.8826e-03, 2.4459e-01,
        5.7564e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 133, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2737, 0.0064, 0.6956, 0.0070, 0.0067, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 134, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0078, 0.0085, 0.4237, 0.0065, 0.5239, 0.0063, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.040

[Epoch: 134, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.2866e-02, 6.1968e-01, 6.0385e-03, 3.0750e-01, 4.4030e-06, 7.8534e-03,
        3.6061e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.013

[Epoch: 134, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0050, 0.2213, 0.5060, 0.0227, 0.2133, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.012

[Epoch: 134, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8928e-01, 4.1599e-05, 4.8467e-03, 2.2574e-02, 5.6408e-03, 2.7147e-01,
        6.1508e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 134, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.1553, 0.0044, 0.8214, 0.0046, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 135, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0053, 0.0051, 0.3767, 0.0053, 0.5808, 0.0047, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 135, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4597e-02, 5.5660e-01, 4.7016e-03, 3.8873e-01, 1.8559e-06, 5.9960e-03,
        2.9373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 135, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0105, 0.0043, 0.2270, 0.5056, 0.0254, 0.2066, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 135, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4787e-01, 3.1436e-05, 4.1186e-03, 1.8412e-02, 4.5741e-03, 2.2116e-01,
        3.8342e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 135, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.2586, 0.0051, 0.7126, 0.0089, 0.0059, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 136, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0045, 0.0066, 0.3590, 0.0060, 0.5994, 0.0043, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 136, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2392e-02, 6.4095e-01, 3.9912e-03, 3.1907e-01, 1.8173e-06, 4.0292e-03,
        1.9563e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 136, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0095, 0.0038, 0.2093, 0.5185, 0.0253, 0.2119, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 136, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.1959e-01, 4.5158e-05, 6.2594e-03, 2.2732e-02, 6.3637e-03, 3.3717e-01,
        7.8348e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 136, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.1940, 0.0083, 0.7752, 0.0056, 0.0054, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 137, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0051, 0.4360, 0.0058, 0.5194, 0.0047, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 137, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5402e-02, 6.4510e-01, 4.6117e-03, 3.0456e-01, 3.3802e-06, 5.1277e-03,
        2.5197e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 137, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0057, 0.2523, 0.4656, 0.0219, 0.2278, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 137, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.5449e-01, 6.4295e-05, 5.5534e-03, 2.2253e-02, 6.2450e-03, 2.0391e-01,
        7.4830e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 137, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.2063, 0.0064, 0.7631, 0.0065, 0.0064, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 138, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0069, 0.0070, 0.3642, 0.0049, 0.5929, 0.0055, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.042

[Epoch: 138, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7517e-02, 6.0499e-01, 6.1381e-03, 3.3204e-01, 1.9963e-06, 7.9121e-03,
        3.1401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 138, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0111, 0.0048, 0.2029, 0.5263, 0.0285, 0.2050, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 138, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.4629e-01, 4.6833e-05, 5.2878e-03, 1.6602e-02, 5.7676e-03, 3.2099e-01,
        5.0134e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 138, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.2153, 0.0051, 0.7611, 0.0046, 0.0045, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 139, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0041, 0.4446, 0.0047, 0.5167, 0.0046, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 139, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9171e-02, 5.8182e-01, 6.8047e-03, 3.5603e-01, 2.3748e-06, 8.9199e-03,
        2.7250e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 139, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0090, 0.0040, 0.2438, 0.4861, 0.0218, 0.2163, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 139, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([8.3213e-01, 2.7411e-05, 4.0973e-03, 1.1912e-02, 3.8856e-03, 1.4237e-01,
        5.5718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 139, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.2055, 0.0056, 0.7710, 0.0051, 0.0047, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 140, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0034, 0.0047, 0.3611, 0.0027, 0.6091, 0.0033, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 140, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4493e-02, 6.2727e-01, 6.5924e-03, 3.2400e-01, 2.2148e-06, 4.6506e-03,
        2.2989e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 140, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0103, 0.0048, 0.2332, 0.4886, 0.0257, 0.2140, 0.0234],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 140, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.0339e-01, 2.8463e-05, 5.3708e-03, 2.6313e-02, 6.7301e-03, 4.5216e-01,
        6.0062e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.027

[Epoch: 140, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.2154, 0.0067, 0.7498, 0.0089, 0.0069, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 141, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0066, 0.0080, 0.4156, 0.0085, 0.5334, 0.0064, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.036

[Epoch: 141, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3284e-02, 6.2822e-01, 3.4508e-03, 3.2756e-01, 1.9033e-06, 4.6541e-03,
        2.2829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 141, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0113, 0.0055, 0.2036, 0.5012, 0.0281, 0.2255, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 141, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.9918e-01, 3.9507e-05, 5.3114e-03, 2.4637e-02, 6.8126e-03, 1.5493e-01,
        9.0876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 141, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.1954, 0.0066, 0.7727, 0.0063, 0.0062, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 142, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0061, 0.3585, 0.0061, 0.6004, 0.0052, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.038

[Epoch: 142, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([2.0063e-02, 6.1776e-01, 6.1304e-03, 3.2716e-01, 4.7683e-06, 4.6915e-03,
        2.4188e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 142, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0115, 0.0046, 0.2399, 0.4799, 0.0216, 0.2220, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.019

[Epoch: 142, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8777e-01, 4.0906e-05, 4.7336e-03, 1.6683e-02, 7.1061e-03, 2.7660e-01,
        7.0688e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 142, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0081, 0.1996, 0.0051, 0.7748, 0.0044, 0.0048, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 143, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0071, 0.4395, 0.0059, 0.5106, 0.0074, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.043

[Epoch: 143, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3803e-02, 6.0057e-01, 3.9928e-03, 3.4835e-01, 2.5306e-06, 6.9308e-03,
        2.6353e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.014

[Epoch: 143, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0119, 0.0057, 0.1964, 0.5374, 0.0228, 0.2060, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.016

[Epoch: 143, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1063e-01, 2.6537e-05, 4.4970e-03, 2.0602e-02, 5.0041e-03, 2.5416e-01,
        5.0803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.044

[Epoch: 143, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0031, 0.2212, 0.0052, 0.7552, 0.0060, 0.0055, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 144, batch: 40/204] total loss per batch: 0.847
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0046, 0.0055, 0.3624, 0.0039, 0.6028, 0.0039, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 144, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8881e-02, 6.4229e-01, 8.6850e-03, 2.9095e-01, 2.7876e-06, 7.4830e-03,
        3.1703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 144, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0098, 0.0040, 0.2601, 0.4790, 0.0249, 0.2066, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.017

[Epoch: 144, batch: 160/204] total loss per batch: 0.868
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9624e-01, 3.3144e-05, 4.8610e-03, 1.8096e-02, 5.9194e-03, 2.6972e-01,
        5.1325e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.026

[Epoch: 144, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.1731, 0.0040, 0.8050, 0.0059, 0.0040, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 145, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0050, 0.4087, 0.0052, 0.5563, 0.0036, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 145, batch: 80/204] total loss per batch: 0.880
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.0915e-02, 5.7731e-01, 2.9469e-03, 3.8874e-01, 1.2523e-06, 3.5486e-03,
        1.6535e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 145, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0116, 0.0059, 0.2192, 0.4795, 0.0268, 0.2354, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 145, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7279e-01, 3.3289e-05, 5.8399e-03, 2.3764e-02, 7.4451e-03, 2.8332e-01,
        6.8031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.032

[Epoch: 145, batch: 200/204] total loss per batch: 0.852
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2793, 0.0057, 0.6941, 0.0060, 0.0055, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 146, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0069, 0.4213, 0.0051, 0.5343, 0.0046, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.043

[Epoch: 146, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6066e-02, 6.7742e-01, 5.7328e-03, 2.6909e-01, 3.2194e-06, 6.1477e-03,
        2.5541e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.016

[Epoch: 146, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0043, 0.2129, 0.5254, 0.0228, 0.2041, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 146, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1325e-01, 4.4593e-05, 6.0670e-03, 2.0302e-02, 6.8374e-03, 2.4674e-01,
        6.7594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.026

[Epoch: 146, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.1588, 0.0046, 0.8152, 0.0052, 0.0067, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 147, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0047, 0.3646, 0.0062, 0.5977, 0.0057, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.041

[Epoch: 147, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4916e-02, 5.6406e-01, 5.7792e-03, 3.8644e-01, 2.6060e-06, 7.4901e-03,
        2.1308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.016

[Epoch: 147, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0126, 0.0051, 0.2272, 0.4722, 0.0268, 0.2315, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.018

[Epoch: 147, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9681e-01, 4.0955e-05, 4.6406e-03, 2.1848e-02, 6.7067e-03, 2.6011e-01,
        9.8456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.031

[Epoch: 147, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.2432, 0.0068, 0.7299, 0.0049, 0.0056, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 148, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0067, 0.4024, 0.0039, 0.5552, 0.0044, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 148, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4472e-02, 6.0284e-01, 5.1992e-03, 3.4651e-01, 1.8440e-06, 4.9794e-03,
        2.5999e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.015

[Epoch: 148, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0043, 0.2251, 0.5139, 0.0192, 0.2120, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 148, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9004e-01, 3.1015e-05, 4.4247e-03, 1.5528e-02, 4.6612e-03, 2.8051e-01,
        4.8066e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.026

[Epoch: 148, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.1998, 0.0048, 0.7764, 0.0055, 0.0057, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 149, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0037, 0.4282, 0.0051, 0.5317, 0.0051, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.038

[Epoch: 149, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6225e-02, 6.2761e-01, 6.5453e-03, 3.1839e-01, 3.3987e-06, 6.9810e-03,
        2.4248e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.016

[Epoch: 149, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0040, 0.2121, 0.5131, 0.0246, 0.2171, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 149, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1250e-01, 2.3967e-05, 5.0922e-03, 2.3362e-02, 5.1646e-03, 2.4841e-01,
        5.4536e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.032

[Epoch: 149, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.1874, 0.0052, 0.7869, 0.0072, 0.0050, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.024

[Epoch: 150, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0062, 0.3824, 0.0045, 0.5758, 0.0046, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.037

[Epoch: 150, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2383e-02, 6.2797e-01, 3.9222e-03, 3.3201e-01, 1.2667e-06, 3.4745e-03,
        2.0241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.016

[Epoch: 150, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0095, 0.0039, 0.2278, 0.4967, 0.0284, 0.2165, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 150, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9927e-01, 3.4159e-05, 5.4196e-03, 2.2581e-02, 7.4406e-03, 2.5726e-01,
        7.9914e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 150, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.2485, 0.0059, 0.7163, 0.0079, 0.0085, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 151, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0062, 0.0061, 0.3615, 0.0059, 0.5981, 0.0052, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 151, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7071e-02, 5.9639e-01, 5.8951e-03, 3.4360e-01, 3.6183e-06, 7.0338e-03,
        3.0012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.016

[Epoch: 151, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0111, 0.0052, 0.2319, 0.5001, 0.0218, 0.2055, 0.0244],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 151, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6269e-01, 3.3089e-05, 6.8199e-03, 2.2027e-02, 6.2371e-03, 2.9606e-01,
        6.1304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 151, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.1860, 0.0057, 0.7897, 0.0054, 0.0047, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 152, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0056, 0.0057, 0.3904, 0.0049, 0.5693, 0.0054, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.040

[Epoch: 152, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6618e-02, 6.2452e-01, 4.6009e-03, 3.2176e-01, 2.4132e-06, 6.1001e-03,
        2.6391e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 152, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0108, 0.0045, 0.2307, 0.4876, 0.0252, 0.2219, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 152, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3222e-01, 2.0348e-05, 3.7724e-03, 1.6956e-02, 4.5883e-03, 2.3759e-01,
        4.8561e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 152, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.2133, 0.0054, 0.7626, 0.0060, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 153, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0056, 0.0046, 0.4068, 0.0037, 0.5542, 0.0043, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 153, batch: 80/204] total loss per batch: 0.877
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4353e-02, 6.0951e-01, 4.8632e-03, 3.4221e-01, 2.2336e-06, 4.8935e-03,
        2.4173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 153, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0105, 0.0038, 0.2120, 0.5171, 0.0244, 0.2101, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.000

[Epoch: 153, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6528e-01, 1.9071e-05, 5.1498e-03, 2.1187e-02, 5.1155e-03, 2.9766e-01,
        5.5850e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 153, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.2131, 0.0057, 0.7598, 0.0062, 0.0058, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 154, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0051, 0.0056, 0.3711, 0.0065, 0.5859, 0.0047, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 154, batch: 80/204] total loss per batch: 0.877
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3716e-02, 6.4364e-01, 4.5823e-03, 3.1030e-01, 1.7961e-06, 4.6918e-03,
        2.3071e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 154, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0098, 0.0051, 0.2357, 0.4855, 0.0249, 0.2178, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 154, batch: 160/204] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1656e-01, 2.1455e-05, 5.2880e-03, 2.1618e-02, 5.5029e-03, 2.4493e-01,
        6.0716e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.032

[Epoch: 154, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.2039, 0.0064, 0.7656, 0.0067, 0.0068, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 155, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0062, 0.0052, 0.4140, 0.0060, 0.5415, 0.0059, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 155, batch: 80/204] total loss per batch: 0.877
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4802e-02, 6.0538e-01, 4.8755e-03, 3.4662e-01, 1.9986e-06, 5.4074e-03,
        2.2911e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 155, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0118, 0.0052, 0.2271, 0.4873, 0.0268, 0.2211, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 155, batch: 160/204] total loss per batch: 0.864
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0441e-01, 1.8319e-05, 5.2870e-03, 1.8995e-02, 5.6358e-03, 2.6065e-01,
        5.0043e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 155, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.2155, 0.0052, 0.7570, 0.0061, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 156, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0061, 0.0055, 0.3892, 0.0046, 0.5700, 0.0052, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 156, batch: 80/204] total loss per batch: 0.877
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5426e-02, 5.9891e-01, 5.1230e-03, 3.4844e-01, 1.9374e-06, 6.6708e-03,
        2.5425e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 156, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0099, 0.0043, 0.2103, 0.5168, 0.0228, 0.2183, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 156, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0081e-01, 1.6636e-05, 4.0352e-03, 1.7564e-02, 4.9219e-03, 2.6802e-01,
        4.6304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 156, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.1866, 0.0047, 0.7924, 0.0042, 0.0043, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 157, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0047, 0.0056, 0.3986, 0.0052, 0.5592, 0.0047, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 157, batch: 80/204] total loss per batch: 0.877
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2745e-02, 6.0782e-01, 4.4726e-03, 3.4710e-01, 1.3983e-06, 4.6847e-03,
        2.3183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 157, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0090, 0.0033, 0.2350, 0.4944, 0.0245, 0.2156, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 157, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0163e-01, 1.1482e-05, 5.1648e-03, 2.1757e-02, 5.5293e-03, 2.6077e-01,
        5.1361e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 157, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.2347, 0.0057, 0.7382, 0.0064, 0.0051, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 158, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0048, 0.3958, 0.0052, 0.5678, 0.0042, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 158, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4499e-02, 6.6522e-01, 4.4808e-03, 2.8755e-01, 1.4293e-06, 4.3180e-03,
        2.3923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 158, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0055, 0.2219, 0.4865, 0.0285, 0.2242, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.001

[Epoch: 158, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7189e-01, 2.7456e-05, 5.9734e-03, 2.2947e-02, 7.2873e-03, 2.8504e-01,
        6.8339e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 158, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.1888, 0.0051, 0.7852, 0.0050, 0.0067, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 159, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0060, 0.3628, 0.0078, 0.5929, 0.0052, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.043

[Epoch: 159, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6378e-02, 6.0578e-01, 6.5694e-03, 3.3843e-01, 2.5307e-06, 6.2159e-03,
        2.6625e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 159, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0118, 0.0069, 0.2327, 0.4517, 0.0274, 0.2442, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.006

[Epoch: 159, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0527e-01, 2.2310e-05, 5.4048e-03, 1.9337e-02, 5.4018e-03, 2.5870e-01,
        5.8656e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 159, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2233, 0.0056, 0.7511, 0.0053, 0.0056, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 160, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0064, 0.4492, 0.0056, 0.5044, 0.0066, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 160, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9706e-02, 5.2639e-01, 6.7535e-03, 4.1314e-01, 2.2049e-06, 8.5682e-03,
        2.5441e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 160, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0106, 0.0036, 0.2197, 0.5445, 0.0227, 0.1784, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 160, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2738e-01, 2.0646e-05, 3.9504e-03, 1.4807e-02, 5.5677e-03, 2.4379e-01,
        4.4834e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 160, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2250, 0.0038, 0.7527, 0.0059, 0.0043, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 161, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0042, 0.0056, 0.3699, 0.0050, 0.5903, 0.0037, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 161, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4083e-02, 6.4568e-01, 4.6903e-03, 3.0278e-01, 2.6490e-06, 5.9445e-03,
        2.6817e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 161, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0090, 0.0045, 0.2321, 0.5086, 0.0234, 0.2049, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.000

[Epoch: 161, batch: 160/204] total loss per batch: 0.867
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7226e-01, 1.4565e-05, 5.2042e-03, 1.9636e-02, 6.4362e-03, 2.9077e-01,
        5.6793e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.046

[Epoch: 161, batch: 200/204] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.1848, 0.0051, 0.7901, 0.0057, 0.0048, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 162, batch: 40/204] total loss per batch: 0.846
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0053, 0.0056, 0.3859, 0.0067, 0.5713, 0.0066, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 162, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4967e-02, 6.2427e-01, 5.9185e-03, 3.2404e-01, 2.5940e-06, 5.5124e-03,
        2.5281e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 162, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0057, 0.2208, 0.5023, 0.0252, 0.2178, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.013

[Epoch: 162, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0448e-01, 3.1495e-05, 8.3181e-03, 2.4403e-02, 7.0102e-03, 2.4980e-01,
        5.9610e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.027

[Epoch: 162, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.2199, 0.0051, 0.7507, 0.0060, 0.0069, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 163, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0071, 0.0065, 0.4178, 0.0065, 0.5310, 0.0072, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 163, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3431e-02, 6.0842e-01, 5.4431e-03, 3.4193e-01, 2.2487e-06, 5.3620e-03,
        2.5419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 163, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0121, 0.0055, 0.2139, 0.4985, 0.0272, 0.2227, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 163, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9551e-01, 2.4646e-05, 4.2869e-03, 1.6850e-02, 4.6136e-03, 2.7397e-01,
        4.7491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.040

[Epoch: 163, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2015, 0.0048, 0.7762, 0.0044, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 164, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0037, 0.0057, 0.3819, 0.0035, 0.5866, 0.0036, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 164, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5692e-02, 6.2349e-01, 6.1333e-03, 3.1949e-01, 2.2071e-06, 7.2907e-03,
        2.7900e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 164, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0083, 0.0045, 0.2324, 0.4939, 0.0229, 0.2193, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 164, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2456e-01, 2.0626e-05, 5.2314e-03, 1.8428e-02, 5.1214e-03, 2.4150e-01,
        5.1395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 164, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.2117, 0.0051, 0.7655, 0.0051, 0.0047, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 165, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0048, 0.0049, 0.3882, 0.0043, 0.5728, 0.0040, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 165, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3980e-02, 6.2607e-01, 4.3459e-03, 3.2919e-01, 1.8512e-06, 4.2893e-03,
        2.2126e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 165, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0048, 0.2086, 0.5284, 0.0245, 0.2009, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.010

[Epoch: 165, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([5.9891e-01, 2.3965e-05, 7.5153e-03, 2.7867e-02, 6.7734e-03, 3.5124e-01,
        7.6653e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 165, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2235, 0.0066, 0.7443, 0.0066, 0.0072, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.019

[Epoch: 166, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0058, 0.0059, 0.4138, 0.0066, 0.5396, 0.0060, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 166, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5030e-02, 6.0308e-01, 5.4202e-03, 3.4736e-01, 2.1942e-06, 5.0869e-03,
        2.4023e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 166, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0060, 0.2301, 0.4799, 0.0280, 0.2244, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 166, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.9945e-01, 2.1858e-05, 4.1811e-03, 1.6202e-02, 5.9373e-03, 1.6999e-01,
        4.2179e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 166, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.2044, 0.0064, 0.7675, 0.0057, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 167, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0059, 0.0058, 0.3719, 0.0060, 0.5851, 0.0049, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 167, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7395e-02, 5.9357e-01, 6.9010e-03, 3.4532e-01, 2.4067e-06, 7.5145e-03,
        2.9293e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 167, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0046, 0.2275, 0.5046, 0.0217, 0.2137, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 167, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.3101e-01, 2.2398e-05, 5.7835e-03, 2.0176e-02, 8.5801e-03, 3.2936e-01,
        5.0663e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.049

[Epoch: 167, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2031, 0.0049, 0.7719, 0.0049, 0.0055, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 168, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0047, 0.0049, 0.4112, 0.0042, 0.5506, 0.0052, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 168, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2741e-02, 6.3331e-01, 4.1645e-03, 3.2299e-01, 1.3417e-06, 3.8360e-03,
        2.2958e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 168, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0092, 0.0045, 0.2104, 0.5076, 0.0258, 0.2196, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.004

[Epoch: 168, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4909e-01, 1.7361e-05, 5.1572e-03, 1.9175e-02, 8.1409e-03, 2.1336e-01,
        5.0568e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.043

[Epoch: 168, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.2169, 0.0048, 0.7621, 0.0039, 0.0041, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 169, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0046, 0.0066, 0.3668, 0.0043, 0.5966, 0.0034, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 169, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3083e-02, 6.1856e-01, 5.2541e-03, 3.3664e-01, 2.3147e-06, 4.7026e-03,
        2.1757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 169, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0079, 0.0047, 0.2349, 0.4928, 0.0227, 0.2181, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 169, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7788e-01, 1.9550e-05, 5.3052e-03, 2.2631e-02, 6.3398e-03, 2.8255e-01,
        5.2758e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.042

[Epoch: 169, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.2037, 0.0061, 0.7677, 0.0059, 0.0067, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 170, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0068, 0.0054, 0.4070, 0.0075, 0.5433, 0.0069, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 170, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3223e-02, 6.3933e-01, 5.0570e-03, 3.0911e-01, 1.7533e-06, 5.6979e-03,
        2.7574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 170, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0103, 0.0045, 0.2491, 0.4794, 0.0254, 0.2091, 0.0222],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 170, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7334e-01, 2.6205e-05, 5.6622e-03, 2.0422e-02, 6.6829e-03, 2.8885e-01,
        5.0175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 170, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.2192, 0.0062, 0.7531, 0.0052, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 171, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0060, 0.0055, 0.4013, 0.0065, 0.5528, 0.0066, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 171, batch: 80/204] total loss per batch: 0.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.9959e-02, 5.7732e-01, 5.2097e-03, 3.6074e-01, 3.3459e-06, 5.9608e-03,
        3.0809e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 171, batch: 120/204] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0121, 0.0055, 0.1977, 0.5127, 0.0293, 0.2187, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 171, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4707e-01, 1.9736e-05, 4.4722e-03, 1.6618e-02, 5.2780e-03, 2.2183e-01,
        4.7194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.043

[Epoch: 171, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.1979, 0.0042, 0.7805, 0.0045, 0.0052, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 172, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0039, 0.0039, 0.3794, 0.0048, 0.5876, 0.0036, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 172, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2845e-02, 6.3769e-01, 4.0666e-03, 3.1893e-01, 1.2671e-06, 4.9541e-03,
        2.1521e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 172, batch: 120/204] total loss per batch: 0.830
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0046, 0.2369, 0.5084, 0.0214, 0.2009, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 172, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9579e-01, 1.9607e-05, 3.9716e-03, 1.7092e-02, 4.8066e-03, 2.7423e-01,
        4.0869e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 172, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2160, 0.0063, 0.7576, 0.0050, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.019

[Epoch: 173, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0063, 0.0053, 0.4193, 0.0071, 0.5352, 0.0057, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 173, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3028e-02, 6.0102e-01, 5.6329e-03, 3.5010e-01, 2.2450e-06, 5.0145e-03,
        2.5204e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 173, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0095, 0.0045, 0.2425, 0.4793, 0.0265, 0.2168, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.003

[Epoch: 173, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6302e-01, 2.8483e-05, 6.7614e-03, 2.6850e-02, 7.7360e-03, 2.8937e-01,
        6.2315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 173, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2108, 0.0056, 0.7607, 0.0066, 0.0058, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 174, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0062, 0.0059, 0.3946, 0.0060, 0.5613, 0.0052, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 174, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6429e-02, 6.3145e-01, 5.4989e-03, 3.1333e-01, 2.3346e-06, 5.4203e-03,
        2.7867e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 174, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0116, 0.0056, 0.2230, 0.4835, 0.0242, 0.2307, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 174, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2319e-01, 2.1834e-05, 5.1723e-03, 1.8410e-02, 5.4467e-03, 2.4406e-01,
        3.6998e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 174, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.2076, 0.0049, 0.7634, 0.0052, 0.0069, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 175, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0049, 0.0050, 0.3757, 0.0055, 0.5873, 0.0048, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 175, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5367e-02, 5.7625e-01, 5.2569e-03, 3.7279e-01, 1.7247e-06, 4.6542e-03,
        2.5672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 175, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0050, 0.2316, 0.5063, 0.0258, 0.1997, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 175, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0581e-01, 1.4766e-05, 4.1699e-03, 1.9035e-02, 5.9478e-03, 2.5936e-01,
        5.6617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 175, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.2017, 0.0053, 0.7762, 0.0040, 0.0041, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 176, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0044, 0.0041, 0.4171, 0.0043, 0.5464, 0.0044, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 176, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6100e-02, 6.1636e-01, 6.4102e-03, 3.3037e-01, 2.2665e-06, 5.7429e-03,
        2.5013e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.017

[Epoch: 176, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0051, 0.2268, 0.4848, 0.0284, 0.2230, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.005

[Epoch: 176, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8560e-01, 1.4856e-05, 5.6346e-03, 2.0165e-02, 5.7499e-03, 2.7736e-01,
        5.4721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.038

[Epoch: 176, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.2180, 0.0052, 0.7549, 0.0056, 0.0056, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 177, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0045, 0.3841, 0.0055, 0.5763, 0.0047, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 177, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1401e-02, 6.6240e-01, 3.7877e-03, 2.9594e-01, 1.6402e-06, 3.5180e-03,
        2.2949e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 177, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0106, 0.0045, 0.2166, 0.5157, 0.0280, 0.2023, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.004

[Epoch: 177, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9791e-01, 1.7851e-05, 5.0573e-03, 2.2854e-02, 5.9074e-03, 2.6232e-01,
        5.9249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 177, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.2236, 0.0064, 0.7450, 0.0060, 0.0066, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 178, batch: 40/204] total loss per batch: 0.844
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0070, 0.0065, 0.3865, 0.0059, 0.5643, 0.0060, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 178, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8365e-02, 5.8345e-01, 7.6150e-03, 3.5615e-01, 2.8064e-06, 7.5804e-03,
        2.6840e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 178, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0115, 0.0059, 0.2333, 0.4798, 0.0265, 0.2230, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 178, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.0750e-01, 1.8498e-05, 5.0058e-03, 1.7694e-02, 5.1608e-03, 2.5902e-01,
        5.5993e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 178, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.1882, 0.0046, 0.7892, 0.0047, 0.0048, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 179, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0055, 0.0050, 0.3919, 0.0049, 0.5673, 0.0055, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.033

[Epoch: 179, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5546e-02, 5.9342e-01, 4.4424e-03, 3.5581e-01, 2.2359e-06, 5.8643e-03,
        2.4922e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 179, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0102, 0.0043, 0.2381, 0.4926, 0.0224, 0.2125, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.002

[Epoch: 179, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1467e-01, 1.9573e-05, 4.8801e-03, 1.9633e-02, 4.1304e-03, 2.5256e-01,
        4.1050e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.032

[Epoch: 179, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.2262, 0.0056, 0.7473, 0.0058, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 180, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0055, 0.0054, 0.4145, 0.0039, 0.5462, 0.0046, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.026

[Epoch: 180, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5595e-02, 6.0897e-01, 5.1309e-03, 3.3958e-01, 1.4353e-06, 4.8569e-03,
        2.5863e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.018

[Epoch: 180, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0092, 0.0039, 0.1979, 0.5316, 0.0243, 0.2150, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 180, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7157e-01, 1.5455e-05, 4.1984e-03, 2.0528e-02, 4.5206e-03, 2.9401e-01,
        5.1653e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.033

[Epoch: 180, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.1978, 0.0050, 0.7763, 0.0048, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 181, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0047, 0.0047, 0.3636, 0.0062, 0.5961, 0.0050, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 181, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2649e-02, 6.2585e-01, 3.3485e-03, 3.3316e-01, 1.7186e-06, 4.8232e-03,
        2.0164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 181, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0049, 0.2459, 0.4888, 0.0241, 0.2066, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.000

[Epoch: 181, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2349e-01, 1.6569e-05, 6.0868e-03, 2.0834e-02, 6.3723e-03, 2.3569e-01,
        7.5119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.034

[Epoch: 181, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.2274, 0.0064, 0.7444, 0.0069, 0.0055, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 182, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0069, 0.0068, 0.4437, 0.0048, 0.5112, 0.0059, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.032

[Epoch: 182, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5498e-02, 6.8073e-01, 5.5046e-03, 2.6951e-01, 1.8999e-06, 5.4756e-03,
        2.3285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 182, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0128, 0.0054, 0.2295, 0.4899, 0.0302, 0.2103, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 182, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7020e-01, 2.4628e-05, 5.8300e-03, 1.9533e-02, 6.1893e-03, 2.9343e-01,
        4.7956e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 182, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2037, 0.0049, 0.7725, 0.0035, 0.0062, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 183, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0059, 0.3552, 0.0049, 0.6026, 0.0061, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.038

[Epoch: 183, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8477e-02, 6.0102e-01, 6.6937e-03, 3.3386e-01, 2.8711e-06, 7.2363e-03,
        3.2713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.019

[Epoch: 183, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0103, 0.0065, 0.2127, 0.4979, 0.0236, 0.2290, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 0.008

[Epoch: 183, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.3743e-01, 1.1400e-05, 2.7948e-03, 1.7542e-02, 4.6397e-03, 2.3354e-01,
        4.0458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 183, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.2195, 0.0052, 0.7569, 0.0048, 0.0040, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 184, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0050, 0.0043, 0.4166, 0.0048, 0.5429, 0.0045, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 184, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4997e-02, 5.2814e-01, 4.3672e-03, 4.2799e-01, 1.5001e-06, 5.1687e-03,
        1.9332e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 184, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0048, 0.2447, 0.5039, 0.0240, 0.1938, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 184, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6131e-01, 1.8366e-05, 5.2904e-03, 2.4365e-02, 4.5370e-03, 2.9953e-01,
        4.9491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 184, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.2085, 0.0044, 0.7684, 0.0053, 0.0046, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 185, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0057, 0.3994, 0.0046, 0.5571, 0.0054, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.030

[Epoch: 185, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5207e-02, 6.0795e-01, 5.8740e-03, 3.3777e-01, 2.5495e-06, 5.9330e-03,
        2.7265e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.020

[Epoch: 185, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0094, 0.0053, 0.2046, 0.5107, 0.0291, 0.2176, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 185, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2672e-01, 2.1852e-05, 5.4907e-03, 1.9081e-02, 6.4415e-03, 2.3475e-01,
        7.4962e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 185, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.1952, 0.0074, 0.7737, 0.0059, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 186, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0066, 0.0048, 0.3588, 0.0058, 0.6017, 0.0056, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.035

[Epoch: 186, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3538e-02, 6.3983e-01, 3.9412e-03, 3.1644e-01, 2.3174e-06, 4.4730e-03,
        2.1778e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 186, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0053, 0.2409, 0.5037, 0.0224, 0.2001, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 186, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5936e-01, 3.0541e-05, 5.5346e-03, 2.2499e-02, 5.0005e-03, 3.0286e-01,
        4.7122e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 186, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.2066, 0.0044, 0.7687, 0.0052, 0.0058, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 187, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0065, 0.0050, 0.4307, 0.0048, 0.5266, 0.0052, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 187, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.8632e-02, 6.3485e-01, 5.5373e-03, 3.0126e-01, 2.5326e-06, 6.8958e-03,
        3.2827e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 187, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0100, 0.0048, 0.2191, 0.4913, 0.0239, 0.2310, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.011

[Epoch: 187, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.6932e-01, 1.9583e-05, 4.9811e-03, 1.5758e-02, 5.1584e-03, 2.0023e-01,
        4.5290e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 187, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.2242, 0.0044, 0.7549, 0.0039, 0.0047, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 188, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0043, 0.0048, 0.3775, 0.0041, 0.5862, 0.0042, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 188, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7179e-02, 6.3584e-01, 5.2309e-03, 3.1247e-01, 2.5514e-06, 5.5092e-03,
        2.3774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 188, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0088, 0.0045, 0.2219, 0.5025, 0.0230, 0.2225, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 188, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.2301e-01, 1.3376e-05, 4.9515e-03, 2.0299e-02, 6.2316e-03, 3.4061e-01,
        4.8856e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 188, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.2036, 0.0046, 0.7746, 0.0046, 0.0042, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 189, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0052, 0.0050, 0.4063, 0.0053, 0.5532, 0.0048, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 189, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.1402e-02, 5.9673e-01, 3.6838e-03, 3.5946e-01, 1.3806e-06, 4.2596e-03,
        2.4465e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 189, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0101, 0.0048, 0.2300, 0.5061, 0.0257, 0.2012, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.009

[Epoch: 189, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4092e-01, 1.4871e-05, 5.5102e-03, 2.2467e-02, 7.3417e-03, 2.1707e-01,
        6.6763e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.030

[Epoch: 189, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.2154, 0.0065, 0.7570, 0.0052, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 190, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0055, 0.0061, 0.4039, 0.0048, 0.5563, 0.0052, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.029

[Epoch: 190, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7547e-02, 6.2471e-01, 5.3388e-03, 3.1945e-01, 3.0225e-06, 6.7879e-03,
        2.6164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 190, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0067, 0.2309, 0.4835, 0.0259, 0.2210, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 190, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7252e-01, 1.9820e-05, 5.5004e-03, 1.8384e-02, 4.8854e-03, 2.9364e-01,
        5.0539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 190, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.2326, 0.0047, 0.7402, 0.0063, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 191, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0057, 0.0051, 0.3850, 0.0042, 0.5779, 0.0046, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.031

[Epoch: 191, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6272e-02, 6.0358e-01, 4.6375e-03, 3.4461e-01, 1.6321e-06, 5.2335e-03,
        2.5671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 191, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0043, 0.2230, 0.4995, 0.0230, 0.2184, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 191, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4393e-01, 1.5642e-05, 3.8669e-03, 1.9454e-02, 4.6377e-03, 2.2280e-01,
        5.2892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.041

[Epoch: 191, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.1868, 0.0062, 0.7872, 0.0049, 0.0055, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 192, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0056, 0.0046, 0.4034, 0.0051, 0.5554, 0.0057, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.036

[Epoch: 192, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5844e-02, 6.0863e-01, 5.8662e-03, 3.4031e-01, 2.7119e-06, 5.7971e-03,
        2.3557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 192, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0097, 0.0048, 0.2288, 0.4989, 0.0221, 0.2183, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.004

[Epoch: 192, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.5052e-01, 1.9199e-05, 6.1704e-03, 2.1931e-02, 6.2215e-03, 3.0979e-01,
        5.3463e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.042

[Epoch: 192, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.2370, 0.0045, 0.7359, 0.0060, 0.0054, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 193, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0066, 0.3810, 0.0061, 0.5717, 0.0061, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.023

[Epoch: 193, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3829e-02, 6.0107e-01, 5.0192e-03, 3.5247e-01, 1.9913e-06, 5.3564e-03,
        2.2253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 193, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0094, 0.0053, 0.2151, 0.5097, 0.0287, 0.2090, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.010

[Epoch: 193, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.2700e-01, 1.4264e-05, 4.3283e-03, 2.4294e-02, 4.8937e-03, 2.3394e-01,
        5.5285e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 193, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.1894, 0.0052, 0.7845, 0.0062, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 194, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0049, 0.0049, 0.3891, 0.0044, 0.5750, 0.0045, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.034

[Epoch: 194, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.3677e-02, 6.1898e-01, 5.1258e-03, 3.3365e-01, 1.8590e-06, 4.6607e-03,
        2.3898e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 194, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0096, 0.0049, 0.2218, 0.5002, 0.0249, 0.2170, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.007

[Epoch: 194, batch: 160/204] total loss per batch: 0.866
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8868e-01, 2.8483e-05, 6.7102e-03, 2.1022e-02, 7.5167e-03, 2.7001e-01,
        6.0321e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 194, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2012, 0.0054, 0.7696, 0.0053, 0.0071, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 195, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0047, 0.3932, 0.0041, 0.5658, 0.0056, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 195, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.6685e-02, 6.2892e-01, 5.9610e-03, 3.1288e-01, 2.2896e-06, 6.6579e-03,
        2.8888e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 195, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0121, 0.0066, 0.2133, 0.4750, 0.0258, 0.2461, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.008

[Epoch: 195, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.8363e-01, 1.5842e-05, 4.3544e-03, 1.8821e-02, 4.8772e-03, 2.8326e-01,
        5.0465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 195, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.2053, 0.0040, 0.7758, 0.0043, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 196, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0045, 0.0045, 0.4040, 0.0042, 0.5610, 0.0040, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 196, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.5639e-02, 6.2398e-01, 3.9220e-03, 3.2745e-01, 1.6483e-06, 3.5775e-03,
        2.5422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.022

[Epoch: 196, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0094, 0.0042, 0.2265, 0.5096, 0.0259, 0.2023, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.003

[Epoch: 196, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.4238e-01, 2.3523e-05, 5.1235e-03, 1.8196e-02, 5.4438e-03, 2.2196e-01,
        6.8779e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 196, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.2182, 0.0047, 0.7578, 0.0047, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 197, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0054, 0.3768, 0.0056, 0.5814, 0.0050, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 197, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.2301e-02, 5.9693e-01, 5.1895e-03, 3.6091e-01, 2.3547e-06, 4.7280e-03,
        1.9941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 197, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0110, 0.0048, 0.2476, 0.4782, 0.0216, 0.2169, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 197, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.6445e-01, 1.5433e-05, 4.9064e-03, 2.2402e-02, 6.0338e-03, 2.9700e-01,
        5.1960e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.037

[Epoch: 197, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.2276, 0.0054, 0.7412, 0.0081, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 198, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0049, 0.0049, 0.4128, 0.0063, 0.5391, 0.0064, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 198, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4961e-02, 6.0704e-01, 4.8187e-03, 3.3711e-01, 2.2499e-06, 5.8669e-03,
        3.0194e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.022

[Epoch: 198, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0104, 0.0059, 0.2126, 0.5056, 0.0304, 0.2121, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.005

[Epoch: 198, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.9758e-01, 1.8399e-05, 5.4745e-03, 1.9980e-02, 5.5933e-03, 2.6485e-01,
        6.4987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.036

[Epoch: 198, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.1805, 0.0044, 0.7933, 0.0068, 0.0053, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 199, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0053, 0.0057, 0.3961, 0.0050, 0.5661, 0.0047, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.028

[Epoch: 199, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.7000e-02, 6.1495e-01, 4.6280e-03, 3.3134e-01, 2.0518e-06, 6.1053e-03,
        2.5975e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.021

[Epoch: 199, batch: 120/204] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0089, 0.0048, 0.2310, 0.5026, 0.0225, 0.2134, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.002

[Epoch: 199, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([7.1642e-01, 2.7639e-05, 6.7421e-03, 1.9245e-02, 5.5924e-03, 2.4701e-01,
        4.9577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.035

[Epoch: 199, batch: 200/204] total loss per batch: 0.849
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.2209, 0.0047, 0.7569, 0.0050, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 200, batch: 40/204] total loss per batch: 0.845
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.3950, 0.0050, 0.5650, 0.0050, 0.0200])
Policy pred: tensor([0.0054, 0.0057, 0.4021, 0.0061, 0.5554, 0.0049, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.025

[Epoch: 200, batch: 80/204] total loss per batch: 0.878
Policy (actual, predicted): 1 1
Policy data: tensor([0.0150, 0.6150, 0.0050, 0.3350, 0.0000, 0.0050, 0.0250])
Policy pred: tensor([1.4347e-02, 6.2829e-01, 4.4265e-03, 3.2417e-01, 1.5523e-06, 5.0464e-03,
        2.3726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.028 -0.022

[Epoch: 200, batch: 120/204] total loss per batch: 0.829
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.2250, 0.5000, 0.0250, 0.2150, 0.0200])
Policy pred: tensor([0.0091, 0.0038, 0.2103, 0.5308, 0.0217, 0.2066, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.010 -0.000

[Epoch: 200, batch: 160/204] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0000, 0.0050, 0.0200, 0.0050, 0.2650, 0.0050])
Policy pred: tensor([6.7398e-01, 1.6594e-05, 5.8555e-03, 1.9909e-02, 5.3963e-03, 2.8959e-01,
        5.2456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.037 0.039

[Epoch: 200, batch: 200/204] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.2100, 0.0050, 0.7650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.2025, 0.0048, 0.7727, 0.0054, 0.0043, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

