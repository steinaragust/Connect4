Training set samples: 6282
Batch size: 32
[Epoch: 1, batch: 39/197] total loss per batch: 2.051
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0379, 0.0245, 0.1948, 0.1292, 0.0630, 0.5180, 0.0325],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 -0.033

[Epoch: 1, batch: 78/197] total loss per batch: 2.004
Policy (actual, predicted): 3 0
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1652, 0.1308, 0.1437, 0.1343, 0.1570, 0.1479, 0.1212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.053

[Epoch: 1, batch: 117/197] total loss per batch: 1.957
Policy (actual, predicted): 0 4
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.1194, 0.1073, 0.1670, 0.0941, 0.1781, 0.1565, 0.1776],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 1, batch: 156/197] total loss per batch: 1.968
Policy (actual, predicted): 6 2
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1036, 0.1093, 0.2182, 0.0963, 0.1726, 0.1851, 0.1149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.054

[Epoch: 1, batch: 195/197] total loss per batch: 1.942
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0668, 0.0786, 0.1427, 0.0972, 0.1838, 0.2966, 0.1343],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.019

[Epoch: 2, batch: 39/197] total loss per batch: 1.897
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0763, 0.0645, 0.0789, 0.0943, 0.1477, 0.4131, 0.1252],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.025

[Epoch: 2, batch: 78/197] total loss per batch: 1.871
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1822, 0.1584, 0.1512, 0.1156, 0.0861, 0.1887, 0.1177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.020

[Epoch: 2, batch: 117/197] total loss per batch: 1.850
Policy (actual, predicted): 0 5
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.1101, 0.0953, 0.1505, 0.1076, 0.1408, 0.2086, 0.1871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.007

[Epoch: 2, batch: 156/197] total loss per batch: 1.809
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0702, 0.1093, 0.2011, 0.1240, 0.1734, 0.2450, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.003

[Epoch: 2, batch: 195/197] total loss per batch: 1.795
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0682, 0.0720, 0.1352, 0.0845, 0.1934, 0.2639, 0.1829],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.026

[Epoch: 3, batch: 39/197] total loss per batch: 1.766
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0839, 0.0685, 0.0879, 0.1375, 0.2210, 0.3041, 0.0970],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.019

[Epoch: 3, batch: 78/197] total loss per batch: 1.753
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1706, 0.1686, 0.1107, 0.1807, 0.0336, 0.2646, 0.0713],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.013

[Epoch: 3, batch: 117/197] total loss per batch: 1.731
Policy (actual, predicted): 0 5
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.1119, 0.0901, 0.1282, 0.1188, 0.1556, 0.1978, 0.1975],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.037

[Epoch: 3, batch: 156/197] total loss per batch: 1.713
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0818, 0.0810, 0.1549, 0.1375, 0.1742, 0.2800, 0.0905],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.016

[Epoch: 3, batch: 195/197] total loss per batch: 1.688
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0609, 0.0768, 0.1128, 0.1064, 0.2081, 0.2963, 0.1386],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.025

[Epoch: 4, batch: 39/197] total loss per batch: 1.701
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0740, 0.0602, 0.0987, 0.1171, 0.2278, 0.3470, 0.0752],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.009

[Epoch: 4, batch: 78/197] total loss per batch: 1.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1592, 0.1148, 0.1310, 0.3237, 0.0140, 0.1604, 0.0970],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.028

[Epoch: 4, batch: 117/197] total loss per batch: 1.675
Policy (actual, predicted): 0 6
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.1282, 0.0692, 0.1535, 0.0877, 0.1319, 0.1624, 0.2671],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 4, batch: 156/197] total loss per batch: 1.670
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1153, 0.0678, 0.1563, 0.1285, 0.1744, 0.2682, 0.0894],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.014

[Epoch: 4, batch: 195/197] total loss per batch: 1.638
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0694, 0.1010, 0.1202, 0.1090, 0.1610, 0.3206, 0.1188],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 5, batch: 39/197] total loss per batch: 1.658
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0542, 0.0387, 0.0657, 0.0795, 0.2216, 0.4551, 0.0851],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 -0.000

[Epoch: 5, batch: 78/197] total loss per batch: 1.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1124, 0.0905, 0.1186, 0.3803, 0.0123, 0.1965, 0.0893],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.034

[Epoch: 5, batch: 117/197] total loss per batch: 1.631
Policy (actual, predicted): 0 6
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.1690, 0.0591, 0.1765, 0.0764, 0.1253, 0.1178, 0.2759],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 5, batch: 156/197] total loss per batch: 1.617
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1193, 0.0616, 0.1808, 0.1107, 0.1308, 0.2385, 0.1584],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.011

[Epoch: 5, batch: 195/197] total loss per batch: 1.591
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0755, 0.1073, 0.1332, 0.1543, 0.1517, 0.2338, 0.1441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 6, batch: 39/197] total loss per batch: 1.615
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0533, 0.0399, 0.0502, 0.0935, 0.2273, 0.4568, 0.0790],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.004

[Epoch: 6, batch: 78/197] total loss per batch: 1.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1126, 0.0898, 0.1818, 0.4116, 0.0221, 0.0951, 0.0871],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.046

[Epoch: 6, batch: 117/197] total loss per batch: 1.577
Policy (actual, predicted): 0 6
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.2105, 0.0591, 0.1714, 0.0690, 0.1047, 0.1210, 0.2643],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.013

[Epoch: 6, batch: 156/197] total loss per batch: 1.571
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1510, 0.0632, 0.1541, 0.1200, 0.1252, 0.2486, 0.1381],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.006

[Epoch: 6, batch: 195/197] total loss per batch: 1.527
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0594, 0.1263, 0.1011, 0.1334, 0.1738, 0.2471, 0.1588],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.014

[Epoch: 7, batch: 39/197] total loss per batch: 1.565
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0467, 0.0220, 0.0215, 0.1021, 0.2824, 0.4616, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.025

[Epoch: 7, batch: 78/197] total loss per batch: 1.562
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1037, 0.1306, 0.1473, 0.4831, 0.0128, 0.0486, 0.0740],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.004

[Epoch: 7, batch: 117/197] total loss per batch: 1.525
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.2878, 0.0342, 0.1320, 0.0835, 0.0957, 0.1046, 0.2623],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.028

[Epoch: 7, batch: 156/197] total loss per batch: 1.523
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1130, 0.0449, 0.0748, 0.1190, 0.1103, 0.2807, 0.2573],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.000

[Epoch: 7, batch: 195/197] total loss per batch: 1.474
Policy (actual, predicted): 1 4
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0704, 0.1700, 0.1126, 0.1156, 0.1827, 0.1761, 0.1726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.010

[Epoch: 8, batch: 39/197] total loss per batch: 1.525
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0618, 0.0360, 0.0344, 0.1130, 0.2468, 0.4487, 0.0593],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.021

[Epoch: 8, batch: 78/197] total loss per batch: 1.508
Policy (actual, predicted): 3 2
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1232, 0.1400, 0.2811, 0.2531, 0.0140, 0.0971, 0.0916],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.048

[Epoch: 8, batch: 117/197] total loss per batch: 1.480
Policy (actual, predicted): 0 6
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.2781, 0.0345, 0.1327, 0.0611, 0.0550, 0.1179, 0.3207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.055

[Epoch: 8, batch: 156/197] total loss per batch: 1.466
Policy (actual, predicted): 6 5
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0939, 0.0269, 0.0402, 0.0825, 0.0714, 0.4145, 0.2706],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.035

[Epoch: 8, batch: 195/197] total loss per batch: 1.424
Policy (actual, predicted): 1 3
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0519, 0.1912, 0.1724, 0.2123, 0.2029, 0.0627, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 9, batch: 39/197] total loss per batch: 1.477
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0481, 0.0217, 0.0164, 0.0727, 0.2191, 0.5615, 0.0606],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.019

[Epoch: 9, batch: 78/197] total loss per batch: 1.459
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1566, 0.0702, 0.1590, 0.3055, 0.0221, 0.2293, 0.0574],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.010

[Epoch: 9, batch: 117/197] total loss per batch: 1.414
Policy (actual, predicted): 0 6
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.2916, 0.0225, 0.1309, 0.1026, 0.0412, 0.0877, 0.3235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.067

[Epoch: 9, batch: 156/197] total loss per batch: 1.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1188, 0.0250, 0.0404, 0.0566, 0.0959, 0.1939, 0.4694],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.014

[Epoch: 9, batch: 195/197] total loss per batch: 1.364
Policy (actual, predicted): 1 2
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0326, 0.2356, 0.2871, 0.2086, 0.1134, 0.0383, 0.0843],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.013

[Epoch: 10, batch: 39/197] total loss per batch: 1.424
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0450, 0.0264, 0.0170, 0.0680, 0.2552, 0.5560, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 -0.006

[Epoch: 10, batch: 78/197] total loss per batch: 1.405
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1769, 0.0998, 0.2468, 0.3501, 0.0051, 0.0570, 0.0643],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.021

[Epoch: 10, batch: 117/197] total loss per batch: 1.336
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.3083, 0.0229, 0.1343, 0.2192, 0.0990, 0.0362, 0.1800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.076

[Epoch: 10, batch: 156/197] total loss per batch: 1.358
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.1366, 0.0173, 0.0280, 0.0593, 0.0791, 0.1872, 0.4925],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.028

[Epoch: 10, batch: 195/197] total loss per batch: 1.309
Policy (actual, predicted): 1 3
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0206, 0.2486, 0.1324, 0.3800, 0.0798, 0.0174, 0.1212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.024

[Epoch: 11, batch: 39/197] total loss per batch: 1.354
Policy (actual, predicted): 0 4
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0409, 0.0328, 0.0121, 0.0365, 0.4912, 0.3706, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.008

[Epoch: 11, batch: 78/197] total loss per batch: 1.321
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.2306, 0.0863, 0.0856, 0.4935, 0.0009, 0.0567, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.016

[Epoch: 11, batch: 117/197] total loss per batch: 1.248
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.4446, 0.0108, 0.1547, 0.1500, 0.0270, 0.0272, 0.1858],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.057

[Epoch: 11, batch: 156/197] total loss per batch: 1.287
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0588, 0.0043, 0.0145, 0.0165, 0.0862, 0.1459, 0.6738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.024

[Epoch: 11, batch: 195/197] total loss per batch: 1.208
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0141, 0.5201, 0.0441, 0.1142, 0.0913, 0.0466, 0.1695],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.062

[Epoch: 12, batch: 39/197] total loss per batch: 1.264
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0415, 0.0168, 0.0102, 0.0265, 0.4454, 0.4462, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 -0.008

[Epoch: 12, batch: 78/197] total loss per batch: 1.248
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.6334e-01, 8.4685e-02, 1.7575e-02, 6.8427e-01, 4.7421e-04, 2.5632e-02,
        2.4018e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.053

[Epoch: 12, batch: 117/197] total loss per batch: 1.154
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5124, 0.0108, 0.1609, 0.0970, 0.0162, 0.0353, 0.1674],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.131

[Epoch: 12, batch: 156/197] total loss per batch: 1.208
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0297, 0.0016, 0.0090, 0.0088, 0.0310, 0.1016, 0.8183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.004

[Epoch: 12, batch: 195/197] total loss per batch: 1.128
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0133, 0.5671, 0.0917, 0.1412, 0.0818, 0.0286, 0.0764],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.029

[Epoch: 13, batch: 39/197] total loss per batch: 1.182
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0392, 0.0132, 0.0128, 0.0079, 0.3209, 0.5970, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.006

[Epoch: 13, batch: 78/197] total loss per batch: 1.189
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.5833e-01, 1.2623e-01, 2.0753e-02, 4.0629e-01, 4.0450e-04, 1.6774e-01,
        2.0253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.003

[Epoch: 13, batch: 117/197] total loss per batch: 1.086
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6036, 0.0084, 0.1426, 0.0900, 0.0168, 0.0316, 0.1070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.083

[Epoch: 13, batch: 156/197] total loss per batch: 1.139
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0405, 0.0042, 0.0076, 0.0067, 0.0867, 0.0528, 0.8016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.011

[Epoch: 13, batch: 195/197] total loss per batch: 1.061
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0033, 0.7799, 0.0361, 0.0741, 0.0392, 0.0119, 0.0556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.036

[Epoch: 14, batch: 39/197] total loss per batch: 1.109
Policy (actual, predicted): 0 4
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0210, 0.0106, 0.0093, 0.0100, 0.7211, 0.2246, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.001

[Epoch: 14, batch: 78/197] total loss per batch: 1.136
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1968, 0.0625, 0.0322, 0.6322, 0.0008, 0.0557, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.030

[Epoch: 14, batch: 117/197] total loss per batch: 1.037
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6008, 0.0089, 0.1237, 0.0985, 0.0252, 0.0338, 0.1091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.095

[Epoch: 14, batch: 156/197] total loss per batch: 1.093
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0692, 0.0222, 0.0243, 0.0180, 0.1560, 0.1486, 0.5617],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.043

[Epoch: 14, batch: 195/197] total loss per batch: 1.026
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0115, 0.4016, 0.0358, 0.2383, 0.0966, 0.0294, 0.1869],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.031

[Epoch: 15, batch: 39/197] total loss per batch: 1.044
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.0546, 0.0065, 0.0254, 0.0098, 0.0968, 0.8002, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.015

[Epoch: 15, batch: 78/197] total loss per batch: 1.072
Policy (actual, predicted): 3 5
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1787, 0.1360, 0.0197, 0.3054, 0.0004, 0.3106, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.008

[Epoch: 15, batch: 117/197] total loss per batch: 1.003
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6766, 0.0032, 0.2219, 0.0391, 0.0065, 0.0129, 0.0397],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.053

[Epoch: 15, batch: 156/197] total loss per batch: 1.050
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0242, 0.0280, 0.0081, 0.0156, 0.0833, 0.0412, 0.7994],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.041

[Epoch: 15, batch: 195/197] total loss per batch: 0.971
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0061, 0.5686, 0.0792, 0.0418, 0.0616, 0.0435, 0.1992],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.042

[Epoch: 16, batch: 39/197] total loss per batch: 1.002
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1136, 0.0083, 0.0363, 0.0165, 0.2782, 0.5312, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.019

[Epoch: 16, batch: 78/197] total loss per batch: 1.020
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.1187, 0.1072, 0.0764, 0.6167, 0.0015, 0.0450, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.071

[Epoch: 16, batch: 117/197] total loss per batch: 0.963
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7955, 0.0036, 0.1068, 0.0245, 0.0040, 0.0386, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.062

[Epoch: 16, batch: 156/197] total loss per batch: 1.005
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0941, 0.0270, 0.0219, 0.0130, 0.3457, 0.0798, 0.4185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.024

[Epoch: 16, batch: 195/197] total loss per batch: 0.947
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0071, 0.5212, 0.0834, 0.0407, 0.0655, 0.0172, 0.2649],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.024

[Epoch: 17, batch: 39/197] total loss per batch: 0.973
Policy (actual, predicted): 0 4
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1892, 0.0042, 0.0682, 0.0159, 0.3796, 0.3343, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.055

[Epoch: 17, batch: 78/197] total loss per batch: 1.004
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.8020e-02, 3.3126e-02, 2.3948e-03, 9.0567e-01, 4.1709e-04, 3.4491e-02,
        5.8821e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.007

[Epoch: 17, batch: 117/197] total loss per batch: 0.928
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.8033, 0.0058, 0.1056, 0.0059, 0.0030, 0.0279, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.047

[Epoch: 17, batch: 156/197] total loss per batch: 0.970
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0474, 0.0162, 0.0114, 0.0086, 0.0878, 0.1263, 0.7023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.029

[Epoch: 17, batch: 195/197] total loss per batch: 0.893
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0052, 0.6624, 0.0117, 0.2358, 0.0290, 0.0264, 0.0295],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.005

[Epoch: 18, batch: 39/197] total loss per batch: 0.933
Policy (actual, predicted): 0 4
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1006, 0.0056, 0.0179, 0.0117, 0.5977, 0.2634, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.026

[Epoch: 18, batch: 78/197] total loss per batch: 0.959
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.4592e-02, 8.7644e-02, 4.9804e-02, 7.9920e-01, 7.5907e-04, 3.4290e-02,
        1.3716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.016

[Epoch: 18, batch: 117/197] total loss per batch: 0.891
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.4895, 0.0060, 0.2760, 0.0997, 0.0158, 0.0823, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.018

[Epoch: 18, batch: 156/197] total loss per batch: 0.926
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0435, 0.0372, 0.0030, 0.0060, 0.3704, 0.0825, 0.4575],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.007

[Epoch: 18, batch: 195/197] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0055, 0.7563, 0.1135, 0.0130, 0.0221, 0.0176, 0.0721],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 19, batch: 39/197] total loss per batch: 0.901
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1022, 0.0139, 0.0273, 0.0104, 0.3932, 0.4493, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.016

[Epoch: 19, batch: 78/197] total loss per batch: 0.925
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([3.8717e-02, 5.6676e-02, 1.0768e-02, 7.3961e-01, 2.7890e-04, 1.4223e-01,
        1.1721e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.006

[Epoch: 19, batch: 117/197] total loss per batch: 0.880
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7161, 0.0115, 0.1928, 0.0168, 0.0134, 0.0288, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.011

[Epoch: 19, batch: 156/197] total loss per batch: 0.907
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0295, 0.0193, 0.0268, 0.0151, 0.0836, 0.1004, 0.7253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.004

[Epoch: 19, batch: 195/197] total loss per batch: 0.860
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0104, 0.8276, 0.0939, 0.0156, 0.0281, 0.0118, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 20, batch: 39/197] total loss per batch: 0.865
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4102, 0.0065, 0.0883, 0.0310, 0.1202, 0.3372, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 20, batch: 78/197] total loss per batch: 0.905
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0547, 0.2641, 0.0879, 0.4502, 0.0015, 0.0981, 0.0435],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 20, batch: 117/197] total loss per batch: 0.850
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6290, 0.0118, 0.1908, 0.0191, 0.0071, 0.0985, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.039

[Epoch: 20, batch: 156/197] total loss per batch: 0.852
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0352, 0.0072, 0.0032, 0.0017, 0.0383, 0.0207, 0.8936],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.089

[Epoch: 20, batch: 195/197] total loss per batch: 0.843
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0106, 0.7943, 0.0249, 0.0359, 0.0147, 0.0705, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 21, batch: 39/197] total loss per batch: 0.843
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1397, 0.0142, 0.0393, 0.0604, 0.1951, 0.5488, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.019

[Epoch: 21, batch: 78/197] total loss per batch: 0.859
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.5304e-02, 4.8761e-02, 1.7065e-02, 8.2704e-01, 3.2391e-04, 3.2191e-02,
        4.9318e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 21, batch: 117/197] total loss per batch: 0.824
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6224, 0.0075, 0.3316, 0.0069, 0.0061, 0.0172, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.022

[Epoch: 21, batch: 156/197] total loss per batch: 0.845
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0173, 0.0171, 0.0099, 0.0081, 0.0386, 0.0677, 0.8413],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.014

[Epoch: 21, batch: 195/197] total loss per batch: 0.805
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0137, 0.8823, 0.0354, 0.0051, 0.0209, 0.0148, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 22, batch: 39/197] total loss per batch: 0.799
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3048, 0.0078, 0.0302, 0.0450, 0.2057, 0.4000, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 22, batch: 78/197] total loss per batch: 0.827
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0247, 0.1019, 0.0400, 0.6612, 0.0020, 0.0956, 0.0747],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.043

[Epoch: 22, batch: 117/197] total loss per batch: 0.797
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5891, 0.0085, 0.3269, 0.0106, 0.0065, 0.0397, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 22, batch: 156/197] total loss per batch: 0.797
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0324, 0.0052, 0.0112, 0.0105, 0.0756, 0.0293, 0.8356],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.005

[Epoch: 22, batch: 195/197] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0221, 0.8419, 0.0424, 0.0146, 0.0199, 0.0367, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 23, batch: 39/197] total loss per batch: 0.782
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5533, 0.0114, 0.0765, 0.0742, 0.0681, 0.2019, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.017

[Epoch: 23, batch: 78/197] total loss per batch: 0.811
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0100, 0.0254, 0.0069, 0.9021, 0.0010, 0.0503, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.039

[Epoch: 23, batch: 117/197] total loss per batch: 0.770
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5971, 0.0132, 0.2758, 0.0270, 0.0117, 0.0571, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.022

[Epoch: 23, batch: 156/197] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0102, 0.0020, 0.0058, 0.0118, 0.0114, 0.0444, 0.9145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.010

[Epoch: 23, batch: 195/197] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0045, 0.9352, 0.0213, 0.0052, 0.0054, 0.0155, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 24, batch: 39/197] total loss per batch: 0.773
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3351, 0.0075, 0.0473, 0.0312, 0.0584, 0.5108, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 24, batch: 78/197] total loss per batch: 0.797
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0558, 0.2517, 0.0875, 0.3714, 0.0083, 0.1403, 0.0849],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.039

[Epoch: 24, batch: 117/197] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5813, 0.0084, 0.3348, 0.0236, 0.0069, 0.0234, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 24, batch: 156/197] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0404, 0.0280, 0.0152, 0.0138, 0.1222, 0.0694, 0.7110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.008

[Epoch: 24, batch: 195/197] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0046, 0.9234, 0.0237, 0.0025, 0.0164, 0.0184, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.012

[Epoch: 25, batch: 39/197] total loss per batch: 0.772
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4210, 0.0030, 0.0371, 0.0322, 0.0789, 0.4213, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.022

[Epoch: 25, batch: 78/197] total loss per batch: 0.789
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.9580e-02, 1.9958e-02, 2.1591e-02, 8.9911e-01, 7.8467e-04, 2.0654e-02,
        8.3262e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.054

[Epoch: 25, batch: 117/197] total loss per batch: 0.757
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6282, 0.0138, 0.2854, 0.0096, 0.0161, 0.0388, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.032

[Epoch: 25, batch: 156/197] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0228, 0.0030, 0.0072, 0.0124, 0.0136, 0.0234, 0.9177],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.083

[Epoch: 25, batch: 195/197] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0292, 0.7769, 0.0496, 0.0501, 0.0250, 0.0403, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.021

[Epoch: 26, batch: 39/197] total loss per batch: 0.756
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.6531, 0.0044, 0.0243, 0.0249, 0.1605, 0.1289, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 26, batch: 78/197] total loss per batch: 0.781
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0261, 0.0800, 0.0309, 0.7604, 0.0030, 0.0789, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.044

[Epoch: 26, batch: 117/197] total loss per batch: 0.748
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6827, 0.0171, 0.2353, 0.0101, 0.0196, 0.0228, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 26, batch: 156/197] total loss per batch: 0.753
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0271, 0.0084, 0.0202, 0.0380, 0.0706, 0.0385, 0.7972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.003

[Epoch: 26, batch: 195/197] total loss per batch: 0.739
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0044, 0.9381, 0.0256, 0.0023, 0.0053, 0.0073, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.001

[Epoch: 27, batch: 39/197] total loss per batch: 0.742
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.1714, 0.0117, 0.0242, 0.0512, 0.0346, 0.6958, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.024

[Epoch: 27, batch: 78/197] total loss per batch: 0.772
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0284, 0.1876, 0.0318, 0.6212, 0.0028, 0.1185, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.052

[Epoch: 27, batch: 117/197] total loss per batch: 0.738
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6988, 0.0081, 0.2302, 0.0164, 0.0132, 0.0227, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 27, batch: 156/197] total loss per batch: 0.747
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0265, 0.0272, 0.0158, 0.0244, 0.0491, 0.0605, 0.7965],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.031

[Epoch: 27, batch: 195/197] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0123, 0.6893, 0.0354, 0.1198, 0.0280, 0.0433, 0.0719],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 28, batch: 39/197] total loss per batch: 0.737
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.7630, 0.0035, 0.0230, 0.0312, 0.0351, 0.1397, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.027

[Epoch: 28, batch: 78/197] total loss per batch: 0.767
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0664, 0.1899, 0.0442, 0.5478, 0.0047, 0.0977, 0.0493],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.044

[Epoch: 28, batch: 117/197] total loss per batch: 0.738
Policy (actual, predicted): 0 2
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.4113, 0.0281, 0.4873, 0.0080, 0.0273, 0.0286, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.025

[Epoch: 28, batch: 156/197] total loss per batch: 0.745
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0410, 0.0069, 0.0101, 0.0360, 0.0302, 0.0255, 0.8503],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.077

[Epoch: 28, batch: 195/197] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([2.9751e-03, 9.6894e-01, 1.5665e-02, 4.6222e-04, 2.1680e-03, 2.7624e-03,
        7.0232e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.008

[Epoch: 29, batch: 39/197] total loss per batch: 0.735
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.2714, 0.0179, 0.0344, 0.0762, 0.0443, 0.5469, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.021

[Epoch: 29, batch: 78/197] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0124, 0.0685, 0.0158, 0.8399, 0.0012, 0.0507, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.042

[Epoch: 29, batch: 117/197] total loss per batch: 0.733
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6057, 0.0099, 0.2589, 0.0127, 0.0168, 0.0688, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 29, batch: 156/197] total loss per batch: 0.733
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0347, 0.0114, 0.0110, 0.0082, 0.0254, 0.0349, 0.8744],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.017

[Epoch: 29, batch: 195/197] total loss per batch: 0.724
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0199, 0.8650, 0.0307, 0.0240, 0.0137, 0.0210, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 30, batch: 39/197] total loss per batch: 0.725
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5991, 0.0095, 0.0604, 0.0125, 0.1008, 0.2093, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.026

[Epoch: 30, batch: 78/197] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0219, 0.1489, 0.0331, 0.7016, 0.0038, 0.0732, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.046

[Epoch: 30, batch: 117/197] total loss per batch: 0.727
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7705, 0.0136, 0.1205, 0.0033, 0.0285, 0.0446, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.026

[Epoch: 30, batch: 156/197] total loss per batch: 0.741
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0126, 0.0264, 0.0184, 0.0168, 0.0526, 0.0053, 0.8679],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.021

[Epoch: 30, batch: 195/197] total loss per batch: 0.720
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0172, 0.8858, 0.0275, 0.0094, 0.0169, 0.0227, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.012

[Epoch: 31, batch: 39/197] total loss per batch: 0.722
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4496, 0.0144, 0.0416, 0.0588, 0.0336, 0.3940, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 31, batch: 78/197] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0362, 0.2572, 0.0290, 0.4913, 0.0040, 0.1683, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 31, batch: 117/197] total loss per batch: 0.712
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6276, 0.0149, 0.2970, 0.0031, 0.0156, 0.0308, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 31, batch: 156/197] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0210, 0.0146, 0.0167, 0.0297, 0.0613, 0.0498, 0.8070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.009

[Epoch: 31, batch: 195/197] total loss per batch: 0.717
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0082, 0.9556, 0.0077, 0.0031, 0.0080, 0.0037, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.017

[Epoch: 32, batch: 39/197] total loss per batch: 0.704
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4003, 0.0122, 0.0180, 0.0213, 0.0332, 0.5093, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.020

[Epoch: 32, batch: 78/197] total loss per batch: 0.733
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0119, 0.0155, 0.0242, 0.9045, 0.0011, 0.0309, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.039

[Epoch: 32, batch: 117/197] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5703, 0.0249, 0.3556, 0.0055, 0.0177, 0.0177, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.004

[Epoch: 32, batch: 156/197] total loss per batch: 0.702
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0401, 0.0144, 0.0177, 0.0179, 0.0227, 0.0248, 0.8624],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.043

[Epoch: 32, batch: 195/197] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0149, 0.9008, 0.0285, 0.0136, 0.0083, 0.0072, 0.0266],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.007

[Epoch: 33, batch: 39/197] total loss per batch: 0.696
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4206, 0.0146, 0.0376, 0.0149, 0.0463, 0.4623, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 33, batch: 78/197] total loss per batch: 0.725
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0422, 0.2115, 0.0673, 0.5451, 0.0030, 0.1145, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.048

[Epoch: 33, batch: 117/197] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6012, 0.0149, 0.3303, 0.0042, 0.0193, 0.0193, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.010

[Epoch: 33, batch: 156/197] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0404, 0.0192, 0.0069, 0.0228, 0.0376, 0.0199, 0.8533],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.003

[Epoch: 33, batch: 195/197] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0195, 0.8812, 0.0225, 0.0084, 0.0203, 0.0081, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 34, batch: 39/197] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5848, 0.0110, 0.0228, 0.0155, 0.0250, 0.3376, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 34, batch: 78/197] total loss per batch: 0.722
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0220, 0.0774, 0.0338, 0.7583, 0.0020, 0.0841, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.041

[Epoch: 34, batch: 117/197] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5901, 0.0232, 0.3219, 0.0046, 0.0227, 0.0273, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.015

[Epoch: 34, batch: 156/197] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0364, 0.0270, 0.0155, 0.0335, 0.0343, 0.0121, 0.8412],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.006

[Epoch: 34, batch: 195/197] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0174, 0.9019, 0.0146, 0.0138, 0.0276, 0.0086, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 35, batch: 39/197] total loss per batch: 0.695
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.2782, 0.0118, 0.0381, 0.0166, 0.0436, 0.6083, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.034

[Epoch: 35, batch: 78/197] total loss per batch: 0.724
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0271, 0.1036, 0.0340, 0.7108, 0.0016, 0.1002, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.041

[Epoch: 35, batch: 117/197] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6753, 0.0121, 0.2485, 0.0052, 0.0184, 0.0278, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 35, batch: 156/197] total loss per batch: 0.700
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0412, 0.0356, 0.0115, 0.0254, 0.0434, 0.0334, 0.8095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.007

[Epoch: 35, batch: 195/197] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0130, 0.8830, 0.0535, 0.0074, 0.0108, 0.0116, 0.0207],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 36, batch: 39/197] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.6914, 0.0146, 0.0387, 0.0279, 0.0629, 0.1593, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 36, batch: 78/197] total loss per batch: 0.725
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0220, 0.2612, 0.0683, 0.5009, 0.0041, 0.1208, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.047

[Epoch: 36, batch: 117/197] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5914, 0.0224, 0.3363, 0.0042, 0.0186, 0.0192, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 36, batch: 156/197] total loss per batch: 0.700
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0387, 0.0161, 0.0112, 0.0251, 0.0277, 0.0121, 0.8692],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.030

[Epoch: 36, batch: 195/197] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0125, 0.9523, 0.0043, 0.0033, 0.0064, 0.0041, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.000

[Epoch: 37, batch: 39/197] total loss per batch: 0.697
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3122, 0.0177, 0.0335, 0.0139, 0.0308, 0.5858, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.036

[Epoch: 37, batch: 78/197] total loss per batch: 0.723
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0252, 0.0853, 0.0205, 0.7981, 0.0011, 0.0452, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.035

[Epoch: 37, batch: 117/197] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6210, 0.0187, 0.2783, 0.0187, 0.0226, 0.0270, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 37, batch: 156/197] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0632, 0.0187, 0.0128, 0.0413, 0.0532, 0.0371, 0.7738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.025

[Epoch: 37, batch: 195/197] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0358, 0.7314, 0.0788, 0.0333, 0.0171, 0.0453, 0.0584],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.008

[Epoch: 38, batch: 39/197] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5860, 0.0135, 0.0308, 0.0193, 0.0377, 0.3084, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 38, batch: 78/197] total loss per batch: 0.727
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0108, 0.2154, 0.0253, 0.6101, 0.0021, 0.1261, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.031

[Epoch: 38, batch: 117/197] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5741, 0.0160, 0.3609, 0.0026, 0.0219, 0.0142, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.001

[Epoch: 38, batch: 156/197] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0159, 0.0142, 0.0078, 0.0177, 0.0221, 0.0128, 0.9094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.004

[Epoch: 38, batch: 195/197] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0091, 0.9627, 0.0066, 0.0027, 0.0033, 0.0029, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 39, batch: 39/197] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4604, 0.0203, 0.0284, 0.0304, 0.0713, 0.3809, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 39, batch: 78/197] total loss per batch: 0.728
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0261, 0.1550, 0.1061, 0.5727, 0.0032, 0.1182, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.045

[Epoch: 39, batch: 117/197] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6216, 0.0206, 0.2972, 0.0129, 0.0157, 0.0181, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 39, batch: 156/197] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0235, 0.0270, 0.0178, 0.0315, 0.0599, 0.0345, 0.8059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.013

[Epoch: 39, batch: 195/197] total loss per batch: 0.685
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0166, 0.9266, 0.0180, 0.0069, 0.0077, 0.0082, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 40, batch: 39/197] total loss per batch: 0.695
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4549, 0.0099, 0.0183, 0.0151, 0.0072, 0.4897, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.038

[Epoch: 40, batch: 78/197] total loss per batch: 0.725
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.3639e-02, 5.4250e-02, 2.0885e-02, 7.9188e-01, 7.4633e-04, 1.0098e-01,
        1.7624e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.023

[Epoch: 40, batch: 117/197] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.5063, 0.0252, 0.3998, 0.0053, 0.0278, 0.0259, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.002

[Epoch: 40, batch: 156/197] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0145, 0.0106, 0.0037, 0.0097, 0.0109, 0.0097, 0.9410],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.024

[Epoch: 40, batch: 195/197] total loss per batch: 0.683
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0283, 0.8984, 0.0197, 0.0105, 0.0070, 0.0122, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.000

[Epoch: 41, batch: 39/197] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5040, 0.0199, 0.0295, 0.0167, 0.0334, 0.3867, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 41, batch: 78/197] total loss per batch: 0.718
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0271, 0.0891, 0.0253, 0.7908, 0.0009, 0.0500, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.037

[Epoch: 41, batch: 117/197] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6968, 0.0198, 0.2355, 0.0092, 0.0160, 0.0138, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 41, batch: 156/197] total loss per batch: 0.689
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0230, 0.0226, 0.0113, 0.0191, 0.0255, 0.0092, 0.8892],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.010

[Epoch: 41, batch: 195/197] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0278, 0.8641, 0.0362, 0.0090, 0.0104, 0.0121, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 42, batch: 39/197] total loss per batch: 0.680
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4398, 0.0154, 0.0333, 0.0089, 0.0272, 0.4706, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 42, batch: 78/197] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0273, 0.2527, 0.0285, 0.5110, 0.0022, 0.1440, 0.0343],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.025

[Epoch: 42, batch: 117/197] total loss per batch: 0.675
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6498, 0.0189, 0.2801, 0.0035, 0.0202, 0.0174, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.011

[Epoch: 42, batch: 156/197] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0316, 0.0317, 0.0113, 0.0424, 0.0363, 0.0309, 0.8158],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.032

[Epoch: 42, batch: 195/197] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0188, 0.9157, 0.0164, 0.0079, 0.0102, 0.0064, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 43, batch: 39/197] total loss per batch: 0.678
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.6489, 0.0169, 0.0300, 0.0310, 0.0251, 0.2393, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 43, batch: 78/197] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0158, 0.0718, 0.0214, 0.8179, 0.0009, 0.0545, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.035

[Epoch: 43, batch: 117/197] total loss per batch: 0.673
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6370, 0.0236, 0.2944, 0.0061, 0.0168, 0.0127, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 43, batch: 156/197] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0206, 0.0165, 0.0082, 0.0212, 0.0129, 0.0075, 0.9132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.020

[Epoch: 43, batch: 195/197] total loss per batch: 0.663
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0191, 0.8996, 0.0217, 0.0093, 0.0098, 0.0104, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 44, batch: 39/197] total loss per batch: 0.678
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.2484, 0.0119, 0.0214, 0.0057, 0.0163, 0.6914, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 44, batch: 78/197] total loss per batch: 0.704
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0274, 0.2145, 0.0323, 0.5878, 0.0011, 0.1048, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 44, batch: 117/197] total loss per batch: 0.672
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6178, 0.0132, 0.3181, 0.0067, 0.0176, 0.0171, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 44, batch: 156/197] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0216, 0.0492, 0.0158, 0.0363, 0.0371, 0.0332, 0.8067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.018

[Epoch: 44, batch: 195/197] total loss per batch: 0.664
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0175, 0.9118, 0.0210, 0.0078, 0.0093, 0.0075, 0.0252],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 45, batch: 39/197] total loss per batch: 0.678
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.6623, 0.0196, 0.0263, 0.0250, 0.0383, 0.2203, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 45, batch: 78/197] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0302, 0.1022, 0.0236, 0.7428, 0.0013, 0.0840, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.044

[Epoch: 45, batch: 117/197] total loss per batch: 0.674
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7076, 0.0223, 0.2327, 0.0061, 0.0134, 0.0118, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.010

[Epoch: 45, batch: 156/197] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0264, 0.0204, 0.0100, 0.0184, 0.0200, 0.0087, 0.8961],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.009

[Epoch: 45, batch: 195/197] total loss per batch: 0.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0356, 0.8664, 0.0288, 0.0102, 0.0119, 0.0149, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 46, batch: 39/197] total loss per batch: 0.679
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4927, 0.0158, 0.0227, 0.0099, 0.0256, 0.4288, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 46, batch: 78/197] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0187, 0.1753, 0.0346, 0.6750, 0.0015, 0.0707, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.031

[Epoch: 46, batch: 117/197] total loss per batch: 0.676
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6414, 0.0155, 0.2888, 0.0044, 0.0220, 0.0169, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 46, batch: 156/197] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0150, 0.0212, 0.0105, 0.0338, 0.0284, 0.0333, 0.8578],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.018

[Epoch: 46, batch: 195/197] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0185, 0.9069, 0.0205, 0.0110, 0.0099, 0.0103, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 47, batch: 39/197] total loss per batch: 0.681
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4831, 0.0199, 0.0179, 0.0181, 0.0157, 0.4341, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.027

[Epoch: 47, batch: 78/197] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0559, 0.1481, 0.0337, 0.6461, 0.0015, 0.0846, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.042

[Epoch: 47, batch: 117/197] total loss per batch: 0.677
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6703, 0.0205, 0.2635, 0.0059, 0.0187, 0.0136, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 47, batch: 156/197] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0262, 0.0229, 0.0112, 0.0231, 0.0361, 0.0128, 0.8677],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.050

[Epoch: 47, batch: 195/197] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0170, 0.8991, 0.0186, 0.0079, 0.0130, 0.0189, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 48, batch: 39/197] total loss per batch: 0.680
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4923, 0.0288, 0.0389, 0.0266, 0.0444, 0.3623, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.021

[Epoch: 48, batch: 78/197] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0216, 0.1788, 0.0336, 0.6557, 0.0015, 0.0862, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.028

[Epoch: 48, batch: 117/197] total loss per batch: 0.676
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6688, 0.0133, 0.2684, 0.0063, 0.0157, 0.0153, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 48, batch: 156/197] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0205, 0.0230, 0.0182, 0.0422, 0.0220, 0.0246, 0.8495],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.004

[Epoch: 48, batch: 195/197] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0346, 0.8357, 0.0500, 0.0195, 0.0133, 0.0145, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 49, batch: 39/197] total loss per batch: 0.679
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4512, 0.0159, 0.0210, 0.0130, 0.0108, 0.4828, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 49, batch: 78/197] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0321, 0.0714, 0.0183, 0.7987, 0.0022, 0.0607, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.031

[Epoch: 49, batch: 117/197] total loss per batch: 0.676
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6612, 0.0168, 0.2687, 0.0041, 0.0260, 0.0150, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 0.001

[Epoch: 49, batch: 156/197] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0329, 0.0291, 0.0100, 0.0247, 0.0328, 0.0133, 0.8570],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.040

[Epoch: 49, batch: 195/197] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0085, 0.9374, 0.0093, 0.0078, 0.0103, 0.0083, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 50, batch: 39/197] total loss per batch: 0.680
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5687, 0.0191, 0.0440, 0.0222, 0.0203, 0.3189, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 50, batch: 78/197] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0247, 0.3634, 0.0579, 0.4392, 0.0012, 0.0918, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.037

[Epoch: 50, batch: 117/197] total loss per batch: 0.677
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6887, 0.0121, 0.2602, 0.0033, 0.0151, 0.0123, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 50, batch: 156/197] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0207, 0.0239, 0.0112, 0.0479, 0.0253, 0.0282, 0.8430],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.011

[Epoch: 50, batch: 195/197] total loss per batch: 0.669
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0289, 0.8397, 0.0352, 0.0120, 0.0160, 0.0236, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.011

[Epoch: 51, batch: 39/197] total loss per batch: 0.679
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3884, 0.0272, 0.0221, 0.0156, 0.0234, 0.5170, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.025

[Epoch: 51, batch: 78/197] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.4388e-02, 2.5180e-02, 1.6970e-02, 8.8339e-01, 7.5860e-04, 4.0013e-02,
        1.9302e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.025

[Epoch: 51, batch: 117/197] total loss per batch: 0.676
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6352, 0.0199, 0.2950, 0.0049, 0.0195, 0.0157, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 51, batch: 156/197] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0235, 0.0220, 0.0144, 0.0218, 0.0343, 0.0227, 0.8612],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 51, batch: 195/197] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0115, 0.9230, 0.0174, 0.0096, 0.0129, 0.0101, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 52, batch: 39/197] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4690, 0.0155, 0.0251, 0.0126, 0.0154, 0.4570, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 52, batch: 78/197] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0191, 0.1198, 0.0453, 0.7083, 0.0012, 0.0810, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 52, batch: 117/197] total loss per batch: 0.667
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6766, 0.0152, 0.2636, 0.0042, 0.0173, 0.0128, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 52, batch: 156/197] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0191, 0.0161, 0.0081, 0.0262, 0.0202, 0.0170, 0.8934],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.031

[Epoch: 52, batch: 195/197] total loss per batch: 0.657
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0133, 0.9177, 0.0212, 0.0091, 0.0123, 0.0098, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 53, batch: 39/197] total loss per batch: 0.669
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5871, 0.0237, 0.0275, 0.0196, 0.0181, 0.3164, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 53, batch: 78/197] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0255, 0.1615, 0.0305, 0.6438, 0.0013, 0.1135, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.027

[Epoch: 53, batch: 117/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6876, 0.0172, 0.2525, 0.0048, 0.0163, 0.0126, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 53, batch: 156/197] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0210, 0.0231, 0.0113, 0.0237, 0.0249, 0.0159, 0.8800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.031

[Epoch: 53, batch: 195/197] total loss per batch: 0.656
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0214, 0.9105, 0.0187, 0.0090, 0.0096, 0.0099, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.001

[Epoch: 54, batch: 39/197] total loss per batch: 0.667
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4396, 0.0181, 0.0227, 0.0185, 0.0171, 0.4770, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 54, batch: 78/197] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0154, 0.1020, 0.0320, 0.7736, 0.0009, 0.0576, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.038

[Epoch: 54, batch: 117/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6671, 0.0176, 0.2711, 0.0042, 0.0188, 0.0126, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.009

[Epoch: 54, batch: 156/197] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0194, 0.0191, 0.0111, 0.0301, 0.0191, 0.0153, 0.8860],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.006

[Epoch: 54, batch: 195/197] total loss per batch: 0.656
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0222, 0.8701, 0.0318, 0.0215, 0.0171, 0.0190, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 55, batch: 39/197] total loss per batch: 0.668
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5016, 0.0190, 0.0281, 0.0111, 0.0166, 0.4158, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 55, batch: 78/197] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0248, 0.2233, 0.0360, 0.6008, 0.0012, 0.0880, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 55, batch: 117/197] total loss per batch: 0.666
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6957, 0.0170, 0.2359, 0.0057, 0.0204, 0.0136, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.006

[Epoch: 55, batch: 156/197] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0210, 0.0222, 0.0109, 0.0239, 0.0289, 0.0150, 0.8781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.024

[Epoch: 55, batch: 195/197] total loss per batch: 0.657
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0110, 0.9341, 0.0158, 0.0060, 0.0088, 0.0078, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.003

[Epoch: 56, batch: 39/197] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5781, 0.0182, 0.0221, 0.0189, 0.0154, 0.3409, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.026

[Epoch: 56, batch: 78/197] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0158, 0.0920, 0.0360, 0.7886, 0.0008, 0.0502, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 56, batch: 117/197] total loss per batch: 0.667
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6754, 0.0151, 0.2663, 0.0037, 0.0186, 0.0136, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.003

[Epoch: 56, batch: 156/197] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0186, 0.0206, 0.0098, 0.0183, 0.0223, 0.0119, 0.8986],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.007

[Epoch: 56, batch: 195/197] total loss per batch: 0.659
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0225, 0.8815, 0.0283, 0.0173, 0.0161, 0.0183, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 57, batch: 39/197] total loss per batch: 0.673
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3216, 0.0160, 0.0322, 0.0092, 0.0222, 0.5924, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.035

[Epoch: 57, batch: 78/197] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0257, 0.2120, 0.0398, 0.5950, 0.0018, 0.0997, 0.0261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 57, batch: 117/197] total loss per batch: 0.670
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6299, 0.0186, 0.2932, 0.0058, 0.0255, 0.0128, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.014

[Epoch: 57, batch: 156/197] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0284, 0.0197, 0.0128, 0.0280, 0.0282, 0.0160, 0.8670],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.007

[Epoch: 57, batch: 195/197] total loss per batch: 0.660
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0118, 0.9304, 0.0168, 0.0058, 0.0101, 0.0103, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 58, batch: 39/197] total loss per batch: 0.674
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.7070, 0.0185, 0.0187, 0.0205, 0.0101, 0.2165, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 58, batch: 78/197] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.7518e-02, 9.1054e-02, 2.6158e-02, 7.9022e-01, 7.4215e-04, 5.6332e-02,
        1.7980e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.034

[Epoch: 58, batch: 117/197] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6712, 0.0118, 0.2712, 0.0048, 0.0173, 0.0156, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.000

[Epoch: 58, batch: 156/197] total loss per batch: 0.671
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0128, 0.0217, 0.0103, 0.0203, 0.0201, 0.0194, 0.8954],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.045

[Epoch: 58, batch: 195/197] total loss per batch: 0.661
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0210, 0.8662, 0.0375, 0.0192, 0.0138, 0.0206, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.006

[Epoch: 59, batch: 39/197] total loss per batch: 0.673
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3485, 0.0227, 0.0298, 0.0138, 0.0229, 0.5544, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 59, batch: 78/197] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0398, 0.2820, 0.0590, 0.4991, 0.0021, 0.0931, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.035

[Epoch: 59, batch: 117/197] total loss per batch: 0.670
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6191, 0.0162, 0.3001, 0.0049, 0.0318, 0.0151, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 59, batch: 156/197] total loss per batch: 0.671
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0314, 0.0336, 0.0205, 0.0346, 0.0623, 0.0199, 0.7977],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.001

[Epoch: 59, batch: 195/197] total loss per batch: 0.661
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0104, 0.9441, 0.0110, 0.0040, 0.0102, 0.0099, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 60, batch: 39/197] total loss per batch: 0.672
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5206, 0.0207, 0.0187, 0.0238, 0.0132, 0.3947, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 60, batch: 78/197] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.2270e-02, 4.4875e-02, 1.7503e-02, 8.5699e-01, 5.2377e-04, 4.9193e-02,
        1.8645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.025

[Epoch: 60, batch: 117/197] total loss per batch: 0.668
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6415, 0.0128, 0.2945, 0.0055, 0.0196, 0.0171, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.005

[Epoch: 60, batch: 156/197] total loss per batch: 0.670
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0112, 0.0158, 0.0071, 0.0151, 0.0090, 0.0127, 0.9291],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.089

[Epoch: 60, batch: 195/197] total loss per batch: 0.661
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0191, 0.8701, 0.0357, 0.0184, 0.0176, 0.0204, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.012

[Epoch: 61, batch: 39/197] total loss per batch: 0.670
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5952, 0.0307, 0.0250, 0.0153, 0.0179, 0.3075, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 61, batch: 78/197] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0306, 0.2584, 0.0502, 0.5238, 0.0017, 0.1060, 0.0293],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 61, batch: 117/197] total loss per batch: 0.666
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7034, 0.0142, 0.2321, 0.0042, 0.0227, 0.0121, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.010

[Epoch: 61, batch: 156/197] total loss per batch: 0.669
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0213, 0.0339, 0.0196, 0.0233, 0.0489, 0.0177, 0.8354],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.007

[Epoch: 61, batch: 195/197] total loss per batch: 0.658
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0076, 0.9401, 0.0118, 0.0061, 0.0144, 0.0067, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 62, batch: 39/197] total loss per batch: 0.665
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4439, 0.0222, 0.0180, 0.0163, 0.0151, 0.4773, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 62, batch: 78/197] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.0596e-02, 8.1429e-02, 2.8839e-02, 7.8869e-01, 5.9611e-04, 6.0943e-02,
        1.8909e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 62, batch: 117/197] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6701, 0.0161, 0.2642, 0.0053, 0.0220, 0.0131, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.012

[Epoch: 62, batch: 156/197] total loss per batch: 0.663
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0189, 0.0203, 0.0101, 0.0198, 0.0171, 0.0138, 0.9001],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.025

[Epoch: 62, batch: 195/197] total loss per batch: 0.652
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0204, 0.8720, 0.0310, 0.0120, 0.0173, 0.0212, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 63, batch: 39/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4604, 0.0292, 0.0245, 0.0159, 0.0160, 0.4449, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 63, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0229, 0.2106, 0.0422, 0.6032, 0.0011, 0.0924, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.031

[Epoch: 63, batch: 117/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7194, 0.0129, 0.2207, 0.0050, 0.0199, 0.0134, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.015

[Epoch: 63, batch: 156/197] total loss per batch: 0.661
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0191, 0.0300, 0.0147, 0.0219, 0.0317, 0.0156, 0.8670],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.014

[Epoch: 63, batch: 195/197] total loss per batch: 0.651
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0138, 0.9207, 0.0168, 0.0096, 0.0144, 0.0104, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.000

[Epoch: 64, batch: 39/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5887, 0.0200, 0.0157, 0.0144, 0.0138, 0.3410, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.027

[Epoch: 64, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.7362e-02, 1.1238e-01, 3.6361e-02, 7.5433e-01, 6.7058e-04, 5.7446e-02,
        2.1457e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.030

[Epoch: 64, batch: 117/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6658, 0.0140, 0.2729, 0.0051, 0.0214, 0.0114, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.007

[Epoch: 64, batch: 156/197] total loss per batch: 0.661
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0206, 0.0235, 0.0131, 0.0240, 0.0229, 0.0177, 0.8782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.017

[Epoch: 64, batch: 195/197] total loss per batch: 0.651
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0160, 0.8857, 0.0273, 0.0101, 0.0158, 0.0235, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.000

[Epoch: 65, batch: 39/197] total loss per batch: 0.663
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4154, 0.0311, 0.0231, 0.0161, 0.0154, 0.4889, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 65, batch: 78/197] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0250, 0.1796, 0.0366, 0.6480, 0.0008, 0.0864, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 65, batch: 117/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6873, 0.0133, 0.2475, 0.0053, 0.0218, 0.0149, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.020

[Epoch: 65, batch: 156/197] total loss per batch: 0.662
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0184, 0.0262, 0.0126, 0.0205, 0.0286, 0.0160, 0.8777],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.030

[Epoch: 65, batch: 195/197] total loss per batch: 0.652
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0134, 0.9152, 0.0186, 0.0113, 0.0161, 0.0110, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.002

[Epoch: 66, batch: 39/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5397, 0.0204, 0.0182, 0.0133, 0.0141, 0.3872, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 66, batch: 78/197] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0184, 0.1451, 0.0407, 0.7129, 0.0008, 0.0584, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.026

[Epoch: 66, batch: 117/197] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6699, 0.0153, 0.2645, 0.0069, 0.0214, 0.0124, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.011

[Epoch: 66, batch: 156/197] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0175, 0.0216, 0.0162, 0.0229, 0.0293, 0.0187, 0.8738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.021

[Epoch: 66, batch: 195/197] total loss per batch: 0.653
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0175, 0.8766, 0.0295, 0.0108, 0.0166, 0.0267, 0.0223],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 67, batch: 39/197] total loss per batch: 0.666
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5589, 0.0295, 0.0175, 0.0160, 0.0160, 0.3545, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 67, batch: 78/197] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0273, 0.1579, 0.0459, 0.6754, 0.0008, 0.0673, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.033

[Epoch: 67, batch: 117/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6569, 0.0109, 0.2774, 0.0052, 0.0229, 0.0160, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 67, batch: 156/197] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0202, 0.0305, 0.0118, 0.0242, 0.0299, 0.0135, 0.8699],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.029

[Epoch: 67, batch: 195/197] total loss per batch: 0.655
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0114, 0.9231, 0.0180, 0.0089, 0.0171, 0.0099, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.003

[Epoch: 68, batch: 39/197] total loss per batch: 0.668
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.3794, 0.0227, 0.0234, 0.0180, 0.0142, 0.5335, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.026

[Epoch: 68, batch: 78/197] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0149, 0.1520, 0.0408, 0.7172, 0.0010, 0.0545, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.019

[Epoch: 68, batch: 117/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6662, 0.0137, 0.2768, 0.0052, 0.0197, 0.0104, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.008

[Epoch: 68, batch: 156/197] total loss per batch: 0.668
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0155, 0.0200, 0.0170, 0.0200, 0.0287, 0.0230, 0.8758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 68, batch: 195/197] total loss per batch: 0.656
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0143, 0.8682, 0.0299, 0.0157, 0.0218, 0.0276, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 69, batch: 39/197] total loss per batch: 0.668
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5695, 0.0281, 0.0177, 0.0149, 0.0229, 0.3385, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 69, batch: 78/197] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0311, 0.1753, 0.0437, 0.6437, 0.0011, 0.0783, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 69, batch: 117/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6718, 0.0104, 0.2674, 0.0067, 0.0212, 0.0137, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 69, batch: 156/197] total loss per batch: 0.667
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0214, 0.0200, 0.0119, 0.0246, 0.0257, 0.0123, 0.8841],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.041

[Epoch: 69, batch: 195/197] total loss per batch: 0.655
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0114, 0.9280, 0.0163, 0.0090, 0.0126, 0.0100, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 70, batch: 39/197] total loss per batch: 0.666
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4427, 0.0283, 0.0291, 0.0245, 0.0151, 0.4489, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.028

[Epoch: 70, batch: 78/197] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.5316e-02, 1.3018e-01, 3.6065e-02, 7.3014e-01, 6.6694e-04, 7.0266e-02,
        1.7366e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.019

[Epoch: 70, batch: 117/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6150, 0.0132, 0.3051, 0.0086, 0.0326, 0.0133, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.010

[Epoch: 70, batch: 156/197] total loss per batch: 0.666
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0145, 0.0298, 0.0189, 0.0251, 0.0263, 0.0249, 0.8605],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.033

[Epoch: 70, batch: 195/197] total loss per batch: 0.654
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0090, 0.9053, 0.0234, 0.0108, 0.0154, 0.0173, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.000

[Epoch: 71, batch: 39/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5400, 0.0208, 0.0131, 0.0131, 0.0121, 0.3938, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 71, batch: 78/197] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0297, 0.1658, 0.0359, 0.6732, 0.0008, 0.0726, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.030

[Epoch: 71, batch: 117/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.7021, 0.0099, 0.2469, 0.0046, 0.0156, 0.0114, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 71, batch: 156/197] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0167, 0.0184, 0.0110, 0.0208, 0.0256, 0.0120, 0.8955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.031

[Epoch: 71, batch: 195/197] total loss per batch: 0.653
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0150, 0.8829, 0.0339, 0.0141, 0.0168, 0.0149, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 72, batch: 39/197] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5053, 0.0240, 0.0195, 0.0164, 0.0160, 0.4110, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 72, batch: 78/197] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.7607e-02, 1.5020e-01, 4.0709e-02, 7.0566e-01, 6.3828e-04, 6.4419e-02,
        2.0767e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 72, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6409, 0.0127, 0.2903, 0.0072, 0.0248, 0.0142, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 72, batch: 156/197] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0182, 0.0262, 0.0158, 0.0258, 0.0262, 0.0213, 0.8665],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.010

[Epoch: 72, batch: 195/197] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0115, 0.9001, 0.0232, 0.0118, 0.0201, 0.0167, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.004

[Epoch: 73, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5110, 0.0213, 0.0164, 0.0127, 0.0132, 0.4177, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 73, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.0190e-02, 1.7318e-01, 3.4902e-02, 6.8439e-01, 6.3633e-04, 6.8174e-02,
        1.8521e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 73, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6813, 0.0103, 0.2627, 0.0048, 0.0188, 0.0123, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.019

[Epoch: 73, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0179, 0.0250, 0.0148, 0.0221, 0.0273, 0.0139, 0.8789],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.028

[Epoch: 73, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0120, 0.9083, 0.0231, 0.0116, 0.0136, 0.0153, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 74, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5348, 0.0232, 0.0173, 0.0143, 0.0136, 0.3897, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 74, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.2844e-02, 1.7984e-01, 3.8042e-02, 6.6052e-01, 6.1708e-04, 7.6723e-02,
        2.1420e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 74, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6594, 0.0128, 0.2755, 0.0076, 0.0221, 0.0137, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.017

[Epoch: 74, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0227, 0.0263, 0.0165, 0.0253, 0.0272, 0.0189, 0.8631],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 0.004

[Epoch: 74, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0121, 0.8941, 0.0235, 0.0127, 0.0214, 0.0178, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.002

[Epoch: 75, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4551, 0.0222, 0.0165, 0.0115, 0.0130, 0.4740, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 75, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.7842e-02, 1.5310e-01, 3.6395e-02, 7.1733e-01, 5.8348e-04, 5.6997e-02,
        1.7754e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 75, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6647, 0.0116, 0.2712, 0.0048, 0.0229, 0.0139, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.019

[Epoch: 75, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0148, 0.0207, 0.0140, 0.0216, 0.0230, 0.0134, 0.8925],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.043

[Epoch: 75, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0132, 0.8988, 0.0302, 0.0119, 0.0136, 0.0185, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 76, batch: 39/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5903, 0.0253, 0.0162, 0.0149, 0.0110, 0.3348, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 76, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.4933e-02, 1.7745e-01, 3.5476e-02, 6.6628e-01, 4.5525e-04, 7.3806e-02,
        2.1594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 76, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6693, 0.0111, 0.2650, 0.0084, 0.0233, 0.0137, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.019

[Epoch: 76, batch: 156/197] total loss per batch: 0.661
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0231, 0.0258, 0.0169, 0.0225, 0.0335, 0.0199, 0.8582],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.004

[Epoch: 76, batch: 195/197] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0102, 0.9060, 0.0187, 0.0114, 0.0199, 0.0169, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.001

[Epoch: 77, batch: 39/197] total loss per batch: 0.662
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4132, 0.0255, 0.0165, 0.0128, 0.0173, 0.5073, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 77, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.6515e-02, 1.7460e-01, 4.0335e-02, 6.9365e-01, 6.0992e-04, 5.5911e-02,
        1.8377e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.028

[Epoch: 77, batch: 117/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6690, 0.0123, 0.2652, 0.0047, 0.0221, 0.0152, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.022

[Epoch: 77, batch: 156/197] total loss per batch: 0.662
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0184, 0.0239, 0.0173, 0.0247, 0.0274, 0.0170, 0.8712],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.024

[Epoch: 77, batch: 195/197] total loss per batch: 0.652
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0146, 0.8869, 0.0346, 0.0123, 0.0165, 0.0181, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 78, batch: 39/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5587, 0.0275, 0.0183, 0.0132, 0.0093, 0.3655, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 78, batch: 78/197] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.9987e-02, 1.5016e-01, 3.9568e-02, 6.8473e-01, 4.8085e-04, 7.0349e-02,
        2.4726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 78, batch: 117/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6437, 0.0085, 0.2927, 0.0063, 0.0269, 0.0122, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 78, batch: 156/197] total loss per batch: 0.663
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0195, 0.0191, 0.0134, 0.0179, 0.0305, 0.0150, 0.8847],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.020

[Epoch: 78, batch: 195/197] total loss per batch: 0.653
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0084, 0.9103, 0.0182, 0.0119, 0.0173, 0.0167, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 79, batch: 39/197] total loss per batch: 0.664
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5187, 0.0264, 0.0150, 0.0163, 0.0178, 0.3978, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 79, batch: 78/197] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.5741e-02, 1.4447e-01, 3.1475e-02, 7.3108e-01, 5.2612e-04, 6.0097e-02,
        1.6610e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.030

[Epoch: 79, batch: 117/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6650, 0.0127, 0.2687, 0.0073, 0.0202, 0.0152, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.017

[Epoch: 79, batch: 156/197] total loss per batch: 0.662
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0205, 0.0338, 0.0235, 0.0298, 0.0291, 0.0208, 0.8426],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.032

[Epoch: 79, batch: 195/197] total loss per batch: 0.653
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0135, 0.8972, 0.0283, 0.0115, 0.0173, 0.0168, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 -0.000

[Epoch: 80, batch: 39/197] total loss per batch: 0.664
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4041, 0.0267, 0.0186, 0.0116, 0.0118, 0.5197, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 80, batch: 78/197] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0368, 0.1943, 0.0553, 0.6105, 0.0007, 0.0736, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 80, batch: 117/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6094, 0.0096, 0.3197, 0.0049, 0.0306, 0.0151, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 80, batch: 156/197] total loss per batch: 0.662
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0163, 0.0151, 0.0114, 0.0140, 0.0238, 0.0120, 0.9073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.033

[Epoch: 80, batch: 195/197] total loss per batch: 0.652
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0103, 0.8813, 0.0277, 0.0148, 0.0197, 0.0222, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 81, batch: 39/197] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.6833, 0.0200, 0.0123, 0.0137, 0.0131, 0.2505, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 81, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.4382e-02, 8.1639e-02, 2.0989e-02, 8.2180e-01, 4.2632e-04, 4.9185e-02,
        1.1578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.032

[Epoch: 81, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6921, 0.0120, 0.2436, 0.0070, 0.0211, 0.0149, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.016

[Epoch: 81, batch: 156/197] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0216, 0.0343, 0.0205, 0.0264, 0.0318, 0.0214, 0.8441],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.011

[Epoch: 81, batch: 195/197] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0114, 0.9074, 0.0243, 0.0099, 0.0176, 0.0178, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 82, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4110, 0.0254, 0.0179, 0.0117, 0.0141, 0.5116, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 82, batch: 78/197] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([0.0327, 0.2404, 0.0446, 0.5814, 0.0007, 0.0767, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.027

[Epoch: 82, batch: 117/197] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6682, 0.0101, 0.2705, 0.0051, 0.0222, 0.0147, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 82, batch: 156/197] total loss per batch: 0.658
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0164, 0.0217, 0.0138, 0.0173, 0.0246, 0.0152, 0.8910],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.035

[Epoch: 82, batch: 195/197] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0109, 0.8936, 0.0256, 0.0131, 0.0167, 0.0215, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.002

[Epoch: 83, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5809, 0.0245, 0.0143, 0.0141, 0.0119, 0.3468, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 83, batch: 78/197] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.7175e-02, 1.1467e-01, 3.1463e-02, 7.6753e-01, 4.0465e-04, 5.2947e-02,
        1.5819e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.030

[Epoch: 83, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6696, 0.0109, 0.2700, 0.0070, 0.0211, 0.0124, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.017

[Epoch: 83, batch: 156/197] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0201, 0.0276, 0.0157, 0.0206, 0.0263, 0.0179, 0.8718],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.022

[Epoch: 83, batch: 195/197] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0120, 0.8993, 0.0251, 0.0123, 0.0177, 0.0192, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 84, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4453, 0.0246, 0.0161, 0.0122, 0.0124, 0.4806, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 84, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.6345e-02, 2.0371e-01, 3.9931e-02, 6.3788e-01, 5.8027e-04, 7.0391e-02,
        2.1165e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.028

[Epoch: 84, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6784, 0.0107, 0.2634, 0.0053, 0.0204, 0.0132, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 84, batch: 156/197] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0178, 0.0233, 0.0153, 0.0182, 0.0249, 0.0161, 0.8844],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.026

[Epoch: 84, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0102, 0.9075, 0.0218, 0.0104, 0.0157, 0.0190, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 85, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5498, 0.0261, 0.0147, 0.0139, 0.0119, 0.3759, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 85, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.8950e-02, 1.4375e-01, 3.5948e-02, 7.2833e-01, 3.8986e-04, 5.5477e-02,
        1.7152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.028

[Epoch: 85, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6679, 0.0098, 0.2738, 0.0067, 0.0209, 0.0117, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.017

[Epoch: 85, batch: 156/197] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0197, 0.0262, 0.0160, 0.0209, 0.0261, 0.0173, 0.8738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.012

[Epoch: 85, batch: 195/197] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0132, 0.8857, 0.0284, 0.0151, 0.0185, 0.0223, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 86, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4675, 0.0251, 0.0151, 0.0123, 0.0115, 0.4591, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.034

[Epoch: 86, batch: 78/197] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.5876e-02, 1.7466e-01, 3.7344e-02, 6.7260e-01, 5.0400e-04, 6.9164e-02,
        1.9852e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.025

[Epoch: 86, batch: 117/197] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6791, 0.0113, 0.2613, 0.0052, 0.0215, 0.0131, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 86, batch: 156/197] total loss per batch: 0.658
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0172, 0.0235, 0.0158, 0.0185, 0.0297, 0.0167, 0.8787],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.026

[Epoch: 86, batch: 195/197] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0089, 0.9182, 0.0194, 0.0087, 0.0148, 0.0168, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 87, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5566, 0.0258, 0.0150, 0.0149, 0.0104, 0.3703, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 87, batch: 78/197] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.0426e-02, 1.6372e-01, 3.9533e-02, 6.9693e-01, 3.3847e-04, 6.1179e-02,
        1.7874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.026

[Epoch: 87, batch: 117/197] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6551, 0.0098, 0.2859, 0.0069, 0.0213, 0.0114, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.018

[Epoch: 87, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0214, 0.0236, 0.0168, 0.0223, 0.0259, 0.0188, 0.8713],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.014

[Epoch: 87, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0156, 0.8750, 0.0320, 0.0177, 0.0188, 0.0233, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 88, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4327, 0.0243, 0.0131, 0.0104, 0.0116, 0.4982, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.032

[Epoch: 88, batch: 78/197] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.9414e-02, 1.2820e-01, 3.5689e-02, 7.1816e-01, 4.4272e-04, 6.9240e-02,
        1.8852e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.027

[Epoch: 88, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6748, 0.0110, 0.2671, 0.0052, 0.0212, 0.0128, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 88, batch: 156/197] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0168, 0.0252, 0.0168, 0.0175, 0.0321, 0.0175, 0.8743],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 88, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0070, 0.9268, 0.0178, 0.0078, 0.0142, 0.0153, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 89, batch: 39/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5965, 0.0293, 0.0174, 0.0196, 0.0108, 0.3200, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.031

[Epoch: 89, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.2373e-02, 1.9335e-01, 4.5634e-02, 6.5048e-01, 3.4326e-04, 6.7308e-02,
        2.0513e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.023

[Epoch: 89, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6353, 0.0091, 0.3057, 0.0070, 0.0207, 0.0120, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.020

[Epoch: 89, batch: 156/197] total loss per batch: 0.661
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0213, 0.0213, 0.0153, 0.0220, 0.0253, 0.0181, 0.8767],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.020

[Epoch: 89, batch: 195/197] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0188, 0.8598, 0.0378, 0.0182, 0.0218, 0.0253, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 90, batch: 39/197] total loss per batch: 0.661
Policy (actual, predicted): 0 5
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4431, 0.0222, 0.0135, 0.0119, 0.0131, 0.4869, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.027

[Epoch: 90, batch: 78/197] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([3.0616e-02, 1.1656e-01, 3.3579e-02, 7.2980e-01, 4.5905e-04, 6.8826e-02,
        2.0170e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.029

[Epoch: 90, batch: 117/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6651, 0.0111, 0.2731, 0.0054, 0.0227, 0.0142, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.024

[Epoch: 90, batch: 156/197] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0181, 0.0236, 0.0171, 0.0181, 0.0299, 0.0169, 0.8763],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 90, batch: 195/197] total loss per batch: 0.650
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0068, 0.9358, 0.0135, 0.0081, 0.0124, 0.0124, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 91, batch: 39/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5499, 0.0302, 0.0168, 0.0138, 0.0096, 0.3723, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 91, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.3695e-02, 2.0989e-01, 4.5171e-02, 6.3386e-01, 3.4375e-04, 6.6580e-02,
        2.0464e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.024

[Epoch: 91, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6581, 0.0096, 0.2786, 0.0067, 0.0228, 0.0141, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.024

[Epoch: 91, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0205, 0.0255, 0.0172, 0.0235, 0.0313, 0.0196, 0.8623],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.038

[Epoch: 91, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0130, 0.8801, 0.0310, 0.0126, 0.0216, 0.0256, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.009

[Epoch: 92, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5034, 0.0232, 0.0140, 0.0131, 0.0122, 0.4258, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.027

[Epoch: 92, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.4029e-02, 1.4571e-01, 3.6688e-02, 7.1085e-01, 3.8704e-04, 6.3491e-02,
        1.8850e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.023

[Epoch: 92, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6764, 0.0096, 0.2682, 0.0056, 0.0192, 0.0126, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 92, batch: 156/197] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0208, 0.0230, 0.0175, 0.0209, 0.0246, 0.0165, 0.8767],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 92, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0096, 0.9099, 0.0192, 0.0112, 0.0172, 0.0183, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.004

[Epoch: 93, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4902, 0.0257, 0.0147, 0.0122, 0.0101, 0.4389, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 93, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.3137e-02, 1.7515e-01, 4.0334e-02, 6.7863e-01, 3.2854e-04, 6.3092e-02,
        1.9331e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.024

[Epoch: 93, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6658, 0.0096, 0.2741, 0.0063, 0.0219, 0.0132, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.024

[Epoch: 93, batch: 156/197] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0209, 0.0255, 0.0181, 0.0215, 0.0304, 0.0189, 0.8647],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.019

[Epoch: 93, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0111, 0.8986, 0.0242, 0.0106, 0.0188, 0.0221, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 94, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5324, 0.0244, 0.0136, 0.0138, 0.0105, 0.3971, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 94, batch: 78/197] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.4401e-02, 1.7105e-01, 3.8221e-02, 6.8260e-01, 3.6833e-04, 6.5234e-02,
        1.8133e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.021

[Epoch: 94, batch: 117/197] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6745, 0.0101, 0.2689, 0.0059, 0.0199, 0.0122, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.023

[Epoch: 94, batch: 156/197] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0211, 0.0239, 0.0181, 0.0198, 0.0242, 0.0165, 0.8764],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.015

[Epoch: 94, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0108, 0.9047, 0.0221, 0.0116, 0.0168, 0.0188, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.005

[Epoch: 95, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4942, 0.0250, 0.0139, 0.0126, 0.0104, 0.4359, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 95, batch: 78/197] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.1494e-02, 1.5569e-01, 3.8597e-02, 7.0494e-01, 2.9563e-04, 6.0749e-02,
        1.8241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.023

[Epoch: 95, batch: 117/197] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6533, 0.0099, 0.2863, 0.0065, 0.0217, 0.0129, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.025

[Epoch: 95, batch: 156/197] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0190, 0.0233, 0.0169, 0.0201, 0.0278, 0.0177, 0.8752],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.018

[Epoch: 95, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0113, 0.8911, 0.0270, 0.0112, 0.0196, 0.0240, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 96, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5316, 0.0252, 0.0140, 0.0145, 0.0097, 0.3963, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 96, batch: 78/197] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([2.7286e-02, 1.8195e-01, 4.1447e-02, 6.6129e-01, 3.7149e-04, 6.9260e-02,
        1.8394e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.020

[Epoch: 96, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6785, 0.0104, 0.2647, 0.0060, 0.0203, 0.0118, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.024

[Epoch: 96, batch: 156/197] total loss per batch: 0.656
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0214, 0.0244, 0.0184, 0.0199, 0.0260, 0.0171, 0.8728],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.018

[Epoch: 96, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0106, 0.9111, 0.0211, 0.0108, 0.0154, 0.0175, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.006

[Epoch: 97, batch: 39/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4963, 0.0249, 0.0139, 0.0134, 0.0109, 0.4327, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.034

[Epoch: 97, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.9145e-02, 1.3214e-01, 3.7387e-02, 7.3504e-01, 2.7384e-04, 5.7999e-02,
        1.8011e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.021

[Epoch: 97, batch: 117/197] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6414, 0.0096, 0.2981, 0.0068, 0.0217, 0.0128, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.026

[Epoch: 97, batch: 156/197] total loss per batch: 0.657
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0188, 0.0237, 0.0177, 0.0194, 0.0288, 0.0183, 0.8734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.012

[Epoch: 97, batch: 195/197] total loss per batch: 0.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0115, 0.8887, 0.0271, 0.0117, 0.0208, 0.0236, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 98, batch: 39/197] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5275, 0.0256, 0.0138, 0.0142, 0.0088, 0.4013, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.029

[Epoch: 98, batch: 78/197] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([3.3324e-02, 1.9512e-01, 4.1344e-02, 6.3860e-01, 3.5633e-04, 7.2927e-02,
        1.8336e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.021

[Epoch: 98, batch: 117/197] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6707, 0.0101, 0.2731, 0.0060, 0.0203, 0.0113, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.027

[Epoch: 98, batch: 156/197] total loss per batch: 0.658
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0187, 0.0223, 0.0173, 0.0183, 0.0233, 0.0158, 0.8844],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.017

[Epoch: 98, batch: 195/197] total loss per batch: 0.648
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0113, 0.9043, 0.0248, 0.0106, 0.0161, 0.0187, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.008

[Epoch: 99, batch: 39/197] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.4845, 0.0263, 0.0151, 0.0154, 0.0124, 0.4378, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.033

[Epoch: 99, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([1.6486e-02, 1.1149e-01, 3.7095e-02, 7.5572e-01, 2.5478e-04, 6.0557e-02,
        1.8401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.019

[Epoch: 99, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6420, 0.0109, 0.2936, 0.0071, 0.0233, 0.0136, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.021

[Epoch: 99, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0186, 0.0233, 0.0199, 0.0195, 0.0341, 0.0210, 0.8636],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.003

[Epoch: 99, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0101, 0.9023, 0.0229, 0.0114, 0.0197, 0.0186, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

[Epoch: 100, batch: 39/197] total loss per batch: 0.661
Policy (actual, predicted): 0 0
Policy data: tensor([0.5100, 0.0250, 0.0100, 0.0150, 0.0100, 0.4200, 0.0100])
Policy pred: tensor([0.5165, 0.0247, 0.0138, 0.0139, 0.0088, 0.4130, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.041 0.030

[Epoch: 100, batch: 78/197] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0250, 0.1650, 0.0400, 0.6900, 0.0000, 0.0650, 0.0150])
Policy pred: tensor([4.0370e-02, 2.0107e-01, 4.0350e-02, 6.2426e-01, 3.8873e-04, 7.5041e-02,
        1.8520e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.022

[Epoch: 100, batch: 117/197] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.6650, 0.0100, 0.2800, 0.0050, 0.0200, 0.0100, 0.0100])
Policy pred: tensor([0.6507, 0.0094, 0.2913, 0.0060, 0.0213, 0.0122, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.020 -0.027

[Epoch: 100, batch: 156/197] total loss per batch: 0.659
Policy (actual, predicted): 6 6
Policy data: tensor([0.0200, 0.0250, 0.0200, 0.0150, 0.0250, 0.0200, 0.8750])
Policy pred: tensor([0.0190, 0.0225, 0.0151, 0.0177, 0.0197, 0.0151, 0.8909],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.025 -0.020

[Epoch: 100, batch: 195/197] total loss per batch: 0.649
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9000, 0.0250, 0.0100, 0.0200, 0.0200, 0.0150])
Policy pred: tensor([0.0112, 0.8959, 0.0271, 0.0106, 0.0184, 0.0225, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.010 0.007

