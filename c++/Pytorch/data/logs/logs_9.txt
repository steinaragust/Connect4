Training set samples: 6566
Batch size: 32
[Epoch: 1, batch: 41/206] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.7331e-05, 7.3082e-01, 1.2547e-06, 1.1733e-06, 2.6914e-01, 7.8875e-07,
        2.5618e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.314

[Epoch: 1, batch: 82/206] total loss per batch: 0.750
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.2680e-02, 1.0134e-01, 1.2152e-07, 1.3631e-01, 5.0699e-02, 1.1782e-06,
        6.8898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.025

[Epoch: 1, batch: 123/206] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0106, 0.0040, 0.0132, 0.9260, 0.0146, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 -0.004

[Epoch: 1, batch: 164/206] total loss per batch: 0.791
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([6.2751e-03, 1.0140e-02, 3.0065e-06, 9.1085e-01, 1.2265e-02, 3.5505e-05,
        6.0428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.043

[Epoch: 1, batch: 205/206] total loss per batch: 0.778
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0020, 0.0017, 0.0165, 0.9626, 0.0087, 0.0024, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 2, batch: 41/206] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.4973e-06, 5.5437e-01, 4.2497e-06, 2.1645e-06, 4.4552e-01, 7.6002e-06,
        1.0074e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.976

[Epoch: 2, batch: 82/206] total loss per batch: 0.605
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.1840e-02, 5.8454e-02, 4.4728e-07, 1.4106e-01, 2.3860e-02, 3.2799e-06,
        7.6478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.048

[Epoch: 2, batch: 123/206] total loss per batch: 0.607
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0038, 0.0040, 0.0151, 0.9478, 0.0130, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 -0.006

[Epoch: 2, batch: 164/206] total loss per batch: 0.618
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.6193e-03, 1.8267e-03, 4.9168e-07, 9.8706e-01, 3.9075e-03, 2.7442e-05,
        3.5599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.015

[Epoch: 2, batch: 205/206] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0019, 0.0260, 0.9474, 0.0159, 0.0022, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 3, batch: 41/206] total loss per batch: 0.548
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.8001e-06, 5.4479e-01, 1.9677e-06, 2.6250e-06, 4.5515e-01, 7.2406e-06,
        3.8648e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.983

[Epoch: 3, batch: 82/206] total loss per batch: 0.541
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.4034e-03, 5.0514e-02, 1.2366e-06, 8.7971e-02, 1.4514e-02, 4.6310e-06,
        8.4159e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.005

[Epoch: 3, batch: 123/206] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0062, 0.0035, 0.0109, 0.9474, 0.0071, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 -0.005

[Epoch: 3, batch: 164/206] total loss per batch: 0.565
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([7.4362e-03, 5.5856e-03, 5.1945e-06, 9.2663e-01, 1.0839e-02, 3.1223e-05,
        4.9478e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.003

[Epoch: 3, batch: 205/206] total loss per batch: 0.569
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0035, 0.0436, 0.9142, 0.0203, 0.0047, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 4, batch: 41/206] total loss per batch: 0.525
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.5741e-06, 5.6269e-01, 2.4279e-05, 1.3197e-05, 4.3718e-01, 1.7196e-05,
        7.2024e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.971

[Epoch: 4, batch: 82/206] total loss per batch: 0.520
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8425e-03, 1.8193e-02, 2.5257e-06, 4.3719e-02, 1.1507e-02, 6.1598e-06,
        9.2173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 4, batch: 123/206] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0076, 0.0026, 0.0127, 0.9524, 0.0054, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.001

[Epoch: 4, batch: 164/206] total loss per batch: 0.549
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([1.0626e-02, 5.8655e-03, 1.5786e-06, 9.6595e-01, 8.6815e-03, 2.3933e-05,
        8.8560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.004

[Epoch: 4, batch: 205/206] total loss per batch: 0.556
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0024, 0.0019, 0.0204, 0.9579, 0.0096, 0.0022, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 5, batch: 41/206] total loss per batch: 0.521
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.5157e-06, 4.4401e-01, 3.8188e-05, 4.0405e-06, 5.5581e-01, 2.4612e-05,
        1.1376e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.968

[Epoch: 5, batch: 82/206] total loss per batch: 0.518
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.3417e-03, 6.4957e-03, 2.6985e-06, 2.6818e-02, 5.4968e-03, 4.6293e-06,
        9.5884e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.008

[Epoch: 5, batch: 123/206] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0115, 0.0037, 0.0155, 0.9379, 0.0106, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.004

[Epoch: 5, batch: 164/206] total loss per batch: 0.542
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([6.1676e-03, 4.3904e-03, 3.0399e-06, 9.6454e-01, 1.3936e-02, 2.6300e-05,
        1.0942e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.003

[Epoch: 5, batch: 205/206] total loss per batch: 0.550
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0035, 0.0116, 0.9621, 0.0100, 0.0025, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 6, batch: 41/206] total loss per batch: 0.520
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([8.0796e-06, 5.8014e-01, 4.3674e-05, 8.6511e-06, 4.1969e-01, 5.0980e-05,
        6.4972e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.979

[Epoch: 6, batch: 82/206] total loss per batch: 0.516
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.3428e-03, 1.8053e-02, 6.2104e-06, 1.1458e-01, 2.4510e-02, 7.7403e-06,
        8.4050e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 6, batch: 123/206] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0059, 0.0022, 0.0071, 0.9628, 0.0062, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 -0.005

[Epoch: 6, batch: 164/206] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.2180e-03, 3.7298e-03, 1.9691e-06, 9.8219e-01, 5.5989e-03, 2.6571e-05,
        5.2346e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.005

[Epoch: 6, batch: 205/206] total loss per batch: 0.541
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0030, 0.0233, 0.9408, 0.0206, 0.0026, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 7, batch: 41/206] total loss per batch: 0.512
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.6483e-06, 3.9839e-01, 4.0168e-05, 1.2148e-05, 6.0145e-01, 3.4673e-05,
        6.9838e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.977

[Epoch: 7, batch: 82/206] total loss per batch: 0.513
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([7.5909e-03, 2.5259e-02, 5.1029e-06, 1.6900e-02, 1.1271e-02, 1.5597e-05,
        9.3896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 7, batch: 123/206] total loss per batch: 0.525
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0044, 0.0035, 0.0113, 0.9668, 0.0031, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 -0.001

[Epoch: 7, batch: 164/206] total loss per batch: 0.530
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([2.5909e-03, 3.1199e-03, 2.5692e-06, 9.6522e-01, 6.3894e-03, 2.3698e-05,
        2.2651e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.002

[Epoch: 7, batch: 205/206] total loss per batch: 0.533
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0067, 0.0085, 0.9601, 0.0126, 0.0023, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 8, batch: 41/206] total loss per batch: 0.506
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.0431e-05, 7.1030e-01, 1.0475e-05, 4.8178e-06, 2.8960e-01, 2.3360e-05,
        6.0152e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.951

[Epoch: 8, batch: 82/206] total loss per batch: 0.503
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.1159e-03, 1.4983e-02, 2.2025e-06, 1.8062e-02, 1.0538e-02, 4.1841e-06,
        9.5329e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 8, batch: 123/206] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0067, 0.0056, 0.0066, 0.9661, 0.0044, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.003

[Epoch: 8, batch: 164/206] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([1.2033e-02, 5.1740e-03, 5.5601e-06, 9.5322e-01, 1.7228e-02, 5.6485e-05,
        1.2285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.005

[Epoch: 8, batch: 205/206] total loss per batch: 0.531
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0027, 0.0167, 0.9612, 0.0079, 0.0025, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 9, batch: 41/206] total loss per batch: 0.502
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.7393e-06, 4.2486e-01, 7.9013e-06, 5.8271e-06, 5.7507e-01, 1.6952e-05,
        3.8218e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 9, batch: 82/206] total loss per batch: 0.499
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.8367e-03, 1.3364e-02, 7.8688e-06, 1.6191e-02, 6.5919e-03, 7.3358e-06,
        9.6200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 9, batch: 123/206] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0044, 0.0045, 0.0099, 0.9644, 0.0035, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.021

[Epoch: 9, batch: 164/206] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.9257e-03, 3.4899e-03, 3.2627e-06, 9.6648e-01, 7.1182e-03, 7.3499e-05,
        1.6910e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.002

[Epoch: 9, batch: 205/206] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0046, 0.0086, 0.9632, 0.0127, 0.0026, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 10, batch: 41/206] total loss per batch: 0.501
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.5315e-05, 6.7046e-01, 1.7557e-05, 3.8882e-06, 3.2945e-01, 1.9110e-05,
        3.7469e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.941

[Epoch: 10, batch: 82/206] total loss per batch: 0.498
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.7749e-03, 1.0936e-02, 4.5851e-06, 1.6708e-02, 5.6737e-03, 5.9652e-06,
        9.6490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 10, batch: 123/206] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0062, 0.0057, 0.0125, 0.9551, 0.0086, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.035

[Epoch: 10, batch: 164/206] total loss per batch: 0.521
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.0268e-03, 3.3557e-03, 2.0412e-06, 9.7959e-01, 7.2425e-03, 3.0900e-05,
        6.7567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.015

[Epoch: 10, batch: 205/206] total loss per batch: 0.521
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0034, 0.0120, 0.9571, 0.0141, 0.0021, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 11, batch: 41/206] total loss per batch: 0.498
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([8.2642e-06, 5.4295e-01, 3.0846e-05, 6.2919e-06, 4.5693e-01, 2.7118e-05,
        4.7746e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.954

[Epoch: 11, batch: 82/206] total loss per batch: 0.496
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.0542e-03, 1.5080e-02, 5.2801e-06, 1.0094e-02, 1.1142e-02, 6.0375e-06,
        9.6162e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.050

[Epoch: 11, batch: 123/206] total loss per batch: 0.508
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0062, 0.0042, 0.0096, 0.9639, 0.0046, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.023

[Epoch: 11, batch: 164/206] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.9894e-03, 3.3625e-03, 1.5180e-06, 9.7285e-01, 8.6687e-03, 3.7587e-05,
        1.1090e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.005

[Epoch: 11, batch: 205/206] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0039, 0.0092, 0.9667, 0.0096, 0.0020, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 12, batch: 41/206] total loss per batch: 0.493
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.3802e-05, 5.4002e-01, 2.1529e-05, 5.9517e-06, 4.5988e-01, 1.6404e-05,
        4.6139e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.952

[Epoch: 12, batch: 82/206] total loss per batch: 0.490
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.1561e-03, 1.1838e-02, 2.5313e-06, 1.0159e-02, 6.4275e-03, 4.3325e-06,
        9.6941e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 12, batch: 123/206] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0044, 0.0077, 0.9680, 0.0057, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.021

[Epoch: 12, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.9252e-03, 3.9802e-03, 1.7361e-06, 9.7670e-01, 6.8752e-03, 3.2543e-05,
        8.4819e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.009

[Epoch: 12, batch: 205/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0048, 0.0080, 0.9661, 0.0087, 0.0024, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 13, batch: 41/206] total loss per batch: 0.490
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.5248e-05, 4.8484e-01, 8.1296e-06, 3.0961e-06, 5.1510e-01, 8.4821e-06,
        2.6743e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.953

[Epoch: 13, batch: 82/206] total loss per batch: 0.488
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.7637e-03, 9.5642e-03, 2.0246e-06, 1.2875e-02, 5.7933e-03, 3.2001e-06,
        9.7000e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.042

[Epoch: 13, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0045, 0.0038, 0.0060, 0.9710, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 13, batch: 164/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.4684e-03, 2.9002e-03, 1.6900e-06, 9.7783e-01, 8.0406e-03, 4.0111e-05,
        7.7182e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.015

[Epoch: 13, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0040, 0.0079, 0.9665, 0.0095, 0.0025, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 14, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.6880e-06, 5.5379e-01, 7.6723e-06, 3.6845e-06, 4.4615e-01, 1.1406e-05,
        3.2920e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.953

[Epoch: 14, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.5562e-03, 1.5534e-02, 2.1809e-06, 1.0287e-02, 7.2048e-03, 4.8463e-06,
        9.6441e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 14, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0037, 0.0063, 0.9692, 0.0058, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 14, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.3615e-03, 4.5345e-03, 2.1540e-06, 9.7348e-01, 7.5567e-03, 2.2819e-05,
        9.0381e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.020

[Epoch: 14, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0055, 0.0078, 0.9668, 0.0082, 0.0024, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 15, batch: 41/206] total loss per batch: 0.490
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([8.8456e-06, 5.4858e-01, 4.1208e-06, 3.0711e-06, 4.5137e-01, 7.0922e-06,
        2.5758e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.950

[Epoch: 15, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.9815e-03, 6.1092e-03, 1.7102e-06, 9.6203e-03, 4.7707e-03, 2.6195e-06,
        9.7751e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 15, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0057, 0.0039, 0.0052, 0.9703, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.045

[Epoch: 15, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.7283e-03, 3.3641e-03, 1.1990e-06, 9.7817e-01, 5.7519e-03, 2.6386e-05,
        8.9596e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.017

[Epoch: 15, batch: 205/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0039, 0.0085, 0.9657, 0.0089, 0.0030, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 16, batch: 41/206] total loss per batch: 0.491
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.3422e-06, 5.2069e-01, 5.1945e-06, 3.7697e-06, 4.7925e-01, 7.8309e-06,
        3.6039e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.952

[Epoch: 16, batch: 82/206] total loss per batch: 0.488
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.5793e-03, 1.6519e-02, 2.1089e-06, 1.0784e-02, 8.5489e-03, 4.6203e-06,
        9.6056e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 16, batch: 123/206] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0057, 0.0044, 0.0070, 0.9671, 0.0054, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 16, batch: 164/206] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5754e-03, 4.9506e-03, 2.1484e-06, 9.7786e-01, 6.0474e-03, 2.1547e-05,
        6.5423e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 16, batch: 205/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0029, 0.0039, 0.0073, 0.9721, 0.0071, 0.0021, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 17, batch: 41/206] total loss per batch: 0.492
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.6417e-05, 6.4573e-01, 6.4406e-06, 4.1785e-06, 3.5419e-01, 8.7123e-06,
        3.4573e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.941

[Epoch: 17, batch: 82/206] total loss per batch: 0.490
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.0897e-03, 7.5073e-03, 2.0102e-06, 1.1905e-02, 5.8924e-03, 3.0795e-06,
        9.7260e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.047

[Epoch: 17, batch: 123/206] total loss per batch: 0.504
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0057, 0.0047, 0.0069, 0.9678, 0.0045, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.037

[Epoch: 17, batch: 164/206] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.3232e-03, 3.1192e-03, 1.8522e-06, 9.7595e-01, 7.2542e-03, 3.1316e-05,
        9.3219e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 17, batch: 205/206] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0041, 0.0075, 0.9682, 0.0073, 0.0032, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 18, batch: 41/206] total loss per batch: 0.494
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.9306e-06, 5.4300e-01, 8.3913e-06, 2.4521e-06, 4.5695e-01, 6.5402e-06,
        2.6963e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.957

[Epoch: 18, batch: 82/206] total loss per batch: 0.492
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.8791e-03, 1.5359e-02, 3.4186e-06, 8.1768e-03, 8.5430e-03, 5.5537e-06,
        9.6503e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.045

[Epoch: 18, batch: 123/206] total loss per batch: 0.507
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0053, 0.0078, 0.0063, 0.9630, 0.0065, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.017

[Epoch: 18, batch: 164/206] total loss per batch: 0.518
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.1996e-03, 5.5352e-03, 1.6062e-06, 9.7852e-01, 6.0526e-03, 3.0036e-05,
        6.6655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.003

[Epoch: 18, batch: 205/206] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0052, 0.0096, 0.9629, 0.0095, 0.0032, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 19, batch: 41/206] total loss per batch: 0.495
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.8720e-05, 5.4180e-01, 8.9781e-06, 3.6568e-06, 4.5810e-01, 1.5139e-05,
        5.4606e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.944

[Epoch: 19, batch: 82/206] total loss per batch: 0.493
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.3958e-03, 1.6865e-02, 2.5228e-06, 2.2348e-02, 2.1204e-02, 4.9768e-06,
        9.3518e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.087

[Epoch: 19, batch: 123/206] total loss per batch: 0.510
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0055, 0.0029, 0.0051, 0.9697, 0.0053, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.010

[Epoch: 19, batch: 164/206] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([6.8060e-03, 5.7777e-03, 5.7930e-06, 9.5443e-01, 5.2482e-03, 2.5331e-05,
        2.7711e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.040

[Epoch: 19, batch: 205/206] total loss per batch: 0.520
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0037, 0.0168, 0.9620, 0.0057, 0.0034, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 20, batch: 41/206] total loss per batch: 0.498
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.4851e-05, 5.7964e-01, 1.2414e-05, 2.6930e-06, 4.2029e-01, 1.5619e-05,
        2.9220e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.969

[Epoch: 20, batch: 82/206] total loss per batch: 0.495
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([1.4112e-03, 1.3298e-02, 3.0226e-06, 6.2525e-03, 6.3187e-03, 5.7632e-06,
        9.7271e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.073

[Epoch: 20, batch: 123/206] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0035, 0.0077, 0.0041, 0.9688, 0.0059, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.006

[Epoch: 20, batch: 164/206] total loss per batch: 0.525
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([2.0517e-03, 2.9854e-03, 2.5450e-06, 9.8735e-01, 3.9183e-03, 1.5263e-05,
        3.6752e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.002

[Epoch: 20, batch: 205/206] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0029, 0.0134, 0.9636, 0.0074, 0.0037, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 21, batch: 41/206] total loss per batch: 0.502
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([9.8952e-06, 4.9726e-01, 3.0145e-05, 8.3424e-06, 5.0262e-01, 1.1833e-05,
        6.8553e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.980

[Epoch: 21, batch: 82/206] total loss per batch: 0.500
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0516e-03, 5.2337e-03, 8.7171e-06, 9.2872e-03, 6.8136e-03, 9.3971e-06,
        9.7360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.052

[Epoch: 21, batch: 123/206] total loss per batch: 0.511
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0056, 0.0060, 0.0119, 0.9573, 0.0063, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 21, batch: 164/206] total loss per batch: 0.523
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0200e-03, 4.4991e-03, 2.0928e-06, 9.7485e-01, 4.7016e-03, 2.4247e-05,
        1.0899e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.043

[Epoch: 21, batch: 205/206] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.0032, 0.0057, 0.9777, 0.0041, 0.0021, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 22, batch: 41/206] total loss per batch: 0.496
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.1298e-05, 4.5457e-01, 6.5247e-06, 5.0125e-06, 5.4534e-01, 1.7496e-05,
        4.4244e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.981

[Epoch: 22, batch: 82/206] total loss per batch: 0.492
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.8994e-03, 1.2462e-02, 2.5180e-06, 1.6896e-02, 1.2128e-02, 3.7964e-06,
        9.5461e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.091

[Epoch: 22, batch: 123/206] total loss per batch: 0.507
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0031, 0.0041, 0.0046, 0.9762, 0.0033, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.009

[Epoch: 22, batch: 164/206] total loss per batch: 0.522
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.2554e-03, 3.8894e-03, 1.0735e-06, 9.8468e-01, 3.4625e-03, 3.2949e-05,
        4.6773e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.003

[Epoch: 22, batch: 205/206] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0044, 0.0089, 0.9665, 0.0087, 0.0026, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 23, batch: 41/206] total loss per batch: 0.492
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.0434e-05, 5.6809e-01, 8.6183e-06, 5.1320e-06, 4.3182e-01, 1.1970e-05,
        4.9266e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 23, batch: 82/206] total loss per batch: 0.489
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.1574e-03, 9.9069e-03, 3.1294e-06, 4.6974e-03, 5.4838e-03, 4.8092e-06,
        9.7575e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.053

[Epoch: 23, batch: 123/206] total loss per batch: 0.502
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0080, 0.0051, 0.0071, 0.0056, 0.9626, 0.0048, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.017

[Epoch: 23, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.1976e-03, 4.3415e-03, 1.5022e-06, 9.8032e-01, 5.1558e-03, 1.4378e-05,
        5.9741e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.013

[Epoch: 23, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0063, 0.0096, 0.9619, 0.0086, 0.0028, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 24, batch: 41/206] total loss per batch: 0.490
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.0446e-05, 5.3483e-01, 2.9491e-06, 3.1068e-06, 4.6511e-01, 6.0570e-06,
        3.4160e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 24, batch: 82/206] total loss per batch: 0.489
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8121e-03, 7.7611e-03, 3.4578e-06, 6.4934e-03, 6.2089e-03, 4.6056e-06,
        9.7472e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.044

[Epoch: 24, batch: 123/206] total loss per batch: 0.499
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0030, 0.0054, 0.0045, 0.9710, 0.0050, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.013

[Epoch: 24, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8266e-03, 5.3195e-03, 1.7142e-06, 9.7743e-01, 6.0052e-03, 1.5707e-05,
        6.4002e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.019

[Epoch: 24, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0038, 0.0065, 0.9717, 0.0066, 0.0026, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 25, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([8.9130e-06, 5.7604e-01, 2.8704e-06, 2.8571e-06, 4.2391e-01, 4.1442e-06,
        3.5113e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 25, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4170e-03, 8.7105e-03, 1.8477e-06, 5.3614e-03, 6.2282e-03, 3.7179e-06,
        9.7528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.031

[Epoch: 25, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0055, 0.0049, 0.0052, 0.9666, 0.0048, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.019

[Epoch: 25, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.3286e-03, 4.9086e-03, 2.2994e-06, 9.8177e-01, 4.5961e-03, 1.8081e-05,
        5.3722e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.013

[Epoch: 25, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0081, 0.9677, 0.0063, 0.0029, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 26, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([7.7043e-06, 5.1247e-01, 3.6149e-06, 2.9803e-06, 4.8748e-01, 6.0431e-06,
        3.4852e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 26, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.7675e-03, 1.1721e-02, 1.8353e-06, 5.1307e-03, 6.3519e-03, 3.8453e-06,
        9.7202e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 26, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.0046, 0.0050, 0.9705, 0.0056, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.015

[Epoch: 26, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5576e-03, 4.9032e-03, 1.5124e-06, 9.8001e-01, 4.8597e-03, 1.7534e-05,
        5.6502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.010

[Epoch: 26, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0044, 0.0053, 0.9723, 0.0065, 0.0027, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 27, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.1069e-05, 5.7930e-01, 3.1068e-06, 1.8135e-06, 4.2065e-01, 3.3505e-06,
        2.3116e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.954

[Epoch: 27, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.0783e-03, 1.0600e-02, 1.8952e-06, 7.1232e-03, 6.8188e-03, 3.3038e-06,
        9.7137e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 27, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0053, 0.0047, 0.0055, 0.9669, 0.0055, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 27, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.7048e-03, 5.3588e-03, 1.4360e-06, 9.7985e-01, 5.2188e-03, 1.5718e-05,
        5.8542e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 27, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0058, 0.0062, 0.9703, 0.0052, 0.0033, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 28, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([7.8335e-06, 5.0726e-01, 2.9799e-06, 2.8426e-06, 4.9269e-01, 4.7799e-06,
        2.7716e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.957

[Epoch: 28, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.8849e-03, 1.0124e-02, 1.9059e-06, 4.8461e-03, 5.6341e-03, 3.7860e-06,
        9.7551e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 28, batch: 123/206] total loss per batch: 0.499
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0046, 0.0052, 0.0054, 0.9685, 0.0051, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 28, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2522e-03, 5.7073e-03, 2.0860e-06, 9.7754e-01, 5.5504e-03, 1.5430e-05,
        5.9327e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 28, batch: 205/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.0072, 0.9694, 0.0064, 0.0030, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 29, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([8.7542e-06, 5.7368e-01, 3.6722e-06, 2.1173e-06, 4.2628e-01, 4.9107e-06,
        2.2787e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.949

[Epoch: 29, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.5302e-03, 1.7062e-02, 2.4361e-06, 9.3423e-03, 8.8816e-03, 3.7101e-06,
        9.6118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 29, batch: 123/206] total loss per batch: 0.499
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0043, 0.0039, 0.0060, 0.9720, 0.0049, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.019

[Epoch: 29, batch: 164/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.6685e-03, 4.4528e-03, 2.0718e-06, 9.7964e-01, 5.5477e-03, 2.1757e-05,
        6.6708e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 29, batch: 205/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0059, 0.0068, 0.9692, 0.0058, 0.0028, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 30, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.4921e-05, 5.2044e-01, 3.5248e-06, 2.3332e-06, 4.7948e-01, 8.3766e-06,
        4.8882e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.951

[Epoch: 30, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.6931e-03, 7.0124e-03, 3.7814e-06, 6.4896e-03, 7.0549e-03, 6.9107e-06,
        9.7374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 30, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0062, 0.0069, 0.0064, 0.9646, 0.0048, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 30, batch: 164/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8980e-03, 6.1409e-03, 2.5451e-06, 9.7581e-01, 5.2341e-03, 1.8834e-05,
        7.8964e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.050

[Epoch: 30, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0044, 0.0046, 0.9734, 0.0058, 0.0029, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 31, batch: 41/206] total loss per batch: 0.490
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([9.6300e-06, 4.8901e-01, 1.4089e-05, 3.5539e-06, 5.1092e-01, 9.2047e-06,
        2.6531e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.968

[Epoch: 31, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.1421e-03, 1.1502e-02, 3.1353e-06, 6.5226e-03, 5.8623e-03, 3.2138e-06,
        9.7296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 31, batch: 123/206] total loss per batch: 0.501
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0049, 0.0034, 0.0062, 0.9719, 0.0040, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 31, batch: 164/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.1934e-03, 3.2551e-03, 2.1443e-06, 9.8416e-01, 4.4890e-03, 2.5231e-05,
        4.8768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.009

[Epoch: 31, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0073, 0.9688, 0.0059, 0.0032, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 32, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.5755e-05, 5.8477e-01, 3.8973e-06, 2.2382e-06, 4.1516e-01, 9.4441e-06,
        4.4071e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.952

[Epoch: 32, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4885e-03, 1.2987e-02, 2.8841e-06, 5.8391e-03, 6.5819e-03, 5.3646e-06,
        9.7010e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 32, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0055, 0.0040, 0.0045, 0.9715, 0.0054, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.023

[Epoch: 32, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.5656e-03, 5.6787e-03, 3.1232e-06, 9.7340e-01, 7.7461e-03, 2.2921e-05,
        7.5838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.013

[Epoch: 32, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0052, 0.0046, 0.9709, 0.0069, 0.0029, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 33, batch: 41/206] total loss per batch: 0.487
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([7.3537e-06, 4.9077e-01, 4.0546e-06, 2.0649e-06, 5.0918e-01, 5.1053e-06,
        2.8033e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.956

[Epoch: 33, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.1335e-03, 8.6712e-03, 2.1407e-06, 6.7641e-03, 5.8879e-03, 3.7100e-06,
        9.7454e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 33, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.0053, 0.0070, 0.9655, 0.0055, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 33, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.3515e-03, 4.6163e-03, 2.0536e-06, 9.8180e-01, 4.3668e-03, 2.0808e-05,
        5.8424e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.019

[Epoch: 33, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0048, 0.0068, 0.9710, 0.0055, 0.0029, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 34, batch: 41/206] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([6.8500e-06, 5.7313e-01, 2.7314e-06, 1.8896e-06, 4.2683e-01, 4.2713e-06,
        2.2793e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 34, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4664e-03, 1.1394e-02, 2.1598e-06, 4.7065e-03, 6.8909e-03, 3.8994e-06,
        9.7254e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 34, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0044, 0.0053, 0.9707, 0.0054, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.030

[Epoch: 34, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1226e-03, 4.2232e-03, 1.5144e-06, 9.7958e-01, 6.0691e-03, 1.7062e-05,
        4.9854e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.016

[Epoch: 34, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0045, 0.0051, 0.9723, 0.0067, 0.0030, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 35, batch: 41/206] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([6.1077e-06, 5.2101e-01, 1.9038e-06, 1.2722e-06, 4.7895e-01, 4.6897e-06,
        2.5972e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.956

[Epoch: 35, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.3624e-03, 1.0081e-02, 1.7031e-06, 6.1188e-03, 5.9405e-03, 3.3685e-06,
        9.7349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 35, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0048, 0.0066, 0.9661, 0.0064, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 35, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7239e-03, 6.6788e-03, 2.7566e-06, 9.7555e-01, 5.8239e-03, 1.6907e-05,
        7.2031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.024

[Epoch: 35, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0061, 0.9709, 0.0051, 0.0032, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 36, batch: 41/206] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.0278e-06, 5.6752e-01, 1.2784e-06, 1.5760e-06, 4.3246e-01, 2.5698e-06,
        1.7676e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 36, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.9197e-03, 7.7634e-03, 1.7566e-06, 5.5424e-03, 5.5646e-03, 3.1062e-06,
        9.7721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.034

[Epoch: 36, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0050, 0.0062, 0.9688, 0.0055, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.023

[Epoch: 36, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.9716e-03, 4.6656e-03, 1.7993e-06, 9.8006e-01, 4.9886e-03, 2.1331e-05,
        5.2957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 36, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0050, 0.0060, 0.9720, 0.0061, 0.0027, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 37, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.6457e-06, 4.8713e-01, 2.3535e-06, 1.2522e-06, 5.1282e-01, 7.5927e-06,
        3.5322e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.957

[Epoch: 37, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4147e-03, 1.3527e-02, 1.7818e-06, 6.9758e-03, 7.3663e-03, 3.1247e-06,
        9.6771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 37, batch: 123/206] total loss per batch: 0.499
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0065, 0.0050, 0.0056, 0.9658, 0.0054, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.021

[Epoch: 37, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.5301e-03, 7.5038e-03, 2.1684e-06, 9.7366e-01, 5.9144e-03, 1.5030e-05,
        7.3730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 37, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0054, 0.0061, 0.9673, 0.0062, 0.0042, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 38, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.3900e-05, 5.4752e-01, 2.9724e-06, 2.7096e-06, 4.5243e-01, 3.4863e-06,
        2.4154e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.957

[Epoch: 38, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1412e-03, 1.3118e-02, 2.0714e-06, 1.0416e-02, 7.0169e-03, 3.9469e-06,
        9.6430e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 38, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0057, 0.0065, 0.0085, 0.9638, 0.0061, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 38, batch: 164/206] total loss per batch: 0.512
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.0630e-03, 4.3360e-03, 2.6491e-06, 9.8281e-01, 4.3805e-03, 1.6239e-05,
        4.3895e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.013

[Epoch: 38, batch: 205/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0033, 0.0036, 0.0048, 0.9739, 0.0070, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 39, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.1313e-06, 4.8134e-01, 5.7297e-06, 2.0426e-06, 5.1861e-01, 8.0878e-06,
        2.7443e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.952

[Epoch: 39, batch: 82/206] total loss per batch: 0.488
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([2.4892e-03, 1.0036e-02, 1.5016e-06, 2.8431e-03, 5.3361e-03, 2.7984e-06,
        9.7929e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 39, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0042, 0.0057, 0.9697, 0.0047, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.016

[Epoch: 39, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.9597e-03, 3.8551e-03, 1.3634e-06, 9.7770e-01, 7.5245e-03, 2.3274e-05,
        5.9318e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 39, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0074, 0.9694, 0.0050, 0.0039, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 40, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.1378e-05, 5.9402e-01, 2.6506e-06, 2.3580e-06, 4.0592e-01, 5.0639e-06,
        3.2368e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.971

[Epoch: 40, batch: 82/206] total loss per batch: 0.488
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.7835e-03, 8.2991e-03, 2.7681e-06, 4.2001e-02, 7.2507e-03, 5.1200e-06,
        9.3866e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 40, batch: 123/206] total loss per batch: 0.500
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0084, 0.0068, 0.0061, 0.9595, 0.0061, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 40, batch: 164/206] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.4091e-03, 5.1178e-03, 3.3062e-06, 9.8134e-01, 3.7749e-03, 2.0665e-05,
        5.3316e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 40, batch: 205/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0041, 0.0052, 0.9715, 0.0062, 0.0028, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 41, batch: 41/206] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.3074e-06, 5.8188e-01, 5.9677e-06, 1.1040e-06, 4.1808e-01, 5.4937e-06,
        2.1420e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 41, batch: 82/206] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.1204e-03, 9.8787e-03, 2.2331e-06, 1.0716e-02, 5.0769e-03, 3.1605e-06,
        9.7120e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 41, batch: 123/206] total loss per batch: 0.499
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0042, 0.0039, 0.0065, 0.9724, 0.0048, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 41, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.2140e-03, 4.5271e-03, 1.2723e-06, 9.8352e-01, 5.0083e-03, 1.7897e-05,
        3.7158e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 41, batch: 205/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0065, 0.9680, 0.0060, 0.0043, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 42, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([9.0038e-06, 5.0853e-01, 4.8738e-06, 2.4326e-06, 4.9142e-01, 7.0586e-06,
        2.4392e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.964

[Epoch: 42, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.9433e-03, 6.6055e-03, 1.9893e-06, 6.3036e-03, 3.5415e-03, 2.7432e-06,
        9.7760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.048

[Epoch: 42, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0071, 0.0050, 0.0065, 0.9649, 0.0049, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.023

[Epoch: 42, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5068e-03, 4.7191e-03, 1.9401e-06, 9.8176e-01, 4.6377e-03, 1.7167e-05,
        4.3547e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.051

[Epoch: 42, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0052, 0.0053, 0.9690, 0.0062, 0.0038, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 43, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.3627e-06, 5.5866e-01, 2.0534e-06, 9.4805e-07, 4.4131e-01, 3.9128e-06,
        1.7791e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 43, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.5000e-03, 7.0791e-03, 1.3608e-06, 7.1209e-03, 4.4829e-03, 1.8683e-06,
        9.7681e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 43, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0045, 0.0047, 0.0056, 0.9708, 0.0053, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 43, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.3253e-03, 5.4859e-03, 1.9786e-06, 9.7848e-01, 5.5430e-03, 2.2573e-05,
        5.1451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 43, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.0057, 0.9683, 0.0063, 0.0042, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 44, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.1853e-06, 5.4203e-01, 1.4746e-06, 9.7061e-07, 4.5795e-01, 3.2664e-06,
        1.2674e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 44, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.3480e-03, 8.5383e-03, 1.0659e-06, 6.5490e-03, 4.5892e-03, 1.9546e-06,
        9.7597e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 44, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0051, 0.0045, 0.0056, 0.9698, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 44, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6040e-03, 5.0989e-03, 1.5054e-06, 9.8046e-01, 4.8121e-03, 1.6661e-05,
        5.0110e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.042

[Epoch: 44, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0053, 0.9695, 0.0056, 0.0043, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 45, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.2012e-06, 5.2954e-01, 1.4270e-06, 8.4434e-07, 4.7044e-01, 2.3437e-06,
        1.2639e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.955

[Epoch: 45, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.6925e-03, 9.4699e-03, 1.1375e-06, 5.7098e-03, 4.5404e-03, 1.6048e-06,
        9.7558e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 45, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0049, 0.0048, 0.0055, 0.9712, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.031

[Epoch: 45, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.5464e-03, 5.7960e-03, 1.7245e-06, 9.7804e-01, 5.5011e-03, 1.8108e-05,
        5.0917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 45, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0052, 0.9710, 0.0057, 0.0040, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 46, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.1662e-06, 5.3140e-01, 1.0961e-06, 9.1651e-07, 4.6858e-01, 2.3892e-06,
        1.2414e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 46, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.8601e-03, 8.9827e-03, 7.1054e-07, 7.4598e-03, 4.8731e-03, 1.5477e-06,
        9.7482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 46, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0045, 0.0054, 0.9689, 0.0062, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 46, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.3775e-03, 5.4956e-03, 1.2974e-06, 9.8005e-01, 4.7793e-03, 1.6152e-05,
        5.2814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.045

[Epoch: 46, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0050, 0.9700, 0.0056, 0.0042, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 47, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.4037e-06, 5.7428e-01, 1.0494e-06, 6.4060e-07, 4.2570e-01, 1.7680e-06,
        1.0255e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.955

[Epoch: 47, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1121e-03, 8.0778e-03, 9.8918e-07, 4.3849e-03, 4.1921e-03, 1.2741e-06,
        9.7823e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 47, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0045, 0.0049, 0.0049, 0.9723, 0.0043, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.034

[Epoch: 47, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6334e-03, 5.2154e-03, 1.5750e-06, 9.8083e-01, 4.1772e-03, 1.4097e-05,
        5.1258e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.037

[Epoch: 47, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0050, 0.9716, 0.0053, 0.0039, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 48, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.3987e-06, 5.0333e-01, 1.2381e-06, 8.5164e-07, 4.9665e-01, 2.5805e-06,
        9.9626e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 48, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.5527e-03, 1.0992e-02, 1.0562e-06, 9.0690e-03, 6.6078e-03, 2.1564e-06,
        9.6878e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.035

[Epoch: 48, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0057, 0.0051, 0.0070, 0.9650, 0.0067, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.034

[Epoch: 48, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5236e-03, 4.5943e-03, 1.0441e-06, 9.8111e-01, 4.8536e-03, 1.5616e-05,
        4.9022e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.039

[Epoch: 48, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.0055, 0.9669, 0.0066, 0.0055, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 49, batch: 41/206] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.2607e-06, 5.7822e-01, 9.7998e-07, 7.4809e-07, 4.2176e-01, 1.6735e-06,
        1.0193e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.955

[Epoch: 49, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.6356e-03, 8.9391e-03, 1.0697e-06, 4.1755e-03, 4.5965e-03, 1.3112e-06,
        9.7665e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 49, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.0049, 0.0046, 0.9727, 0.0039, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 49, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0084e-03, 5.7312e-03, 1.8025e-06, 9.7793e-01, 4.5378e-03, 2.1799e-05,
        6.7736e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.026

[Epoch: 49, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0046, 0.9728, 0.0040, 0.0035, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 50, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 4
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.7656e-06, 4.7284e-01, 2.6629e-06, 1.4451e-06, 5.2713e-01, 4.6241e-06,
        1.3788e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 50, batch: 82/206] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.7822e-03, 1.0614e-02, 1.4326e-06, 6.3253e-03, 6.6083e-03, 2.3499e-06,
        9.7267e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.038

[Epoch: 50, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0066, 0.0060, 0.0064, 0.9630, 0.0069, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.035

[Epoch: 50, batch: 164/206] total loss per batch: 0.511
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([3.7904e-03, 4.0576e-03, 1.3692e-06, 9.8527e-01, 4.0080e-03, 1.5024e-05,
        2.8529e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.022

[Epoch: 50, batch: 205/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.0065, 0.9669, 0.0073, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 51, batch: 41/206] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.5289e-06, 5.8376e-01, 1.9622e-06, 6.9019e-07, 4.1621e-01, 3.0298e-06,
        1.7498e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.952

[Epoch: 51, batch: 82/206] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4051e-03, 9.7964e-03, 1.7014e-06, 5.0772e-03, 4.0832e-03, 2.3611e-06,
        9.7663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.002

[Epoch: 51, batch: 123/206] total loss per batch: 0.498
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0045, 0.0054, 0.9710, 0.0045, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.031

[Epoch: 51, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([6.9927e-03, 7.2648e-03, 2.6485e-06, 9.6985e-01, 6.9421e-03, 2.7427e-05,
        8.9215e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 51, batch: 205/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0052, 0.0039, 0.9728, 0.0046, 0.0037, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 52, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.5893e-06, 5.3221e-01, 1.3013e-06, 1.0982e-06, 4.6776e-01, 3.8419e-06,
        1.6151e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 52, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.8763e-03, 9.6915e-03, 1.2418e-06, 6.0539e-03, 5.4352e-03, 1.8228e-06,
        9.7294e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 52, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0053, 0.0049, 0.0053, 0.9687, 0.0056, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.037

[Epoch: 52, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.1725e-03, 4.8431e-03, 2.0337e-06, 9.8070e-01, 5.3932e-03, 1.9877e-05,
        4.8672e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 52, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0052, 0.0051, 0.9685, 0.0058, 0.0044, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 53, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.0075e-06, 5.2924e-01, 2.0579e-06, 8.3765e-07, 4.7073e-01, 4.0401e-06,
        1.5033e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 53, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0219e-03, 1.0474e-02, 1.1107e-06, 5.4712e-03, 5.4548e-03, 1.6146e-06,
        9.7357e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 53, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0049, 0.0050, 0.9705, 0.0050, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.030

[Epoch: 53, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.3035e-03, 5.7238e-03, 1.7439e-06, 9.7727e-01, 5.7344e-03, 1.9235e-05,
        5.9479e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 53, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.0046, 0.9702, 0.0053, 0.0043, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 54, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.4259e-06, 5.5401e-01, 1.3263e-06, 7.1529e-07, 4.4597e-01, 2.8125e-06,
        1.3330e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.965

[Epoch: 54, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.5584e-03, 8.7250e-03, 8.6952e-07, 5.0782e-03, 4.9709e-03, 1.4581e-06,
        9.7667e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 54, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.0049, 0.0056, 0.9700, 0.0054, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.031

[Epoch: 54, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6174e-03, 4.9573e-03, 1.4229e-06, 9.8091e-01, 4.3397e-03, 1.4073e-05,
        5.1611e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 54, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0046, 0.9705, 0.0053, 0.0044, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 55, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([5.5111e-06, 5.6138e-01, 1.5253e-06, 6.7147e-07, 4.3860e-01, 3.1301e-06,
        1.0537e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.954

[Epoch: 55, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8711e-03, 1.0592e-02, 6.9400e-07, 5.1408e-03, 4.2507e-03, 1.0654e-06,
        9.7514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 55, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0049, 0.0052, 0.9702, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.033

[Epoch: 55, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.3181e-03, 5.6075e-03, 1.5400e-06, 9.7814e-01, 5.3782e-03, 1.7650e-05,
        5.5385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 55, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0048, 0.9700, 0.0052, 0.0044, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 56, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.6509e-06, 5.1732e-01, 1.0230e-06, 5.8244e-07, 4.8266e-01, 2.3594e-06,
        9.5557e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 56, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4506e-03, 9.1561e-03, 6.1895e-07, 4.6493e-03, 5.1147e-03, 1.0339e-06,
        9.7663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 56, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.0048, 0.0055, 0.9698, 0.0057, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.030

[Epoch: 56, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5678e-03, 4.9263e-03, 1.1861e-06, 9.8141e-01, 4.1804e-03, 1.1609e-05,
        4.8981e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.038

[Epoch: 56, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0047, 0.9722, 0.0050, 0.0039, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 57, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.2658e-06, 5.4909e-01, 1.5269e-06, 7.1617e-07, 4.5090e-01, 2.3299e-06,
        9.4416e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.955

[Epoch: 57, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.5320e-03, 1.0123e-02, 5.3429e-07, 4.9106e-03, 4.2831e-03, 8.0561e-07,
        9.7515e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 57, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.0046, 0.0045, 0.9718, 0.0049, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.030

[Epoch: 57, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.6783e-03, 5.5397e-03, 1.5078e-06, 9.7662e-01, 6.0345e-03, 1.4784e-05,
        6.1084e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.028

[Epoch: 57, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0055, 0.9685, 0.0050, 0.0054, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 58, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.8279e-06, 5.4099e-01, 9.3646e-07, 4.7219e-07, 4.5900e-01, 1.9499e-06,
        7.6562e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 58, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.9857e-03, 9.7335e-03, 5.7587e-07, 5.7193e-03, 6.5000e-03, 1.0451e-06,
        9.7406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 58, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0046, 0.0050, 0.9718, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 58, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5543e-03, 5.6092e-03, 9.9815e-07, 9.8166e-01, 3.9140e-03, 1.2300e-05,
        4.2523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.023

[Epoch: 58, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0049, 0.9723, 0.0050, 0.0036, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 59, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.4735e-06, 5.3350e-01, 1.4318e-06, 7.3084e-07, 4.6649e-01, 2.2454e-06,
        8.5971e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 59, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3093e-03, 9.1975e-03, 7.8523e-07, 4.8828e-03, 4.1686e-03, 1.0680e-06,
        9.7644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 59, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0057, 0.0043, 0.0050, 0.9707, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 59, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0550e-03, 3.8134e-03, 1.8939e-06, 9.7862e-01, 6.0372e-03, 1.4552e-05,
        6.4562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 59, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0047, 0.9695, 0.0050, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 60, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.6871e-06, 5.5018e-01, 1.0335e-06, 6.3367e-07, 4.4980e-01, 2.4311e-06,
        7.7722e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.957

[Epoch: 60, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1133e-03, 1.4337e-02, 6.0727e-07, 7.7722e-03, 7.8803e-03, 1.2213e-06,
        9.6490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 60, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0052, 0.0052, 0.0054, 0.9667, 0.0057, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.035

[Epoch: 60, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.8977e-03, 5.9073e-03, 1.4285e-06, 9.7926e-01, 4.2540e-03, 1.1484e-05,
        4.6635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 60, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0057, 0.9719, 0.0048, 0.0037, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 61, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.4344e-06, 5.0877e-01, 1.5515e-06, 7.8438e-07, 4.9121e-01, 2.9676e-06,
        1.1154e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 61, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.2752e-03, 6.6227e-03, 1.0501e-06, 5.2392e-03, 4.2582e-03, 1.3696e-06,
        9.7960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.034

[Epoch: 61, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0044, 0.0044, 0.0054, 0.9726, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 61, batch: 164/206] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5369e-03, 4.0943e-03, 1.5361e-06, 9.8198e-01, 4.8831e-03, 1.4522e-05,
        4.4919e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 61, batch: 205/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0046, 0.9703, 0.0053, 0.0045, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 62, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([4.7550e-06, 5.5488e-01, 1.6743e-06, 7.1249e-07, 4.4509e-01, 2.9154e-06,
        1.1549e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.964

[Epoch: 62, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.5722e-03, 1.0951e-02, 8.8633e-07, 4.4891e-03, 4.7539e-03, 1.1417e-06,
        9.7523e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 62, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0042, 0.0044, 0.0045, 0.9737, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 62, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([6.5864e-03, 5.1504e-03, 1.6503e-06, 9.7746e-01, 5.1503e-03, 1.2095e-05,
        5.6437e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 62, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.0059, 0.9673, 0.0055, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 63, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.0649e-06, 5.2319e-01, 1.2879e-06, 6.4068e-07, 4.7679e-01, 2.5024e-06,
        9.9350e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 63, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.3933e-03, 9.5515e-03, 5.9485e-07, 4.8500e-03, 4.8291e-03, 8.8294e-07,
        9.7637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 63, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0051, 0.0052, 0.9700, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 63, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7902e-03, 4.7018e-03, 1.2649e-06, 9.8043e-01, 4.9290e-03, 1.1358e-05,
        5.1321e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 63, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.0053, 0.9700, 0.0050, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 64, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.5631e-06, 5.5230e-01, 1.1272e-06, 6.6608e-07, 4.4768e-01, 2.3025e-06,
        9.6151e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 64, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8302e-03, 1.0072e-02, 5.7326e-07, 4.7142e-03, 4.8776e-03, 9.3892e-07,
        9.7550e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 64, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0045, 0.0048, 0.0051, 0.9712, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 64, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2266e-03, 4.6348e-03, 1.1683e-06, 9.8070e-01, 4.4914e-03, 1.0175e-05,
        4.9329e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 64, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0054, 0.9694, 0.0052, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 65, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.9984e-06, 5.4693e-01, 1.0338e-06, 5.4909e-07, 4.5306e-01, 1.9884e-06,
        7.3695e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 65, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9815e-03, 1.0227e-02, 4.3589e-07, 5.0274e-03, 4.8798e-03, 7.1988e-07,
        9.7488e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 65, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0048, 0.0050, 0.9713, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 65, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2634e-03, 5.1707e-03, 1.2215e-06, 9.7898e-01, 5.3196e-03, 9.9049e-06,
        5.2593e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 65, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.0052, 0.9691, 0.0051, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 66, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.0130e-06, 5.3472e-01, 7.0826e-07, 4.8069e-07, 4.6526e-01, 1.6976e-06,
        7.4427e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 66, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.6199e-03, 9.2278e-03, 4.2974e-07, 4.7095e-03, 4.8931e-03, 7.3855e-07,
        9.7655e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 66, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.0052, 0.0051, 0.9693, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 66, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8174e-03, 4.6755e-03, 9.4519e-07, 9.8122e-01, 4.5098e-03, 1.0096e-05,
        4.7629e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 66, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0054, 0.9690, 0.0052, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 67, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.6694e-06, 5.3993e-01, 8.6208e-07, 4.5000e-07, 4.6006e-01, 1.6888e-06,
        6.8828e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 67, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8454e-03, 1.1378e-02, 3.5751e-07, 5.7894e-03, 5.0961e-03, 6.2987e-07,
        9.7289e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 67, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0049, 0.0050, 0.0053, 0.9704, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 67, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.6268e-03, 5.4222e-03, 1.1614e-06, 9.7798e-01, 5.3641e-03, 8.7754e-06,
        5.5945e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.035

[Epoch: 67, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0050, 0.9695, 0.0050, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 68, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.3850e-06, 5.3394e-01, 6.0493e-07, 4.6453e-07, 4.6604e-01, 1.6009e-06,
        6.6091e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 68, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0250e-03, 8.5559e-03, 3.8791e-07, 4.3081e-03, 5.1208e-03, 6.0930e-07,
        9.7699e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 68, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0045, 0.0044, 0.9722, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 68, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1528e-03, 4.7746e-03, 9.6402e-07, 9.8102e-01, 4.4654e-03, 8.5710e-06,
        4.5800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 68, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.0055, 0.9683, 0.0056, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 69, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.7543e-06, 5.4034e-01, 7.3433e-07, 4.5121e-07, 4.5965e-01, 1.9492e-06,
        8.2594e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 69, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4139e-03, 1.0821e-02, 4.6768e-07, 7.5962e-03, 5.5618e-03, 7.5901e-07,
        9.7161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 69, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0054, 0.0052, 0.9714, 0.0046, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 69, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2027e-03, 5.1967e-03, 1.6966e-06, 9.7890e-01, 5.0577e-03, 9.0587e-06,
        5.6362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 69, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0049, 0.0050, 0.9731, 0.0042, 0.0041, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 70, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.3947e-06, 5.6549e-01, 7.6915e-07, 5.0895e-07, 4.3450e-01, 1.9691e-06,
        6.6504e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.970

[Epoch: 70, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1464e-03, 8.1849e-03, 4.2626e-07, 4.0126e-03, 4.5958e-03, 6.6017e-07,
        9.7806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 70, batch: 123/206] total loss per batch: 0.497
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0046, 0.0049, 0.0052, 0.9702, 0.0050, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 70, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.9491e-03, 5.2941e-03, 1.1080e-06, 9.8013e-01, 4.8079e-03, 8.0308e-06,
        4.8060e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 70, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0058, 0.0062, 0.9641, 0.0060, 0.0058, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 71, batch: 41/206] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.1511e-06, 5.2713e-01, 7.1413e-07, 5.7917e-07, 4.7285e-01, 2.2594e-06,
        8.4485e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 71, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0389e-03, 1.0935e-02, 4.9518e-07, 6.8637e-03, 5.7989e-03, 1.0731e-06,
        9.7136e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 71, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.0050, 0.0050, 0.9699, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.029

[Epoch: 71, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1586e-03, 5.1065e-03, 1.2565e-06, 9.7998e-01, 4.4629e-03, 8.3180e-06,
        5.2853e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 71, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0058, 0.9696, 0.0050, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 72, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.1152e-06, 5.4535e-01, 9.2029e-07, 5.3747e-07, 4.5463e-01, 2.1558e-06,
        8.4476e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.965

[Epoch: 72, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.2128e-03, 9.3063e-03, 4.7201e-07, 5.3380e-03, 5.4402e-03, 7.9756e-07,
        9.7470e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 72, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0055, 0.0055, 0.0057, 0.9677, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 72, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.4644e-03, 4.9602e-03, 1.0785e-06, 9.8187e-01, 4.2399e-03, 7.9330e-06,
        4.4529e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 72, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0054, 0.9689, 0.0052, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 73, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.9569e-06, 5.1372e-01, 6.6419e-07, 4.9038e-07, 4.8626e-01, 2.0941e-06,
        7.6322e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 73, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3202e-03, 1.0211e-02, 4.0584e-07, 4.9128e-03, 5.0777e-03, 7.2279e-07,
        9.7448e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 73, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0050, 0.0050, 0.9694, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 73, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7361e-03, 4.8435e-03, 9.2097e-07, 9.8113e-01, 4.5685e-03, 6.8105e-06,
        4.7153e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 73, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0053, 0.9694, 0.0049, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 74, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.6147e-06, 5.4255e-01, 5.8393e-07, 3.9734e-07, 4.5744e-01, 1.7079e-06,
        6.3174e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 74, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9084e-03, 1.0430e-02, 3.5834e-07, 5.4246e-03, 5.2490e-03, 6.2521e-07,
        9.7399e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 74, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.0052, 0.0055, 0.9683, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 74, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6355e-03, 5.0859e-03, 8.9993e-07, 9.8112e-01, 4.6031e-03, 7.0217e-06,
        4.5438e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 74, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0054, 0.9692, 0.0050, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 75, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.3117e-06, 5.3282e-01, 4.9533e-07, 3.7032e-07, 4.6717e-01, 1.5838e-06,
        5.9334e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 75, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8776e-03, 9.7314e-03, 3.2277e-07, 4.8512e-03, 5.3115e-03, 5.8212e-07,
        9.7523e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 75, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0053, 0.0051, 0.0052, 0.9690, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 75, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6632e-03, 4.7726e-03, 8.1300e-07, 9.8134e-01, 4.5693e-03, 6.2207e-06,
        4.6487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 75, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0051, 0.9699, 0.0048, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 76, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.4560e-06, 5.4393e-01, 4.6587e-07, 3.0960e-07, 4.5606e-01, 1.3567e-06,
        5.0047e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 76, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.7960e-03, 1.0152e-02, 2.7074e-07, 5.0164e-03, 4.9561e-03, 5.0449e-07,
        9.7508e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 76, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0051, 0.0052, 0.9692, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 76, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8409e-03, 5.2386e-03, 8.3935e-07, 9.8027e-01, 4.8817e-03, 6.6912e-06,
        4.7596e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.027

[Epoch: 76, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.0055, 0.9689, 0.0051, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 77, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.9692e-06, 5.3585e-01, 3.8179e-07, 3.0045e-07, 4.6414e-01, 1.2596e-06,
        5.1783e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 77, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0263e-03, 9.6821e-03, 2.7226e-07, 4.7866e-03, 5.2078e-03, 4.9444e-07,
        9.7530e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 77, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.0052, 0.0052, 0.9689, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 77, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7666e-03, 4.9677e-03, 7.5978e-07, 9.8062e-01, 4.8335e-03, 6.2607e-06,
        4.8012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 77, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.0050, 0.9697, 0.0048, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 78, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.4818e-06, 5.4095e-01, 4.3101e-07, 2.7602e-07, 4.5905e-01, 1.1868e-06,
        4.7012e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 78, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8143e-03, 9.7916e-03, 1.8850e-07, 5.1161e-03, 5.1113e-03, 4.0874e-07,
        9.7517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 78, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.0053, 0.0055, 0.9689, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 78, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7137e-03, 5.1250e-03, 7.0496e-07, 9.8080e-01, 4.5600e-03, 5.2295e-06,
        4.7942e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 78, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0053, 0.9681, 0.0054, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 79, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.7175e-06, 5.3965e-01, 3.9258e-07, 2.7985e-07, 4.6034e-01, 1.2156e-06,
        4.8402e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.964

[Epoch: 79, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.6750e-03, 1.0275e-02, 2.8619e-07, 4.9343e-03, 5.0632e-03, 4.6625e-07,
        9.7505e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 79, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0057, 0.0049, 0.0052, 0.9682, 0.0056, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 79, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0511e-03, 5.1142e-03, 8.1668e-07, 9.7848e-01, 5.6961e-03, 8.4598e-06,
        5.6526e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.044

[Epoch: 79, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.0052, 0.9699, 0.0048, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 80, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([3.0469e-06, 5.4267e-01, 4.9473e-07, 3.0733e-07, 4.5732e-01, 1.5251e-06,
        5.6319e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 80, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.8220e-03, 8.4662e-03, 2.4100e-07, 5.0283e-03, 4.9513e-03, 5.4601e-07,
        9.7573e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 80, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0050, 0.0053, 0.0051, 0.9706, 0.0050, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 80, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7413e-03, 4.7949e-03, 6.9528e-07, 9.8170e-01, 4.5109e-03, 6.1529e-06,
        4.2432e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.025

[Epoch: 80, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.0048, 0.9693, 0.0057, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 81, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.4259e-06, 5.3597e-01, 4.5391e-07, 3.6546e-07, 4.6402e-01, 1.2810e-06,
        5.8038e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 81, batch: 82/206] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([3.7863e-03, 1.3202e-02, 2.8625e-07, 4.7500e-03, 4.8878e-03, 4.2681e-07,
        9.7337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 81, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0057, 0.0055, 0.0054, 0.9665, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.028

[Epoch: 81, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2166e-03, 5.3139e-03, 8.8827e-07, 9.7805e-01, 5.6027e-03, 8.3063e-06,
        5.8048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.024

[Epoch: 81, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0050, 0.9709, 0.0047, 0.0047, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 82, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.2832e-06, 5.4300e-01, 4.8235e-07, 3.3993e-07, 4.5699e-01, 1.4079e-06,
        5.5368e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 82, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9492e-03, 9.2812e-03, 3.0027e-07, 4.8155e-03, 4.7292e-03, 6.0177e-07,
        9.7622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 82, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0050, 0.0052, 0.9699, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 82, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2273e-03, 5.4528e-03, 9.0986e-07, 9.7873e-01, 5.2338e-03, 7.2605e-06,
        5.3437e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 82, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0053, 0.0050, 0.9701, 0.0049, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 83, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.4477e-06, 5.4508e-01, 4.4293e-07, 3.3326e-07, 4.5491e-01, 1.3587e-06,
        5.5567e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 83, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.4705e-03, 9.8284e-03, 2.2890e-07, 4.9378e-03, 4.5940e-03, 4.2347e-07,
        9.7617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 83, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.0050, 0.0050, 0.9704, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 83, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2270e-03, 5.2550e-03, 8.6829e-07, 9.7916e-01, 5.1535e-03, 7.0971e-06,
        5.1937e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 83, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0050, 0.9703, 0.0050, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 84, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.1619e-06, 5.3751e-01, 3.9759e-07, 3.0453e-07, 4.6248e-01, 1.2788e-06,
        4.7979e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 84, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.7662e-03, 9.5677e-03, 2.0511e-07, 4.6330e-03, 4.5841e-03, 3.9586e-07,
        9.7645e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 84, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0047, 0.0050, 0.0049, 0.9709, 0.0052, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 84, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2814e-03, 5.4418e-03, 8.0283e-07, 9.7871e-01, 5.2937e-03, 7.1528e-06,
        5.2660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 84, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0050, 0.9707, 0.0048, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 85, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.2367e-06, 5.3773e-01, 3.7855e-07, 2.8002e-07, 4.6226e-01, 1.1105e-06,
        4.5611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.958

[Epoch: 85, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.6928e-03, 9.9397e-03, 1.7401e-07, 4.9348e-03, 4.8281e-03, 3.5861e-07,
        9.7560e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 85, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0050, 0.0048, 0.9709, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 85, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1994e-03, 5.5499e-03, 8.1453e-07, 9.7882e-01, 5.1448e-03, 6.8432e-06,
        5.2779e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 85, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0049, 0.9710, 0.0049, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 86, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.1235e-06, 5.4162e-01, 3.6316e-07, 2.5933e-07, 4.5837e-01, 1.0689e-06,
        4.2744e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 86, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9582e-03, 9.8127e-03, 1.5402e-07, 4.8151e-03, 4.5520e-03, 3.1572e-07,
        9.7586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 86, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0049, 0.0047, 0.9710, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 86, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1044e-03, 5.2085e-03, 7.3601e-07, 9.7944e-01, 5.1064e-03, 6.4359e-06,
        5.1317e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 86, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0048, 0.9712, 0.0048, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 87, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.8448e-06, 5.3679e-01, 3.1930e-07, 2.3806e-07, 4.6320e-01, 9.0811e-07,
        3.9925e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 87, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9929e-03, 1.0041e-02, 1.4570e-07, 5.3815e-03, 4.9589e-03, 3.0722e-07,
        9.7462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 87, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0046, 0.0047, 0.0047, 0.9723, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 87, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0798e-03, 5.3654e-03, 7.3634e-07, 9.7910e-01, 5.1775e-03, 5.7691e-06,
        5.2745e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 87, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0047, 0.0047, 0.9718, 0.0047, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 88, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.0367e-06, 5.4880e-01, 3.1770e-07, 2.1729e-07, 4.5119e-01, 8.1712e-07,
        3.7681e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.959

[Epoch: 88, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.9944e-03, 9.9107e-03, 1.3692e-07, 5.1096e-03, 4.8249e-03, 2.9990e-07,
        9.7516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 88, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0046, 0.0046, 0.9725, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.025

[Epoch: 88, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0252e-03, 5.1665e-03, 6.7108e-07, 9.7977e-01, 5.0887e-03, 6.7748e-06,
        4.9404e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 88, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0047, 0.9711, 0.0049, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 89, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.7228e-06, 5.3124e-01, 2.8637e-07, 2.1232e-07, 4.6876e-01, 7.9192e-07,
        4.0252e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 89, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3569e-03, 1.0254e-02, 1.3271e-07, 6.0078e-03, 5.2430e-03, 2.9712e-07,
        9.7314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 89, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0045, 0.0045, 0.9721, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 89, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.2872e-03, 5.4534e-03, 7.0109e-07, 9.7846e-01, 5.1446e-03, 5.2598e-06,
        5.6498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 89, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0045, 0.0048, 0.9718, 0.0047, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 90, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.0615e-06, 5.4991e-01, 4.4688e-07, 2.5042e-07, 4.5008e-01, 9.8139e-07,
        4.3533e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 90, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.7835e-03, 9.7635e-03, 1.1094e-07, 4.5593e-03, 4.9701e-03, 2.9350e-07,
        9.7592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 90, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.0047, 0.0047, 0.9734, 0.0045, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.024

[Epoch: 90, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.1244e-03, 5.1573e-03, 7.3694e-07, 9.7948e-01, 4.9797e-03, 6.9034e-06,
        5.2522e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.036

[Epoch: 90, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.0045, 0.9713, 0.0050, 0.0047, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 91, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.9496e-06, 5.3927e-01, 3.5151e-07, 2.6096e-07, 4.6072e-01, 1.0129e-06,
        4.4474e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 91, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3151e-03, 1.0122e-02, 1.7667e-07, 6.6045e-03, 5.3846e-03, 3.8394e-07,
        9.7257e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 91, batch: 123/206] total loss per batch: 0.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0043, 0.0045, 0.0047, 0.9727, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.022

[Epoch: 91, batch: 164/206] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7146e-03, 5.2480e-03, 7.7067e-07, 9.7948e-01, 5.0896e-03, 6.0490e-06,
        5.4610e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.029

[Epoch: 91, batch: 205/206] total loss per batch: 0.507
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0047, 0.0048, 0.9717, 0.0048, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 92, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([2.0208e-06, 5.3690e-01, 5.5983e-07, 2.8709e-07, 4.6309e-01, 1.0610e-06,
        4.4612e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.960

[Epoch: 92, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.6440e-03, 9.8457e-03, 1.4307e-07, 4.4013e-03, 4.8777e-03, 3.5655e-07,
        9.7623e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 92, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0043, 0.0042, 0.0043, 0.9747, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 92, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([5.0563e-03, 5.1919e-03, 7.7643e-07, 9.7922e-01, 5.1147e-03, 6.3626e-06,
        5.4133e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.034

[Epoch: 92, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0049, 0.9711, 0.0050, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 93, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.9261e-06, 5.3853e-01, 4.8261e-07, 2.7881e-07, 4.6146e-01, 1.1075e-06,
        4.4416e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 93, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.2438e-03, 9.8515e-03, 1.4861e-07, 4.9663e-03, 5.0026e-03, 3.6350e-07,
        9.7494e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 93, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0045, 0.0044, 0.0046, 0.9736, 0.0045, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 93, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.7481e-03, 5.0018e-03, 6.9352e-07, 9.7995e-01, 4.9142e-03, 6.1330e-06,
        5.3784e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 93, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0050, 0.9710, 0.0050, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 94, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.7062e-06, 5.3946e-01, 4.2077e-07, 2.5052e-07, 4.6054e-01, 1.0090e-06,
        4.1952e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 94, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3456e-03, 9.9763e-03, 1.4047e-07, 4.9587e-03, 4.7825e-03, 3.2151e-07,
        9.7494e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 94, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0046, 0.0045, 0.0047, 0.9731, 0.0044, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 94, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8818e-03, 5.0024e-03, 6.8800e-07, 9.7986e-01, 5.0452e-03, 5.7648e-06,
        5.2034e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 94, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0051, 0.9699, 0.0050, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 95, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.5851e-06, 5.3575e-01, 3.9438e-07, 2.3709e-07, 4.6424e-01, 9.8669e-07,
        4.0865e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 95, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.2526e-03, 1.0128e-02, 1.3306e-07, 4.9028e-03, 5.0993e-03, 3.2217e-07,
        9.7462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 95, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0048, 0.0046, 0.0049, 0.9720, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 95, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.9272e-03, 4.9981e-03, 6.5264e-07, 9.7973e-01, 5.0467e-03, 5.7648e-06,
        5.2946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 95, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0052, 0.9694, 0.0051, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 96, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.4558e-06, 5.3713e-01, 3.4299e-07, 2.1954e-07, 4.6287e-01, 9.1936e-07,
        3.9474e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 96, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1871e-03, 9.9588e-03, 1.2709e-07, 5.0462e-03, 4.9572e-03, 2.9978e-07,
        9.7485e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 96, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0049, 0.0050, 0.9711, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 96, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.8180e-03, 4.9306e-03, 5.9252e-07, 9.8045e-01, 4.8442e-03, 5.0691e-06,
        4.9498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.032

[Epoch: 96, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0051, 0.9697, 0.0050, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 97, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.2853e-06, 5.4220e-01, 3.0601e-07, 2.0400e-07, 4.5780e-01, 9.3257e-07,
        3.8865e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.963

[Epoch: 97, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.0937e-03, 1.0106e-02, 1.0973e-07, 4.7474e-03, 5.0308e-03, 2.6159e-07,
        9.7502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 97, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0049, 0.0050, 0.9703, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 97, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.9100e-03, 5.0471e-03, 5.5047e-07, 9.8016e-01, 4.7702e-03, 4.8019e-06,
        5.1048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 97, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.0053, 0.9690, 0.0052, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 98, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.1669e-06, 5.3786e-01, 2.8315e-07, 2.0405e-07, 4.6213e-01, 8.8669e-07,
        3.8046e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.962

[Epoch: 98, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.1326e-03, 1.0006e-02, 1.1842e-07, 4.9861e-03, 5.1329e-03, 2.6952e-07,
        9.7474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 98, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.0051, 0.0054, 0.9686, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 98, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.5594e-03, 4.8245e-03, 4.9302e-07, 9.8117e-01, 4.6118e-03, 4.4324e-06,
        4.8278e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.031

[Epoch: 98, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0052, 0.9694, 0.0050, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 99, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([1.1436e-06, 5.4150e-01, 2.7969e-07, 1.9161e-07, 4.5849e-01, 9.6793e-07,
        4.0507e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.964

[Epoch: 99, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([4.8550e-03, 1.0290e-02, 1.0444e-07, 4.6790e-03, 5.1311e-03, 2.1919e-07,
        9.7504e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 99, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0058, 0.0053, 0.0055, 0.9674, 0.0055, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.026

[Epoch: 99, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.6173e-03, 4.7146e-03, 4.3465e-07, 9.8170e-01, 4.2101e-03, 3.9859e-06,
        4.7560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.030

[Epoch: 99, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.0054, 0.9682, 0.0054, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 100, batch: 41/206] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.5400, 0.0000, 0.0000, 0.4600, 0.0000, 0.0000])
Policy pred: tensor([9.8518e-07, 5.3233e-01, 2.8043e-07, 2.1691e-07, 4.6766e-01, 9.1785e-07,
        3.9667e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.961 0.961

[Epoch: 100, batch: 82/206] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0100, 0.0000, 0.0050, 0.0050, 0.0000, 0.9750])
Policy pred: tensor([5.3690e-03, 1.0175e-02, 1.2544e-07, 5.6219e-03, 5.3166e-03, 2.8993e-07,
        9.7352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 100, batch: 123/206] total loss per batch: 0.495
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0056, 0.0056, 0.0056, 0.9664, 0.0054, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.024 0.027

[Epoch: 100, batch: 164/206] total loss per batch: 0.508
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0000, 0.9800, 0.0050, 0.0000, 0.0050])
Policy pred: tensor([4.3757e-03, 4.5494e-03, 4.4628e-07, 9.8187e-01, 4.5779e-03, 4.1322e-06,
        4.6267e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.032 -0.033

[Epoch: 100, batch: 205/206] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0051, 0.0056, 0.9678, 0.0055, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

