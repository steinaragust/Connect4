Training set samples: 6790
Batch size: 32
[Epoch: 1, batch: 42/213] total loss per batch: 1.024
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0133, 0.0100, 0.0215, 0.0264, 0.0329, 0.8811, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 1, batch: 84/213] total loss per batch: 1.012
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.0585e-01, 1.5329e-01, 3.5683e-02, 5.9644e-07, 1.0213e-06, 3.9397e-02,
        5.6578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.040

[Epoch: 1, batch: 126/213] total loss per batch: 0.975
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0921e-03, 3.5576e-02, 3.4792e-02, 8.9754e-01, 7.3654e-03, 1.5638e-02,
        4.9242e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.002

[Epoch: 1, batch: 168/213] total loss per batch: 1.008
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0073, 0.0091, 0.9532, 0.0177, 0.0047, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.017

[Epoch: 1, batch: 210/213] total loss per batch: 0.990
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.4602e-03, 9.5717e-01, 7.9930e-03, 1.1852e-02, 4.6288e-09, 5.1421e-03,
        1.1380e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.010

[Epoch: 2, batch: 42/213] total loss per batch: 0.731
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0077, 0.0045, 0.0242, 0.0110, 0.0138, 0.9272, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.001

[Epoch: 2, batch: 84/213] total loss per batch: 0.710
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([5.1649e-02, 3.4977e-02, 1.6201e-02, 2.1270e-07, 1.5761e-07, 1.8697e-02,
        8.7848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 2, batch: 126/213] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.6326e-03, 3.2876e-02, 3.0256e-02, 9.1489e-01, 4.5512e-03, 1.3795e-02,
        7.6770e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.012

[Epoch: 2, batch: 168/213] total loss per batch: 0.717
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0078, 0.0081, 0.9302, 0.0435, 0.0029, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 2, batch: 210/213] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.0061e-03, 9.7253e-01, 9.8316e-03, 4.4493e-03, 2.9402e-10, 2.8466e-03,
        7.3372e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.011

[Epoch: 3, batch: 42/213] total loss per batch: 0.608
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0028, 0.0028, 0.0090, 0.0037, 0.0074, 0.9667, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 3, batch: 84/213] total loss per batch: 0.586
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4426e-02, 2.7077e-02, 7.4436e-03, 4.0725e-08, 1.1785e-07, 1.2248e-02,
        9.1880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.040

[Epoch: 3, batch: 126/213] total loss per batch: 0.596
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.2083e-03, 2.5646e-02, 1.8754e-02, 9.3399e-01, 3.5722e-03, 1.4834e-02,
        2.0956e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.015

[Epoch: 3, batch: 168/213] total loss per batch: 0.586
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0086, 0.0138, 0.9082, 0.0581, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.001

[Epoch: 3, batch: 210/213] total loss per batch: 0.577
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.3442e-03, 9.6457e-01, 1.1358e-02, 5.3682e-03, 4.1188e-10, 5.3508e-03,
        1.0005e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.006

[Epoch: 4, batch: 42/213] total loss per batch: 0.559
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0030, 0.0030, 0.0068, 0.0034, 0.0055, 0.9700, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 4, batch: 84/213] total loss per batch: 0.546
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.4279e-02, 1.5727e-02, 4.0637e-03, 4.0192e-08, 1.3051e-07, 8.5236e-03,
        9.5741e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 4, batch: 126/213] total loss per batch: 0.545
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.8419e-03, 3.0724e-02, 3.1578e-02, 9.1358e-01, 4.8861e-03, 1.3393e-02,
        4.2302e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.003

[Epoch: 4, batch: 168/213] total loss per batch: 0.549
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0087, 0.0126, 0.9144, 0.0545, 0.0028, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.001

[Epoch: 4, batch: 210/213] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.3656e-03, 9.6618e-01, 1.0658e-02, 4.3169e-03, 4.4908e-10, 5.9698e-03,
        8.5045e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.005

[Epoch: 5, batch: 42/213] total loss per batch: 0.539
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0037, 0.0044, 0.0111, 0.0036, 0.0062, 0.9581, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 5, batch: 84/213] total loss per batch: 0.525
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([5.1643e-02, 5.0809e-02, 1.7650e-02, 1.2637e-07, 1.9322e-07, 1.8075e-02,
        8.6182e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 5, batch: 126/213] total loss per batch: 0.529
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2091e-02, 4.9395e-02, 2.2568e-02, 8.8145e-01, 1.0084e-02, 2.4416e-02,
        2.5147e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 5, batch: 168/213] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0081, 0.0147, 0.9039, 0.0631, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.002

[Epoch: 5, batch: 210/213] total loss per batch: 0.525
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.2107e-03, 9.6646e-01, 1.2494e-02, 5.5668e-03, 2.1243e-09, 4.6801e-03,
        7.5893e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.002

[Epoch: 6, batch: 42/213] total loss per batch: 0.525
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0050, 0.0060, 0.0054, 0.0044, 0.0054, 0.9636, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 6, batch: 84/213] total loss per batch: 0.509
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7236e-02, 4.3454e-02, 8.7249e-03, 1.2322e-07, 1.0253e-07, 1.2631e-02,
        9.0795e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 6, batch: 126/213] total loss per batch: 0.516
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([2.5814e-02, 2.5441e-02, 1.8831e-02, 9.0075e-01, 6.7247e-03, 2.2438e-02,
        3.9052e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 6, batch: 168/213] total loss per batch: 0.513
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0068, 0.0107, 0.9183, 0.0537, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.002

[Epoch: 6, batch: 210/213] total loss per batch: 0.508
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.0096e-03, 9.7257e-01, 9.1492e-03, 3.7158e-03, 2.0118e-09, 4.6973e-03,
        6.8551e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 7, batch: 42/213] total loss per batch: 0.514
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0060, 0.0055, 0.0108, 0.0085, 0.0057, 0.9452, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 7, batch: 84/213] total loss per batch: 0.498
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([6.0312e-02, 7.0283e-02, 3.6943e-02, 4.3134e-07, 7.9035e-07, 2.1772e-02,
        8.1069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 7, batch: 126/213] total loss per batch: 0.506
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.5546e-02, 3.7539e-02, 1.7314e-02, 8.7254e-01, 9.1912e-03, 2.7866e-02,
        1.6623e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.001

[Epoch: 7, batch: 168/213] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0061, 0.0137, 0.9036, 0.0659, 0.0038, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 7, batch: 210/213] total loss per batch: 0.499
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.3699e-03, 9.7017e-01, 1.0566e-02, 6.0440e-03, 2.2985e-09, 3.9541e-03,
        5.8957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 8, batch: 42/213] total loss per batch: 0.507
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0044, 0.0032, 0.0042, 0.0036, 0.0039, 0.9701, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 8, batch: 84/213] total loss per batch: 0.494
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([7.8533e-03, 6.7641e-03, 4.7864e-03, 4.9429e-08, 1.0608e-07, 6.0429e-03,
        9.7455e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 8, batch: 126/213] total loss per batch: 0.499
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.7430e-02, 2.6416e-02, 1.4991e-02, 8.5281e-01, 7.4009e-03, 2.0950e-02,
        3.0405e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 8, batch: 168/213] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0062, 0.0124, 0.9070, 0.0640, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 8, batch: 210/213] total loss per batch: 0.496
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.5274e-03, 9.7235e-01, 9.0823e-03, 4.5866e-03, 8.4538e-10, 5.4073e-03,
        5.0439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 9, batch: 42/213] total loss per batch: 0.502
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0073, 0.0055, 0.0105, 0.0047, 0.0042, 0.9550, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 9, batch: 84/213] total loss per batch: 0.490
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6228e-02, 3.6685e-02, 1.4887e-02, 8.3058e-08, 3.5689e-07, 1.3533e-02,
        8.9867e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 9, batch: 126/213] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.7602e-02, 2.8241e-02, 1.5726e-02, 8.6654e-01, 1.1075e-02, 2.0816e-02,
        3.8072e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.002

[Epoch: 9, batch: 168/213] total loss per batch: 0.491
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0062, 0.0117, 0.9134, 0.0562, 0.0046, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.000

[Epoch: 9, batch: 210/213] total loss per batch: 0.494
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.8933e-03, 9.7232e-01, 9.9028e-03, 5.4662e-03, 1.6378e-09, 4.9318e-03,
        4.4863e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 10, batch: 42/213] total loss per batch: 0.500
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0061, 0.0040, 0.0056, 0.0064, 0.0039, 0.9642, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 10, batch: 84/213] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([5.5369e-02, 4.2811e-02, 3.9634e-02, 1.7105e-07, 8.1068e-07, 3.4270e-02,
        8.2791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 10, batch: 126/213] total loss per batch: 0.493
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0058e-02, 3.1732e-02, 2.2119e-02, 8.2341e-01, 1.1208e-02, 2.1471e-02,
        1.7017e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 10, batch: 168/213] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0049, 0.0122, 0.9025, 0.0678, 0.0049, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 10, batch: 210/213] total loss per batch: 0.490
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.5190e-03, 9.7484e-01, 8.0390e-03, 4.5612e-03, 1.6183e-09, 4.5969e-03,
        4.4407e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 11, batch: 42/213] total loss per batch: 0.498
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0113, 0.0055, 0.0067, 0.0105, 0.0056, 0.9489, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 11, batch: 84/213] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.1232e-02, 1.0663e-02, 8.9150e-03, 1.0369e-07, 2.8110e-07, 9.7656e-03,
        9.4942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 11, batch: 126/213] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.0351e-02, 2.3729e-02, 1.6240e-02, 8.6031e-01, 1.0918e-02, 1.8454e-02,
        1.2454e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.000

[Epoch: 11, batch: 168/213] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0046, 0.0098, 0.9195, 0.0546, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 11, batch: 210/213] total loss per batch: 0.493
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.9231e-03, 9.7019e-01, 1.0995e-02, 8.3353e-03, 3.3296e-09, 3.4692e-03,
        4.0840e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.003

[Epoch: 12, batch: 42/213] total loss per batch: 0.498
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0054, 0.0043, 0.0047, 0.0054, 0.0033, 0.9644, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 12, batch: 84/213] total loss per batch: 0.484
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3321e-02, 3.8835e-02, 2.9023e-02, 1.3686e-07, 4.7337e-07, 4.2742e-02,
        8.5608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 12, batch: 126/213] total loss per batch: 0.489
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.1373e-02, 2.0903e-02, 1.7372e-02, 8.6151e-01, 7.6994e-03, 2.1142e-02,
        5.2091e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.003

[Epoch: 12, batch: 168/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0060, 0.0116, 0.8802, 0.0874, 0.0055, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 12, batch: 210/213] total loss per batch: 0.492
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.2198e-03, 9.7359e-01, 8.2560e-03, 5.5924e-03, 7.7036e-09, 4.0727e-03,
        5.2722e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 13, batch: 42/213] total loss per batch: 0.497
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0080, 0.0040, 0.0071, 0.0079, 0.0048, 0.9554, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 13, batch: 84/213] total loss per batch: 0.486
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.9052e-02, 1.3981e-02, 1.4278e-02, 5.6601e-08, 3.9327e-07, 1.3201e-02,
        9.3949e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.043

[Epoch: 13, batch: 126/213] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7004e-02, 2.4245e-02, 1.2303e-02, 8.5071e-01, 9.7695e-03, 1.5973e-02,
        1.3044e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 13, batch: 168/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0056, 0.0127, 0.9053, 0.0636, 0.0045, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.002

[Epoch: 13, batch: 210/213] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8887e-03, 9.6782e-01, 1.1202e-02, 6.6024e-03, 3.4528e-09, 4.4537e-03,
        5.0351e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 14, batch: 42/213] total loss per batch: 0.496
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0067, 0.0042, 0.0069, 0.0064, 0.0043, 0.9641, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 14, batch: 84/213] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([6.0370e-02, 5.2231e-02, 5.9501e-02, 2.5826e-07, 6.9451e-07, 3.8842e-02,
        7.8905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 14, batch: 126/213] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1741e-01, 2.9379e-02, 1.8529e-02, 8.1047e-01, 1.1587e-02, 1.2623e-02,
        4.8713e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.005

[Epoch: 14, batch: 168/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0051, 0.0129, 0.9031, 0.0653, 0.0049, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 14, batch: 210/213] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.3693e-03, 9.7325e-01, 8.2776e-03, 6.2620e-03, 9.8480e-09, 4.6856e-03,
        4.1600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.004

[Epoch: 15, batch: 42/213] total loss per batch: 0.495
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0104, 0.0036, 0.0041, 0.0047, 0.0026, 0.9648, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 15, batch: 84/213] total loss per batch: 0.482
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.8971e-02, 8.0686e-03, 5.8462e-03, 1.2059e-07, 9.9240e-07, 1.0415e-02,
        9.5670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 15, batch: 126/213] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.2056e-02, 1.9853e-02, 1.6826e-02, 8.5433e-01, 1.3470e-02, 1.3464e-02,
        6.0049e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 15, batch: 168/213] total loss per batch: 0.484
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.0111, 0.9180, 0.0534, 0.0047, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 15, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.5899e-03, 9.7579e-01, 7.9951e-03, 5.4165e-03, 8.4578e-09, 4.4497e-03,
        3.7567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.000

[Epoch: 16, batch: 42/213] total loss per batch: 0.495
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0094, 0.0061, 0.0087, 0.0072, 0.0062, 0.9485, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 16, batch: 84/213] total loss per batch: 0.481
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.0150e-02, 1.1696e-02, 2.0943e-02, 1.1906e-07, 3.8299e-07, 3.3689e-02,
        9.1352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 16, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.2225e-02, 1.5500e-02, 1.3880e-02, 8.7504e-01, 1.1190e-02, 1.2169e-02,
        1.5231e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.007

[Epoch: 16, batch: 168/213] total loss per batch: 0.483
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.0090, 0.9021, 0.0700, 0.0053, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 16, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.3878e-03, 9.7372e-01, 8.7287e-03, 5.4100e-03, 1.6501e-08, 3.0812e-03,
        4.6753e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 17, batch: 42/213] total loss per batch: 0.492
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0058, 0.0054, 0.0059, 0.0051, 0.9549, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 17, batch: 84/213] total loss per batch: 0.481
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.2559e-02, 2.3067e-02, 1.9868e-02, 2.3639e-07, 1.9688e-06, 3.2037e-02,
        8.8247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 17, batch: 126/213] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.5576e-02, 2.4915e-02, 2.0773e-02, 8.2955e-01, 1.5893e-02, 1.3296e-02,
        5.7610e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 17, batch: 168/213] total loss per batch: 0.482
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0060, 0.0152, 0.8769, 0.0866, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 17, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.1044e-03, 9.6848e-01, 1.2045e-02, 6.3195e-03, 1.0150e-08, 5.5209e-03,
        4.5339e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.003

[Epoch: 18, batch: 42/213] total loss per batch: 0.492
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0106, 0.0069, 0.0036, 0.0115, 0.0035, 0.9546, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 18, batch: 84/213] total loss per batch: 0.480
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0059e-02, 2.3408e-02, 2.9794e-02, 2.1389e-07, 8.8606e-07, 2.9364e-02,
        8.8737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 18, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.3298e-02, 1.6080e-02, 1.2220e-02, 8.5645e-01, 1.1068e-02, 1.0887e-02,
        4.6194e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.008

[Epoch: 18, batch: 168/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0054, 0.0100, 0.9183, 0.0528, 0.0047, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 18, batch: 210/213] total loss per batch: 0.485
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0848e-03, 9.7315e-01, 8.9886e-03, 4.7317e-03, 1.3677e-08, 4.9337e-03,
        4.1095e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 19, batch: 42/213] total loss per batch: 0.492
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0114, 0.0066, 0.0083, 0.0078, 0.0056, 0.9477, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 19, batch: 84/213] total loss per batch: 0.479
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.8760e-02, 4.1409e-02, 5.8353e-02, 4.2805e-07, 2.9745e-06, 3.8614e-02,
        8.1286e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 19, batch: 126/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.9632e-02, 1.3616e-02, 1.6313e-02, 8.6437e-01, 1.4702e-02, 1.1368e-02,
        7.4747e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.000

[Epoch: 19, batch: 168/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0059, 0.0147, 0.8945, 0.0700, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 19, batch: 210/213] total loss per batch: 0.484
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.5661e-03, 9.7122e-01, 9.1246e-03, 4.6068e-03, 1.5790e-08, 6.1847e-03,
        5.3025e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.009

[Epoch: 20, batch: 42/213] total loss per batch: 0.491
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0097, 0.0079, 0.0054, 0.0124, 0.0061, 0.9484, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 20, batch: 84/213] total loss per batch: 0.478
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.3737e-02, 1.2661e-02, 1.0494e-02, 1.8717e-07, 4.3797e-07, 1.8700e-02,
        9.4441e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 20, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2957e-01, 2.2192e-02, 1.1139e-02, 8.1619e-01, 1.2472e-02, 8.4342e-03,
        7.6502e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 20, batch: 168/213] total loss per batch: 0.483
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0063, 0.0137, 0.9054, 0.0579, 0.0064, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 20, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.4978e-03, 9.7441e-01, 7.9748e-03, 5.3951e-03, 1.6110e-08, 3.5032e-03,
        5.2196e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 21, batch: 42/213] total loss per batch: 0.492
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0180, 0.0081, 0.0037, 0.0077, 0.0052, 0.9425, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 21, batch: 84/213] total loss per batch: 0.479
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([5.3954e-02, 2.0766e-02, 3.9492e-02, 1.0403e-07, 2.1962e-06, 3.7205e-02,
        8.4858e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 21, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.1746e-02, 1.0684e-02, 1.8453e-02, 9.2358e-01, 8.6132e-03, 6.9258e-03,
        2.5480e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.009

[Epoch: 21, batch: 168/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0055, 0.0098, 0.9097, 0.0595, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.001

[Epoch: 21, batch: 210/213] total loss per batch: 0.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.4688e-03, 9.7646e-01, 5.0688e-03, 7.4893e-03, 8.2190e-09, 3.3786e-03,
        5.1310e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 22, batch: 42/213] total loss per batch: 0.493
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0061, 0.0032, 0.0076, 0.0090, 0.0044, 0.9584, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 22, batch: 84/213] total loss per batch: 0.480
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.0405e-02, 2.0331e-02, 8.8766e-03, 2.0874e-07, 6.6349e-07, 2.9552e-02,
        9.2083e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 22, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1962e-01, 2.1187e-02, 1.5766e-02, 8.1840e-01, 1.1851e-02, 1.3177e-02,
        1.4222e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.005

[Epoch: 22, batch: 168/213] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0059, 0.0153, 0.8876, 0.0734, 0.0048, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 22, batch: 210/213] total loss per batch: 0.489
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.4067e-03, 9.7129e-01, 9.6543e-03, 5.2678e-03, 4.1483e-08, 4.4515e-03,
        4.9284e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.004

[Epoch: 23, batch: 42/213] total loss per batch: 0.496
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0117, 0.0046, 0.0064, 0.0107, 0.0054, 0.9473, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 23, batch: 84/213] total loss per batch: 0.492
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.6067e-02, 2.4234e-02, 1.9970e-02, 7.7558e-07, 1.2950e-06, 1.5694e-02,
        9.2403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.032

[Epoch: 23, batch: 126/213] total loss per batch: 0.496
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9025e-02, 3.0103e-02, 2.2471e-02, 8.2661e-01, 1.7504e-02, 1.4285e-02,
        2.9372e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 23, batch: 168/213] total loss per batch: 0.509
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0067, 0.0103, 0.8797, 0.0891, 0.0053, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.022

[Epoch: 23, batch: 210/213] total loss per batch: 0.493
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.7280e-03, 9.7459e-01, 1.0250e-02, 6.3024e-03, 1.0755e-08, 3.2979e-03,
        2.8299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 24, batch: 42/213] total loss per batch: 0.499
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0082, 0.0022, 0.0039, 0.0059, 0.0024, 0.9695, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 24, batch: 84/213] total loss per batch: 0.493
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([6.1981e-02, 1.7123e-02, 1.4951e-02, 3.2128e-07, 1.9769e-06, 2.1739e-02,
        8.8420e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 24, batch: 126/213] total loss per batch: 0.503
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.1620e-02, 1.5690e-02, 2.3110e-02, 8.8488e-01, 8.5011e-03, 1.6198e-02,
        6.6192e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 24, batch: 168/213] total loss per batch: 0.515
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0045, 0.0099, 0.9221, 0.0482, 0.0072, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.019

[Epoch: 24, batch: 210/213] total loss per batch: 0.511
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.9708e-03, 9.7402e-01, 7.8126e-03, 6.3040e-03, 5.8875e-09, 4.7502e-03,
        4.1377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.001

[Epoch: 25, batch: 42/213] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0110, 0.0106, 0.0069, 0.0040, 0.0091, 0.9415, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 25, batch: 84/213] total loss per batch: 0.516
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([6.0959e-02, 6.6186e-02, 5.7066e-02, 2.6784e-07, 4.5381e-06, 2.6640e-02,
        7.8914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 25, batch: 126/213] total loss per batch: 0.514
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.3664e-02, 2.9823e-02, 3.0669e-02, 8.2213e-01, 2.8098e-02, 1.5612e-02,
        3.9655e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.007

[Epoch: 25, batch: 168/213] total loss per batch: 0.528
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0049, 0.0091, 0.9221, 0.0545, 0.0024, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.000

[Epoch: 25, batch: 210/213] total loss per batch: 0.536
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.8835e-03, 9.6193e-01, 1.0246e-02, 1.1021e-02, 3.2800e-07, 4.3389e-03,
        4.5803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 26, batch: 42/213] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0074, 0.0047, 0.0051, 0.0045, 0.0122, 0.9500, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 26, batch: 84/213] total loss per batch: 0.512
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.5738e-02, 1.2645e-02, 1.0025e-02, 7.1651e-08, 4.2618e-06, 5.1774e-02,
        9.0981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 26, batch: 126/213] total loss per batch: 0.519
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([2.0959e-02, 3.2874e-03, 1.2127e-02, 9.4326e-01, 1.6444e-02, 3.9203e-03,
        1.8550e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.014

[Epoch: 26, batch: 168/213] total loss per batch: 0.519
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0040, 0.0103, 0.9301, 0.0407, 0.0034, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 26, batch: 210/213] total loss per batch: 0.516
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.0245e-03, 9.7503e-01, 1.0390e-02, 5.8959e-03, 1.1562e-07, 1.5429e-03,
        4.1193e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 27, batch: 42/213] total loss per batch: 0.523
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0105, 0.0054, 0.0080, 0.0074, 0.0139, 0.9468, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 27, batch: 84/213] total loss per batch: 0.497
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.2624e-02, 1.6704e-02, 8.7399e-03, 1.6914e-07, 1.7940e-06, 2.8926e-02,
        9.3300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.024

[Epoch: 27, batch: 126/213] total loss per batch: 0.501
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.5917e-02, 8.7131e-03, 3.4538e-02, 8.4842e-01, 1.2747e-02, 9.6607e-03,
        1.4821e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.000

[Epoch: 27, batch: 168/213] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0042, 0.0105, 0.9277, 0.0430, 0.0045, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 27, batch: 210/213] total loss per batch: 0.494
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.2013e-03, 9.7083e-01, 1.4982e-02, 5.0644e-03, 5.8597e-08, 1.8053e-03,
        5.1195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.006

[Epoch: 28, batch: 42/213] total loss per batch: 0.500
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0073, 0.0040, 0.0064, 0.0107, 0.0073, 0.9541, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 28, batch: 84/213] total loss per batch: 0.482
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.4933e-02, 4.0582e-02, 4.3304e-02, 1.8887e-07, 4.3690e-06, 3.5115e-02,
        8.6606e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 28, batch: 126/213] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.4903e-02, 8.3009e-03, 2.3230e-02, 8.7943e-01, 1.6491e-02, 7.6494e-03,
        1.5621e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.000

[Epoch: 28, batch: 168/213] total loss per batch: 0.483
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0060, 0.0153, 0.8955, 0.0680, 0.0053, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 28, batch: 210/213] total loss per batch: 0.484
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.4733e-03, 9.7535e-01, 1.0660e-02, 3.8191e-03, 9.3117e-08, 3.0298e-03,
        3.6702e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.000

[Epoch: 29, batch: 42/213] total loss per batch: 0.490
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0078, 0.0038, 0.0070, 0.0098, 0.0055, 0.9586, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.003

[Epoch: 29, batch: 84/213] total loss per batch: 0.476
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.0312e-02, 2.3354e-02, 2.6190e-02, 1.4588e-07, 4.8456e-06, 1.9691e-02,
        9.1045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 29, batch: 126/213] total loss per batch: 0.482
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7129e-02, 1.0458e-02, 1.7143e-02, 8.6254e-01, 1.4506e-02, 8.2260e-03,
        3.7464e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 29, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0047, 0.0120, 0.9093, 0.0591, 0.0047, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 29, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.7645e-03, 9.7169e-01, 1.2038e-02, 4.3201e-03, 6.0669e-08, 3.1199e-03,
        5.0699e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 30, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0071, 0.0036, 0.0053, 0.0068, 0.0048, 0.9638, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.002

[Epoch: 30, batch: 84/213] total loss per batch: 0.473
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6105e-02, 2.3742e-02, 2.5191e-02, 1.2586e-07, 4.5964e-06, 2.5176e-02,
        8.9978e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.027

[Epoch: 30, batch: 126/213] total loss per batch: 0.484
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0466e-01, 1.3866e-02, 1.9550e-02, 8.3840e-01, 1.4095e-02, 9.4315e-03,
        1.5431e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 30, batch: 168/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.0116, 0.9000, 0.0673, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.017

[Epoch: 30, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.4897e-03, 9.7187e-01, 1.0707e-02, 4.5598e-03, 7.2332e-08, 3.1368e-03,
        5.2379e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.001

[Epoch: 31, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0083, 0.0042, 0.0059, 0.0079, 0.0058, 0.9602, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 31, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7110e-02, 2.0667e-02, 2.8520e-02, 1.2503e-07, 2.4085e-06, 2.4122e-02,
        8.9958e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 31, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.3002e-02, 1.0886e-02, 1.8251e-02, 8.7473e-01, 1.5254e-02, 7.8736e-03,
        2.5026e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.000

[Epoch: 31, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0044, 0.0120, 0.9037, 0.0652, 0.0051, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 31, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.0082e-03, 9.7417e-01, 9.5704e-03, 4.7944e-03, 9.6780e-08, 2.9993e-03,
        5.4599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 32, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0085, 0.0038, 0.0055, 0.0074, 0.0061, 0.9580, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 32, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.7584e-02, 2.5791e-02, 2.9713e-02, 1.7686e-07, 2.1591e-06, 2.8178e-02,
        8.7873e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 32, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.5441e-02, 1.4380e-02, 1.7686e-02, 8.4865e-01, 1.6379e-02, 7.4668e-03,
        1.4808e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 32, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.0118, 0.8957, 0.0735, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 32, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5040e-03, 9.7497e-01, 7.9398e-03, 4.5026e-03, 7.8603e-08, 3.1087e-03,
        4.9738e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 33, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0097, 0.0042, 0.0063, 0.0085, 0.0058, 0.9545, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 33, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7560e-02, 2.3407e-02, 3.0685e-02, 1.3182e-07, 1.9594e-06, 2.5989e-02,
        8.9236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 33, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.6413e-02, 1.3432e-02, 1.7068e-02, 8.6254e-01, 1.3223e-02, 7.3208e-03,
        1.4207e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 33, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0049, 0.0124, 0.8976, 0.0694, 0.0050, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 33, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.7357e-03, 9.7554e-01, 8.3852e-03, 4.2475e-03, 6.0376e-08, 3.1473e-03,
        4.9429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 34, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0105, 0.0038, 0.0053, 0.0091, 0.0052, 0.9554, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 34, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5967e-02, 2.4498e-02, 2.8449e-02, 1.3864e-07, 1.6284e-06, 2.7236e-02,
        8.8385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.024

[Epoch: 34, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4476e-02, 1.4272e-02, 1.4680e-02, 8.5340e-01, 1.5912e-02, 7.2625e-03,
        2.0805e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 34, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0049, 0.0117, 0.9038, 0.0640, 0.0052, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 34, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.3183e-03, 9.7575e-01, 6.6149e-03, 4.5324e-03, 5.8924e-08, 3.2085e-03,
        5.5801e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 35, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0043, 0.0059, 0.0087, 0.0053, 0.9552, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 35, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1469e-02, 2.6251e-02, 3.2019e-02, 1.3229e-07, 2.6183e-06, 2.7722e-02,
        8.8254e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 35, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.6354e-02, 1.5274e-02, 1.6164e-02, 8.8345e-01, 1.1930e-02, 6.8265e-03,
        2.0672e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.003

[Epoch: 35, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0046, 0.0104, 0.9085, 0.0612, 0.0044, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 35, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.5859e-03, 9.7584e-01, 8.4063e-03, 4.4118e-03, 4.2391e-08, 3.5798e-03,
        4.1744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 36, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0129, 0.0038, 0.0055, 0.0096, 0.0051, 0.9514, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 36, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2238e-02, 2.4984e-02, 3.0431e-02, 1.2505e-07, 1.0505e-06, 3.3617e-02,
        8.7873e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 36, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2628e-01, 1.6066e-02, 1.4198e-02, 8.2035e-01, 1.6267e-02, 6.8340e-03,
        2.6876e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.005

[Epoch: 36, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0054, 0.0119, 0.8940, 0.0720, 0.0056, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.018

[Epoch: 36, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2192e-03, 9.7702e-01, 6.2618e-03, 4.2796e-03, 5.7851e-08, 2.6776e-03,
        5.5467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 37, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0040, 0.0052, 0.0103, 0.0053, 0.9521, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 37, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5729e-02, 2.2824e-02, 2.4593e-02, 1.5334e-07, 1.4789e-06, 2.2372e-02,
        9.0448e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 37, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.2580e-02, 1.4784e-02, 1.5326e-02, 9.0023e-01, 1.0530e-02, 6.5546e-03,
        2.2892e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 37, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0040, 0.0112, 0.9127, 0.0566, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 37, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6114e-03, 9.7222e-01, 7.9984e-03, 5.0084e-03, 3.8064e-08, 4.4643e-03,
        5.6993e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 38, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0117, 0.0047, 0.0057, 0.0099, 0.0056, 0.9506, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 38, batch: 84/213] total loss per batch: 0.473
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.9460e-02, 3.0031e-02, 3.1685e-02, 2.7588e-07, 1.9452e-06, 3.4421e-02,
        8.6440e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 38, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1742e-01, 1.6421e-02, 1.4170e-02, 8.3169e-01, 1.2788e-02, 7.5139e-03,
        3.8995e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.002

[Epoch: 38, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.0110, 0.8829, 0.0835, 0.0058, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 38, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0780e-03, 9.7641e-01, 6.3156e-03, 4.3652e-03, 5.3841e-08, 3.3524e-03,
        5.4782e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 39, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0152, 0.0047, 0.0086, 0.0134, 0.0076, 0.9341, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 39, batch: 84/213] total loss per batch: 0.474
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.2952e-02, 2.1198e-02, 2.4078e-02, 2.0317e-07, 2.3575e-06, 3.5050e-02,
        8.9672e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 39, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.1726e-02, 1.3464e-02, 1.3091e-02, 8.9454e-01, 1.1018e-02, 6.1553e-03,
        4.8876e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.021

[Epoch: 39, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0044, 0.0114, 0.9103, 0.0594, 0.0045, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 39, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.9098e-03, 9.7479e-01, 7.9118e-03, 3.4281e-03, 3.3336e-08, 3.7703e-03,
        6.1865e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 40, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0084, 0.0041, 0.0053, 0.0103, 0.0037, 0.9552, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 40, batch: 84/213] total loss per batch: 0.474
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3379e-02, 2.5867e-02, 3.5671e-02, 2.0948e-07, 8.8268e-07, 2.0376e-02,
        8.8471e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.034

[Epoch: 40, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1723e-01, 2.2027e-02, 1.8167e-02, 8.1959e-01, 1.4817e-02, 8.1689e-03,
        1.8100e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.003

[Epoch: 40, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0046, 0.0122, 0.8959, 0.0711, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 40, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6421e-03, 9.7182e-01, 7.1978e-03, 6.8632e-03, 1.4394e-07, 2.4624e-03,
        7.0158e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 41, batch: 42/213] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0128, 0.0033, 0.0057, 0.0062, 0.0059, 0.9519, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 41, batch: 84/213] total loss per batch: 0.475
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9382e-02, 2.0496e-02, 2.2852e-02, 3.7388e-07, 1.5021e-06, 3.8371e-02,
        8.8890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.034

[Epoch: 41, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.9122e-02, 1.2600e-02, 1.8218e-02, 8.7905e-01, 1.2856e-02, 8.1532e-03,
        7.3086e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.005

[Epoch: 41, batch: 168/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0054, 0.0097, 0.9164, 0.0526, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.019

[Epoch: 41, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.8028e-03, 9.7410e-01, 6.4279e-03, 4.4306e-03, 4.8715e-08, 4.4053e-03,
        6.8359e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 42, batch: 42/213] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0086, 0.0036, 0.0049, 0.0139, 0.0052, 0.9545, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 42, batch: 84/213] total loss per batch: 0.475
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3855e-02, 3.2165e-02, 3.7488e-02, 6.2307e-07, 2.3076e-06, 3.0922e-02,
        8.6557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.027

[Epoch: 42, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.9052e-02, 1.5295e-02, 1.4472e-02, 8.5109e-01, 1.3543e-02, 6.5434e-03,
        1.0832e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.007

[Epoch: 42, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0052, 0.0144, 0.8848, 0.0774, 0.0058, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 42, batch: 210/213] total loss per batch: 0.481
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9001e-03, 9.6795e-01, 9.9949e-03, 6.7392e-03, 1.9072e-07, 3.7321e-03,
        6.6833e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 43, batch: 42/213] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0107, 0.0031, 0.0050, 0.0081, 0.0042, 0.9509, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.001

[Epoch: 43, batch: 84/213] total loss per batch: 0.475
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2059e-02, 1.5821e-02, 1.5258e-02, 2.2699e-07, 1.6983e-06, 2.0497e-02,
        9.1636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.037

[Epoch: 43, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.0489e-02, 1.0809e-02, 1.8791e-02, 8.8518e-01, 1.7253e-02, 7.4811e-03,
        3.0070e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.002

[Epoch: 43, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0040, 0.0097, 0.9178, 0.0573, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 43, batch: 210/213] total loss per batch: 0.483
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.6710e-03, 9.8016e-01, 4.0569e-03, 3.6956e-03, 3.9632e-08, 2.6301e-03,
        5.7905e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 44, batch: 42/213] total loss per batch: 0.489
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0080, 0.0033, 0.0062, 0.0070, 0.0033, 0.9632, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.001

[Epoch: 44, batch: 84/213] total loss per batch: 0.482
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0053e-02, 3.3712e-02, 4.8887e-02, 4.4686e-07, 4.6358e-06, 2.8427e-02,
        8.5891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.046

[Epoch: 44, batch: 126/213] total loss per batch: 0.498
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.6009e-01, 1.1419e-02, 1.5808e-02, 7.7738e-01, 2.6067e-02, 9.2414e-03,
        1.3164e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.021

[Epoch: 44, batch: 168/213] total loss per batch: 0.510
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0079, 0.0119, 0.9429, 0.0210, 0.0045, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.001

[Epoch: 44, batch: 210/213] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.7169e-03, 9.8049e-01, 4.3576e-03, 5.7115e-03, 3.1818e-09, 2.6160e-03,
        4.1039e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 45, batch: 42/213] total loss per batch: 0.535
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0063, 0.0013, 0.0060, 0.0032, 0.0016, 0.9738, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 45, batch: 84/213] total loss per batch: 0.538
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.7648e-02, 1.7604e-02, 3.5582e-02, 6.3279e-07, 1.7841e-06, 1.4080e-02,
        8.9508e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.024

[Epoch: 45, batch: 126/213] total loss per batch: 0.523
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([2.1956e-02, 8.7082e-03, 1.5057e-02, 9.3284e-01, 1.1593e-02, 9.8438e-03,
        5.7832e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.030

[Epoch: 45, batch: 168/213] total loss per batch: 0.526
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0096, 0.0130, 0.8841, 0.0794, 0.0032, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.008

[Epoch: 45, batch: 210/213] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([1.9687e-03, 9.7545e-01, 7.8528e-03, 4.5581e-03, 4.2541e-09, 2.8131e-03,
        7.3587e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 46, batch: 42/213] total loss per batch: 0.529
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0169, 0.0066, 0.0043, 0.0029, 0.0038, 0.9569, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 46, batch: 84/213] total loss per batch: 0.513
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5384e-02, 2.5817e-02, 1.3109e-02, 2.7153e-06, 4.3854e-06, 1.9978e-02,
        9.1570e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.081

[Epoch: 46, batch: 126/213] total loss per batch: 0.520
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([2.6891e-02, 5.1594e-02, 2.0624e-02, 8.8337e-01, 4.6068e-03, 1.2918e-02,
        2.2247e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.003

[Epoch: 46, batch: 168/213] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0065, 0.0106, 0.8716, 0.0930, 0.0053, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.001

[Epoch: 46, batch: 210/213] total loss per batch: 0.504
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.3068e-03, 9.8264e-01, 4.1178e-03, 3.7111e-03, 1.2077e-08, 2.7857e-03,
        4.4382e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 47, batch: 42/213] total loss per batch: 0.505
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0072, 0.0059, 0.0048, 0.0046, 0.0044, 0.9672, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 -0.008

[Epoch: 47, batch: 84/213] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.7655e-02, 2.3284e-02, 1.8206e-02, 7.8621e-07, 1.5322e-06, 3.7273e-02,
        9.0358e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.089

[Epoch: 47, batch: 126/213] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.8582e-02, 2.8510e-02, 1.3189e-02, 8.4117e-01, 8.8850e-03, 9.6642e-03,
        1.0136e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.012

[Epoch: 47, batch: 168/213] total loss per batch: 0.482
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0065, 0.0116, 0.8878, 0.0745, 0.0064, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 47, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.7472e-03, 9.8100e-01, 4.4581e-03, 4.3884e-03, 5.9250e-09, 1.8977e-03,
        5.5048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 48, batch: 42/213] total loss per batch: 0.494
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0172, 0.0064, 0.0077, 0.0088, 0.0053, 0.9429, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 48, batch: 84/213] total loss per batch: 0.476
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1350e-02, 2.3136e-02, 2.6365e-02, 1.3363e-06, 5.7837e-07, 3.7564e-02,
        8.8158e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.048

[Epoch: 48, batch: 126/213] total loss per batch: 0.482
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.6206e-02, 2.0108e-02, 1.8640e-02, 8.7748e-01, 9.4867e-03, 8.0807e-03,
        2.6467e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.017

[Epoch: 48, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.0069, 0.0117, 0.8867, 0.0759, 0.0067, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 48, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0781e-03, 9.7400e-01, 6.0625e-03, 6.0918e-03, 5.7072e-09, 3.5487e-03,
        6.2216e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 49, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0129, 0.0065, 0.0078, 0.0073, 0.0044, 0.9483, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.001

[Epoch: 49, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5981e-02, 3.0159e-02, 3.0341e-02, 1.1381e-06, 6.3688e-07, 3.9430e-02,
        8.6409e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 49, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.5616e-02, 1.9173e-02, 1.5531e-02, 8.6036e-01, 1.0647e-02, 8.6725e-03,
        3.9733e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.017

[Epoch: 49, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0071, 0.0113, 0.8951, 0.0692, 0.0061, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 49, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.7542e-03, 9.7410e-01, 6.0298e-03, 6.6423e-03, 4.0159e-09, 3.6357e-03,
        5.8390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 50, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0108, 0.0060, 0.0067, 0.0071, 0.0045, 0.9525, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 50, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9505e-02, 2.3535e-02, 2.7529e-02, 9.1280e-07, 3.9562e-07, 3.2951e-02,
        8.8648e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.040

[Epoch: 50, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9124e-02, 1.8054e-02, 1.6406e-02, 8.5760e-01, 1.0891e-02, 7.9258e-03,
        2.6135e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.016

[Epoch: 50, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0066, 0.0112, 0.8911, 0.0731, 0.0063, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 50, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0306e-03, 9.7318e-01, 6.2339e-03, 6.1658e-03, 3.5411e-09, 3.6378e-03,
        5.7560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 51, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0058, 0.0068, 0.0076, 0.0041, 0.9521, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.001

[Epoch: 51, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2930e-02, 2.8446e-02, 2.9332e-02, 7.0857e-07, 3.7792e-07, 3.0817e-02,
        8.7848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.035

[Epoch: 51, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.4228e-02, 1.6538e-02, 1.4616e-02, 8.6603e-01, 1.1300e-02, 7.2864e-03,
        4.5107e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.015

[Epoch: 51, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0069, 0.0115, 0.8966, 0.0674, 0.0062, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 51, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.1890e-03, 9.7337e-01, 6.4014e-03, 6.3127e-03, 2.7497e-09, 3.8480e-03,
        5.8756e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 52, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0055, 0.0061, 0.0083, 0.0040, 0.9526, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 52, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0579e-02, 2.5872e-02, 3.0610e-02, 6.9238e-07, 3.0032e-07, 2.8875e-02,
        8.8406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 52, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.2112e-02, 1.6345e-02, 1.5707e-02, 8.5732e-01, 1.1393e-02, 7.1241e-03,
        3.0467e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.015

[Epoch: 52, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0065, 0.0113, 0.8980, 0.0668, 0.0061, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 52, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9880e-03, 9.7284e-01, 6.4065e-03, 6.0560e-03, 2.2392e-09, 4.0648e-03,
        5.6468e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 53, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0055, 0.0061, 0.0084, 0.0041, 0.9515, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 53, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9173e-02, 2.5631e-02, 2.9025e-02, 5.3264e-07, 2.4308e-07, 2.9875e-02,
        8.8630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 53, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.6902e-02, 1.5507e-02, 1.4375e-02, 8.6459e-01, 1.2140e-02, 6.4861e-03,
        3.4537e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.014

[Epoch: 53, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0067, 0.0113, 0.8955, 0.0687, 0.0062, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 53, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5740e-03, 9.7305e-01, 6.4099e-03, 6.0920e-03, 2.1348e-09, 3.8118e-03,
        6.0616e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 54, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0052, 0.0058, 0.0090, 0.0042, 0.9517, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.004

[Epoch: 54, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0955e-02, 2.6326e-02, 2.9979e-02, 4.9194e-07, 2.1909e-07, 3.0615e-02,
        8.8212e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 54, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.2881e-02, 1.6199e-02, 1.5860e-02, 8.5557e-01, 1.2523e-02, 6.9631e-03,
        2.7520e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.013

[Epoch: 54, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0064, 0.0112, 0.8990, 0.0661, 0.0060, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 54, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0592e-03, 9.7306e-01, 6.0571e-03, 6.0092e-03, 1.5063e-09, 4.1616e-03,
        5.6511e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 55, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0054, 0.0059, 0.0096, 0.0042, 0.9495, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.004

[Epoch: 55, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7335e-02, 2.5681e-02, 3.0040e-02, 3.9360e-07, 1.8090e-07, 2.8534e-02,
        8.8841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 55, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.8056e-02, 1.4054e-02, 1.3816e-02, 8.7553e-01, 1.2647e-02, 5.8944e-03,
        2.7936e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.011

[Epoch: 55, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0066, 0.0112, 0.8973, 0.0673, 0.0063, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 55, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7328e-03, 9.7296e-01, 6.3069e-03, 5.9459e-03, 1.7324e-09, 4.1915e-03,
        5.8650e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 56, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0049, 0.0059, 0.0090, 0.0043, 0.9514, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 56, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3147e-02, 2.6486e-02, 3.0084e-02, 3.9442e-07, 1.5947e-07, 3.1157e-02,
        8.7913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 56, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1125e-01, 1.8083e-02, 1.7585e-02, 8.3108e-01, 1.4595e-02, 7.4021e-03,
        3.2041e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.009

[Epoch: 56, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0063, 0.0108, 0.9021, 0.0643, 0.0057, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 56, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9463e-03, 9.7328e-01, 6.0512e-03, 6.1388e-03, 1.2753e-09, 3.8543e-03,
        5.7262e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 57, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0104, 0.0051, 0.0058, 0.0102, 0.0041, 0.9506, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 57, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6029e-02, 2.1962e-02, 3.0925e-02, 3.7302e-07, 1.5648e-07, 3.1599e-02,
        8.8948e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 57, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.3443e-02, 1.3830e-02, 1.2400e-02, 8.9382e-01, 1.1194e-02, 5.3144e-03,
        2.1980e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.008

[Epoch: 57, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0065, 0.0114, 0.8945, 0.0691, 0.0065, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 57, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6185e-03, 9.7469e-01, 5.8442e-03, 5.4199e-03, 1.7204e-09, 4.4144e-03,
        5.0094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 58, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0104, 0.0052, 0.0059, 0.0098, 0.0044, 0.9478, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 58, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5782e-02, 3.0600e-02, 2.9829e-02, 4.5006e-07, 1.9237e-07, 2.5370e-02,
        8.7842e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 58, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2311e-01, 2.1429e-02, 1.7342e-02, 8.1217e-01, 1.6391e-02, 9.5536e-03,
        5.9530e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.006

[Epoch: 58, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0060, 0.0107, 0.9061, 0.0616, 0.0054, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 58, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7086e-03, 9.7499e-01, 5.2750e-03, 5.8599e-03, 1.3985e-09, 3.4820e-03,
        5.6825e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 59, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0090, 0.0048, 0.0063, 0.0105, 0.0043, 0.9537, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 59, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8081e-02, 2.5012e-02, 2.9061e-02, 3.2883e-07, 1.5630e-07, 3.3353e-02,
        8.8449e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 59, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.3093e-02, 1.3628e-02, 1.3754e-02, 9.0272e-01, 1.1098e-02, 5.7124e-03,
        1.6312e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.006

[Epoch: 59, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0069, 0.0121, 0.8926, 0.0695, 0.0067, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 59, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0467e-03, 9.7705e-01, 5.1743e-03, 4.2275e-03, 2.4130e-09, 4.5052e-03,
        4.9967e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 60, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0114, 0.0046, 0.0058, 0.0088, 0.0044, 0.9504, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.004

[Epoch: 60, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0759e-02, 2.8550e-02, 2.7010e-02, 9.3475e-07, 2.1893e-07, 2.9499e-02,
        8.8418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 60, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0473e-01, 1.8537e-02, 1.8003e-02, 8.3346e-01, 1.5686e-02, 9.5927e-03,
        5.3129e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.002

[Epoch: 60, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0063, 0.0108, 0.8941, 0.0718, 0.0058, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 60, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0273e-03, 9.7169e-01, 6.9744e-03, 7.3080e-03, 1.1283e-09, 4.0032e-03,
        4.9973e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 61, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0096, 0.0045, 0.0062, 0.0101, 0.0050, 0.9497, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 61, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3875e-02, 2.1265e-02, 2.5078e-02, 3.9458e-07, 1.8920e-07, 2.7410e-02,
        8.9237e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.057

[Epoch: 61, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.3772e-02, 1.8616e-02, 1.2683e-02, 8.8351e-01, 1.4915e-02, 6.4997e-03,
        1.6822e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.003

[Epoch: 61, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0060, 0.0118, 0.9033, 0.0616, 0.0060, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 61, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0359e-03, 9.7644e-01, 5.1349e-03, 4.4670e-03, 3.8649e-09, 4.3537e-03,
        4.5727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 62, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0120, 0.0050, 0.0059, 0.0077, 0.0045, 0.9500, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 62, batch: 84/213] total loss per batch: 0.473
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9799e-02, 3.5628e-02, 3.7333e-02, 1.2575e-06, 3.3102e-07, 2.8103e-02,
        8.6914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.027

[Epoch: 62, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2758e-01, 1.5647e-02, 1.2098e-02, 8.2429e-01, 1.4496e-02, 5.8887e-03,
        2.3458e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 62, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0078, 0.0111, 0.8917, 0.0722, 0.0066, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 62, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.1144e-03, 9.7350e-01, 5.9973e-03, 5.7822e-03, 1.1882e-09, 5.1602e-03,
        5.4475e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 63, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0127, 0.0050, 0.0059, 0.0068, 0.0054, 0.9512, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 63, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6238e-02, 1.8868e-02, 1.9708e-02, 8.9345e-07, 2.9925e-07, 3.7050e-02,
        8.8813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.034

[Epoch: 63, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.5442e-02, 1.7650e-02, 1.4852e-02, 8.8985e-01, 1.6472e-02, 5.7388e-03,
        4.5731e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 63, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0059, 0.0126, 0.8962, 0.0679, 0.0059, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 63, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7229e-03, 9.7687e-01, 5.8554e-03, 5.0108e-03, 3.0267e-09, 3.1006e-03,
        4.4430e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 64, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0052, 0.0063, 0.0132, 0.0065, 0.9420, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 64, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1243e-02, 3.2428e-02, 3.3399e-02, 1.0800e-06, 3.2632e-07, 2.6713e-02,
        8.7622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 64, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.8942e-02, 1.7971e-02, 1.1680e-02, 8.5493e-01, 1.0099e-02, 6.3744e-03,
        8.6242e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 64, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0075, 0.0116, 0.8983, 0.0651, 0.0061, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 64, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.2445e-03, 9.6601e-01, 7.1716e-03, 7.6218e-03, 3.3724e-09, 7.3331e-03,
        5.6149e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 65, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0172, 0.0056, 0.0078, 0.0082, 0.0044, 0.9353, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 65, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9933e-02, 1.9168e-02, 2.3518e-02, 8.3565e-07, 3.0381e-07, 2.7256e-02,
        9.0012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.035

[Epoch: 65, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0818e-01, 2.0045e-02, 1.6487e-02, 8.3050e-01, 1.7316e-02, 7.4627e-03,
        4.4673e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 65, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.0112, 0.8985, 0.0701, 0.0055, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 65, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7071e-03, 9.7757e-01, 5.1033e-03, 4.4612e-03, 3.8634e-09, 3.5995e-03,
        4.5569e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 66, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0055, 0.0070, 0.0084, 0.0052, 0.9538, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 66, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9073e-02, 2.9550e-02, 2.8109e-02, 1.1647e-06, 2.9044e-07, 3.5693e-02,
        8.7757e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 66, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([4.1240e-02, 1.4102e-02, 1.4685e-02, 9.1146e-01, 1.2424e-02, 6.0887e-03,
        1.3965e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 66, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0102, 0.9014, 0.0680, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 66, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6445e-03, 9.7666e-01, 4.9051e-03, 4.6769e-03, 1.6624e-09, 5.1004e-03,
        4.0086e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 67, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0044, 0.0057, 0.0083, 0.0050, 0.9498, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 67, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1728e-02, 2.5385e-02, 3.4044e-02, 8.9079e-07, 3.8941e-07, 2.8208e-02,
        8.8063e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.027

[Epoch: 67, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.6809e-01, 1.7782e-02, 1.7923e-02, 7.6907e-01, 1.7581e-02, 9.5468e-03,
        4.1895e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 67, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0056, 0.0107, 0.8958, 0.0734, 0.0050, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 67, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.3752e-03, 9.7554e-01, 5.3319e-03, 6.1371e-03, 7.0671e-09, 4.1249e-03,
        4.4878e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.005

[Epoch: 68, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0078, 0.0066, 0.0055, 0.0100, 0.0052, 0.9531, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 68, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9821e-02, 1.8802e-02, 1.9259e-02, 7.4603e-07, 2.2415e-07, 3.0303e-02,
        9.0181e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 68, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.8663e-02, 1.3877e-02, 1.6773e-02, 9.0576e-01, 1.7732e-02, 7.1920e-03,
        1.3304e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 68, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0060, 0.0120, 0.9005, 0.0661, 0.0058, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 68, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.3055e-03, 9.7459e-01, 6.0919e-03, 5.6845e-03, 4.7496e-09, 4.6782e-03,
        4.6547e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 69, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0136, 0.0048, 0.0069, 0.0090, 0.0053, 0.9419, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 69, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.1966e-02, 4.1976e-02, 4.8764e-02, 1.1921e-06, 8.2914e-07, 3.8343e-02,
        8.2895e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 69, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.1579e-02, 1.9168e-02, 1.7216e-02, 8.6353e-01, 1.2903e-02, 5.6020e-03,
        5.6064e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 69, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0058, 0.0117, 0.8886, 0.0765, 0.0057, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 69, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2451e-03, 9.7644e-01, 4.9314e-03, 4.0185e-03, 4.1773e-09, 4.5471e-03,
        5.8154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 70, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0084, 0.0039, 0.0050, 0.0120, 0.0046, 0.9520, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 70, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.7439e-02, 1.1039e-02, 1.0997e-02, 6.4094e-07, 3.3914e-07, 1.9061e-02,
        9.4146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.064

[Epoch: 70, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0508e-01, 1.4611e-02, 1.1332e-02, 8.4918e-01, 1.2855e-02, 6.9363e-03,
        3.8842e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 70, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0057, 0.0109, 0.8994, 0.0678, 0.0059, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 70, batch: 210/213] total loss per batch: 0.478
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7222e-03, 9.7359e-01, 5.9444e-03, 6.0636e-03, 3.2843e-09, 4.4000e-03,
        5.2766e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 71, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0151, 0.0048, 0.0074, 0.0083, 0.0061, 0.9390, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 71, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.9263e-02, 4.3150e-02, 3.8159e-02, 1.3296e-06, 7.5906e-07, 3.6742e-02,
        8.4268e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 71, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.8710e-02, 1.0575e-02, 1.7129e-02, 8.9517e-01, 1.3254e-02, 5.1611e-03,
        7.6070e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 71, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0064, 0.0121, 0.8937, 0.0717, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 71, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0869e-03, 9.7688e-01, 5.4008e-03, 3.6323e-03, 5.6682e-09, 3.5490e-03,
        5.4478e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 72, batch: 42/213] total loss per batch: 0.485
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0088, 0.0044, 0.0068, 0.0088, 0.0047, 0.9580, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 72, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.4351e-02, 1.7053e-02, 2.8840e-02, 1.0985e-06, 2.4801e-07, 2.4766e-02,
        9.0499e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.034

[Epoch: 72, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.3042e-01, 1.9236e-02, 1.5250e-02, 8.1274e-01, 1.4703e-02, 7.6533e-03,
        5.1998e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.023

[Epoch: 72, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0055, 0.0125, 0.8996, 0.0665, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.017

[Epoch: 72, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2073e-03, 9.7453e-01, 4.9818e-03, 5.3055e-03, 9.0382e-09, 5.1109e-03,
        5.8610e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 73, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0081, 0.0044, 0.0052, 0.0101, 0.0039, 0.9553, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 73, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3552e-02, 2.6346e-02, 2.3546e-02, 7.7851e-07, 7.8421e-07, 3.6307e-02,
        8.8025e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.035

[Epoch: 73, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.1366e-02, 1.5702e-02, 2.1915e-02, 8.7630e-01, 1.7992e-02, 6.7213e-03,
        1.2954e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 73, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0063, 0.0114, 0.8984, 0.0680, 0.0057, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 73, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2342e-03, 9.7719e-01, 5.7338e-03, 4.7184e-03, 3.9692e-09, 3.5190e-03,
        4.6070e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.002

[Epoch: 74, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0115, 0.0035, 0.0060, 0.0101, 0.0064, 0.9465, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 74, batch: 84/213] total loss per batch: 0.472
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.1590e-02, 2.5882e-02, 3.9113e-02, 1.0055e-06, 5.8280e-07, 2.7511e-02,
        8.8590e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 74, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4005e-02, 1.3497e-02, 7.9050e-03, 8.6297e-01, 1.3161e-02, 8.4659e-03,
        9.1412e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 74, batch: 168/213] total loss per batch: 0.474
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0122, 0.9018, 0.0649, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 74, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5653e-03, 9.7597e-01, 3.6556e-03, 5.5509e-03, 2.3620e-09, 4.9524e-03,
        5.3020e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 75, batch: 42/213] total loss per batch: 0.489
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0087, 0.0028, 0.0054, 0.0045, 0.0034, 0.9669, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 75, batch: 84/213] total loss per batch: 0.477
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.8454e-02, 1.7588e-02, 1.9682e-02, 2.4025e-07, 5.6433e-07, 3.0392e-02,
        8.9388e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 75, batch: 126/213] total loss per batch: 0.483
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4912e-02, 1.8279e-02, 1.2186e-02, 8.5139e-01, 1.3984e-02, 9.2537e-03,
        2.1880e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 75, batch: 168/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0068, 0.0120, 0.8934, 0.0724, 0.0065, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.024

[Epoch: 75, batch: 210/213] total loss per batch: 0.484
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6504e-03, 9.7317e-01, 6.7418e-03, 4.0129e-03, 6.2518e-09, 4.9980e-03,
        6.4270e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 76, batch: 42/213] total loss per batch: 0.495
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0053, 0.0070, 0.0112, 0.0052, 0.9415, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.032

[Epoch: 76, batch: 84/213] total loss per batch: 0.479
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5897e-02, 3.4515e-02, 1.8739e-02, 5.2305e-07, 1.9167e-07, 1.5330e-02,
        9.0552e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.049

[Epoch: 76, batch: 126/213] total loss per batch: 0.487
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.7307e-02, 2.3755e-02, 1.4649e-02, 8.6086e-01, 2.2110e-02, 1.1317e-02,
        7.7890e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.025

[Epoch: 76, batch: 168/213] total loss per batch: 0.482
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.0093, 0.9219, 0.0496, 0.0042, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 76, batch: 210/213] total loss per batch: 0.487
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([2.5338e-03, 9.7997e-01, 5.2546e-03, 5.8643e-03, 7.3124e-10, 3.4913e-03,
        2.8899e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 77, batch: 42/213] total loss per batch: 0.497
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0169, 0.0029, 0.0025, 0.0117, 0.0039, 0.9550, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 77, batch: 84/213] total loss per batch: 0.495
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6993e-02, 2.0552e-02, 2.5426e-02, 9.4175e-07, 8.6539e-07, 5.5648e-02,
        8.7138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.039

[Epoch: 77, batch: 126/213] total loss per batch: 0.490
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1116e-01, 1.0448e-02, 1.7593e-02, 8.2175e-01, 3.1132e-02, 7.9149e-03,
        8.4384e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 77, batch: 168/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0069, 0.0129, 0.8973, 0.0668, 0.0064, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 77, batch: 210/213] total loss per batch: 0.486
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6003e-03, 9.7255e-01, 7.6320e-03, 5.4574e-03, 1.2333e-08, 4.4881e-03,
        5.2746e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 78, batch: 42/213] total loss per batch: 0.497
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0067, 0.0069, 0.0092, 0.0045, 0.9515, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.037

[Epoch: 78, batch: 84/213] total loss per batch: 0.483
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.0002e-02, 3.1785e-02, 2.4449e-02, 3.1888e-07, 6.6726e-07, 3.7486e-02,
        8.6628e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 78, batch: 126/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.4509e-02, 9.5433e-03, 1.5787e-02, 8.7585e-01, 1.6826e-02, 7.4846e-03,
        6.1049e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 78, batch: 168/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0064, 0.0119, 0.9003, 0.0657, 0.0055, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 78, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9749e-03, 9.7535e-01, 6.4297e-03, 3.4754e-03, 3.0851e-09, 4.8998e-03,
        4.8751e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 79, batch: 42/213] total loss per batch: 0.487
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0078, 0.0049, 0.0067, 0.0091, 0.0046, 0.9541, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.030

[Epoch: 79, batch: 84/213] total loss per batch: 0.490
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.4265e-02, 2.1731e-02, 1.5572e-02, 1.4907e-07, 4.2551e-07, 1.1488e-02,
        9.2694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 79, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.4422e-02, 1.3877e-02, 1.2254e-02, 8.6480e-01, 1.6213e-02, 8.4387e-03,
        1.1396e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 79, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.0125, 0.9064, 0.0600, 0.0051, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 79, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1876e-03, 9.7317e-01, 7.1085e-03, 6.3376e-03, 2.2616e-09, 3.0755e-03,
        5.1192e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 80, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0080, 0.0055, 0.0067, 0.0066, 0.0051, 0.9604, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 80, batch: 84/213] total loss per batch: 0.476
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9214e-02, 3.4179e-02, 2.7527e-02, 5.7791e-07, 6.7172e-07, 3.9417e-02,
        8.6966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 80, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.2772e-02, 1.1823e-02, 1.3612e-02, 8.8763e-01, 1.4948e-02, 9.2121e-03,
        2.1610e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 80, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.0118, 0.9119, 0.0571, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 80, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.9658e-03, 9.7232e-01, 6.2341e-03, 6.1367e-03, 2.0269e-09, 4.0289e-03,
        5.3126e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.025

[Epoch: 81, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0080, 0.0051, 0.0074, 0.0093, 0.0059, 0.9529, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.025

[Epoch: 81, batch: 84/213] total loss per batch: 0.473
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2374e-02, 2.3551e-02, 2.7619e-02, 3.4380e-07, 6.0162e-07, 3.5464e-02,
        8.8099e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 81, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6324e-02, 1.3379e-02, 2.1910e-02, 8.4420e-01, 1.5513e-02, 8.6789e-03,
        2.9694e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 81, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0047, 0.0123, 0.9060, 0.0616, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 81, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.6974e-03, 9.7556e-01, 5.4675e-03, 5.2546e-03, 1.6509e-09, 3.6490e-03,
        4.3706e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 82, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0095, 0.0051, 0.0065, 0.0091, 0.0048, 0.9517, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.025

[Epoch: 82, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6461e-02, 2.3450e-02, 2.8240e-02, 3.7805e-07, 5.5125e-07, 2.9057e-02,
        8.9279e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 82, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.8767e-02, 1.2999e-02, 1.6830e-02, 8.4907e-01, 1.4190e-02, 8.1413e-03,
        3.0431e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.008

[Epoch: 82, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0051, 0.0123, 0.9007, 0.0651, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 82, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.4023e-03, 9.7316e-01, 6.3932e-03, 5.3584e-03, 1.8565e-09, 4.1359e-03,
        4.5544e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 83, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0111, 0.0052, 0.0060, 0.0096, 0.0054, 0.9489, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 83, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1098e-02, 2.7243e-02, 3.0304e-02, 2.9562e-07, 4.5481e-07, 3.2998e-02,
        8.7836e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 83, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.5751e-02, 1.3043e-02, 1.6253e-02, 8.6311e-01, 1.4495e-02, 7.3441e-03,
        3.3016e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 83, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.0118, 0.8992, 0.0679, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 83, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8994e-03, 9.7621e-01, 5.9865e-03, 4.9609e-03, 1.8508e-09, 3.8244e-03,
        4.1236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 84, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0045, 0.0055, 0.0088, 0.0050, 0.9529, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 84, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7901e-02, 2.5088e-02, 3.0558e-02, 2.5080e-07, 3.3290e-07, 2.8513e-02,
        8.8794e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 84, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9674e-02, 1.3375e-02, 1.5737e-02, 8.6016e-01, 1.4219e-02, 6.8349e-03,
        2.5222e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 84, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0121, 0.9035, 0.0635, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 84, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.5133e-03, 9.7533e-01, 5.4188e-03, 4.9296e-03, 1.6348e-09, 4.2095e-03,
        4.6012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 85, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0112, 0.0047, 0.0053, 0.0094, 0.0048, 0.9501, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 85, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9355e-02, 2.5752e-02, 2.8902e-02, 2.2534e-07, 3.3353e-07, 2.9686e-02,
        8.8630e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 85, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.5322e-02, 1.3643e-02, 1.6184e-02, 8.5309e-01, 1.4828e-02, 6.9368e-03,
        2.5884e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 85, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0118, 0.8974, 0.0697, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 85, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0031e-03, 9.7664e-01, 5.4945e-03, 4.7659e-03, 1.4099e-09, 3.9630e-03,
        4.1286e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 86, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0046, 0.0054, 0.0092, 0.0052, 0.9515, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 86, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1502e-02, 2.6176e-02, 3.3704e-02, 2.3673e-07, 2.6766e-07, 3.2482e-02,
        8.7614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 86, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.4707e-02, 1.3631e-02, 1.5401e-02, 8.6588e-01, 1.4310e-02, 6.0653e-03,
        2.8737e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 86, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0048, 0.0117, 0.9039, 0.0638, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 86, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2250e-03, 9.7542e-01, 5.5773e-03, 4.9322e-03, 1.5333e-09, 4.3835e-03,
        4.4638e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 87, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0121, 0.0048, 0.0052, 0.0097, 0.0050, 0.9475, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 87, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8109e-02, 2.3979e-02, 2.5725e-02, 1.7441e-07, 2.4935e-07, 2.6697e-02,
        8.9549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 87, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.8094e-02, 1.4102e-02, 1.5254e-02, 8.5149e-01, 1.4690e-02, 6.3733e-03,
        2.2405e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 87, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0050, 0.0119, 0.8987, 0.0686, 0.0050, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 87, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2516e-03, 9.7705e-01, 5.0525e-03, 4.7462e-03, 1.1224e-09, 3.7840e-03,
        4.1191e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 88, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0087, 0.0042, 0.0052, 0.0092, 0.0051, 0.9534, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 88, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0113e-02, 2.4128e-02, 3.2687e-02, 2.2511e-07, 2.3012e-07, 2.9327e-02,
        8.8375e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 88, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.0032e-02, 1.3547e-02, 1.4568e-02, 8.7197e-01, 1.4092e-02, 5.7897e-03,
        2.3046e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 88, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0123, 0.8969, 0.0694, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 88, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9185e-03, 9.7437e-01, 5.9596e-03, 5.1312e-03, 1.8518e-09, 4.6644e-03,
        4.9589e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 89, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0133, 0.0050, 0.0052, 0.0095, 0.0048, 0.9464, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 89, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0904e-02, 2.6649e-02, 2.7448e-02, 2.4585e-07, 2.8571e-07, 3.0971e-02,
        8.8403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 89, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.3580e-02, 1.4983e-02, 1.4440e-02, 8.5599e-01, 1.5308e-02, 5.7032e-03,
        2.3508e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 89, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0048, 0.0103, 0.9056, 0.0633, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.020

[Epoch: 89, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8114e-03, 9.7913e-01, 4.5285e-03, 3.7676e-03, 1.3693e-09, 3.8346e-03,
        3.9242e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 90, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0073, 0.0039, 0.0060, 0.0085, 0.0051, 0.9544, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 90, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8631e-02, 2.2383e-02, 3.1879e-02, 3.2042e-07, 1.8453e-07, 2.4773e-02,
        8.9233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 90, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8862e-02, 1.4116e-02, 1.4088e-02, 8.6379e-01, 1.3016e-02, 6.1242e-03,
        4.4110e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 90, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0128, 0.8985, 0.0684, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.017

[Epoch: 90, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.4859e-03, 9.7285e-01, 7.2099e-03, 5.6900e-03, 3.2198e-09, 4.8183e-03,
        4.9470e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 91, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0115, 0.0054, 0.0049, 0.0115, 0.0048, 0.9484, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 91, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2232e-02, 2.6393e-02, 3.2059e-02, 4.0243e-07, 3.7610e-07, 2.9875e-02,
        8.7944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 91, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.8898e-02, 1.3048e-02, 1.4883e-02, 8.5257e-01, 1.5021e-02, 5.5838e-03,
        4.9425e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.020

[Epoch: 91, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0094, 0.9145, 0.0564, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.014

[Epoch: 91, batch: 210/213] total loss per batch: 0.477
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.4027e-03, 9.7548e-01, 4.7688e-03, 4.7082e-03, 2.4483e-09, 4.7157e-03,
        3.9208e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 92, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0108, 0.0043, 0.0050, 0.0085, 0.0053, 0.9457, 0.0205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 92, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5687e-02, 2.1959e-02, 3.1195e-02, 4.9039e-07, 2.2377e-07, 2.8986e-02,
        8.9217e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 92, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.6097e-02, 1.6358e-02, 1.4127e-02, 8.7324e-01, 1.4092e-02, 6.0906e-03,
        3.8351e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 92, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0059, 0.0127, 0.8989, 0.0667, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 92, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.7697e-03, 9.7785e-01, 5.3854e-03, 4.2270e-03, 5.0258e-09, 4.1144e-03,
        4.6490e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 93, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0045, 0.0056, 0.0100, 0.0043, 0.9528, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 93, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6517e-02, 3.0432e-02, 2.6705e-02, 5.3058e-07, 2.7507e-07, 2.9005e-02,
        8.7734e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 93, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0667e-01, 1.5292e-02, 1.6873e-02, 8.3874e-01, 1.5502e-02, 6.9280e-03,
        1.1677e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 93, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.0112, 0.9050, 0.0636, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.026

[Epoch: 93, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7779e-03, 9.7642e-01, 5.0791e-03, 4.5965e-03, 5.1802e-09, 4.9002e-03,
        4.2254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 94, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0096, 0.0047, 0.0052, 0.0084, 0.0044, 0.9548, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 94, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5282e-02, 2.7773e-02, 3.7625e-02, 7.0939e-07, 3.6727e-07, 4.5331e-02,
        8.5399e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 94, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.7402e-02, 1.2445e-02, 1.3701e-02, 8.9862e-01, 1.2094e-02, 5.7369e-03,
        6.6280e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 94, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.0118, 0.8980, 0.0689, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 94, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0301e-03, 9.7720e-01, 4.7088e-03, 4.6365e-03, 3.5956e-09, 4.6134e-03,
        4.8145e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.003

[Epoch: 95, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0085, 0.0048, 0.0061, 0.0087, 0.0048, 0.9510, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 95, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5975e-02, 1.8280e-02, 2.0250e-02, 4.2426e-07, 2.3771e-07, 1.3205e-02,
        9.2229e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.036

[Epoch: 95, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.4394e-01, 2.0507e-02, 1.5674e-02, 7.9283e-01, 2.0217e-02, 6.8305e-03,
        7.7145e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 95, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0057, 0.0122, 0.8912, 0.0753, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.020

[Epoch: 95, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7438e-03, 9.7373e-01, 5.6133e-03, 5.0507e-03, 7.1635e-09, 5.6310e-03,
        5.2330e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 96, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0113, 0.0047, 0.0051, 0.0100, 0.0048, 0.9508, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 96, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6984e-02, 2.4108e-02, 2.6686e-02, 1.8968e-07, 2.6684e-07, 3.3095e-02,
        8.8913e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.028

[Epoch: 96, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([4.1132e-02, 1.3893e-02, 1.7199e-02, 9.1123e-01, 1.0351e-02, 6.1970e-03,
        3.2108e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.020

[Epoch: 96, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.0113, 0.9092, 0.0590, 0.0056, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.017

[Epoch: 96, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9229e-03, 9.7582e-01, 5.4634e-03, 4.7563e-03, 4.6366e-09, 4.4120e-03,
        4.6244e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 97, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0044, 0.0047, 0.0081, 0.0050, 0.9503, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 97, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.7572e-02, 2.3211e-02, 3.7118e-02, 1.3798e-06, 3.1738e-07, 2.9630e-02,
        8.7247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 97, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1762e-01, 1.5941e-02, 1.6746e-02, 8.2491e-01, 1.6518e-02, 8.2598e-03,
        3.1874e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.004

[Epoch: 97, batch: 168/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0075, 0.0197, 0.8686, 0.0803, 0.0085, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 97, batch: 210/213] total loss per batch: 0.503
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.7643e-03, 9.7128e-01, 8.4322e-03, 4.0357e-03, 6.9342e-09, 7.1131e-03,
        5.3710e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.007

[Epoch: 98, batch: 42/213] total loss per batch: 0.505
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0164, 0.0049, 0.0046, 0.0279, 0.0226, 0.8806, 0.0430],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.001

[Epoch: 98, batch: 84/213] total loss per batch: 0.501
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([5.4337e-02, 2.2652e-02, 2.2380e-02, 1.8282e-07, 2.0823e-07, 1.7721e-02,
        8.8291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 98, batch: 126/213] total loss per batch: 0.524
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.4664e-02, 3.2861e-02, 1.2319e-02, 8.6312e-01, 5.2516e-03, 2.1782e-02,
        1.6252e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 98, batch: 168/213] total loss per batch: 0.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0071, 0.0096, 0.9003, 0.0687, 0.0066, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.018

[Epoch: 98, batch: 210/213] total loss per batch: 0.521
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6650e-03, 9.7909e-01, 4.7833e-03, 2.7301e-03, 4.5579e-08, 2.2993e-03,
        6.4367e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 99, batch: 42/213] total loss per batch: 0.509
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0036, 0.0040, 0.0048, 0.0057, 0.0030, 0.9690, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 99, batch: 84/213] total loss per batch: 0.487
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.7093e-02, 9.2692e-03, 2.1798e-02, 4.0278e-07, 1.3534e-06, 2.5355e-02,
        9.2648e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.039

[Epoch: 99, batch: 126/213] total loss per batch: 0.502
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.1364e-02, 3.7583e-02, 2.4157e-02, 8.3514e-01, 1.0338e-02, 1.1422e-02,
        1.5522e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.009

[Epoch: 99, batch: 168/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0071, 0.0119, 0.8675, 0.0995, 0.0032, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 99, batch: 210/213] total loss per batch: 0.493
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.6701e-03, 9.7412e-01, 6.4968e-03, 4.6266e-03, 3.5370e-08, 2.5474e-03,
        5.5402e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 100, batch: 42/213] total loss per batch: 0.493
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0073, 0.0039, 0.0081, 0.0104, 0.0052, 0.9587, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 100, batch: 84/213] total loss per batch: 0.476
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0401e-02, 2.1143e-02, 3.4424e-02, 5.8345e-07, 1.0682e-06, 3.7044e-02,
        8.7699e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.042

[Epoch: 100, batch: 126/213] total loss per batch: 0.485
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8731e-02, 2.2411e-02, 1.9477e-02, 8.4689e-01, 1.0408e-02, 1.2082e-02,
        7.4774e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 -0.010

[Epoch: 100, batch: 168/213] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0064, 0.0127, 0.8897, 0.0759, 0.0044, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 100, batch: 210/213] total loss per batch: 0.479
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.9738e-03, 9.7079e-01, 6.6867e-03, 4.2520e-03, 5.6123e-08, 3.9484e-03,
        6.3447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 101, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0083, 0.0042, 0.0110, 0.0095, 0.0042, 0.9537, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 101, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.2354e-02, 2.0114e-02, 2.9542e-02, 6.6590e-07, 7.9461e-07, 3.4506e-02,
        8.9348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.047

[Epoch: 101, batch: 126/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.7842e-02, 2.0949e-02, 1.6051e-02, 8.6447e-01, 9.8627e-03, 1.0827e-02,
        1.0037e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 101, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0059, 0.0123, 0.8915, 0.0750, 0.0047, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 101, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.7075e-03, 9.6975e-01, 7.1193e-03, 4.5552e-03, 4.0675e-08, 4.3655e-03,
        6.4998e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 102, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0089, 0.0045, 0.0099, 0.0089, 0.0043, 0.9534, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 102, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.4317e-02, 2.2406e-02, 3.2996e-02, 5.9915e-07, 7.5591e-07, 3.4343e-02,
        8.8594e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.038

[Epoch: 102, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7217e-02, 1.8909e-02, 1.5159e-02, 8.5887e-01, 9.9305e-03, 9.9129e-03,
        6.6764e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 102, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.0123, 0.8926, 0.0742, 0.0048, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 102, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.5431e-03, 9.7049e-01, 6.7966e-03, 4.5039e-03, 3.3359e-08, 4.2901e-03,
        6.3734e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 103, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0045, 0.0093, 0.0090, 0.0043, 0.9533, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 103, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5078e-02, 2.2183e-02, 3.2335e-02, 5.1394e-07, 6.4102e-07, 3.2949e-02,
        8.8745e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.037

[Epoch: 103, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7376e-02, 1.8120e-02, 1.4579e-02, 8.6045e-01, 1.0319e-02, 9.1536e-03,
        6.2553e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 103, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0122, 0.8935, 0.0733, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 103, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.3181e-03, 9.7112e-01, 6.5653e-03, 4.4478e-03, 2.9289e-08, 4.3677e-03,
        6.1833e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 104, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0094, 0.0045, 0.0087, 0.0091, 0.0044, 0.9533, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 104, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6509e-02, 2.3383e-02, 3.2506e-02, 4.7981e-07, 5.8750e-07, 3.2386e-02,
        8.8521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.036

[Epoch: 104, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7851e-02, 1.7597e-02, 1.3940e-02, 8.6125e-01, 1.0753e-02, 8.6069e-03,
        5.7659e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 104, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0121, 0.8947, 0.0723, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 104, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([7.0874e-03, 9.7172e-01, 6.4142e-03, 4.4057e-03, 2.4898e-08, 4.3484e-03,
        6.0293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 105, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0095, 0.0045, 0.0081, 0.0092, 0.0044, 0.9532, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 105, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6206e-02, 2.2995e-02, 3.1361e-02, 4.0659e-07, 4.9277e-07, 3.1446e-02,
        8.8799e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.035

[Epoch: 105, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8392e-02, 1.7021e-02, 1.3666e-02, 8.6154e-01, 1.1293e-02, 8.0839e-03,
        5.6775e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.001

[Epoch: 105, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0121, 0.8950, 0.0720, 0.0049, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 105, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.9648e-03, 9.7224e-01, 6.2235e-03, 4.3566e-03, 2.1427e-08, 4.3390e-03,
        5.8771e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 106, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0044, 0.0079, 0.0094, 0.0044, 0.9530, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 106, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7220e-02, 2.3842e-02, 3.1200e-02, 3.8348e-07, 4.3692e-07, 3.1851e-02,
        8.8589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.034

[Epoch: 106, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9853e-02, 1.6806e-02, 1.3150e-02, 8.6060e-01, 1.1894e-02, 7.6973e-03,
        5.3493e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 106, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0120, 0.8957, 0.0715, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 106, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.8127e-03, 9.7270e-01, 6.1291e-03, 4.3170e-03, 1.8747e-08, 4.3246e-03,
        5.7143e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 107, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0044, 0.0076, 0.0095, 0.0045, 0.9525, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 107, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7501e-02, 2.3946e-02, 3.0919e-02, 3.3618e-07, 3.8350e-07, 3.1653e-02,
        8.8598e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.033

[Epoch: 107, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9144e-02, 1.6675e-02, 1.3089e-02, 8.6115e-01, 1.2517e-02, 7.4280e-03,
        5.3396e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.002

[Epoch: 107, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0118, 0.8968, 0.0706, 0.0049, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 107, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.6316e-03, 9.7305e-01, 6.0429e-03, 4.3167e-03, 1.6764e-08, 4.3518e-03,
        5.6092e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 108, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0042, 0.0074, 0.0097, 0.0045, 0.9524, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 108, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7996e-02, 2.4104e-02, 3.0504e-02, 3.0668e-07, 3.3924e-07, 3.0965e-02,
        8.8643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.032

[Epoch: 108, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0949e-02, 1.6422e-02, 1.3198e-02, 8.5935e-01, 1.2930e-02, 7.1554e-03,
        4.8323e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 108, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0120, 0.8962, 0.0711, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 108, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.5208e-03, 9.7335e-01, 6.0336e-03, 4.2891e-03, 1.4535e-08, 4.3488e-03,
        5.4603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 109, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0042, 0.0070, 0.0098, 0.0045, 0.9523, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 109, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8421e-02, 2.4675e-02, 3.1046e-02, 2.8416e-07, 3.0831e-07, 3.1766e-02,
        8.8409e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.031

[Epoch: 109, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.6954e-02, 1.6045e-02, 1.2985e-02, 8.6384e-01, 1.3364e-02, 6.8093e-03,
        4.9228e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.003

[Epoch: 109, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.0116, 0.8980, 0.0698, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 109, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.3626e-03, 9.7365e-01, 5.8576e-03, 4.3279e-03, 1.3140e-08, 4.3571e-03,
        5.4496e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 110, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0105, 0.0041, 0.0068, 0.0102, 0.0046, 0.9520, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 110, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8338e-02, 2.4512e-02, 2.9536e-02, 2.4406e-07, 2.5370e-07, 3.0249e-02,
        8.8736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 110, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4578e-02, 1.6547e-02, 1.3672e-02, 8.5428e-01, 1.3993e-02, 6.9309e-03,
        4.0773e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 110, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0121, 0.8972, 0.0699, 0.0050, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 110, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.4182e-03, 9.7372e-01, 5.9123e-03, 4.2240e-03, 1.0701e-08, 4.4554e-03,
        5.2689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 111, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0110, 0.0043, 0.0070, 0.0102, 0.0046, 0.9504, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 111, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9230e-02, 2.5437e-02, 3.1883e-02, 2.5775e-07, 2.5392e-07, 3.0834e-02,
        8.8262e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 111, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.5044e-02, 1.5181e-02, 1.3220e-02, 8.6652e-01, 1.3871e-02, 6.1686e-03,
        5.2201e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.005

[Epoch: 111, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0113, 0.9019, 0.0668, 0.0047, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 111, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.9069e-03, 9.7471e-01, 5.6296e-03, 4.3619e-03, 1.0888e-08, 4.1251e-03,
        5.2703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 112, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0038, 0.0061, 0.0103, 0.0044, 0.9535, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 112, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8971e-02, 2.4153e-02, 2.7939e-02, 2.1775e-07, 2.0043e-07, 3.1002e-02,
        8.8794e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 112, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.2679e-02, 1.6318e-02, 1.4453e-02, 8.5531e-01, 1.4634e-02, 6.6076e-03,
        3.8232e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.006

[Epoch: 112, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.0118, 0.8943, 0.0726, 0.0051, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 112, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.1767e-03, 9.7401e-01, 5.6791e-03, 4.1782e-03, 8.0085e-09, 4.8071e-03,
        5.1477e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 113, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0107, 0.0044, 0.0070, 0.0109, 0.0048, 0.9485, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 113, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8583e-02, 2.3224e-02, 3.0828e-02, 2.4859e-07, 2.2831e-07, 3.0658e-02,
        8.8671e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 113, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6938e-02, 1.5776e-02, 1.4511e-02, 8.5217e-01, 1.4245e-02, 6.3600e-03,
        4.9556e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 113, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0042, 0.0110, 0.9039, 0.0653, 0.0045, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 113, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.3447e-03, 9.7508e-01, 5.5852e-03, 4.6600e-03, 1.2225e-08, 4.0462e-03,
        5.2841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 114, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0093, 0.0036, 0.0054, 0.0098, 0.0044, 0.9567, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 114, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8797e-02, 2.6327e-02, 2.9345e-02, 1.7887e-07, 1.4517e-07, 3.2513e-02,
        8.8302e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 114, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.5176e-02, 1.5002e-02, 1.2409e-02, 8.8796e-01, 1.3449e-02, 6.0071e-03,
        7.4940e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 114, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.0115, 0.8902, 0.0765, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 114, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.0609e-03, 9.7514e-01, 5.0791e-03, 4.2566e-03, 1.0178e-08, 4.4751e-03,
        4.9921e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 115, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0123, 0.0046, 0.0071, 0.0107, 0.0049, 0.9456, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 115, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0627e-02, 2.4456e-02, 2.8985e-02, 3.7744e-07, 3.1849e-07, 3.1221e-02,
        8.8471e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.032

[Epoch: 115, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2627e-01, 1.8369e-02, 1.5247e-02, 8.1612e-01, 1.8075e-02, 5.9254e-03,
        7.5468e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.007

[Epoch: 115, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0107, 0.9041, 0.0658, 0.0047, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 115, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2152e-03, 9.7665e-01, 5.5367e-03, 4.1692e-03, 1.2995e-08, 4.1740e-03,
        5.2562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 116, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0039, 0.0048, 0.0088, 0.0043, 0.9588, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 116, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6317e-02, 2.6271e-02, 2.9420e-02, 4.8353e-07, 3.3255e-07, 2.9823e-02,
        8.8817e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 116, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.6778e-02, 1.5304e-02, 1.6064e-02, 8.9348e-01, 1.1489e-02, 6.8881e-03,
        6.7976e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.004

[Epoch: 116, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.0114, 0.8954, 0.0717, 0.0052, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 116, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.7318e-03, 9.7409e-01, 5.3065e-03, 4.6979e-03, 3.0206e-08, 4.7288e-03,
        5.4476e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 117, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0041, 0.0077, 0.0118, 0.0049, 0.9454, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.006

[Epoch: 117, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7417e-02, 2.0021e-02, 2.7699e-02, 3.7815e-07, 2.6647e-07, 2.7913e-02,
        8.9695e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.029

[Epoch: 117, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0937e-01, 1.7769e-02, 1.4327e-02, 8.3698e-01, 1.5450e-02, 6.1088e-03,
        1.2881e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 117, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0116, 0.8998, 0.0684, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.016

[Epoch: 117, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0098e-03, 9.7821e-01, 4.2819e-03, 4.0345e-03, 1.7478e-08, 4.0213e-03,
        4.4385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 118, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0118, 0.0037, 0.0063, 0.0103, 0.0047, 0.9512, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 118, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7814e-02, 3.0063e-02, 3.1152e-02, 3.5163e-07, 2.8981e-07, 3.1631e-02,
        8.7934e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 118, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.9829e-02, 1.8029e-02, 1.6595e-02, 8.8505e-01, 1.2821e-02, 7.6781e-03,
        1.7803e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 118, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0050, 0.0110, 0.8978, 0.0691, 0.0052, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 118, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.4138e-03, 9.7464e-01, 5.1174e-03, 4.5502e-03, 1.5255e-08, 4.6609e-03,
        5.6200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 119, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0089, 0.0042, 0.0056, 0.0106, 0.0053, 0.9548, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 119, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7099e-02, 2.6127e-02, 2.9741e-02, 3.7529e-07, 1.8471e-07, 2.8898e-02,
        8.8813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 119, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1693e-01, 1.5443e-02, 1.3469e-02, 8.2970e-01, 1.8316e-02, 6.1374e-03,
        1.5269e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 119, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.0107, 0.9022, 0.0674, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 119, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1796e-03, 9.7516e-01, 5.3303e-03, 4.7650e-03, 1.4150e-08, 4.9118e-03,
        4.6505e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 120, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0127, 0.0040, 0.0067, 0.0113, 0.0050, 0.9445, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 120, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0628e-02, 2.3424e-02, 2.8772e-02, 4.0802e-07, 2.7562e-07, 3.0387e-02,
        8.8679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 120, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.5637e-02, 1.4555e-02, 1.4858e-02, 8.7641e-01, 1.2607e-02, 5.9336e-03,
        1.6500e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 120, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0110, 0.9043, 0.0650, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 120, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.3814e-03, 9.7558e-01, 4.8716e-03, 4.8036e-03, 1.5447e-08, 4.6548e-03,
        4.7106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 121, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0039, 0.0050, 0.0099, 0.0050, 0.9548, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 121, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6962e-02, 3.0982e-02, 3.2815e-02, 4.0190e-07, 2.1806e-07, 3.0746e-02,
        8.7849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 121, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0315e-01, 1.5400e-02, 1.3257e-02, 8.4804e-01, 1.4721e-02, 5.4352e-03,
        1.6806e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 121, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.0115, 0.9016, 0.0673, 0.0049, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 121, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8720e-03, 9.7526e-01, 5.4568e-03, 4.4764e-03, 1.9918e-08, 4.6006e-03,
        5.3340e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 122, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0040, 0.0053, 0.0106, 0.0049, 0.9528, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 122, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9750e-02, 1.6828e-02, 2.5115e-02, 3.3571e-07, 1.4212e-07, 2.4266e-02,
        9.0404e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 122, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.9958e-02, 1.3944e-02, 1.5092e-02, 8.8208e-01, 1.3099e-02, 5.8266e-03,
        1.2625e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.007

[Epoch: 122, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0047, 0.0105, 0.9017, 0.0678, 0.0047, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 122, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8561e-03, 9.7844e-01, 4.3175e-03, 4.2118e-03, 8.7341e-09, 4.2451e-03,
        3.9247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 123, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0048, 0.0059, 0.0101, 0.0054, 0.9502, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 123, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4036e-02, 4.2424e-02, 4.0940e-02, 6.5239e-07, 3.4166e-07, 4.6164e-02,
        8.3644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 123, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0956e-01, 1.5908e-02, 1.2404e-02, 8.4352e-01, 1.3142e-02, 5.4722e-03,
        2.3478e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.021

[Epoch: 123, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0056, 0.0117, 0.8910, 0.0741, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 123, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2507e-03, 9.7384e-01, 6.1924e-03, 4.7488e-03, 1.6203e-08, 5.0843e-03,
        4.8795e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 124, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0107, 0.0044, 0.0057, 0.0130, 0.0046, 0.9474, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 124, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.1260e-02, 1.4454e-02, 2.2244e-02, 5.3368e-07, 2.0641e-07, 1.5364e-02,
        9.2668e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 124, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.6777e-02, 1.5643e-02, 1.7353e-02, 8.7757e-01, 1.6947e-02, 5.7133e-03,
        1.1876e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 124, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0041, 0.0103, 0.9157, 0.0567, 0.0043, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 124, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.5295e-03, 9.7581e-01, 4.2113e-03, 4.7093e-03, 2.1987e-08, 4.9257e-03,
        4.8155e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 125, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0111, 0.0041, 0.0058, 0.0089, 0.0057, 0.9495, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 125, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0507e-02, 3.0821e-02, 3.9370e-02, 4.5582e-07, 2.6276e-07, 3.6480e-02,
        8.6282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 125, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0572e-01, 1.4821e-02, 1.2096e-02, 8.4857e-01, 1.3743e-02, 5.0475e-03,
        6.5117e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 125, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0047, 0.0106, 0.9004, 0.0684, 0.0050, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 125, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9689e-03, 9.7350e-01, 6.4121e-03, 4.5421e-03, 1.6967e-08, 5.2015e-03,
        5.3742e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 126, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0084, 0.0043, 0.0055, 0.0117, 0.0042, 0.9543, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 126, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0490e-02, 2.0062e-02, 2.7150e-02, 5.6306e-07, 3.6570e-07, 2.0138e-02,
        9.0216e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.024

[Epoch: 126, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.3660e-02, 1.5599e-02, 1.5849e-02, 8.6504e-01, 1.4175e-02, 5.6769e-03,
        2.5120e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.025

[Epoch: 126, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0042, 0.0102, 0.9129, 0.0586, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 126, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.8285e-03, 9.7227e-01, 5.0199e-03, 6.0571e-03, 2.0682e-08, 5.1565e-03,
        5.6706e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 127, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0114, 0.0041, 0.0049, 0.0087, 0.0050, 0.9530, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 127, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.9356e-02, 3.3758e-02, 4.0702e-02, 7.1760e-07, 4.5234e-07, 5.6254e-02,
        8.2993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 127, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.3531e-02, 1.5525e-02, 1.6688e-02, 8.4794e-01, 2.0056e-02, 6.2546e-03,
        4.5017e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 127, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0041, 0.0090, 0.9108, 0.0621, 0.0047, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.011

[Epoch: 127, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8269e-03, 9.7575e-01, 5.0414e-03, 4.5840e-03, 1.3824e-08, 5.3941e-03,
        4.4031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 128, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0112, 0.0049, 0.0073, 0.0111, 0.0065, 0.9430, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 128, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.2988e-02, 1.3244e-02, 1.8001e-02, 2.2849e-07, 2.7061e-07, 9.7053e-03,
        9.4606e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 128, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.2769e-02, 1.5062e-02, 1.2501e-02, 8.7465e-01, 1.0108e-02, 4.9047e-03,
        1.8363e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.005

[Epoch: 128, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.0122, 0.8968, 0.0695, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 128, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7328e-03, 9.7577e-01, 4.9660e-03, 4.7988e-03, 2.0401e-08, 5.0183e-03,
        4.7162e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 129, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0083, 0.0039, 0.0040, 0.0084, 0.0036, 0.9611, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 129, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.2193e-02, 2.7975e-02, 3.2338e-02, 6.1544e-07, 2.7931e-07, 3.9298e-02,
        8.5819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.024

[Epoch: 129, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.4686e-02, 1.2609e-02, 1.5218e-02, 8.6773e-01, 1.3565e-02, 6.1896e-03,
        9.0495e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.021

[Epoch: 129, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0114, 0.8976, 0.0713, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.015

[Epoch: 129, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9586e-03, 9.7515e-01, 4.7493e-03, 5.2763e-03, 1.9625e-08, 4.8302e-03,
        5.0316e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 130, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0094, 0.0048, 0.0060, 0.0111, 0.0058, 0.9480, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 130, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7678e-02, 2.5059e-02, 3.2316e-02, 3.9135e-07, 4.7993e-07, 2.1171e-02,
        8.9377e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 130, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.9450e-02, 1.9257e-02, 1.4459e-02, 8.4274e-01, 1.8262e-02, 5.8329e-03,
        3.9003e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 130, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0047, 0.0092, 0.9054, 0.0632, 0.0056, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 130, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9564e-03, 9.7375e-01, 6.3727e-03, 5.1714e-03, 1.2226e-08, 5.3498e-03,
        4.3984e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 131, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0046, 0.0056, 0.0110, 0.0047, 0.9489, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 131, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3363e-02, 2.3018e-02, 2.2365e-02, 2.9503e-07, 2.5251e-07, 2.9794e-02,
        8.9146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 131, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.8008e-02, 1.1159e-02, 1.5753e-02, 8.7931e-01, 1.0849e-02, 4.9247e-03,
        5.6177e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 131, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0046, 0.0110, 0.9071, 0.0640, 0.0042, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.012

[Epoch: 131, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.8323e-03, 9.7230e-01, 5.1210e-03, 5.3962e-03, 1.5954e-08, 5.3351e-03,
        6.0110e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 132, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0055, 0.0047, 0.0085, 0.0045, 0.9555, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 132, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5065e-02, 2.6054e-02, 4.1386e-02, 6.1147e-07, 3.8748e-07, 3.3846e-02,
        8.6365e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 132, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1556e-01, 1.8028e-02, 1.3738e-02, 8.3015e-01, 1.6144e-02, 6.3836e-03,
        2.3260e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 132, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0048, 0.0111, 0.8926, 0.0756, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 132, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2354e-03, 9.7582e-01, 5.0556e-03, 4.5059e-03, 3.3868e-08, 4.4415e-03,
        4.9425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 133, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0085, 0.0036, 0.0053, 0.0109, 0.0049, 0.9542, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 133, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7959e-02, 2.1484e-02, 2.7873e-02, 6.6096e-07, 4.9655e-07, 2.6486e-02,
        8.9620e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 133, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.0073e-02, 1.4148e-02, 1.5573e-02, 8.7894e-01, 1.5014e-02, 6.2517e-03,
        1.5275e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 133, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0112, 0.9038, 0.0644, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 133, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.9108e-03, 9.7429e-01, 5.7540e-03, 4.5792e-03, 7.3890e-09, 4.5217e-03,
        4.9408e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.026

[Epoch: 134, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0113, 0.0059, 0.0070, 0.0099, 0.0049, 0.9462, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 134, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6137e-02, 2.0697e-02, 2.6792e-02, 3.9204e-07, 3.5063e-07, 2.6414e-02,
        8.9996e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 134, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6139e-02, 1.7952e-02, 1.5503e-02, 8.4789e-01, 1.7665e-02, 4.8470e-03,
        3.7987e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 134, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0116, 0.9052, 0.0633, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 134, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.6582e-03, 9.7289e-01, 5.6355e-03, 5.4348e-03, 2.4280e-08, 4.9843e-03,
        5.4005e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 135, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0038, 0.0056, 0.0106, 0.0051, 0.9505, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 135, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6767e-02, 2.9129e-02, 2.9950e-02, 4.6911e-07, 2.9584e-07, 3.3865e-02,
        8.8029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 135, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.4074e-02, 1.3698e-02, 1.3412e-02, 8.7285e-01, 1.0228e-02, 5.7426e-03,
        9.5425e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 135, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0057, 0.0119, 0.8817, 0.0824, 0.0060, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 135, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2027e-03, 9.7440e-01, 5.6977e-03, 4.8763e-03, 1.6977e-08, 5.2440e-03,
        4.5837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 136, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0041, 0.0057, 0.0109, 0.0042, 0.9516, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 136, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6725e-02, 2.5699e-02, 3.8557e-02, 5.0663e-07, 2.3351e-07, 2.8773e-02,
        8.7024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 136, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4555e-02, 1.5869e-02, 1.8360e-02, 8.4831e-01, 1.7758e-02, 5.1482e-03,
        1.8782e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 136, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0050, 0.0119, 0.9026, 0.0640, 0.0054, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 136, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.1467e-03, 9.7357e-01, 5.8086e-03, 4.6108e-03, 1.9834e-08, 4.5831e-03,
        5.2820e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 137, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0094, 0.0049, 0.0055, 0.0092, 0.0049, 0.9523, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 137, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1866e-02, 2.7732e-02, 2.7317e-02, 5.4587e-07, 3.5389e-07, 3.0807e-02,
        8.8228e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 137, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.2753e-02, 1.4834e-02, 1.3773e-02, 8.7016e-01, 1.2398e-02, 6.0843e-03,
        2.4660e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 137, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.0100, 0.9119, 0.0599, 0.0044, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.004

[Epoch: 137, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8763e-03, 9.7727e-01, 4.8423e-03, 4.4353e-03, 6.2339e-09, 4.2743e-03,
        4.3010e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 138, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0116, 0.0045, 0.0051, 0.0117, 0.0052, 0.9475, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 138, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8286e-02, 2.3803e-02, 2.5533e-02, 4.0613e-07, 2.0397e-07, 3.0651e-02,
        8.9173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 138, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.3085e-02, 1.0875e-02, 1.4994e-02, 8.7360e-01, 1.2176e-02, 5.2736e-03,
        6.6873e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 138, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0048, 0.0115, 0.8993, 0.0684, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 138, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7126e-03, 9.7416e-01, 5.3706e-03, 4.7947e-03, 2.1421e-08, 5.5064e-03,
        5.4606e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 139, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0096, 0.0043, 0.0062, 0.0094, 0.0048, 0.9510, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 139, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8181e-02, 2.1209e-02, 3.2471e-02, 4.8091e-07, 2.0211e-07, 3.0156e-02,
        8.8798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 139, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.2249e-01, 1.9500e-02, 1.8142e-02, 8.1304e-01, 2.0074e-02, 6.7595e-03,
        8.4806e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.022

[Epoch: 139, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0048, 0.0117, 0.9037, 0.0646, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 139, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0112e-03, 9.7890e-01, 4.4573e-03, 3.7262e-03, 8.7744e-09, 3.8520e-03,
        4.0491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 140, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0112, 0.0046, 0.0056, 0.0129, 0.0056, 0.9459, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 140, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2736e-02, 3.0726e-02, 3.4892e-02, 5.4412e-07, 3.0791e-07, 3.7607e-02,
        8.6404e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 140, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([5.8148e-02, 1.4138e-02, 1.2900e-02, 8.9979e-01, 1.0507e-02, 4.5209e-03,
        1.6911e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 140, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0112, 0.8992, 0.0680, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 140, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.4116e-03, 9.7740e-01, 4.9705e-03, 4.5527e-03, 1.2857e-08, 4.7161e-03,
        3.9458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 141, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0082, 0.0043, 0.0050, 0.0096, 0.0048, 0.9536, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 141, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7525e-02, 1.7162e-02, 2.1853e-02, 4.3946e-07, 2.1036e-07, 2.4848e-02,
        9.0861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 141, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0567e-01, 1.8282e-02, 1.6139e-02, 8.3408e-01, 2.0302e-02, 5.5258e-03,
        4.7038e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.010

[Epoch: 141, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0106, 0.8991, 0.0700, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 141, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.8109e-03, 9.7012e-01, 5.3748e-03, 6.7731e-03, 8.7058e-09, 5.4676e-03,
        6.4504e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 142, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0105, 0.0051, 0.0058, 0.0099, 0.0048, 0.9480, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 142, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1470e-02, 3.9018e-02, 3.7309e-02, 6.6060e-07, 5.0204e-07, 2.5490e-02,
        8.6671e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 142, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.3765e-02, 1.6431e-02, 1.3824e-02, 8.5701e-01, 1.1894e-02, 7.0802e-03,
        3.2679e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.020

[Epoch: 142, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.0110, 0.9012, 0.0676, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 142, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.6783e-03, 9.7470e-01, 5.3468e-03, 4.3716e-03, 2.6780e-08, 5.1470e-03,
        4.7567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.029

[Epoch: 143, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0043, 0.0051, 0.0115, 0.0051, 0.9493, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 143, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5830e-02, 1.3770e-02, 2.2475e-02, 6.7612e-07, 1.9149e-07, 3.2308e-02,
        9.0562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 143, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.5301e-02, 1.6060e-02, 1.6255e-02, 8.7453e-01, 1.2775e-02, 5.0821e-03,
        1.2292e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.028

[Epoch: 143, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0044, 0.0101, 0.9173, 0.0545, 0.0043, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.003

[Epoch: 143, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7824e-03, 9.7380e-01, 4.8870e-03, 5.5713e-03, 8.7339e-09, 5.0709e-03,
        5.8900e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 144, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0084, 0.0043, 0.0046, 0.0091, 0.0041, 0.9583, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 144, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4878e-02, 2.5212e-02, 3.5901e-02, 7.3909e-07, 1.4901e-07, 3.7545e-02,
        8.6646e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 144, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6683e-02, 1.3372e-02, 1.0932e-02, 8.6089e-01, 1.3357e-02, 4.7628e-03,
        3.1373e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 144, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0049, 0.0111, 0.9023, 0.0677, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.003

[Epoch: 144, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2580e-03, 9.7527e-01, 5.4605e-03, 4.0041e-03, 1.7973e-08, 4.9571e-03,
        5.0534e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 145, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0040, 0.0057, 0.0082, 0.0051, 0.9538, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 145, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9628e-02, 2.6249e-02, 3.2208e-02, 7.6147e-07, 2.0600e-07, 2.1285e-02,
        8.9063e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 145, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.3927e-02, 1.5273e-02, 1.4709e-02, 8.7175e-01, 1.8522e-02, 5.8197e-03,
        3.0277e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 145, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0053, 0.0108, 0.8946, 0.0726, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 145, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.9863e-03, 9.7600e-01, 5.1032e-03, 4.7950e-03, 1.0744e-08, 4.8896e-03,
        5.2226e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 146, batch: 42/213] total loss per batch: 0.483
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0096, 0.0042, 0.0064, 0.0097, 0.0042, 0.9489, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 146, batch: 84/213] total loss per batch: 0.470
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6681e-02, 2.7278e-02, 2.9870e-02, 4.1441e-07, 2.7925e-07, 3.7324e-02,
        8.7885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 146, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.3419e-01, 1.4642e-02, 1.0499e-02, 8.2372e-01, 1.2195e-02, 4.7555e-03,
        1.6564e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.003

[Epoch: 146, batch: 168/213] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0034, 0.0105, 0.9109, 0.0609, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 146, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.9173e-03, 9.7659e-01, 5.0413e-03, 5.2334e-03, 1.5946e-08, 4.4925e-03,
        4.7228e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.010

[Epoch: 147, batch: 42/213] total loss per batch: 0.488
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0136, 0.0041, 0.0060, 0.0116, 0.0072, 0.9446, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 147, batch: 84/213] total loss per batch: 0.478
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4790e-02, 2.5818e-02, 3.2184e-02, 2.3382e-06, 9.0969e-08, 3.8870e-02,
        8.6834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.039

[Epoch: 147, batch: 126/213] total loss per batch: 0.486
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([3.8624e-02, 1.4179e-02, 1.7540e-02, 9.0117e-01, 2.1773e-02, 6.7162e-03,
        6.0151e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 147, batch: 168/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0040, 0.0097, 0.8991, 0.0725, 0.0049, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 -0.000

[Epoch: 147, batch: 210/213] total loss per batch: 0.483
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.8164e-03, 9.7481e-01, 4.8000e-03, 4.7407e-03, 1.4835e-08, 3.8880e-03,
        4.9452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 148, batch: 42/213] total loss per batch: 0.493
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0088, 0.0043, 0.0048, 0.0092, 0.0042, 0.9564, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 148, batch: 84/213] total loss per batch: 0.485
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([4.0387e-02, 1.5078e-02, 2.6034e-02, 8.7413e-07, 3.9646e-08, 1.7559e-02,
        9.0094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.032

[Epoch: 148, batch: 126/213] total loss per batch: 0.488
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.4151e-01, 2.0904e-02, 1.3585e-02, 8.0253e-01, 1.5267e-02, 6.1946e-03,
        9.1200e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.009

[Epoch: 148, batch: 168/213] total loss per batch: 0.479
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0053, 0.0116, 0.9036, 0.0660, 0.0049, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.000

[Epoch: 148, batch: 210/213] total loss per batch: 0.480
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([6.0259e-03, 9.7883e-01, 5.2515e-03, 3.4054e-03, 1.1892e-08, 2.8299e-03,
        3.6581e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 149, batch: 42/213] total loss per batch: 0.486
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0093, 0.0067, 0.0061, 0.0141, 0.0040, 0.9479, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 149, batch: 84/213] total loss per batch: 0.477
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4116e-02, 2.4949e-02, 3.3953e-02, 2.7706e-07, 2.6670e-08, 2.8587e-02,
        8.7839e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 149, batch: 126/213] total loss per batch: 0.480
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.3414e-02, 1.6473e-02, 1.4264e-02, 8.8322e-01, 1.6383e-02, 6.2415e-03,
        7.0901e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 149, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0052, 0.0113, 0.9005, 0.0689, 0.0048, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 149, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7557e-03, 9.7777e-01, 4.8198e-03, 4.7799e-03, 2.2597e-08, 3.3831e-03,
        4.4930e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.028

[Epoch: 150, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0093, 0.0065, 0.0101, 0.0132, 0.0038, 0.9455, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.025

[Epoch: 150, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.5924e-02, 2.9466e-02, 3.5938e-02, 1.0404e-06, 5.6676e-08, 2.3459e-02,
        8.7521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 150, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.9561e-02, 1.3495e-02, 1.2807e-02, 8.8461e-01, 1.3873e-02, 5.6564e-03,
        6.6853e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 150, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0116, 0.9058, 0.0630, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 150, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7852e-03, 9.7764e-01, 4.3724e-03, 5.3435e-03, 1.9565e-08, 3.3932e-03,
        4.4646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 151, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0089, 0.0059, 0.0076, 0.0121, 0.0042, 0.9486, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 151, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4254e-02, 2.1248e-02, 2.8548e-02, 8.0575e-07, 4.2266e-08, 3.1318e-02,
        8.8463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 151, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7922e-02, 1.5002e-02, 1.3724e-02, 8.6321e-01, 1.4462e-02, 5.6786e-03,
        8.1769e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 151, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.0113, 0.9058, 0.0632, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 151, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8968e-03, 9.7657e-01, 4.8323e-03, 5.4790e-03, 1.8463e-08, 3.5853e-03,
        4.6372e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 152, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0057, 0.0070, 0.0121, 0.0043, 0.9483, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 152, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3942e-02, 2.3464e-02, 3.0174e-02, 7.7083e-07, 4.0702e-08, 3.2176e-02,
        8.8024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 152, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8097e-02, 1.5119e-02, 1.3943e-02, 8.6254e-01, 1.4808e-02, 5.4891e-03,
        7.3590e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 152, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0114, 0.9052, 0.0636, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 152, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0232e-03, 9.7624e-01, 4.8615e-03, 5.5049e-03, 1.5538e-08, 3.6538e-03,
        4.7160e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 153, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0090, 0.0055, 0.0066, 0.0120, 0.0043, 0.9488, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 153, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3373e-02, 2.4378e-02, 3.0811e-02, 7.3973e-07, 3.8995e-08, 3.1983e-02,
        8.7945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 153, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9107e-02, 1.5270e-02, 1.4267e-02, 8.6102e-01, 1.4881e-02, 5.4597e-03,
        6.9839e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 153, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0113, 0.9050, 0.0638, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 153, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0478e-03, 9.7603e-01, 4.8873e-03, 5.5364e-03, 1.3847e-08, 3.7464e-03,
        4.7560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 154, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0090, 0.0055, 0.0064, 0.0121, 0.0043, 0.9484, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 154, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2037e-02, 2.4198e-02, 2.9887e-02, 6.7926e-07, 3.5566e-08, 3.1666e-02,
        8.8221e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 154, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8839e-02, 1.5260e-02, 1.4507e-02, 8.6109e-01, 1.4944e-02, 5.3574e-03,
        6.6008e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 154, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.0113, 0.9050, 0.0639, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 154, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0797e-03, 9.7596e-01, 4.8846e-03, 5.4729e-03, 1.1936e-08, 3.8088e-03,
        4.7932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 155, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0090, 0.0053, 0.0061, 0.0120, 0.0043, 0.9489, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 155, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1298e-02, 2.4427e-02, 2.9920e-02, 6.3446e-07, 3.2392e-08, 3.0939e-02,
        8.8342e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 155, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9411e-02, 1.5346e-02, 1.4648e-02, 8.6032e-01, 1.4973e-02, 5.3036e-03,
        6.1079e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 155, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0112, 0.9049, 0.0641, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 155, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0584e-03, 9.7588e-01, 4.9044e-03, 5.4519e-03, 1.0406e-08, 3.8942e-03,
        4.8087e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 156, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0053, 0.0060, 0.0120, 0.0044, 0.9484, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 156, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0970e-02, 2.4623e-02, 2.9543e-02, 5.8252e-07, 2.9340e-08, 3.0387e-02,
        8.8448e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 156, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9545e-02, 1.5253e-02, 1.4762e-02, 8.6023e-01, 1.4963e-02, 5.2435e-03,
        5.6628e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 156, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0112, 0.9048, 0.0642, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 156, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0799e-03, 9.7585e-01, 4.9195e-03, 5.3593e-03, 9.1910e-09, 3.9358e-03,
        4.8525e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 157, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0052, 0.0057, 0.0117, 0.0044, 0.9493, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 157, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0489e-02, 2.4669e-02, 3.0121e-02, 5.4597e-07, 2.6354e-08, 3.0282e-02,
        8.8444e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 157, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9725e-02, 1.5325e-02, 1.4863e-02, 8.5988e-01, 1.4992e-02, 5.2184e-03,
        5.3808e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 157, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0111, 0.9047, 0.0643, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 157, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0483e-03, 9.7583e-01, 4.9562e-03, 5.3063e-03, 8.1909e-09, 4.0222e-03,
        4.8361e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 158, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0093, 0.0052, 0.0058, 0.0117, 0.0044, 0.9485, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 158, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0464e-02, 2.5045e-02, 3.0193e-02, 5.0927e-07, 2.4074e-08, 3.0501e-02,
        8.8380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 158, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9471e-02, 1.5155e-02, 1.4806e-02, 8.6049e-01, 1.4915e-02, 5.1595e-03,
        5.1361e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 158, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0111, 0.9048, 0.0642, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 158, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0731e-03, 9.7577e-01, 4.9153e-03, 5.2577e-03, 7.0152e-09, 4.0263e-03,
        4.9580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 159, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0093, 0.0051, 0.0054, 0.0114, 0.0044, 0.9499, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 159, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9970e-02, 2.4358e-02, 2.9595e-02, 4.4695e-07, 2.0754e-08, 2.9818e-02,
        8.8626e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 159, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0532e-02, 1.5324e-02, 1.4995e-02, 8.5888e-01, 1.5111e-02, 5.1541e-03,
        4.8165e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 159, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0110, 0.9047, 0.0644, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 159, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0858e-03, 9.7586e-01, 4.9806e-03, 5.1492e-03, 6.2128e-09, 4.1341e-03,
        4.7953e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 160, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0094, 0.0052, 0.0058, 0.0117, 0.0045, 0.9481, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 160, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0441e-02, 2.5803e-02, 3.0783e-02, 4.4384e-07, 1.9770e-08, 3.0552e-02,
        8.8242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 160, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9730e-02, 1.5154e-02, 1.4886e-02, 8.6032e-01, 1.4768e-02, 5.1380e-03,
        4.5273e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 160, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0110, 0.9052, 0.0640, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 160, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1461e-03, 9.7545e-01, 4.8728e-03, 5.2636e-03, 5.7448e-09, 4.1785e-03,
        5.0907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 161, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0095, 0.0050, 0.0052, 0.0111, 0.0045, 0.9502, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 161, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0336e-02, 2.4378e-02, 2.9875e-02, 3.8290e-07, 1.6922e-08, 3.1074e-02,
        8.8434e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 161, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9451e-02, 1.4977e-02, 1.4968e-02, 8.6033e-01, 1.5227e-02, 5.0460e-03,
        4.2632e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 161, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.0110, 0.9044, 0.0650, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 161, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0151e-03, 9.7658e-01, 4.8609e-03, 4.8669e-03, 4.2059e-09, 4.0800e-03,
        4.5944e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 162, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0092, 0.0050, 0.0057, 0.0114, 0.0045, 0.9488, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 162, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8226e-02, 2.3856e-02, 2.7334e-02, 3.3345e-07, 1.3256e-08, 2.6002e-02,
        8.9458e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 162, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.1271e-02, 1.5407e-02, 1.4959e-02, 8.5886e-01, 1.4349e-02, 5.1591e-03,
        4.3627e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 162, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0108, 0.9073, 0.0621, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 162, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2160e-03, 9.7476e-01, 5.0365e-03, 5.3147e-03, 5.7483e-09, 4.4357e-03,
        5.2356e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 163, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0097, 0.0049, 0.0050, 0.0104, 0.0045, 0.9513, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 163, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1805e-02, 2.8978e-02, 3.4257e-02, 4.2824e-07, 1.4446e-08, 3.5370e-02,
        8.6959e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 163, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8842e-02, 1.4743e-02, 1.4452e-02, 8.6164e-01, 1.5360e-02, 4.9644e-03,
        3.9110e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 163, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0112, 0.9003, 0.0685, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 163, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0822e-03, 9.7642e-01, 5.0089e-03, 4.6769e-03, 3.3487e-09, 4.0920e-03,
        4.7178e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 164, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0095, 0.0050, 0.0056, 0.0115, 0.0044, 0.9488, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 164, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9591e-02, 1.8931e-02, 2.6644e-02, 3.1525e-07, 1.2194e-08, 2.7200e-02,
        8.9763e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 164, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7654e-02, 1.5513e-02, 1.5710e-02, 8.6207e-01, 1.3834e-02, 5.2156e-03,
        7.9236e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.023

[Epoch: 164, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0045, 0.0100, 0.9102, 0.0610, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 164, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2264e-03, 9.7471e-01, 4.8526e-03, 5.5328e-03, 5.1037e-09, 4.6385e-03,
        5.0364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.024

[Epoch: 165, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0051, 0.0056, 0.0111, 0.0053, 0.9462, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 165, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1445e-02, 3.5117e-02, 3.6213e-02, 5.4402e-07, 1.6723e-08, 3.4922e-02,
        8.6230e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 165, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.9964e-02, 1.4107e-02, 1.3887e-02, 8.5029e-01, 1.6281e-02, 5.4676e-03,
        4.3493e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 165, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0110, 0.9048, 0.0638, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.006

[Epoch: 165, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5446e-03, 9.7761e-01, 4.8494e-03, 4.2170e-03, 2.5655e-09, 4.0339e-03,
        4.7474e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 166, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0089, 0.0042, 0.0047, 0.0093, 0.0040, 0.9561, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 166, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8651e-02, 2.0085e-02, 2.4375e-02, 3.3823e-07, 2.2353e-08, 2.4889e-02,
        9.0200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 166, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.5902e-02, 1.5898e-02, 1.5452e-02, 8.7276e-01, 1.4973e-02, 5.0143e-03,
        6.1123e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 166, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0112, 0.9017, 0.0673, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 166, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1690e-03, 9.7299e-01, 5.7070e-03, 5.6998e-03, 6.4305e-09, 5.0307e-03,
        5.4023e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.025

[Epoch: 167, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0104, 0.0054, 0.0054, 0.0108, 0.0049, 0.9470, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 167, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7412e-02, 2.8096e-02, 3.3908e-02, 4.0775e-07, 1.2711e-08, 3.6612e-02,
        8.7397e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 167, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.1969e-02, 1.4721e-02, 1.4792e-02, 8.5868e-01, 1.4520e-02, 5.3212e-03,
        3.9984e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 167, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.0112, 0.9030, 0.0661, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 167, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1951e-03, 9.7545e-01, 4.8668e-03, 4.6312e-03, 3.8790e-09, 5.0050e-03,
        4.8506e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 168, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0097, 0.0051, 0.0055, 0.0100, 0.0045, 0.9495, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 168, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2649e-02, 3.0555e-02, 2.9455e-02, 3.7831e-07, 2.0751e-08, 2.1779e-02,
        8.8556e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.030

[Epoch: 168, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0041e-01, 1.4745e-02, 1.2871e-02, 8.5223e-01, 1.4715e-02, 5.0332e-03,
        6.2945e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 168, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0109, 0.9026, 0.0657, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 168, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.2866e-03, 9.7640e-01, 5.1837e-03, 4.4293e-03, 4.1909e-09, 4.4980e-03,
        5.1982e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 169, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0047, 0.0052, 0.0100, 0.0044, 0.9517, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 169, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1322e-02, 2.4164e-02, 3.1307e-02, 4.5947e-07, 2.5025e-08, 3.7886e-02,
        8.7532e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.005

[Epoch: 169, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.1017e-02, 1.4947e-02, 1.5722e-02, 8.7759e-01, 1.5748e-02, 4.9753e-03,
        4.2580e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 169, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0105, 0.9037, 0.0662, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 169, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0355e-03, 9.7459e-01, 5.3099e-03, 5.2571e-03, 4.6802e-09, 4.8996e-03,
        4.9057e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 170, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0084, 0.0053, 0.0047, 0.0092, 0.0047, 0.9548, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 170, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7814e-02, 2.4117e-02, 2.8256e-02, 3.5440e-07, 1.7554e-08, 2.5268e-02,
        8.9455e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 170, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0628e-01, 1.4875e-02, 1.3718e-02, 8.4644e-01, 1.3631e-02, 5.0603e-03,
        3.7992e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 170, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0107, 0.9033, 0.0661, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 170, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.0428e-03, 9.7529e-01, 4.7545e-03, 4.5768e-03, 4.3879e-09, 5.1964e-03,
        5.1355e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.022

[Epoch: 171, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0106, 0.0052, 0.0061, 0.0099, 0.0048, 0.9475, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 171, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9229e-02, 2.7061e-02, 3.1072e-02, 4.0697e-07, 2.3392e-08, 3.0423e-02,
        8.8222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 171, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.8010e-02, 1.6474e-02, 1.8408e-02, 8.6409e-01, 1.7430e-02, 5.5844e-03,
        5.3427e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 171, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0101, 0.9025, 0.0675, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 171, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5923e-03, 9.7518e-01, 5.5072e-03, 5.0524e-03, 3.9513e-09, 4.3474e-03,
        5.3237e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 172, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0055, 0.0054, 0.0123, 0.0053, 0.9461, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 172, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0302e-02, 2.6754e-02, 3.0046e-02, 4.0865e-07, 1.9276e-08, 3.4646e-02,
        8.7825e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 172, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.5888e-02, 1.3717e-02, 1.3493e-02, 8.5851e-01, 1.3039e-02, 5.3525e-03,
        7.0749e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 172, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.0114, 0.8976, 0.0697, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 172, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.4350e-03, 9.7455e-01, 5.0157e-03, 4.7447e-03, 6.1054e-09, 5.2943e-03,
        4.9563e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.023

[Epoch: 173, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0089, 0.0043, 0.0046, 0.0080, 0.0042, 0.9568, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 173, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8044e-02, 2.0711e-02, 2.5748e-02, 4.0165e-07, 2.2016e-08, 2.3440e-02,
        9.0206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 173, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.6080e-02, 1.5186e-02, 1.5520e-02, 8.7360e-01, 1.5200e-02, 4.4142e-03,
        3.6945e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 173, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0099, 0.9105, 0.0608, 0.0046, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 173, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.5784e-03, 9.7593e-01, 5.1074e-03, 4.9432e-03, 3.4700e-09, 4.4593e-03,
        4.9815e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 174, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0059, 0.0064, 0.0122, 0.0051, 0.9442, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 174, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.0453e-02, 2.9988e-02, 3.5446e-02, 3.9561e-07, 2.8761e-08, 3.3051e-02,
        8.7106e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 174, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.1631e-01, 1.3672e-02, 1.4183e-02, 8.3725e-01, 1.3554e-02, 5.0300e-03,
        4.7633e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.021

[Epoch: 174, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0048, 0.0109, 0.9055, 0.0629, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 174, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8981e-03, 9.7298e-01, 5.2120e-03, 5.7927e-03, 7.9507e-09, 5.4210e-03,
        5.6987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 175, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0110, 0.0049, 0.0053, 0.0095, 0.0049, 0.9494, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 175, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.1737e-02, 2.1976e-02, 2.7420e-02, 6.9926e-07, 2.1333e-08, 3.4433e-02,
        8.8443e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 175, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([6.3308e-02, 1.4033e-02, 1.6454e-02, 8.8683e-01, 1.3106e-02, 6.2719e-03,
        1.4305e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 175, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0042, 0.0096, 0.9131, 0.0605, 0.0043, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 175, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8740e-03, 9.7668e-01, 4.5276e-03, 4.7835e-03, 5.1366e-09, 4.2988e-03,
        4.8325e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 176, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0095, 0.0047, 0.0052, 0.0101, 0.0050, 0.9505, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 176, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7417e-02, 2.3178e-02, 2.6534e-02, 4.0191e-07, 3.0231e-08, 1.9318e-02,
        9.0355e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 176, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.7783e-02, 1.3663e-02, 1.4022e-02, 8.5267e-01, 1.7732e-02, 4.1342e-03,
        3.5991e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 176, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0111, 0.9033, 0.0661, 0.0048, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 176, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.0199e-03, 9.7678e-01, 4.5816e-03, 4.8828e-03, 3.8739e-09, 4.5335e-03,
        5.2065e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 177, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0108, 0.0051, 0.0054, 0.0111, 0.0048, 0.9486, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 177, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4236e-02, 3.1919e-02, 4.0391e-02, 5.9395e-07, 3.0123e-08, 4.1585e-02,
        8.5187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 177, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0116e-01, 1.5054e-02, 1.4754e-02, 8.4744e-01, 1.5464e-02, 6.1313e-03,
        2.5909e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 177, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0051, 0.0125, 0.8998, 0.0658, 0.0054, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 177, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.6734e-03, 9.7753e-01, 4.2376e-03, 4.7108e-03, 5.1394e-09, 4.3031e-03,
        4.5438e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 178, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0051, 0.0054, 0.0099, 0.0047, 0.9494, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 178, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.4574e-02, 2.0152e-02, 2.5219e-02, 4.1681e-07, 2.8680e-08, 2.0541e-02,
        9.0951e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 178, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.3083e-02, 1.3950e-02, 1.6237e-02, 8.7507e-01, 1.6303e-02, 5.3525e-03,
        4.8734e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.022

[Epoch: 178, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0046, 0.0096, 0.9090, 0.0624, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 178, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.4630e-03, 9.7600e-01, 4.6940e-03, 5.1637e-03, 4.3644e-09, 4.4111e-03,
        5.2711e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 179, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0050, 0.0056, 0.0095, 0.0052, 0.9505, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 179, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.2790e-02, 3.1016e-02, 3.5390e-02, 7.1879e-07, 3.4888e-08, 3.3172e-02,
        8.6763e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 179, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0983e-01, 1.6541e-02, 1.5586e-02, 8.3760e-01, 1.4515e-02, 5.9269e-03,
        2.7494e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 179, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0050, 0.0117, 0.8956, 0.0716, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 179, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.7216e-03, 9.7445e-01, 4.6069e-03, 5.2574e-03, 3.8547e-09, 5.3413e-03,
        5.6259e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 180, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0115, 0.0047, 0.0052, 0.0116, 0.0048, 0.9457, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 180, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7965e-02, 2.1848e-02, 2.7452e-02, 4.2803e-07, 2.2955e-08, 2.9962e-02,
        8.9277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.026

[Epoch: 180, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.8900e-02, 1.5764e-02, 1.6663e-02, 8.6582e-01, 1.6758e-02, 6.0943e-03,
        3.0314e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 180, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0059, 0.0129, 0.8940, 0.0694, 0.0059, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 180, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.5180e-03, 9.7368e-01, 5.2613e-03, 5.6990e-03, 5.8569e-09, 4.6149e-03,
        5.2250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 181, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0091, 0.0052, 0.0047, 0.0080, 0.0049, 0.9537, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 181, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.4340e-02, 2.5339e-02, 3.2121e-02, 5.5523e-07, 2.9698e-08, 2.8898e-02,
        8.7930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 181, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.7056e-02, 1.4368e-02, 1.5669e-02, 8.6317e-01, 1.4044e-02, 5.6958e-03,
        5.9837e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 181, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0110, 0.9040, 0.0645, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.003

[Epoch: 181, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.1811e-03, 9.7706e-01, 3.9481e-03, 4.6589e-03, 3.5705e-09, 4.9893e-03,
        5.1594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 182, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0038, 0.0051, 0.0097, 0.0046, 0.9521, 0.0148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 182, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.5883e-02, 2.3666e-02, 2.8597e-02, 3.2645e-07, 2.7570e-08, 3.2601e-02,
        8.8925e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 182, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.1108e-02, 1.5399e-02, 1.4302e-02, 8.6032e-01, 1.3973e-02, 4.8935e-03,
        2.1274e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.018

[Epoch: 182, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0050, 0.0109, 0.9012, 0.0681, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.007

[Epoch: 182, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9009e-03, 9.7465e-01, 5.2526e-03, 4.9517e-03, 5.1398e-09, 5.1384e-03,
        5.1051e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 183, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0062, 0.0058, 0.0108, 0.0051, 0.9471, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 183, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.8183e-02, 3.0096e-02, 3.3909e-02, 8.5065e-07, 5.2560e-08, 2.6373e-02,
        8.7144e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.023

[Epoch: 183, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6599e-02, 1.5013e-02, 1.5111e-02, 8.5277e-01, 1.5219e-02, 5.2888e-03,
        3.3009e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 183, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0055, 0.0112, 0.9001, 0.0664, 0.0057, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 183, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.5723e-03, 9.7152e-01, 5.5095e-03, 6.2390e-03, 5.7721e-09, 5.2298e-03,
        5.9308e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 184, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0107, 0.0052, 0.0058, 0.0104, 0.0050, 0.9465, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 184, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([1.8746e-02, 1.4510e-02, 2.0900e-02, 1.9719e-07, 1.6953e-08, 2.4524e-02,
        9.2132e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 184, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([7.7479e-02, 1.7422e-02, 1.6339e-02, 8.6694e-01, 1.6332e-02, 5.4842e-03,
        4.0990e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 184, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.0108, 0.9072, 0.0634, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 184, batch: 210/213] total loss per batch: 0.475
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.8032e-03, 9.8125e-01, 3.9382e-03, 3.4456e-03, 5.4355e-09, 3.7170e-03,
        3.8485e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 185, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0090, 0.0037, 0.0044, 0.0086, 0.0052, 0.9551, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 185, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6817e-02, 4.4203e-02, 4.6712e-02, 9.0994e-07, 6.0036e-08, 4.5325e-02,
        8.2694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 185, batch: 126/213] total loss per batch: 0.477
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.3398e-02, 1.2001e-02, 1.5893e-02, 8.5983e-01, 1.3867e-02, 5.0074e-03,
        3.1903e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 185, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0054, 0.0103, 0.8994, 0.0686, 0.0054, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 185, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([3.9676e-03, 9.7584e-01, 4.7466e-03, 5.0314e-03, 4.9922e-09, 5.2936e-03,
        5.1217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.026

[Epoch: 186, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0107, 0.0059, 0.0052, 0.0119, 0.0053, 0.9446, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 186, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.3247e-02, 2.0289e-02, 2.1824e-02, 4.2822e-07, 5.1747e-08, 1.8833e-02,
        9.1581e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 186, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.6651e-02, 1.8651e-02, 1.4948e-02, 8.4941e-01, 1.5350e-02, 4.9872e-03,
        3.5430e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 186, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0113, 0.9038, 0.0654, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.005

[Epoch: 186, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.7596e-03, 9.7061e-01, 5.3281e-03, 6.6530e-03, 4.9249e-09, 5.3265e-03,
        6.3278e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.021

[Epoch: 187, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0052, 0.0066, 0.0100, 0.0050, 0.9445, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 187, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6561e-02, 2.8570e-02, 2.7805e-02, 2.5431e-07, 5.1725e-08, 2.7389e-02,
        8.8967e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 187, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.3388e-02, 1.2424e-02, 1.4881e-02, 8.7062e-01, 1.2597e-02, 6.0904e-03,
        3.4786e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 187, batch: 168/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0096, 0.9171, 0.0521, 0.0056, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.004

[Epoch: 187, batch: 210/213] total loss per batch: 0.481
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.9596e-03, 9.7600e-01, 5.9941e-03, 3.4480e-03, 1.1293e-09, 4.0541e-03,
        4.5465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.004

[Epoch: 188, batch: 42/213] total loss per batch: 0.488
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0088, 0.0032, 0.0081, 0.0124, 0.0054, 0.9547, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 188, batch: 84/213] total loss per batch: 0.475
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.3360e-02, 2.1795e-02, 3.4593e-02, 2.2009e-07, 1.9671e-08, 2.6891e-02,
        8.8336e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.045

[Epoch: 188, batch: 126/213] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([1.0121e-01, 1.9931e-02, 9.2259e-03, 8.4453e-01, 1.7508e-02, 7.5972e-03,
        4.4209e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.021

[Epoch: 188, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0107, 0.8963, 0.0735, 0.0054, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 188, batch: 210/213] total loss per batch: 0.482
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.8868e-03, 9.7255e-01, 7.8321e-03, 4.5428e-03, 7.5132e-09, 5.1648e-03,
        5.0259e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.009

[Epoch: 189, batch: 42/213] total loss per batch: 0.484
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0087, 0.0034, 0.0053, 0.0097, 0.0043, 0.9567, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.008

[Epoch: 189, batch: 84/213] total loss per batch: 0.471
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([3.6974e-02, 3.1258e-02, 3.9337e-02, 1.0077e-06, 8.2548e-08, 3.7564e-02,
        8.5487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 189, batch: 126/213] total loss per batch: 0.478
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.2022e-02, 1.6077e-02, 1.4340e-02, 8.6892e-01, 1.2974e-02, 5.6680e-03,
        2.0510e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.008

[Epoch: 189, batch: 168/213] total loss per batch: 0.473
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0045, 0.0106, 0.9027, 0.0673, 0.0049, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.013

[Epoch: 189, batch: 210/213] total loss per batch: 0.476
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1411e-03, 9.7339e-01, 6.7030e-03, 4.7373e-03, 7.7286e-09, 5.0176e-03,
        5.0078e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 190, batch: 42/213] total loss per batch: 0.482
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0042, 0.0053, 0.0117, 0.0045, 0.9511, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.011

[Epoch: 190, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.6522e-02, 2.3473e-02, 2.8347e-02, 3.2799e-07, 8.2908e-08, 3.0577e-02,
        8.9108e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.025

[Epoch: 190, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.1921e-02, 1.7771e-02, 1.3656e-02, 8.5204e-01, 1.8291e-02, 6.3252e-03,
        2.8093e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.011

[Epoch: 190, batch: 168/213] total loss per batch: 0.471
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0045, 0.0098, 0.9066, 0.0651, 0.0047, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.010

[Epoch: 190, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.8256e-03, 9.7173e-01, 6.8532e-03, 4.9314e-03, 7.6407e-09, 5.1892e-03,
        5.4755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.012

[Epoch: 191, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0103, 0.0044, 0.0055, 0.0113, 0.0046, 0.9502, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 191, batch: 84/213] total loss per batch: 0.469
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.7967e-02, 2.3206e-02, 2.9722e-02, 3.3776e-07, 6.5994e-08, 2.9173e-02,
        8.8993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 191, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0182e-02, 1.7420e-02, 1.3991e-02, 8.5518e-01, 1.6836e-02, 6.3929e-03,
        2.4670e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 191, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0099, 0.9095, 0.0618, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.009

[Epoch: 191, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.8595e-03, 9.7103e-01, 6.8162e-03, 5.2933e-03, 6.7608e-09, 5.3333e-03,
        5.6686e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 192, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0044, 0.0053, 0.0108, 0.0045, 0.9508, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 192, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.8489e-02, 2.3618e-02, 3.0078e-02, 3.2838e-07, 6.5980e-08, 2.9201e-02,
        8.8861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 192, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8197e-02, 1.6697e-02, 1.3820e-02, 8.5920e-01, 1.5830e-02, 6.2525e-03,
        2.2558e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.012

[Epoch: 192, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0100, 0.9084, 0.0627, 0.0047, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 192, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.7292e-03, 9.7142e-01, 6.6666e-03, 5.2999e-03, 5.9833e-09, 5.3336e-03,
        5.5521e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 193, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0102, 0.0044, 0.0052, 0.0104, 0.0046, 0.9510, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 193, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9250e-02, 2.4262e-02, 3.0651e-02, 3.0533e-07, 5.8454e-08, 2.9605e-02,
        8.8623e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.022

[Epoch: 193, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9142e-02, 1.6328e-02, 1.4141e-02, 8.5886e-01, 1.5372e-02, 6.1597e-03,
        2.0861e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.013

[Epoch: 193, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0100, 0.9076, 0.0634, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 193, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.5901e-03, 9.7180e-01, 6.5437e-03, 5.2886e-03, 5.3519e-09, 5.3187e-03,
        5.4538e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.013

[Epoch: 194, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0101, 0.0045, 0.0051, 0.0103, 0.0046, 0.9512, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 194, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9440e-02, 2.4481e-02, 3.0633e-02, 2.8653e-07, 5.4769e-08, 2.9623e-02,
        8.8582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 194, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9193e-02, 1.6115e-02, 1.4299e-02, 8.5917e-01, 1.5146e-02, 6.0737e-03,
        1.9492e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.014

[Epoch: 194, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0100, 0.9070, 0.0639, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 194, batch: 210/213] total loss per batch: 0.474
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.4693e-03, 9.7226e-01, 6.3820e-03, 5.2210e-03, 4.7187e-09, 5.3231e-03,
        5.3456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 195, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0045, 0.0050, 0.0102, 0.0047, 0.9512, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.017

[Epoch: 195, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9736e-02, 2.4658e-02, 3.0620e-02, 2.6972e-07, 5.0330e-08, 2.9835e-02,
        8.8515e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.021

[Epoch: 195, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.9769e-02, 1.5815e-02, 1.4462e-02, 8.5910e-01, 1.4884e-02, 5.9681e-03,
        1.8220e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.015

[Epoch: 195, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0101, 0.9064, 0.0643, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 195, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.3593e-03, 9.7263e-01, 6.2261e-03, 5.2039e-03, 4.1951e-09, 5.2909e-03,
        5.2917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.014

[Epoch: 196, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0100, 0.0046, 0.0049, 0.0102, 0.0047, 0.9511, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.018

[Epoch: 196, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9826e-02, 2.4794e-02, 3.0492e-02, 2.4519e-07, 4.5838e-08, 2.9866e-02,
        8.8502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 196, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.0468e-02, 1.5668e-02, 1.4686e-02, 8.5850e-01, 1.4800e-02, 5.8786e-03,
        1.7005e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.016

[Epoch: 196, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0101, 0.9063, 0.0644, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 196, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2909e-03, 9.7292e-01, 6.0811e-03, 5.1397e-03, 3.7125e-09, 5.3235e-03,
        5.2431e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 197, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0046, 0.0049, 0.0101, 0.0048, 0.9512, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 197, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9807e-02, 2.4754e-02, 3.0413e-02, 2.3199e-07, 4.2871e-08, 2.9828e-02,
        8.8520e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 197, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.8488e-02, 1.5320e-02, 1.4691e-02, 8.6114e-01, 1.4597e-02, 5.7662e-03,
        1.5686e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 197, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0102, 0.9060, 0.0646, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 197, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.2072e-03, 9.7321e-01, 5.9281e-03, 5.1340e-03, 3.2845e-09, 5.2997e-03,
        5.2187e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 198, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0046, 0.0049, 0.0102, 0.0048, 0.9509, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.019

[Epoch: 198, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9871e-02, 2.4920e-02, 3.0280e-02, 2.0834e-07, 3.8596e-08, 2.9796e-02,
        8.8513e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 198, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.2213e-02, 1.5431e-02, 1.5009e-02, 8.5700e-01, 1.4670e-02, 5.6722e-03,
        1.4507e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 198, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0101, 0.9060, 0.0647, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 198, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1739e-03, 9.7339e-01, 5.8133e-03, 5.1199e-03, 2.9023e-09, 5.2879e-03,
        5.2100e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.015

[Epoch: 199, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0098, 0.0047, 0.0049, 0.0100, 0.0049, 0.9509, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 199, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9899e-02, 2.4850e-02, 3.0235e-02, 1.8997e-07, 3.4914e-08, 3.0174e-02,
        8.8484e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 199, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([8.6440e-02, 1.4958e-02, 1.4779e-02, 8.6353e-01, 1.4645e-02, 5.6426e-03,
        1.3306e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.017

[Epoch: 199, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0102, 0.9062, 0.0644, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 199, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([5.1362e-03, 9.7365e-01, 5.6599e-03, 5.0298e-03, 2.6376e-09, 5.2790e-03,
        5.2402e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

[Epoch: 200, batch: 42/213] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.0100, 0.0050, 0.9500, 0.0150])
Policy pred: tensor([0.0099, 0.0046, 0.0048, 0.0101, 0.0048, 0.9511, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.020

[Epoch: 200, batch: 84/213] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0250, 0.0300, 0.0000, 0.0000, 0.0300, 0.8850])
Policy pred: tensor([2.9688e-02, 2.4765e-02, 3.0222e-02, 1.9145e-07, 3.5583e-08, 2.9682e-02,
        8.8564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 200, batch: 126/213] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0900, 0.0150, 0.0150, 0.8600, 0.0150, 0.0050, 0.0000])
Policy pred: tensor([9.4932e-02, 1.5507e-02, 1.5295e-02, 8.5397e-01, 1.4810e-02, 5.4877e-03,
        1.2870e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.018 0.019

[Epoch: 200, batch: 168/213] total loss per batch: 0.470
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.9050, 0.0650, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0104, 0.9044, 0.0657, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.008 0.008

[Epoch: 200, batch: 210/213] total loss per batch: 0.473
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.9750, 0.0050, 0.0050, 0.0000, 0.0050, 0.0050])
Policy pred: tensor([4.9991e-03, 9.7400e-01, 5.6113e-03, 5.0908e-03, 2.2300e-09, 5.2484e-03,
        5.0545e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.019 0.016

