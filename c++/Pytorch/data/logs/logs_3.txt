Training set samples: 7086
Batch size: 32
[Epoch: 1, batch: 44/222] total loss per batch: 1.393
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.0252e-01, 7.3598e-07, 2.6044e-01, 1.9372e-06, 1.9061e-07, 5.3704e-01,
        2.8625e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.259

[Epoch: 1, batch: 88/222] total loss per batch: 1.256
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.8331e-01, 1.1994e-01, 1.8074e-01, 5.1960e-07, 6.3382e-07, 1.6860e-01,
        3.4740e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.038

[Epoch: 1, batch: 132/222] total loss per batch: 1.305
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2383e-06, 4.0865e-09, 2.1228e-08, 1.0000e+00, 2.4841e-08, 1.3795e-07,
        3.9819e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 0.184

[Epoch: 1, batch: 176/222] total loss per batch: 1.183
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2613e-01, 8.0134e-02, 1.7801e-01, 4.6733e-07, 5.5356e-02, 6.0347e-02,
        1.9173e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.019

[Epoch: 1, batch: 220/222] total loss per batch: 1.143
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0198, 0.0040, 0.0038, 0.9309, 0.0086, 0.0127, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 2, batch: 44/222] total loss per batch: 0.954
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.4027e-01, 7.7525e-08, 3.4109e-01, 1.5758e-06, 4.7880e-07, 4.1863e-01,
        2.5074e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.698

[Epoch: 2, batch: 88/222] total loss per batch: 0.865
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.3089e-02, 4.3837e-02, 6.6246e-02, 1.1725e-07, 1.5862e-08, 9.2387e-03,
        8.2759e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.038

[Epoch: 2, batch: 132/222] total loss per batch: 0.904
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.0556e-07, 7.6487e-09, 2.8648e-08, 1.0000e+00, 1.2728e-08, 8.8104e-08,
        9.8913e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.817

[Epoch: 2, batch: 176/222] total loss per batch: 0.820
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1775e-01, 3.3293e-02, 1.7092e-01, 1.2777e-07, 6.4473e-02, 1.3532e-02,
        3.2315e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 2, batch: 220/222] total loss per batch: 0.796
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.0018, 0.0016, 0.9667, 0.0028, 0.0071, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 3, batch: 44/222] total loss per batch: 0.757
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.0293e-01, 5.9907e-09, 3.9149e-01, 8.9563e-07, 9.5230e-08, 4.0557e-01,
        1.9976e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.842

[Epoch: 3, batch: 88/222] total loss per batch: 0.706
Policy (actual, predicted): 6 1
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.3012e-01, 3.9009e-01, 6.4519e-02, 1.3843e-07, 1.4083e-08, 5.5024e-02,
        3.6024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 3, batch: 132/222] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.7508e-07, 4.8716e-10, 2.6090e-08, 1.0000e+00, 1.0256e-08, 5.6951e-08,
        1.5851e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.825

[Epoch: 3, batch: 176/222] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.7448e-01, 6.5868e-02, 3.6770e-01, 3.7261e-06, 1.5207e-01, 3.9835e-02,
        3.8787e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.009

[Epoch: 3, batch: 220/222] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0027, 0.0022, 0.9719, 0.0032, 0.0044, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 4, batch: 44/222] total loss per batch: 0.698
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.3079e-01, 4.2807e-08, 3.5934e-01, 1.9853e-06, 3.2540e-07, 4.0987e-01,
        4.3905e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.885

[Epoch: 4, batch: 88/222] total loss per batch: 0.663
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.3162e-02, 6.1901e-02, 1.6264e-02, 1.6909e-07, 9.2473e-09, 2.0330e-02,
        8.5834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.031

[Epoch: 4, batch: 132/222] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.8983e-07, 1.2353e-09, 6.2099e-08, 1.0000e+00, 5.2917e-09, 1.3188e-07,
        1.7341e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.912

[Epoch: 4, batch: 176/222] total loss per batch: 0.665
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.9093e-01, 1.4047e-02, 8.3734e-02, 1.5852e-07, 7.4121e-03, 3.8695e-03,
        9.7395e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.009

[Epoch: 4, batch: 220/222] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0026, 0.0038, 0.9693, 0.0032, 0.0057, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.003

[Epoch: 5, batch: 44/222] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.4326e-01, 1.0195e-07, 3.8856e-01, 3.5445e-06, 1.0569e-06, 3.6817e-01,
        1.9811e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.908

[Epoch: 5, batch: 88/222] total loss per batch: 0.660
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.5045e-02, 2.0426e-01, 2.4563e-02, 5.2867e-07, 1.1764e-07, 1.8795e-01,
        5.1818e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.031

[Epoch: 5, batch: 132/222] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4489e-07, 5.1042e-09, 3.1630e-08, 1.0000e+00, 1.9060e-08, 6.3288e-08,
        2.5583e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.919

[Epoch: 5, batch: 176/222] total loss per batch: 0.646
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.7838e-01, 8.9159e-03, 3.3566e-01, 2.7132e-07, 7.1426e-02, 5.6167e-03,
        7.7313e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.008

[Epoch: 5, batch: 220/222] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0111, 0.0018, 0.0030, 0.9705, 0.0038, 0.0057, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 6, batch: 44/222] total loss per batch: 0.676
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.4411e-01, 5.4710e-08, 3.4268e-01, 2.5615e-06, 1.0025e-06, 4.1320e-01,
        9.0346e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.926

[Epoch: 6, batch: 88/222] total loss per batch: 0.649
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.0973e-02, 3.8924e-02, 1.0793e-02, 2.7162e-07, 1.9010e-08, 1.3456e-02,
        8.4585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.007

[Epoch: 6, batch: 132/222] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0363e-07, 4.9095e-09, 4.1580e-08, 1.0000e+00, 1.3641e-08, 1.0432e-07,
        1.1461e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.951

[Epoch: 6, batch: 176/222] total loss per batch: 0.629
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6779e-01, 1.6599e-02, 2.8470e-01, 3.7705e-07, 1.8995e-02, 1.1855e-02,
        5.8114e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.012

[Epoch: 6, batch: 220/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0129, 0.0032, 0.0035, 0.9680, 0.0035, 0.0038, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.006

[Epoch: 7, batch: 44/222] total loss per batch: 0.663
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.2220e-01, 8.7801e-08, 4.1203e-01, 2.9171e-06, 1.0614e-06, 3.6577e-01,
        2.0397e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.880

[Epoch: 7, batch: 88/222] total loss per batch: 0.636
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.5035e-02, 1.9864e-01, 3.4691e-02, 3.2791e-07, 5.8445e-08, 1.1062e-01,
        6.0102e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.022

[Epoch: 7, batch: 132/222] total loss per batch: 0.669
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6013e-06, 1.0620e-08, 1.6716e-07, 1.0000e+00, 5.6231e-08, 2.3201e-07,
        1.9046e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.952

[Epoch: 7, batch: 176/222] total loss per batch: 0.618
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.9349e-01, 9.4617e-03, 1.7218e-01, 6.5799e-07, 2.0114e-02, 4.7365e-03,
        1.7735e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.007

[Epoch: 7, batch: 220/222] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0126, 0.0025, 0.0045, 0.9691, 0.0044, 0.0038, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.002

[Epoch: 8, batch: 44/222] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7019e-01, 5.2775e-08, 3.8263e-01, 4.4524e-06, 7.7884e-07, 3.4718e-01,
        2.1874e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.916

[Epoch: 8, batch: 88/222] total loss per batch: 0.625
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.5621e-01, 7.1550e-02, 1.9143e-02, 6.3577e-07, 8.1209e-08, 6.7383e-02,
        6.8571e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.027

[Epoch: 8, batch: 132/222] total loss per batch: 0.662
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5702e-06, 4.0258e-08, 3.8136e-07, 1.0000e+00, 1.1706e-07, 3.7786e-07,
        1.0291e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.946

[Epoch: 8, batch: 176/222] total loss per batch: 0.613
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.9581e-01, 7.6718e-03, 3.7793e-01, 5.7182e-07, 7.9312e-03, 1.0657e-02,
        7.0947e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.017

[Epoch: 8, batch: 220/222] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0047, 0.0044, 0.9683, 0.0043, 0.0046, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.002

[Epoch: 9, batch: 44/222] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5729e-01, 1.9256e-07, 3.7891e-01, 6.8927e-06, 2.9640e-06, 3.6379e-01,
        4.5625e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.932

[Epoch: 9, batch: 88/222] total loss per batch: 0.622
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([2.8352e-02, 1.7325e-01, 1.5981e-02, 7.3689e-07, 3.0232e-08, 5.3965e-02,
        7.2845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.024

[Epoch: 9, batch: 132/222] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6233e-06, 3.4111e-08, 3.6182e-07, 1.0000e+00, 5.6089e-08, 2.9609e-07,
        1.7469e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.974

[Epoch: 9, batch: 176/222] total loss per batch: 0.607
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.2131e-01, 1.1763e-02, 1.4228e-01, 8.7596e-07, 2.0402e-02, 4.2240e-03,
        1.0924e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.000

[Epoch: 9, batch: 220/222] total loss per batch: 0.620
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0119, 0.0052, 0.0051, 0.9645, 0.0047, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.007

[Epoch: 10, batch: 44/222] total loss per batch: 0.644
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8154e-01, 1.1393e-07, 3.9722e-01, 4.5219e-06, 2.4177e-06, 3.2124e-01,
        2.0407e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.923

[Epoch: 10, batch: 88/222] total loss per batch: 0.622
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0743e-01, 6.4791e-02, 1.8613e-02, 6.8028e-07, 1.5302e-07, 2.2573e-01,
        5.8344e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.037

[Epoch: 10, batch: 132/222] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.6433e-07, 3.1967e-08, 2.1232e-07, 1.0000e+00, 2.1169e-08, 5.1315e-07,
        5.6937e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.936

[Epoch: 10, batch: 176/222] total loss per batch: 0.607
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.1382e-01, 8.0131e-03, 4.5523e-01, 2.5401e-06, 9.9983e-03, 1.2913e-02,
        3.0687e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.021

[Epoch: 10, batch: 220/222] total loss per batch: 0.618
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.0036, 0.0050, 0.9670, 0.0038, 0.0043, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.005

[Epoch: 11, batch: 44/222] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8460e-01, 1.1579e-07, 3.6485e-01, 1.2365e-05, 4.1583e-06, 3.5053e-01,
        2.2039e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.964

[Epoch: 11, batch: 88/222] total loss per batch: 0.620
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.6613e-02, 1.1627e-01, 8.7999e-03, 2.7937e-07, 1.2806e-08, 3.4602e-03,
        7.7486e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.041

[Epoch: 11, batch: 132/222] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.6566e-06, 1.8304e-07, 1.5001e-06, 9.9999e-01, 1.4467e-07, 1.6368e-06,
        1.1382e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.987

[Epoch: 11, batch: 176/222] total loss per batch: 0.605
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.8300e-01, 5.1901e-03, 9.5028e-02, 1.4069e-06, 1.0856e-02, 5.9167e-03,
        7.9892e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 11, batch: 220/222] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0046, 0.0050, 0.9662, 0.0045, 0.0039, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.005

[Epoch: 12, batch: 44/222] total loss per batch: 0.640
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6461e-01, 1.4739e-07, 3.5044e-01, 1.6736e-05, 3.6469e-06, 3.8493e-01,
        1.8699e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.909

[Epoch: 12, batch: 88/222] total loss per batch: 0.615
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.4572e-02, 1.0329e-01, 1.8158e-02, 6.1169e-07, 6.7364e-08, 6.2738e-02,
        7.4124e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.036

[Epoch: 12, batch: 132/222] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2885e-06, 7.0214e-08, 2.6384e-07, 1.0000e+00, 4.1439e-08, 2.9868e-07,
        3.0796e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.945

[Epoch: 12, batch: 176/222] total loss per batch: 0.604
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.4929e-01, 5.6198e-03, 4.1467e-01, 2.0934e-06, 1.8554e-02, 1.1814e-02,
        4.9724e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.021

[Epoch: 12, batch: 220/222] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0042, 0.0039, 0.9713, 0.0038, 0.0043, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.005

[Epoch: 13, batch: 44/222] total loss per batch: 0.638
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5662e-01, 1.1794e-07, 3.5333e-01, 1.1806e-05, 3.6779e-06, 3.9004e-01,
        1.7857e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 13, batch: 88/222] total loss per batch: 0.614
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.8981e-02, 2.0472e-01, 2.5404e-02, 4.5453e-07, 5.3601e-08, 7.7132e-02,
        5.9376e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.047

[Epoch: 13, batch: 132/222] total loss per batch: 0.655
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5706e-06, 6.8374e-08, 4.3859e-07, 1.0000e+00, 4.7429e-08, 5.9699e-07,
        2.2252e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.980

[Epoch: 13, batch: 176/222] total loss per batch: 0.605
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.2024e-01, 6.0550e-03, 1.6204e-01, 3.3066e-06, 7.8157e-03, 3.8266e-03,
        1.0668e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.007

[Epoch: 13, batch: 220/222] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0048, 0.0048, 0.9664, 0.0041, 0.0056, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 14, batch: 44/222] total loss per batch: 0.639
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8058e-01, 2.9625e-07, 3.6387e-01, 1.6448e-05, 7.1595e-06, 3.5552e-01,
        1.0335e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.957

[Epoch: 14, batch: 88/222] total loss per batch: 0.616
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.0545e-02, 6.6489e-02, 1.0399e-02, 7.2515e-07, 2.1998e-07, 6.0059e-02,
        8.2251e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.024

[Epoch: 14, batch: 132/222] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.0829e-06, 6.8016e-08, 9.2676e-07, 1.0000e+00, 6.6314e-08, 6.2062e-07,
        7.6743e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.980

[Epoch: 14, batch: 176/222] total loss per batch: 0.602
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.6926e-01, 4.1671e-03, 4.0319e-01, 2.8920e-06, 1.2502e-02, 1.0848e-02,
        2.2476e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.009

[Epoch: 14, batch: 220/222] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0123, 0.0041, 0.0036, 0.9664, 0.0045, 0.0033, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 15, batch: 44/222] total loss per batch: 0.639
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8218e-01, 2.7396e-07, 3.8089e-01, 3.3268e-05, 2.4432e-06, 3.3690e-01,
        1.4048e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.877

[Epoch: 15, batch: 88/222] total loss per batch: 0.620
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([2.1049e-01, 1.9027e-01, 1.4907e-02, 7.0451e-07, 4.6929e-08, 5.3315e-02,
        5.3102e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.038

[Epoch: 15, batch: 132/222] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3083e-06, 2.5799e-07, 4.2733e-07, 1.0000e+00, 1.3566e-08, 6.2917e-07,
        2.8516e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.989

[Epoch: 15, batch: 176/222] total loss per batch: 0.601
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.6033e-01, 6.7463e-03, 1.1950e-01, 1.8575e-06, 7.6778e-03, 5.7324e-03,
        9.7807e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.004

[Epoch: 15, batch: 220/222] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0042, 0.0044, 0.9698, 0.0049, 0.0036, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 16, batch: 44/222] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.1312e-01, 4.2263e-07, 3.5737e-01, 2.3744e-05, 9.8928e-06, 3.2948e-01,
        1.8435e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.962

[Epoch: 16, batch: 88/222] total loss per batch: 0.617
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([2.0605e-02, 7.3917e-02, 2.1283e-02, 2.6965e-06, 1.8655e-07, 1.3251e-01,
        7.5168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.039

[Epoch: 16, batch: 132/222] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2922e-06, 8.3164e-08, 8.1196e-07, 1.0000e+00, 6.6419e-08, 1.1428e-06,
        4.8456e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.989

[Epoch: 16, batch: 176/222] total loss per batch: 0.601
Policy (actual, predicted): 0 2
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.2015e-01, 5.2909e-03, 6.5679e-01, 4.3399e-06, 8.3366e-03, 9.3985e-03,
        2.1760e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.005

[Epoch: 16, batch: 220/222] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0050, 0.0054, 0.9649, 0.0035, 0.0046, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 17, batch: 44/222] total loss per batch: 0.640
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8534e-01, 4.0349e-07, 3.7598e-01, 4.9045e-05, 2.4461e-06, 3.3863e-01,
        2.4560e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.914

[Epoch: 17, batch: 88/222] total loss per batch: 0.614
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3127e-02, 2.3855e-01, 1.1467e-02, 1.8994e-06, 2.1729e-08, 2.0973e-02,
        6.4588e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.030

[Epoch: 17, batch: 132/222] total loss per batch: 0.653
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2751e-06, 1.3560e-07, 1.5776e-06, 9.9999e-01, 4.3920e-08, 2.2054e-06,
        3.9579e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.985

[Epoch: 17, batch: 176/222] total loss per batch: 0.602
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.0013e-01, 6.9919e-03, 7.0994e-02, 1.1247e-05, 1.7386e-02, 4.4608e-03,
        2.4297e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.006

[Epoch: 17, batch: 220/222] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0044, 0.0052, 0.9709, 0.0043, 0.0035, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.002

[Epoch: 18, batch: 44/222] total loss per batch: 0.634
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8586e-01, 3.7009e-07, 3.8701e-01, 3.1404e-05, 8.0809e-06, 3.2709e-01,
        2.3239e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.892

[Epoch: 18, batch: 88/222] total loss per batch: 0.611
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0974e-02, 5.7938e-02, 1.8526e-02, 9.3603e-07, 8.2686e-08, 6.5632e-02,
        7.7693e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.031

[Epoch: 18, batch: 132/222] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1844e-06, 3.2614e-08, 2.6043e-06, 9.9999e-01, 1.5578e-07, 1.7495e-06,
        1.6991e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.992

[Epoch: 18, batch: 176/222] total loss per batch: 0.600
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8264e-01, 9.2618e-03, 2.6242e-01, 9.4676e-06, 3.8683e-02, 6.9544e-03,
        3.2326e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.028

[Epoch: 18, batch: 220/222] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0111, 0.0052, 0.0053, 0.9614, 0.0043, 0.0062, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 19, batch: 44/222] total loss per batch: 0.632
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9963e-01, 4.3924e-07, 3.6270e-01, 6.6281e-05, 3.9320e-06, 3.3760e-01,
        2.8184e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.961

[Epoch: 19, batch: 88/222] total loss per batch: 0.609
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.5442e-02, 1.2060e-01, 1.3724e-02, 1.8286e-06, 1.6180e-07, 6.9181e-02,
        7.3105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.034

[Epoch: 19, batch: 132/222] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.8587e-07, 6.5950e-08, 1.1042e-06, 1.0000e+00, 3.2961e-08, 6.5655e-07,
        2.1652e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.971

[Epoch: 19, batch: 176/222] total loss per batch: 0.600
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8152e-01, 1.2044e-02, 1.8783e-01, 1.8329e-05, 1.1631e-02, 6.8833e-03,
        7.3562e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.006

[Epoch: 19, batch: 220/222] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0108, 0.0033, 0.0036, 0.9699, 0.0043, 0.0046, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 20, batch: 44/222] total loss per batch: 0.634
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.1763e-01, 5.7707e-07, 3.6102e-01, 6.4708e-05, 6.0994e-06, 3.2128e-01,
        3.4855e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.973

[Epoch: 20, batch: 88/222] total loss per batch: 0.609
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0189e-01, 1.5548e-01, 1.6566e-02, 2.3525e-06, 1.8266e-07, 9.1234e-02,
        6.3482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.044

[Epoch: 20, batch: 132/222] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.9544e-07, 5.3598e-08, 8.5329e-07, 1.0000e+00, 2.6152e-08, 3.9701e-07,
        8.0143e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.978

[Epoch: 20, batch: 176/222] total loss per batch: 0.600
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.8524e-01, 6.6382e-03, 3.7214e-01, 8.5654e-06, 2.8922e-02, 6.9743e-03,
        7.2301e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.039

[Epoch: 20, batch: 220/222] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0116, 0.0044, 0.0050, 0.9640, 0.0042, 0.0057, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.002

[Epoch: 21, batch: 44/222] total loss per batch: 0.633
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7559e-01, 2.4208e-07, 3.4030e-01, 8.4721e-06, 3.5345e-06, 3.8410e-01,
        1.4943e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.955

[Epoch: 21, batch: 88/222] total loss per batch: 0.608
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8780e-02, 6.8492e-02, 1.2641e-02, 6.5140e-07, 1.4870e-07, 6.1674e-02,
        7.7841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.036

[Epoch: 21, batch: 132/222] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.3419e-07, 9.0078e-08, 9.8225e-07, 1.0000e+00, 3.5385e-08, 2.5247e-06,
        1.5486e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.989

[Epoch: 21, batch: 176/222] total loss per batch: 0.597
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.6177e-01, 1.1764e-02, 2.0082e-01, 3.2978e-05, 1.8000e-02, 7.5211e-03,
        8.4002e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.018

[Epoch: 21, batch: 220/222] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0110, 0.0038, 0.0045, 0.9651, 0.0062, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 22, batch: 44/222] total loss per batch: 0.630
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9774e-01, 4.1580e-07, 3.4956e-01, 6.4391e-05, 7.2985e-06, 3.5263e-01,
        5.5272e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.917

[Epoch: 22, batch: 88/222] total loss per batch: 0.607
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.0426e-02, 1.8358e-01, 3.2533e-02, 6.5464e-06, 7.3867e-07, 1.5155e-01,
        5.4191e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.042

[Epoch: 22, batch: 132/222] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5860e-06, 1.4487e-07, 1.2606e-06, 1.0000e+00, 6.8944e-08, 1.3403e-06,
        1.4143e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.967

[Epoch: 22, batch: 176/222] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.8793e-01, 7.8446e-03, 3.8529e-01, 7.6983e-06, 1.3926e-02, 4.9779e-03,
        2.4803e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 22, batch: 220/222] total loss per batch: 0.605
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0117, 0.0043, 0.0045, 0.9636, 0.0056, 0.0056, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.005

[Epoch: 23, batch: 44/222] total loss per batch: 0.629
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7484e-01, 5.4434e-07, 3.5543e-01, 6.1446e-05, 4.2585e-06, 3.6967e-01,
        2.3329e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 23, batch: 88/222] total loss per batch: 0.607
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.3079e-02, 6.5861e-02, 1.2303e-02, 1.1153e-06, 1.3127e-07, 2.8296e-02,
        8.3046e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.015

[Epoch: 23, batch: 132/222] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.8857e-07, 4.1153e-08, 9.8061e-07, 1.0000e+00, 2.8172e-08, 3.0480e-06,
        5.2875e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 23, batch: 176/222] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.7872e-01, 8.0949e-03, 1.8986e-01, 1.0549e-05, 1.9080e-02, 4.1642e-03,
        7.7456e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 23, batch: 220/222] total loss per batch: 0.603
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0043, 0.0050, 0.9679, 0.0045, 0.0049, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 24, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8154e-01, 4.5981e-07, 3.7986e-01, 5.6191e-05, 7.5385e-06, 3.3853e-01,
        4.3154e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.937

[Epoch: 24, batch: 88/222] total loss per batch: 0.607
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.2479e-01, 1.3070e-01, 1.9293e-02, 3.5864e-06, 4.1352e-07, 1.3220e-01,
        5.9302e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.042

[Epoch: 24, batch: 132/222] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2893e-06, 1.2028e-07, 1.1683e-06, 9.9999e-01, 1.6713e-07, 2.2724e-06,
        4.9412e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.979

[Epoch: 24, batch: 176/222] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1659e-01, 5.6264e-03, 2.6335e-01, 3.2639e-05, 6.2039e-03, 8.1482e-03,
        4.6449e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.015

[Epoch: 24, batch: 220/222] total loss per batch: 0.604
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0123, 0.0046, 0.0042, 0.9626, 0.0052, 0.0067, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 25, batch: 44/222] total loss per batch: 0.629
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9359e-01, 1.1952e-06, 3.7634e-01, 2.9372e-05, 6.2255e-06, 3.3003e-01,
        9.9245e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.972

[Epoch: 25, batch: 88/222] total loss per batch: 0.607
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.3176e-02, 1.5670e-01, 1.9854e-02, 3.7727e-06, 1.5182e-07, 7.9332e-02,
        7.0094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.035

[Epoch: 25, batch: 132/222] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.2878e-07, 8.9269e-09, 5.4445e-07, 1.0000e+00, 9.9311e-08, 5.9057e-07,
        1.6707e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 25, batch: 176/222] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3401e-01, 8.3036e-03, 2.3509e-01, 1.0753e-05, 1.7394e-02, 5.1512e-03,
        3.5786e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.002

[Epoch: 25, batch: 220/222] total loss per batch: 0.604
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0044, 0.0045, 0.9668, 0.0041, 0.0047, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 26, batch: 44/222] total loss per batch: 0.630
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7304e-01, 2.5677e-07, 3.5514e-01, 8.7435e-05, 7.1559e-06, 3.7173e-01,
        6.8390e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.918

[Epoch: 26, batch: 88/222] total loss per batch: 0.606
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3501e-02, 8.7932e-02, 1.2587e-02, 2.6592e-06, 5.5124e-07, 8.1510e-02,
        7.2447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.033

[Epoch: 26, batch: 132/222] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.5051e-07, 7.4236e-08, 6.5939e-07, 1.0000e+00, 5.7370e-08, 1.4918e-06,
        3.0202e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 26, batch: 176/222] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.8957e-01, 1.2747e-02, 3.8025e-01, 1.0767e-05, 1.1128e-02, 6.2338e-03,
        5.3207e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.002

[Epoch: 26, batch: 220/222] total loss per batch: 0.604
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0111, 0.0047, 0.0058, 0.9635, 0.0060, 0.0043, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 27, batch: 44/222] total loss per batch: 0.629
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5751e-01, 7.5682e-07, 3.9516e-01, 4.8558e-05, 1.0389e-05, 3.4726e-01,
        1.4419e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 27, batch: 88/222] total loss per batch: 0.610
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.3629e-02, 1.5906e-01, 2.3269e-02, 4.6857e-06, 2.8791e-07, 7.7963e-02,
        6.7607e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.024

[Epoch: 27, batch: 132/222] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.2889e-07, 2.1995e-07, 1.0328e-06, 1.0000e+00, 3.0219e-07, 2.1980e-06,
        5.0059e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.982

[Epoch: 27, batch: 176/222] total loss per batch: 0.598
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2542e-01, 4.1469e-03, 2.4955e-01, 9.9960e-06, 1.6556e-02, 4.2882e-03,
        2.8797e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.018

[Epoch: 27, batch: 220/222] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0064, 0.0038, 0.9681, 0.0031, 0.0034, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.002

[Epoch: 28, batch: 44/222] total loss per batch: 0.632
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6663e-01, 4.6658e-07, 3.3908e-01, 5.4394e-05, 3.5273e-06, 3.9423e-01,
        2.0977e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.908

[Epoch: 28, batch: 88/222] total loss per batch: 0.608
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.1412e-01, 1.1649e-01, 1.6320e-02, 3.0341e-06, 5.7290e-07, 1.1009e-01,
        6.4299e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.044

[Epoch: 28, batch: 132/222] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.5788e-07, 1.3826e-08, 3.1798e-07, 1.0000e+00, 1.1739e-07, 7.9922e-07,
        3.3532e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 28, batch: 176/222] total loss per batch: 0.597
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7490e-01, 6.8068e-03, 3.0436e-01, 1.0473e-04, 9.0767e-03, 4.7151e-03,
        4.3508e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 28, batch: 220/222] total loss per batch: 0.604
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.0041, 0.0039, 0.9719, 0.0041, 0.0049, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 29, batch: 44/222] total loss per batch: 0.631
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6856e-01, 4.1748e-07, 3.9508e-01, 4.5418e-05, 6.3570e-06, 3.3632e-01,
        2.9840e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.929

[Epoch: 29, batch: 88/222] total loss per batch: 0.606
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.5389e-02, 1.1314e-01, 1.1377e-02, 3.6524e-06, 2.0768e-07, 5.5550e-02,
        7.3454e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.038

[Epoch: 29, batch: 132/222] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9981e-07, 4.1171e-08, 1.1025e-06, 1.0000e+00, 4.4653e-08, 1.3390e-06,
        5.7634e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 29, batch: 176/222] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4536e-01, 3.6080e-03, 2.3492e-01, 3.9486e-05, 1.0738e-02, 5.3043e-03,
        2.6457e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 29, batch: 220/222] total loss per batch: 0.604
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0034, 0.0050, 0.9721, 0.0037, 0.0036, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 30, batch: 44/222] total loss per batch: 0.631
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.4315e-01, 4.9927e-07, 3.7796e-01, 3.0948e-05, 6.4388e-06, 3.7885e-01,
        5.1438e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 30, batch: 88/222] total loss per batch: 0.605
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.4589e-02, 1.3643e-01, 2.5930e-02, 5.9364e-06, 4.5240e-07, 8.5017e-02,
        7.0802e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.040

[Epoch: 30, batch: 132/222] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.5169e-07, 5.5059e-08, 8.4682e-07, 1.0000e+00, 1.8537e-07, 2.7913e-06,
        3.3161e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.982

[Epoch: 30, batch: 176/222] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.3602e-01, 1.1038e-02, 3.2077e-01, 1.6248e-04, 2.2619e-02, 9.3772e-03,
        1.4703e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.000

[Epoch: 30, batch: 220/222] total loss per batch: 0.603
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.0045, 0.0045, 0.9660, 0.0047, 0.0047, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.006

[Epoch: 31, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0443e-01, 1.5025e-07, 3.7617e-01, 4.3545e-05, 3.2943e-06, 3.1935e-01,
        5.8692e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.924

[Epoch: 31, batch: 88/222] total loss per batch: 0.603
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([2.0609e-01, 1.3760e-01, 9.4496e-03, 4.1654e-06, 5.2914e-07, 7.6383e-02,
        5.7047e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.040

[Epoch: 31, batch: 132/222] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.3325e-07, 4.7795e-08, 1.0243e-06, 9.9999e-01, 1.1653e-07, 5.3706e-06,
        9.8888e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 31, batch: 176/222] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5870e-01, 4.0546e-03, 2.2368e-01, 5.8753e-05, 9.5762e-03, 3.8872e-03,
        4.0642e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 31, batch: 220/222] total loss per batch: 0.603
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0043, 0.0054, 0.9679, 0.0035, 0.0042, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 32, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.1734e-01, 8.6072e-07, 3.4427e-01, 5.9578e-05, 1.2322e-05, 3.3832e-01,
        5.7792e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 32, batch: 88/222] total loss per batch: 0.603
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([3.2816e-02, 1.1414e-01, 1.7567e-02, 1.0925e-05, 1.8915e-06, 8.9608e-02,
        7.4586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 32, batch: 132/222] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.2738e-07, 3.2484e-08, 2.6412e-06, 1.0000e+00, 4.2431e-08, 1.0659e-06,
        7.8118e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.981

[Epoch: 32, batch: 176/222] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2452e-01, 9.7675e-03, 3.4294e-01, 1.4211e-04, 1.2554e-02, 1.0046e-02,
        2.8091e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 32, batch: 220/222] total loss per batch: 0.601
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0086, 0.0039, 0.0055, 0.9700, 0.0035, 0.0039, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 33, batch: 44/222] total loss per batch: 0.627
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.2165e-01, 8.1809e-07, 3.5541e-01, 7.5350e-05, 1.5178e-05, 3.2285e-01,
        6.4732e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.931

[Epoch: 33, batch: 88/222] total loss per batch: 0.604
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8651e-02, 1.4112e-01, 1.1527e-02, 6.2113e-06, 2.1438e-06, 5.8729e-02,
        7.0997e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.044

[Epoch: 33, batch: 132/222] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.8722e-07, 6.1633e-08, 1.3909e-06, 9.9999e-01, 3.8498e-07, 4.6896e-06,
        5.2999e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.990

[Epoch: 33, batch: 176/222] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7536e-01, 4.8366e-03, 2.9363e-01, 7.3102e-05, 1.8309e-02, 7.7382e-03,
        5.1741e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 33, batch: 220/222] total loss per batch: 0.602
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0035, 0.0047, 0.9680, 0.0054, 0.0050, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 34, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7978e-01, 4.5756e-07, 3.5518e-01, 1.7359e-05, 5.3613e-06, 3.6501e-01,
        5.2805e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.894

[Epoch: 34, batch: 88/222] total loss per batch: 0.603
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0528e-01, 8.7989e-02, 1.5547e-02, 4.6260e-06, 4.8990e-07, 9.3179e-02,
        6.9801e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 34, batch: 132/222] total loss per batch: 0.642
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5441e-07, 4.7818e-08, 1.2150e-06, 1.0000e+00, 4.1407e-08, 1.8007e-06,
        5.2976e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.989

[Epoch: 34, batch: 176/222] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.3461e-01, 5.7755e-03, 1.3829e-01, 1.0548e-04, 1.6842e-02, 4.2168e-03,
        1.5421e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 34, batch: 220/222] total loss per batch: 0.605
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0039, 0.0039, 0.9709, 0.0029, 0.0053, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 35, batch: 44/222] total loss per batch: 0.632
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0722e-01, 1.7578e-07, 3.5847e-01, 3.7046e-05, 7.8193e-06, 3.3426e-01,
        3.8566e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.941

[Epoch: 35, batch: 88/222] total loss per batch: 0.607
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.2276e-02, 1.7387e-01, 2.2922e-02, 2.5820e-06, 7.9900e-07, 6.7683e-02,
        6.7325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.049

[Epoch: 35, batch: 132/222] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.6769e-08, 5.4003e-09, 7.7178e-07, 1.0000e+00, 6.2182e-08, 1.7703e-06,
        1.2027e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.993

[Epoch: 35, batch: 176/222] total loss per batch: 0.596
Policy (actual, predicted): 0 2
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([4.1014e-01, 7.6461e-03, 5.4886e-01, 3.2254e-05, 2.3888e-02, 9.3872e-03,
        4.5134e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 35, batch: 220/222] total loss per batch: 0.602
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0121, 0.0041, 0.0042, 0.9656, 0.0038, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.006

[Epoch: 36, batch: 44/222] total loss per batch: 0.629
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0339e-01, 5.0386e-07, 3.4184e-01, 5.5457e-05, 1.2514e-05, 3.5470e-01,
        3.1201e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.975

[Epoch: 36, batch: 88/222] total loss per batch: 0.605
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.7333e-02, 1.1642e-01, 1.1607e-02, 5.8297e-06, 1.1172e-06, 9.7966e-02,
        7.0667e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.038

[Epoch: 36, batch: 132/222] total loss per batch: 0.644
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.1657e-07, 1.5757e-08, 1.0041e-06, 1.0000e+00, 1.2046e-07, 7.2556e-07,
        7.3991e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 36, batch: 176/222] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.0839e-01, 5.9733e-03, 6.8885e-02, 5.0272e-05, 1.3390e-02, 3.2544e-03,
        5.3324e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 36, batch: 220/222] total loss per batch: 0.602
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0027, 0.0053, 0.9746, 0.0036, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.002

[Epoch: 37, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9383e-01, 3.2263e-07, 3.3534e-01, 6.9652e-05, 1.6207e-05, 3.7075e-01,
        9.6496e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.940

[Epoch: 37, batch: 88/222] total loss per batch: 0.602
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6667e-02, 1.3409e-01, 1.2083e-02, 1.0536e-06, 3.5763e-07, 7.7657e-02,
        6.8950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.050

[Epoch: 37, batch: 132/222] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.5935e-08, 3.1150e-08, 1.2093e-06, 1.0000e+00, 1.3520e-07, 4.2747e-07,
        6.0286e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 37, batch: 176/222] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.4644e-01, 5.5767e-03, 3.2764e-01, 6.7959e-05, 1.6568e-02, 3.6607e-03,
        4.5854e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.002

[Epoch: 37, batch: 220/222] total loss per batch: 0.600
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0044, 0.0056, 0.9688, 0.0030, 0.0041, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 38, batch: 44/222] total loss per batch: 0.626
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9642e-01, 2.7166e-07, 3.6450e-01, 3.7017e-05, 6.1632e-06, 3.3903e-01,
        6.8991e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 38, batch: 88/222] total loss per batch: 0.601
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5952e-02, 1.6781e-01, 1.1589e-02, 1.2116e-05, 3.6529e-06, 1.0882e-01,
        6.3582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.041

[Epoch: 38, batch: 132/222] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.8930e-08, 2.8476e-09, 3.0185e-07, 1.0000e+00, 1.2560e-08, 1.8011e-07,
        4.0584e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 38, batch: 176/222] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9461e-01, 5.7431e-03, 2.8367e-01, 4.1545e-05, 1.0405e-02, 5.4733e-03,
        4.9866e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.000

[Epoch: 38, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0042, 0.0052, 0.9723, 0.0031, 0.0039, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 39, batch: 44/222] total loss per batch: 0.624
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.1030e-01, 1.2334e-07, 3.7171e-01, 3.6008e-05, 6.3958e-06, 3.1794e-01,
        8.0643e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 39, batch: 88/222] total loss per batch: 0.599
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.2136e-02, 9.7430e-02, 1.5269e-02, 5.0440e-06, 7.3633e-07, 5.8991e-02,
        7.3617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 39, batch: 132/222] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.0333e-07, 1.2816e-07, 3.5251e-06, 9.9999e-01, 2.0680e-07, 1.2218e-06,
        6.2746e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.991

[Epoch: 39, batch: 176/222] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3752e-01, 5.8496e-03, 2.4249e-01, 1.0813e-04, 1.0430e-02, 3.5836e-03,
        2.4614e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.003

[Epoch: 39, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0039, 0.0047, 0.9686, 0.0039, 0.0055, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 40, batch: 44/222] total loss per batch: 0.623
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9997e-01, 2.2773e-07, 3.4587e-01, 6.2348e-05, 6.6486e-06, 3.5409e-01,
        5.0460e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.908

[Epoch: 40, batch: 88/222] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6994e-02, 1.4899e-01, 1.1931e-02, 6.7424e-06, 2.1100e-06, 1.0708e-01,
        6.5500e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.043

[Epoch: 40, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9104e-07, 7.5910e-09, 1.0336e-06, 1.0000e+00, 4.4470e-08, 5.6880e-07,
        1.6530e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.987

[Epoch: 40, batch: 176/222] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6485e-01, 6.8603e-03, 3.1054e-01, 8.2563e-05, 9.9841e-03, 7.6649e-03,
        2.0635e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 40, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0033, 0.0039, 0.9727, 0.0038, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 41, batch: 44/222] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0052e-01, 2.4240e-07, 3.6523e-01, 9.2972e-05, 1.0785e-05, 3.3415e-01,
        1.4561e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 41, batch: 88/222] total loss per batch: 0.599
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.5054e-02, 8.1538e-02, 1.7578e-02, 1.4167e-05, 1.6368e-06, 6.4227e-02,
        7.5159e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.070

[Epoch: 41, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2734e-07, 7.9082e-08, 1.4545e-06, 1.0000e+00, 8.2069e-08, 4.7470e-07,
        4.8332e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 41, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0061e-01, 6.9835e-03, 2.8077e-01, 6.1250e-05, 7.2798e-03, 4.2760e-03,
        1.5505e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 41, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0081, 0.0046, 0.0034, 0.9709, 0.0037, 0.0044, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.002

[Epoch: 42, batch: 44/222] total loss per batch: 0.624
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9059e-01, 1.8679e-07, 3.5669e-01, 9.5550e-05, 8.9640e-06, 3.5261e-01,
        7.5101e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.893

[Epoch: 42, batch: 88/222] total loss per batch: 0.600
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.1636e-02, 1.8051e-01, 1.6415e-02, 4.8314e-06, 8.1357e-07, 7.6287e-02,
        6.3515e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 42, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.5538e-07, 1.4360e-08, 1.1957e-06, 1.0000e+00, 3.8494e-07, 5.7634e-07,
        3.2758e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.993

[Epoch: 42, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1666e-01, 4.9202e-03, 2.6554e-01, 1.0337e-04, 7.7333e-03, 5.0060e-03,
        3.7069e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 42, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0038, 0.0055, 0.9676, 0.0042, 0.0048, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 43, batch: 44/222] total loss per batch: 0.624
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8016e-01, 2.8210e-07, 3.8649e-01, 3.6258e-05, 1.3850e-05, 3.3330e-01,
        4.0782e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 43, batch: 88/222] total loss per batch: 0.599
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.0324e-02, 6.5561e-02, 1.3642e-02, 7.2988e-06, 1.3086e-06, 8.7504e-02,
        7.7296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.052

[Epoch: 43, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.3925e-07, 7.3120e-08, 1.1295e-06, 1.0000e+00, 1.5565e-07, 8.7259e-07,
        6.1797e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 43, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5558e-01, 6.9584e-03, 3.2229e-01, 4.5526e-05, 8.4033e-03, 6.7064e-03,
        2.0406e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 43, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0128, 0.0048, 0.0047, 0.9615, 0.0041, 0.0062, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 44, batch: 44/222] total loss per batch: 0.622
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9114e-01, 1.1018e-07, 3.5147e-01, 6.5121e-05, 5.9877e-06, 3.5731e-01,
        8.5057e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.969

[Epoch: 44, batch: 88/222] total loss per batch: 0.598
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0394e-01, 1.9318e-01, 1.9393e-02, 3.8546e-06, 1.7312e-06, 7.0609e-02,
        6.1288e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.042

[Epoch: 44, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4102e-07, 8.5203e-08, 3.0724e-06, 1.0000e+00, 2.3435e-07, 1.1946e-06,
        2.2594e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.990

[Epoch: 44, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8796e-01, 6.8383e-03, 1.9090e-01, 1.7793e-04, 1.0489e-02, 3.6113e-03,
        1.5328e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 44, batch: 220/222] total loss per batch: 0.597
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0040, 0.0051, 0.9713, 0.0045, 0.0038, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 45, batch: 44/222] total loss per batch: 0.622
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8165e-01, 1.8863e-07, 3.9909e-01, 3.4785e-05, 9.6439e-06, 3.1922e-01,
        7.8437e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 45, batch: 88/222] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.2790e-02, 1.1605e-01, 1.3667e-02, 4.9355e-06, 1.2932e-06, 8.8376e-02,
        6.9911e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.067

[Epoch: 45, batch: 132/222] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3030e-07, 9.8337e-08, 1.4331e-06, 1.0000e+00, 1.6972e-07, 7.2914e-07,
        3.7764e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 45, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.1556e-01, 4.4249e-03, 3.6817e-01, 9.3918e-05, 6.9513e-03, 4.7852e-03,
        1.1282e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 45, batch: 220/222] total loss per batch: 0.597
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0136, 0.0057, 0.0050, 0.9576, 0.0052, 0.0062, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 46, batch: 44/222] total loss per batch: 0.622
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6505e-01, 2.0350e-07, 3.7250e-01, 2.8110e-05, 4.7090e-06, 3.6242e-01,
        9.1887e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.901

[Epoch: 46, batch: 88/222] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.6672e-02, 1.3846e-01, 1.4050e-02, 5.9572e-06, 2.1349e-06, 5.5068e-02,
        7.2574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.056

[Epoch: 46, batch: 132/222] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.1742e-08, 1.9727e-08, 6.3152e-07, 1.0000e+00, 3.5455e-08, 3.4358e-07,
        7.9754e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 46, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8145e-01, 8.3103e-03, 1.9250e-01, 7.4526e-05, 1.2798e-02, 4.8608e-03,
        8.7184e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.000

[Epoch: 46, batch: 220/222] total loss per batch: 0.597
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0114, 0.0058, 0.0054, 0.9622, 0.0047, 0.0058, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.007

[Epoch: 47, batch: 44/222] total loss per batch: 0.622
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0081e-01, 3.4184e-07, 3.5321e-01, 5.3803e-05, 1.0306e-05, 3.4592e-01,
        4.4943e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.945

[Epoch: 47, batch: 88/222] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0225e-01, 1.8254e-01, 2.4707e-02, 7.1850e-06, 8.1001e-07, 1.1508e-01,
        5.7542e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 47, batch: 132/222] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.7312e-07, 4.0321e-08, 1.1839e-06, 1.0000e+00, 7.0452e-08, 1.1131e-06,
        1.4136e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 47, batch: 176/222] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.0221e-01, 4.9367e-03, 3.7923e-01, 6.2337e-05, 7.3062e-03, 6.2472e-03,
        7.5234e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.002

[Epoch: 47, batch: 220/222] total loss per batch: 0.597
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0046, 0.0048, 0.9663, 0.0044, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 48, batch: 44/222] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6155e-01, 1.0238e-07, 3.7687e-01, 4.2166e-05, 3.5949e-06, 3.6153e-01,
        1.0990e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.950

[Epoch: 48, batch: 88/222] total loss per batch: 0.599
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.3303e-02, 6.6642e-02, 1.1201e-02, 1.1347e-05, 4.1656e-06, 7.7599e-02,
        7.9124e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.055

[Epoch: 48, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.8479e-07, 1.8147e-07, 9.9727e-07, 9.9999e-01, 1.5114e-07, 5.8499e-06,
        2.8109e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.988

[Epoch: 48, batch: 176/222] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.7820e-01, 5.2314e-03, 2.0203e-01, 8.5641e-05, 1.0573e-02, 3.8832e-03,
        3.9474e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 48, batch: 220/222] total loss per batch: 0.596
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.0051, 0.0038, 0.9647, 0.0057, 0.0057, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.006

[Epoch: 49, batch: 44/222] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7251e-01, 3.3923e-07, 3.6723e-01, 2.4222e-05, 5.8576e-06, 3.6023e-01,
        6.6703e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 49, batch: 88/222] total loss per batch: 0.600
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0469e-01, 1.4159e-01, 1.7693e-02, 8.7617e-06, 1.7756e-06, 6.3357e-02,
        6.7266e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.056

[Epoch: 49, batch: 132/222] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4792e-07, 8.3812e-09, 1.7952e-06, 1.0000e+00, 9.0498e-08, 6.8903e-07,
        2.4902e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 49, batch: 176/222] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8087e-01, 9.0284e-03, 2.9544e-01, 2.4877e-05, 9.7007e-03, 4.9249e-03,
        4.3541e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 49, batch: 220/222] total loss per batch: 0.598
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0045, 0.0056, 0.9652, 0.0042, 0.0048, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 50, batch: 44/222] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0603e-01, 1.0150e-07, 3.5428e-01, 5.2632e-05, 2.3831e-06, 3.3964e-01,
        7.4831e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 50, batch: 88/222] total loss per batch: 0.602
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.3342e-02, 1.2406e-01, 1.3038e-02, 8.9251e-06, 3.1103e-06, 6.8140e-02,
        7.4140e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 50, batch: 132/222] total loss per batch: 0.638
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2883e-07, 1.2056e-07, 1.5890e-06, 1.0000e+00, 4.5418e-07, 2.0881e-06,
        1.6511e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.993

[Epoch: 50, batch: 176/222] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7569e-01, 2.4478e-03, 3.0514e-01, 8.9766e-05, 1.2703e-02, 3.9203e-03,
        1.0979e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 50, batch: 220/222] total loss per batch: 0.602
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0048, 0.0053, 0.9713, 0.0038, 0.0046, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 51, batch: 44/222] total loss per batch: 0.626
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7769e-01, 4.6554e-07, 3.6080e-01, 3.1855e-05, 6.2925e-06, 3.6147e-01,
        1.1652e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.923

[Epoch: 51, batch: 88/222] total loss per batch: 0.606
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0012e-01, 1.1136e-01, 1.9782e-02, 1.8703e-05, 3.9134e-06, 7.8842e-02,
        6.8987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.053

[Epoch: 51, batch: 132/222] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2399e-08, 1.7659e-08, 9.6892e-07, 1.0000e+00, 1.4013e-07, 7.2857e-07,
        1.1104e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 51, batch: 176/222] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.1753e-01, 7.5870e-03, 1.5858e-01, 1.2476e-04, 1.0131e-02, 6.0297e-03,
        2.2291e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 51, batch: 220/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0161, 0.0036, 0.0063, 0.9534, 0.0074, 0.0078, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.020

[Epoch: 52, batch: 44/222] total loss per batch: 0.658
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7949e-01, 5.2739e-07, 3.1686e-01, 9.1292e-05, 1.2016e-05, 4.0354e-01,
        3.5842e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.812

[Epoch: 52, batch: 88/222] total loss per batch: 0.624
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.2841e-02, 1.3809e-01, 2.3041e-02, 2.3553e-06, 2.9599e-07, 6.4314e-02,
        7.0172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.013

[Epoch: 52, batch: 132/222] total loss per batch: 0.673
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.0398e-08, 3.2158e-08, 1.5291e-06, 1.0000e+00, 2.7354e-07, 5.6381e-07,
        4.5092e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 52, batch: 176/222] total loss per batch: 0.621
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.4511e-01, 7.0827e-03, 4.3760e-01, 1.1376e-04, 6.9850e-03, 3.1057e-03,
        9.9713e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.001

[Epoch: 52, batch: 220/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0121, 0.0050, 0.0044, 0.9643, 0.0045, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.022

[Epoch: 53, batch: 44/222] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7460e-01, 3.9181e-06, 3.7457e-01, 1.8938e-04, 3.4717e-06, 3.5063e-01,
        4.0118e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.916

[Epoch: 53, batch: 88/222] total loss per batch: 0.612
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.0015e-02, 1.8979e-01, 1.3319e-02, 8.4650e-06, 6.7261e-07, 6.6063e-02,
        6.4081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.006

[Epoch: 53, batch: 132/222] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5343e-08, 5.7099e-09, 1.1048e-06, 1.0000e+00, 4.8406e-07, 1.5620e-06,
        1.4990e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 53, batch: 176/222] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.6648e-01, 3.5407e-03, 2.2044e-01, 7.6822e-05, 6.1544e-03, 3.2919e-03,
        1.6404e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 53, batch: 220/222] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0033, 0.0052, 0.9727, 0.0034, 0.0043, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 54, batch: 44/222] total loss per batch: 0.628
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0531e-01, 1.8312e-06, 3.0652e-01, 1.4430e-04, 1.8576e-07, 3.8802e-01,
        6.2666e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.899

[Epoch: 54, batch: 88/222] total loss per batch: 0.598
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.9636e-02, 9.3061e-02, 1.2255e-02, 2.6912e-06, 5.0889e-07, 6.4300e-02,
        7.4074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.017

[Epoch: 54, batch: 132/222] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.0882e-08, 8.1520e-09, 1.5956e-06, 1.0000e+00, 9.7969e-08, 1.5068e-06,
        1.3161e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 54, batch: 176/222] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9957e-01, 6.1246e-03, 2.7986e-01, 5.3220e-05, 9.6886e-03, 4.6878e-03,
        9.3648e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 54, batch: 220/222] total loss per batch: 0.595
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0038, 0.0045, 0.9707, 0.0035, 0.0041, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 55, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8865e-01, 6.6222e-07, 3.7322e-01, 1.2101e-04, 2.1677e-07, 3.3801e-01,
        1.7428e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.951

[Epoch: 55, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.9192e-02, 1.1982e-01, 1.5259e-02, 1.5852e-06, 2.9415e-07, 6.6890e-02,
        7.2884e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.019

[Epoch: 55, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.0036e-08, 2.2411e-09, 8.1228e-07, 1.0000e+00, 5.9870e-08, 7.8875e-07,
        1.0608e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 55, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1552e-01, 6.5154e-03, 2.6413e-01, 5.1035e-05, 9.0204e-03, 4.7531e-03,
        4.3622e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 55, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0041, 0.0042, 0.9690, 0.0043, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 56, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7668e-01, 3.7584e-07, 3.6635e-01, 1.0878e-04, 2.4921e-07, 3.5686e-01,
        1.5891e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.960

[Epoch: 56, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6514e-02, 1.0761e-01, 1.5708e-02, 1.2280e-06, 2.6611e-07, 7.5392e-02,
        7.1478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.023

[Epoch: 56, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.1542e-09, 2.2080e-09, 7.2344e-07, 1.0000e+00, 5.8853e-08, 6.1554e-07,
        6.4895e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 56, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7050e-01, 6.9056e-03, 3.0887e-01, 4.0204e-05, 8.9502e-03, 4.7337e-03,
        4.2303e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 56, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0048, 0.0049, 0.9650, 0.0048, 0.0049, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.005

[Epoch: 57, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8345e-01, 2.5772e-07, 3.7410e-01, 7.4839e-05, 1.8513e-07, 3.4238e-01,
        1.2471e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 57, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7665e-02, 1.2822e-01, 1.6385e-02, 1.3978e-06, 1.8994e-07, 7.2370e-02,
        7.0536e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.025

[Epoch: 57, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.6189e-09, 2.1448e-09, 4.7068e-07, 1.0000e+00, 4.5922e-08, 5.4650e-07,
        4.7062e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 57, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3198e-01, 6.2019e-03, 2.4880e-01, 3.3085e-05, 8.5555e-03, 4.4276e-03,
        3.5342e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 57, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0044, 0.0047, 0.9662, 0.0048, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.006

[Epoch: 58, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8025e-01, 2.1918e-07, 3.7360e-01, 7.1077e-05, 1.8076e-07, 3.4608e-01,
        1.1576e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.951

[Epoch: 58, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.7141e-02, 1.2165e-01, 1.5946e-02, 1.1431e-06, 1.9824e-07, 8.2302e-02,
        6.9296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.031

[Epoch: 58, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.3406e-09, 1.2492e-09, 4.4521e-07, 1.0000e+00, 3.6418e-08, 3.7263e-07,
        3.3251e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 58, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7113e-01, 6.3760e-03, 3.0964e-01, 2.6099e-05, 8.0927e-03, 4.7358e-03,
        3.3095e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 58, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0048, 0.0050, 0.9643, 0.0050, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 59, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8286e-01, 1.5249e-07, 3.7415e-01, 5.1947e-05, 1.3738e-07, 3.4293e-01,
        8.4564e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.952

[Epoch: 59, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7746e-02, 1.2739e-01, 1.6419e-02, 1.1236e-06, 1.3518e-07, 7.9088e-02,
        6.9935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.033

[Epoch: 59, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.1858e-09, 1.3414e-09, 2.9228e-07, 1.0000e+00, 2.7184e-08, 3.1536e-07,
        2.6938e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 59, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2377e-01, 5.5409e-03, 2.5844e-01, 2.4431e-05, 7.5818e-03, 4.6345e-03,
        3.0513e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 59, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0045, 0.0050, 0.9640, 0.0051, 0.0053, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 60, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7327e-01, 1.3408e-07, 3.8515e-01, 4.9571e-05, 1.4269e-07, 3.4153e-01,
        7.8298e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 60, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6666e-02, 1.2158e-01, 1.6936e-02, 8.7574e-07, 1.4441e-07, 8.2979e-02,
        6.9184e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.037

[Epoch: 60, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.1561e-09, 7.5543e-10, 2.7235e-07, 1.0000e+00, 2.2689e-08, 2.7993e-07,
        2.2762e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 60, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8003e-01, 5.7424e-03, 3.0173e-01, 1.8989e-05, 7.6106e-03, 4.8571e-03,
        2.8277e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 60, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0049, 0.0050, 0.9638, 0.0050, 0.0056, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 61, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9314e-01, 1.0286e-07, 3.6289e-01, 3.6954e-05, 9.9958e-08, 3.4393e-01,
        8.4487e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 61, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.3207e-02, 1.3570e-01, 1.5866e-02, 8.9086e-07, 9.9678e-08, 7.7409e-02,
        6.9782e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.040

[Epoch: 61, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9400e-09, 9.9003e-10, 2.6927e-07, 1.0000e+00, 1.9895e-08, 2.5681e-07,
        1.5542e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 61, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1572e-01, 5.5156e-03, 2.6801e-01, 2.1421e-05, 6.2163e-03, 4.5159e-03,
        2.5928e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 61, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0045, 0.0051, 0.9641, 0.0055, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 62, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7349e-01, 1.2038e-07, 3.7003e-01, 4.6356e-05, 1.6356e-07, 3.5643e-01,
        6.4262e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 62, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.5254e-02, 1.0723e-01, 1.8629e-02, 9.2743e-07, 1.6403e-07, 9.1866e-02,
        6.8702e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.047

[Epoch: 62, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.1893e-09, 6.3203e-10, 2.5094e-07, 1.0000e+00, 1.6995e-08, 2.6055e-07,
        1.6553e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 62, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9712e-01, 5.0793e-03, 2.8465e-01, 1.4013e-05, 8.3391e-03, 4.7911e-03,
        3.0786e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 62, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.0050, 0.0051, 0.9636, 0.0047, 0.0055, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 63, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8279e-01, 1.0266e-07, 3.7646e-01, 3.3990e-05, 1.1627e-07, 3.4071e-01,
        1.1117e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 63, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.5161e-02, 1.6532e-01, 1.5780e-02, 9.8169e-07, 1.0465e-07, 6.5493e-02,
        6.8824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 63, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.0113e-09, 1.7466e-09, 3.4027e-07, 1.0000e+00, 3.0432e-08, 2.4558e-07,
        1.8290e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 63, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9094e-01, 5.6723e-03, 2.9314e-01, 2.5198e-05, 5.1114e-03, 5.1101e-03,
        2.1896e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 63, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0041, 0.0048, 0.9674, 0.0052, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 64, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8182e-01, 1.1966e-07, 3.7015e-01, 4.0633e-05, 1.8876e-07, 3.4799e-01,
        6.7271e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 64, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0370e-01, 9.4752e-02, 1.9218e-02, 1.9261e-06, 2.8333e-07, 1.1682e-01,
        6.6551e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.053

[Epoch: 64, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1408e-08, 1.6928e-09, 4.3302e-07, 1.0000e+00, 4.3636e-08, 3.0408e-07,
        1.2335e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 64, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2436e-01, 6.0524e-03, 2.5448e-01, 3.6507e-05, 9.9296e-03, 5.1338e-03,
        1.9441e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 64, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0129, 0.0053, 0.0054, 0.9593, 0.0048, 0.0061, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 65, batch: 44/222] total loss per batch: 0.620
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6538e-01, 1.9422e-07, 3.9696e-01, 5.9953e-05, 2.1510e-07, 3.3760e-01,
        1.7527e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.924

[Epoch: 65, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.2907e-02, 1.5408e-01, 1.7087e-02, 1.6192e-06, 1.6455e-07, 4.8872e-02,
        7.1705e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.054

[Epoch: 65, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.4831e-08, 1.0050e-08, 1.1343e-06, 1.0000e+00, 7.5150e-08, 1.1493e-06,
        2.5682e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 65, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.1727e-01, 6.2682e-03, 3.6530e-01, 3.4200e-05, 5.7959e-03, 5.3316e-03,
        4.3834e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.004

[Epoch: 65, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0041, 0.0047, 0.9687, 0.0048, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 66, batch: 44/222] total loss per batch: 0.620
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9636e-01, 1.4999e-07, 3.5794e-01, 6.2046e-05, 2.2389e-07, 3.4563e-01,
        1.2421e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.961

[Epoch: 66, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.9684e-02, 1.1322e-01, 1.5174e-02, 2.8062e-06, 3.1877e-07, 1.0692e-01,
        6.6500e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 66, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4296e-08, 4.2208e-09, 7.7170e-07, 1.0000e+00, 3.3476e-08, 4.5202e-07,
        1.4015e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 66, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8604e-01, 4.8804e-03, 1.9365e-01, 4.5846e-05, 9.9705e-03, 5.4137e-03,
        2.0009e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 66, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0132, 0.0046, 0.0052, 0.9622, 0.0051, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 67, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8564e-01, 2.0775e-07, 3.5384e-01, 9.0784e-05, 5.2169e-07, 3.6043e-01,
        2.7573e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 67, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.8535e-02, 1.4153e-01, 1.4554e-02, 2.2370e-06, 2.5420e-07, 7.1215e-02,
        7.1416e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 67, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2310e-08, 4.6686e-09, 8.6875e-07, 1.0000e+00, 4.8896e-08, 6.1858e-07,
        4.7341e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 67, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7890e-01, 3.9589e-03, 3.0606e-01, 2.5976e-05, 6.4558e-03, 4.5934e-03,
        6.0910e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 67, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0036, 0.0052, 0.9671, 0.0049, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 68, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6074e-01, 2.4598e-07, 4.0093e-01, 5.2667e-05, 9.6356e-07, 3.3828e-01,
        2.8733e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.968

[Epoch: 68, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0153e-01, 1.2185e-01, 2.0042e-02, 3.9766e-06, 3.1899e-07, 8.3791e-02,
        6.7279e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 68, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4846e-08, 7.1566e-09, 8.6221e-07, 1.0000e+00, 2.6140e-08, 8.5397e-07,
        1.2330e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 68, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2676e-01, 5.2456e-03, 2.5033e-01, 7.7512e-05, 1.1230e-02, 6.3439e-03,
        6.1750e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 68, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0125, 0.0052, 0.0046, 0.9621, 0.0055, 0.0056, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 69, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7825e-01, 1.6901e-07, 3.6408e-01, 4.3453e-05, 6.0478e-07, 3.5762e-01,
        1.7397e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 69, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.5186e-02, 1.2543e-01, 1.4429e-02, 5.7122e-06, 2.3849e-07, 8.0671e-02,
        7.2428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 69, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.2401e-08, 8.5682e-09, 7.0932e-07, 1.0000e+00, 2.4052e-08, 5.1968e-07,
        4.3043e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 69, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7916e-01, 5.1235e-03, 3.0483e-01, 4.7279e-05, 6.4530e-03, 4.3844e-03,
        3.1922e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 69, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0040, 0.0046, 0.9672, 0.0050, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 70, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9292e-01, 2.4104e-07, 3.7116e-01, 6.4976e-05, 6.7569e-07, 3.3585e-01,
        2.3098e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.967

[Epoch: 70, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3868e-02, 1.4800e-01, 1.8360e-02, 3.7693e-06, 3.6528e-07, 8.1653e-02,
        6.5811e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 70, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.1260e-08, 4.5263e-09, 1.0838e-06, 1.0000e+00, 4.0813e-08, 5.5515e-07,
        1.8037e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 70, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1854e-01, 5.0628e-03, 2.6440e-01, 9.6941e-05, 6.9533e-03, 4.9468e-03,
        7.8581e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 70, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0048, 0.0049, 0.9641, 0.0050, 0.0062, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 71, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8045e-01, 1.6065e-07, 3.8361e-01, 2.6098e-05, 8.2253e-07, 3.3592e-01,
        1.6470e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 71, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.4179e-02, 8.9645e-02, 1.7548e-02, 3.3972e-06, 3.4824e-07, 6.9586e-02,
        7.5904e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.051

[Epoch: 71, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.5523e-08, 5.1804e-09, 3.5019e-07, 1.0000e+00, 5.8761e-08, 2.3439e-07,
        1.7613e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 71, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5378e-01, 5.6734e-03, 3.2725e-01, 4.4347e-05, 6.6126e-03, 6.6359e-03,
        3.6005e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 71, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0119, 0.0043, 0.0045, 0.9629, 0.0051, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 72, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5934e-01, 1.4236e-07, 3.8162e-01, 1.9186e-05, 6.3098e-07, 3.5901e-01,
        1.8777e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.967

[Epoch: 72, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0020e-01, 1.7336e-01, 1.9226e-02, 4.8311e-06, 4.6471e-07, 1.0881e-01,
        5.9839e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.072

[Epoch: 72, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.0741e-07, 8.6456e-09, 1.4117e-06, 1.0000e+00, 8.8576e-08, 1.0542e-06,
        8.0750e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 72, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.6223e-01, 5.0142e-03, 2.2071e-01, 5.4628e-05, 7.0923e-03, 4.8847e-03,
        5.7610e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 72, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0055, 0.0059, 0.9629, 0.0049, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 73, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0240e-01, 4.6941e-07, 3.5071e-01, 3.9763e-05, 8.0629e-07, 3.4685e-01,
        2.3713e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.929

[Epoch: 73, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.4251e-02, 9.4705e-02, 1.5184e-02, 4.1891e-06, 4.0235e-07, 5.3791e-02,
        7.6206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 73, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.0660e-08, 1.0206e-08, 1.4158e-06, 1.0000e+00, 8.8422e-08, 8.8324e-07,
        1.3355e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 73, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.3677e-01, 9.4071e-03, 3.4099e-01, 6.2479e-05, 7.2425e-03, 5.5218e-03,
        5.6394e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 73, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0084, 0.0034, 0.0042, 0.9694, 0.0048, 0.0050, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 74, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7901e-01, 1.3476e-07, 3.7234e-01, 1.1883e-05, 7.0115e-07, 3.4864e-01,
        3.4280e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.980

[Epoch: 74, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([5.9139e-02, 1.4298e-01, 2.2102e-02, 1.0354e-05, 1.0423e-06, 9.2103e-02,
        6.8367e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.076

[Epoch: 74, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.9998e-08, 5.4020e-09, 1.2938e-06, 1.0000e+00, 4.7515e-08, 5.9349e-07,
        6.7503e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 74, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4422e-01, 3.6223e-03, 2.4009e-01, 7.8593e-05, 6.9437e-03, 5.0441e-03,
        1.0874e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 74, batch: 220/222] total loss per batch: 0.595
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0053, 0.0052, 0.9660, 0.0052, 0.0039, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 75, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9263e-01, 6.0468e-07, 3.6234e-01, 4.8184e-05, 1.0639e-06, 3.4497e-01,
        3.1694e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.910

[Epoch: 75, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3419e-02, 1.1211e-01, 1.5134e-02, 3.2568e-06, 4.0849e-07, 8.0020e-02,
        6.9931e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.072

[Epoch: 75, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.8549e-08, 9.2974e-09, 1.3256e-06, 1.0000e+00, 9.1377e-08, 7.1106e-07,
        2.7877e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 75, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8599e-01, 5.0134e-03, 2.9792e-01, 5.5204e-05, 6.1003e-03, 4.9187e-03,
        4.8891e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 75, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0044, 0.0053, 0.9663, 0.0054, 0.0051, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 76, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8545e-01, 1.4514e-07, 3.6048e-01, 1.9654e-05, 7.8577e-07, 3.5405e-01,
        6.0691e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.963

[Epoch: 76, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4751e-02, 1.4097e-01, 1.5489e-02, 1.1554e-05, 1.1560e-06, 7.0933e-02,
        6.8784e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 76, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1453e-07, 8.1216e-09, 2.1527e-06, 1.0000e+00, 5.8120e-08, 6.4929e-07,
        8.9940e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 76, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6575e-01, 5.6730e-03, 3.1404e-01, 7.9176e-05, 7.3376e-03, 7.1139e-03,
        2.3587e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 76, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0053, 0.0050, 0.9619, 0.0051, 0.0061, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 77, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7460e-01, 2.0894e-07, 3.8117e-01, 2.6926e-05, 1.7437e-06, 3.4420e-01,
        1.8642e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.967

[Epoch: 77, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.4406e-02, 1.1396e-01, 2.1945e-02, 5.3949e-06, 1.0384e-06, 8.9567e-02,
        7.0011e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.087

[Epoch: 77, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.5064e-08, 7.4046e-09, 2.0235e-06, 1.0000e+00, 1.9811e-08, 3.3648e-07,
        6.0818e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 77, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.7027e-01, 4.9478e-03, 2.1425e-01, 1.0635e-04, 5.4264e-03, 4.9878e-03,
        7.9782e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 77, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0046, 0.0053, 0.9633, 0.0051, 0.0058, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 78, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7313e-01, 1.0484e-07, 3.8218e-01, 1.7905e-05, 7.2652e-07, 3.4467e-01,
        2.6868e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 78, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.0781e-01, 1.2650e-01, 1.4267e-02, 1.1072e-05, 6.9808e-07, 6.8990e-02,
        6.8242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 78, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5915e-07, 1.0962e-08, 3.3144e-06, 9.9999e-01, 8.7304e-08, 1.5333e-06,
        2.9402e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 78, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5588e-01, 4.8370e-03, 3.2697e-01, 6.3836e-05, 6.4827e-03, 5.7723e-03,
        3.3099e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 78, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0051, 0.0059, 0.9643, 0.0053, 0.0062, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 79, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7241e-01, 2.1628e-07, 3.7655e-01, 2.8314e-05, 2.5523e-06, 3.5100e-01,
        3.6892e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 79, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.1622e-02, 1.3526e-01, 2.0415e-02, 8.3642e-06, 7.5151e-07, 8.8723e-02,
        7.1397e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 79, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.7754e-07, 9.6775e-09, 2.8697e-06, 1.0000e+00, 5.5704e-08, 7.5578e-07,
        5.6898e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 79, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1216e-01, 5.4697e-03, 2.6694e-01, 6.5211e-05, 8.3344e-03, 7.0306e-03,
        3.0424e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 79, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0135, 0.0054, 0.0048, 0.9599, 0.0052, 0.0049, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 80, batch: 44/222] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6462e-01, 1.1100e-07, 3.7348e-01, 1.9551e-05, 1.9073e-06, 3.6188e-01,
        3.2816e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 80, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([1.3679e-01, 1.0537e-01, 1.6874e-02, 4.6095e-06, 5.6445e-07, 7.6187e-02,
        6.6478e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 80, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1561e-07, 2.2641e-08, 7.9389e-06, 9.9999e-01, 3.0701e-07, 1.4273e-06,
        3.0147e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 80, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9329e-01, 4.9469e-03, 2.9156e-01, 5.4749e-05, 5.9736e-03, 4.1751e-03,
        2.4044e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 80, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0050, 0.0062, 0.9632, 0.0047, 0.0063, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 81, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9396e-01, 2.2300e-07, 3.5929e-01, 4.4781e-05, 2.7246e-06, 3.4670e-01,
        3.2593e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.949

[Epoch: 81, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.2016e-02, 1.5811e-01, 2.1025e-02, 1.1001e-05, 1.1394e-06, 8.1089e-02,
        6.7775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.073

[Epoch: 81, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.4968e-08, 6.7370e-10, 1.0665e-06, 1.0000e+00, 1.2699e-08, 3.1136e-07,
        3.7895e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 81, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0557e-01, 6.2207e-03, 2.7394e-01, 6.3090e-05, 8.7641e-03, 5.4369e-03,
        2.7894e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 81, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0055, 0.0050, 0.9622, 0.0060, 0.0057, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 82, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8688e-01, 1.4221e-07, 3.7887e-01, 1.6882e-05, 2.0550e-06, 3.3422e-01,
        1.7586e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.901

[Epoch: 82, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3488e-02, 1.0217e-01, 1.6975e-02, 7.4782e-06, 6.6833e-07, 7.3040e-02,
        7.1432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.074

[Epoch: 82, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4386e-07, 1.6656e-08, 1.6853e-06, 1.0000e+00, 1.5410e-07, 7.9515e-07,
        2.5558e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 82, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8044e-01, 4.0288e-03, 3.0327e-01, 4.2418e-05, 6.1123e-03, 6.1026e-03,
        2.7073e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 82, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0048, 0.0051, 0.9640, 0.0053, 0.0058, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 83, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7160e-01, 2.0794e-07, 3.7802e-01, 3.2565e-05, 2.6018e-06, 3.5034e-01,
        2.9709e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.968

[Epoch: 83, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6161e-02, 1.4610e-01, 1.8898e-02, 1.0648e-05, 1.9283e-06, 7.7923e-02,
        6.8091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 83, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.7434e-08, 2.5472e-09, 1.2248e-06, 1.0000e+00, 3.5347e-08, 3.6235e-07,
        6.5756e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 83, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4754e-01, 4.8016e-03, 2.3471e-01, 9.5090e-05, 7.7834e-03, 5.0636e-03,
        5.6296e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 83, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0117, 0.0053, 0.0068, 0.9580, 0.0057, 0.0073, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 84, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8945e-01, 1.3378e-07, 3.7243e-01, 2.0238e-05, 1.8010e-06, 3.3810e-01,
        3.2692e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.966

[Epoch: 84, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.9932e-02, 1.1049e-01, 1.8139e-02, 9.0203e-06, 6.7767e-07, 9.3483e-02,
        6.8794e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 84, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.6074e-08, 5.6211e-09, 3.0503e-06, 1.0000e+00, 8.6953e-08, 1.4470e-06,
        6.7965e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 84, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6066e-01, 4.9910e-03, 3.2250e-01, 4.5586e-05, 6.1235e-03, 5.6748e-03,
        3.1627e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.028

[Epoch: 84, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0047, 0.0059, 0.9632, 0.0046, 0.0057, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 85, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7082e-01, 2.3383e-07, 3.8265e-01, 3.6698e-05, 2.2003e-06, 3.4649e-01,
        2.5421e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.920

[Epoch: 85, batch: 88/222] total loss per batch: 0.598
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.8437e-02, 1.8104e-01, 1.6263e-02, 3.7958e-06, 1.6572e-06, 8.9324e-02,
        6.2493e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.073

[Epoch: 85, batch: 132/222] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.9169e-08, 1.6294e-08, 4.0517e-06, 9.9999e-01, 6.7772e-08, 2.0891e-06,
        5.7443e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 85, batch: 176/222] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3308e-01, 3.4327e-03, 2.5127e-01, 5.7051e-05, 7.9974e-03, 4.1419e-03,
        1.9347e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 85, batch: 220/222] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0129, 0.0030, 0.0057, 0.9639, 0.0038, 0.0047, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 86, batch: 44/222] total loss per batch: 0.648
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7391e-01, 6.3424e-08, 3.9822e-01, 3.4764e-05, 4.4619e-07, 3.2783e-01,
        3.8518e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.954

[Epoch: 86, batch: 88/222] total loss per batch: 0.641
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([4.6141e-02, 9.2780e-02, 6.7406e-03, 9.5900e-06, 1.0971e-07, 2.6652e-02,
        8.2768e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.029

[Epoch: 86, batch: 132/222] total loss per batch: 0.672
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2890e-08, 1.4860e-09, 4.8899e-07, 1.0000e+00, 8.5633e-09, 1.4164e-07,
        5.1065e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 86, batch: 176/222] total loss per batch: 0.608
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6818e-01, 5.1167e-03, 3.1836e-01, 5.5776e-05, 4.9705e-03, 3.3149e-03,
        4.0830e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 86, batch: 220/222] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0135, 0.0048, 0.0075, 0.9567, 0.0036, 0.0082, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 87, batch: 44/222] total loss per batch: 0.645
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9236e-01, 4.5992e-08, 3.6190e-01, 4.1484e-05, 4.6093e-07, 3.4570e-01,
        3.6480e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.945

[Epoch: 87, batch: 88/222] total loss per batch: 0.614
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.1206e-02, 1.1673e-01, 2.1593e-02, 6.2124e-06, 1.9656e-07, 8.1982e-02,
        7.0848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 87, batch: 132/222] total loss per batch: 0.648
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1356e-08, 3.2075e-09, 3.1199e-07, 1.0000e+00, 1.3345e-08, 2.5785e-07,
        3.2577e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 87, batch: 176/222] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3098e-01, 2.9306e-03, 2.3876e-01, 2.9528e-05, 2.2410e-02, 4.8875e-03,
        4.2149e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 87, batch: 220/222] total loss per batch: 0.602
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.0041, 0.0052, 0.9687, 0.0053, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 88, batch: 44/222] total loss per batch: 0.626
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8417e-01, 9.3196e-08, 3.6442e-01, 3.0653e-05, 2.7569e-07, 3.5137e-01,
        8.1242e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.964

[Epoch: 88, batch: 88/222] total loss per batch: 0.597
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4225e-02, 1.5386e-01, 1.6447e-02, 1.0540e-05, 6.0113e-07, 7.8945e-02,
        6.6651e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 88, batch: 132/222] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2657e-08, 1.6244e-09, 1.4537e-06, 1.0000e+00, 3.3491e-08, 5.0462e-07,
        2.4532e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 88, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1454e-01, 2.9975e-03, 2.6396e-01, 3.5941e-05, 1.2414e-02, 6.0436e-03,
        5.9548e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 88, batch: 220/222] total loss per batch: 0.595
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0042, 0.0049, 0.9674, 0.0046, 0.0054, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 89, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0088e-01, 1.0609e-07, 3.5604e-01, 2.8749e-05, 5.7645e-07, 3.4305e-01,
        2.4335e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 89, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8274e-02, 1.3382e-01, 1.6997e-02, 1.0333e-05, 4.2046e-07, 8.6419e-02,
        6.8448e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 89, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.9932e-08, 8.5754e-10, 1.0594e-06, 1.0000e+00, 2.1403e-08, 4.3401e-07,
        3.1756e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 89, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2805e-01, 4.5498e-03, 3.4740e-01, 5.4776e-05, 1.2848e-02, 7.0864e-03,
        8.6496e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 89, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0045, 0.0052, 0.9667, 0.0046, 0.0055, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 90, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8822e-01, 1.0035e-07, 3.6512e-01, 2.4102e-05, 5.1801e-07, 3.4663e-01,
        1.6831e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.953

[Epoch: 90, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.9373e-02, 1.2388e-01, 1.7723e-02, 7.3674e-06, 3.5596e-07, 8.3641e-02,
        6.9538e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 90, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6810e-08, 8.1203e-10, 8.9775e-07, 1.0000e+00, 1.7035e-08, 3.6244e-07,
        2.9260e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 90, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5092e-01, 2.9012e-03, 2.3009e-01, 2.5271e-05, 1.0716e-02, 5.3379e-03,
        5.6785e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 90, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0045, 0.0051, 0.9665, 0.0048, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 91, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8351e-01, 7.9789e-08, 3.6750e-01, 2.3937e-05, 4.7020e-07, 3.4897e-01,
        1.3477e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.948

[Epoch: 91, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0555e-02, 1.2620e-01, 1.7860e-02, 6.9607e-06, 3.3016e-07, 8.2606e-02,
        6.9277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 91, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4863e-08, 5.8892e-10, 7.2915e-07, 1.0000e+00, 1.4445e-08, 2.9301e-07,
        2.5679e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 91, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9047e-01, 3.0754e-03, 2.9114e-01, 2.0617e-05, 9.8294e-03, 5.4568e-03,
        6.0048e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 91, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0045, 0.0053, 0.9658, 0.0049, 0.0058, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 92, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8144e-01, 6.5579e-08, 3.7231e-01, 2.0656e-05, 3.8731e-07, 3.4623e-01,
        1.1519e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 92, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1091e-02, 1.2324e-01, 1.8521e-02, 5.9005e-06, 2.7478e-07, 8.4620e-02,
        6.9253e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 92, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2435e-08, 4.9496e-10, 6.6981e-07, 1.0000e+00, 1.1122e-08, 2.5301e-07,
        2.1225e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 92, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0232e-01, 3.1255e-03, 2.8031e-01, 1.6948e-05, 8.9604e-03, 5.2577e-03,
        5.5643e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 92, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0045, 0.0053, 0.9655, 0.0048, 0.0059, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 93, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8180e-01, 5.6232e-08, 3.7145e-01, 1.9687e-05, 3.4764e-07, 3.4673e-01,
        9.7638e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.941

[Epoch: 93, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1662e-02, 1.2304e-01, 1.8650e-02, 4.8903e-06, 2.2038e-07, 8.0367e-02,
        6.9627e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 93, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1191e-08, 4.1189e-10, 4.7449e-07, 1.0000e+00, 9.4262e-09, 2.1145e-07,
        1.8424e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 93, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9640e-01, 3.1523e-03, 2.8748e-01, 1.3284e-05, 7.8685e-03, 5.0854e-03,
        4.7457e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 93, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0044, 0.0053, 0.9652, 0.0050, 0.0060, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 94, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8034e-01, 4.6302e-08, 3.7468e-01, 1.7133e-05, 2.7311e-07, 3.4497e-01,
        9.3334e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.941

[Epoch: 94, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3338e-02, 1.2545e-01, 1.8749e-02, 4.5476e-06, 2.2055e-07, 8.4671e-02,
        6.8779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 94, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.2264e-09, 3.4540e-10, 4.5570e-07, 1.0000e+00, 6.5598e-09, 1.5206e-07,
        1.4523e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 94, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0707e-01, 3.3209e-03, 2.7707e-01, 1.3300e-05, 7.3735e-03, 5.1504e-03,
        4.6571e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 94, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0045, 0.0052, 0.9650, 0.0049, 0.0059, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 95, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8049e-01, 3.7331e-08, 3.7089e-01, 1.4812e-05, 2.6148e-07, 3.4861e-01,
        7.1895e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 95, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1080e-02, 1.2146e-01, 1.9808e-02, 3.6665e-06, 1.8000e-07, 7.8380e-02,
        6.9927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 95, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.3214e-09, 2.5009e-10, 3.1582e-07, 1.0000e+00, 5.2445e-09, 1.5198e-07,
        1.3783e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 95, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8649e-01, 3.3395e-03, 2.9805e-01, 9.6911e-06, 6.9888e-03, 5.1195e-03,
        4.2354e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 95, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0044, 0.0055, 0.9646, 0.0050, 0.0062, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 96, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7778e-01, 3.8189e-08, 3.8241e-01, 1.5225e-05, 2.3036e-07, 3.3980e-01,
        8.1470e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 96, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0256e-02, 1.2871e-01, 1.8856e-02, 3.6513e-06, 1.6688e-07, 8.5268e-02,
        6.8690e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.067

[Epoch: 96, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.8579e-09, 3.2102e-10, 2.9898e-07, 1.0000e+00, 5.2257e-09, 1.1989e-07,
        9.1140e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 96, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1280e-01, 3.5682e-03, 2.7211e-01, 1.1686e-05, 6.6981e-03, 4.8009e-03,
        4.5800e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 96, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0044, 0.0049, 0.9658, 0.0048, 0.0058, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 97, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8046e-01, 2.6284e-08, 3.6127e-01, 9.9576e-06, 1.6205e-07, 3.5826e-01,
        5.4547e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.943

[Epoch: 97, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6698e-02, 1.2186e-01, 1.8258e-02, 3.2503e-06, 2.7548e-07, 8.4325e-02,
        6.8886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 97, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.0510e-08, 3.4452e-10, 4.5689e-07, 1.0000e+00, 4.8928e-09, 1.8788e-07,
        1.5982e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 97, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8806e-01, 3.7620e-03, 2.9583e-01, 9.7664e-06, 6.6255e-03, 5.7070e-03,
        4.0290e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 97, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0046, 0.0060, 0.9632, 0.0052, 0.0065, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 98, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7098e-01, 5.1580e-08, 3.9465e-01, 1.8432e-05, 2.5156e-07, 3.3435e-01,
        1.0384e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.930

[Epoch: 98, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.0381e-02, 1.2203e-01, 2.1363e-02, 4.6874e-06, 1.9093e-07, 7.4207e-02,
        7.1201e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 98, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4216e-08, 4.5889e-10, 4.5913e-07, 1.0000e+00, 7.2694e-09, 1.9679e-07,
        9.7352e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 98, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9488e-01, 4.1258e-03, 2.8966e-01, 2.1677e-05, 6.5454e-03, 4.7524e-03,
        6.4006e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 98, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.0041, 0.0047, 0.9655, 0.0051, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.008

[Epoch: 99, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8157e-01, 5.3058e-08, 3.6209e-01, 1.3449e-05, 3.7781e-07, 3.5633e-01,
        6.7675e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.953

[Epoch: 99, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6534e-02, 1.2997e-01, 1.6137e-02, 5.2403e-06, 5.3656e-07, 1.0201e-01,
        6.6534e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 99, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.9215e-08, 3.4937e-10, 8.1630e-07, 1.0000e+00, 8.8218e-09, 1.9336e-07,
        3.3573e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 99, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3154e-01, 3.8491e-03, 2.5146e-01, 2.3514e-05, 7.6603e-03, 5.4614e-03,
        6.4289e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 99, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0040, 0.0063, 0.9648, 0.0061, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 100, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8756e-01, 2.0816e-07, 3.7216e-01, 3.0282e-05, 7.1611e-07, 3.4025e-01,
        2.7144e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.919

[Epoch: 100, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.3656e-02, 1.1061e-01, 2.1976e-02, 5.6789e-06, 1.0661e-07, 6.6833e-02,
        7.2692e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.074

[Epoch: 100, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.0318e-08, 3.0947e-09, 7.8181e-07, 1.0000e+00, 1.0046e-08, 3.5468e-07,
        8.0062e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 100, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2726e-01, 3.9249e-03, 3.5747e-01, 1.8093e-05, 6.0179e-03, 5.3050e-03,
        6.0805e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 100, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0049, 0.0056, 0.9648, 0.0051, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 101, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6765e-01, 5.5518e-08, 3.8838e-01, 1.2204e-05, 5.7049e-07, 3.4396e-01,
        1.4764e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 101, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3305e-02, 1.4390e-01, 2.1040e-02, 5.9342e-06, 5.3826e-07, 9.1712e-02,
        6.6003e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.067

[Epoch: 101, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.5621e-08, 1.4027e-09, 1.1225e-06, 1.0000e+00, 1.6278e-08, 2.8897e-07,
        2.6529e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 101, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.1029e-01, 3.2968e-03, 1.7623e-01, 3.0226e-05, 5.7523e-03, 4.3957e-03,
        8.0996e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 101, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0048, 0.0054, 0.9639, 0.0050, 0.0062, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 102, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8074e-01, 9.4561e-08, 3.6294e-01, 2.1044e-05, 7.5469e-07, 3.5629e-01,
        1.4479e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 102, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8141e-02, 1.1979e-01, 1.9698e-02, 5.3484e-06, 2.3802e-07, 7.1122e-02,
        7.1125e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 102, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.5290e-08, 1.5086e-09, 1.3988e-06, 1.0000e+00, 1.8303e-08, 4.3748e-07,
        3.5219e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 102, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.3471e-01, 4.5217e-03, 3.4896e-01, 2.1698e-05, 5.7299e-03, 6.0484e-03,
        8.5128e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 102, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0045, 0.0048, 0.9649, 0.0054, 0.0058, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 103, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7875e-01, 8.5726e-08, 3.7848e-01, 1.5248e-05, 7.3407e-07, 3.4275e-01,
        1.3608e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 103, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.2930e-02, 1.2366e-01, 1.9218e-02, 5.2398e-06, 3.4704e-07, 8.5862e-02,
        6.8832e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.072

[Epoch: 103, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.8801e-08, 1.1035e-09, 8.0182e-07, 1.0000e+00, 1.1872e-08, 2.7110e-07,
        1.8485e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 103, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4133e-01, 4.1508e-03, 2.4223e-01, 1.5240e-05, 6.3639e-03, 5.9030e-03,
        6.4580e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 103, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0048, 0.0052, 0.9648, 0.0052, 0.0058, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 104, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8289e-01, 8.0480e-08, 3.7798e-01, 1.1555e-05, 7.9707e-07, 3.3913e-01,
        1.4283e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.926

[Epoch: 104, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0368e-02, 1.2510e-01, 1.9234e-02, 4.4649e-06, 2.3100e-07, 8.0682e-02,
        6.9461e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.070

[Epoch: 104, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1606e-08, 7.7384e-10, 7.8781e-07, 1.0000e+00, 9.7164e-09, 2.2478e-07,
        2.1565e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 104, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6991e-01, 4.7325e-03, 3.1275e-01, 1.6023e-05, 6.2422e-03, 6.3387e-03,
        6.4107e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 104, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.0045, 0.0052, 0.9635, 0.0055, 0.0058, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 105, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8528e-01, 6.5690e-08, 3.6508e-01, 9.7321e-06, 3.3701e-07, 3.4962e-01,
        6.6594e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 105, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0910e-02, 1.2015e-01, 1.9495e-02, 3.9536e-06, 2.4805e-07, 7.7172e-02,
        7.0227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.067

[Epoch: 105, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.5611e-08, 1.3801e-09, 1.1816e-06, 1.0000e+00, 1.1282e-08, 2.9269e-07,
        1.6586e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 105, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2177e-01, 4.7802e-03, 2.6221e-01, 1.2718e-05, 5.6835e-03, 5.5388e-03,
        4.4773e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 105, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0047, 0.0059, 0.9630, 0.0056, 0.0062, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 106, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6880e-01, 6.0903e-08, 3.8275e-01, 1.0729e-05, 6.3862e-07, 3.4844e-01,
        9.9005e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.943

[Epoch: 106, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6282e-02, 1.2761e-01, 2.0414e-02, 5.1054e-06, 3.2637e-07, 8.0136e-02,
        6.9555e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 106, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.0475e-08, 5.6231e-10, 8.1599e-07, 1.0000e+00, 7.4179e-09, 1.2657e-07,
        3.7186e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 106, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7115e-01, 4.7160e-03, 3.1169e-01, 1.1980e-05, 6.3750e-03, 6.0474e-03,
        6.8263e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 106, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0049, 0.0057, 0.9629, 0.0056, 0.0061, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 107, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9391e-01, 5.9456e-08, 3.6286e-01, 1.0547e-05, 3.4437e-07, 3.4322e-01,
        7.8033e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.929

[Epoch: 107, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6698e-02, 1.2834e-01, 2.2870e-02, 5.0691e-06, 3.6087e-07, 8.7578e-02,
        6.7451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 107, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.7541e-08, 2.0717e-09, 1.2171e-06, 1.0000e+00, 1.2093e-08, 3.3582e-07,
        7.8130e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 107, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0800e-01, 5.2826e-03, 2.7574e-01, 1.8232e-05, 5.3953e-03, 5.5561e-03,
        4.2693e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 107, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0043, 0.0049, 0.9643, 0.0052, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 108, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6508e-01, 8.2762e-08, 3.8175e-01, 9.2491e-06, 1.8950e-06, 3.5316e-01,
        9.7809e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.927

[Epoch: 108, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.9583e-02, 1.0953e-01, 1.4262e-02, 5.0016e-06, 3.3621e-07, 7.0827e-02,
        7.3579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 108, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.3294e-08, 8.7916e-10, 6.4233e-07, 1.0000e+00, 8.8086e-09, 1.3591e-07,
        1.5189e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 108, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3933e-01, 5.2183e-03, 2.4229e-01, 2.0736e-05, 7.1717e-03, 5.9563e-03,
        1.3070e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 108, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0078, 0.0045, 0.0058, 0.9649, 0.0055, 0.0058, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 109, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9298e-01, 1.1733e-07, 3.6968e-01, 3.8271e-05, 4.7319e-07, 3.3731e-01,
        1.3381e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.950

[Epoch: 109, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.8311e-02, 1.5373e-01, 2.1635e-02, 1.2025e-05, 7.6214e-07, 9.0714e-02,
        6.3560e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 109, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.5091e-08, 1.2478e-08, 2.9694e-06, 1.0000e+00, 2.2832e-08, 1.1777e-06,
        5.7685e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 109, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6418e-01, 6.1744e-03, 3.1954e-01, 5.3268e-05, 4.3708e-03, 5.6864e-03,
        2.3243e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 109, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0157, 0.0056, 0.0064, 0.9539, 0.0061, 0.0071, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 110, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6086e-01, 2.0511e-07, 3.8233e-01, 2.5708e-05, 2.6205e-06, 3.5678e-01,
        1.3525e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.899

[Epoch: 110, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3762e-02, 1.1977e-01, 1.8884e-02, 8.6948e-06, 7.6562e-07, 6.8217e-02,
        7.0936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.056

[Epoch: 110, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.8182e-08, 2.2960e-09, 1.0500e-06, 1.0000e+00, 1.4508e-08, 4.5449e-07,
        1.7239e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 110, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4013e-01, 4.3814e-03, 2.4171e-01, 2.5740e-05, 8.4832e-03, 5.2565e-03,
        1.3898e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.018

[Epoch: 110, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0043, 0.0055, 0.9654, 0.0052, 0.0053, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 111, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9527e-01, 1.2578e-07, 3.6210e-01, 2.0206e-05, 9.6738e-07, 3.4261e-01,
        1.8721e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.979

[Epoch: 111, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6119e-02, 1.1741e-01, 1.8144e-02, 5.9558e-06, 7.8347e-07, 8.7164e-02,
        7.0116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 111, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2575e-07, 1.0518e-08, 4.2949e-06, 9.9999e-01, 5.8188e-08, 6.0453e-07,
        2.8619e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.992

[Epoch: 111, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8908e-01, 4.6906e-03, 2.9539e-01, 2.7678e-05, 6.2148e-03, 4.6002e-03,
        3.5967e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 111, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0041, 0.0048, 0.9680, 0.0042, 0.0054, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 112, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8789e-01, 1.3467e-07, 3.5704e-01, 1.5849e-05, 1.5351e-06, 3.5505e-01,
        1.1475e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.933

[Epoch: 112, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.1075e-02, 1.4419e-01, 2.1804e-02, 7.6059e-06, 4.1691e-07, 7.3397e-02,
        6.6952e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 112, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.7355e-08, 3.7698e-09, 1.3733e-06, 1.0000e+00, 3.4806e-08, 4.1837e-07,
        1.5539e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 112, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.3004e-01, 5.3066e-03, 3.4916e-01, 2.9041e-05, 1.0024e-02, 5.4265e-03,
        1.1844e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 112, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0044, 0.0044, 0.9675, 0.0050, 0.0055, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 113, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7316e-01, 2.6375e-07, 3.6899e-01, 2.9860e-05, 1.3779e-06, 3.5782e-01,
        2.3287e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.955

[Epoch: 113, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.1363e-02, 1.0369e-01, 1.8542e-02, 7.3386e-06, 5.8615e-07, 8.2847e-02,
        7.3355e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 113, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9517e-08, 2.6588e-09, 2.6039e-06, 1.0000e+00, 2.4395e-08, 6.0455e-07,
        1.3654e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 113, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.6454e-01, 2.8652e-03, 2.2093e-01, 1.5374e-05, 6.8809e-03, 4.7624e-03,
        3.2259e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 113, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0042, 0.0059, 0.9656, 0.0055, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 114, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8230e-01, 1.0789e-07, 3.8478e-01, 1.8767e-05, 9.8046e-07, 3.3290e-01,
        1.6707e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.944

[Epoch: 114, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6881e-02, 1.3678e-01, 1.9375e-02, 8.7142e-06, 4.1198e-07, 7.9311e-02,
        6.7764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 114, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9692e-08, 2.5574e-09, 2.2693e-06, 1.0000e+00, 1.6942e-08, 2.8297e-07,
        2.0062e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 114, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6932e-01, 3.3424e-03, 3.1401e-01, 1.3978e-05, 7.4285e-03, 5.8748e-03,
        8.0222e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 114, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0132, 0.0045, 0.0052, 0.9601, 0.0056, 0.0062, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 115, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7355e-01, 2.7028e-07, 3.8384e-01, 2.3707e-05, 8.9974e-07, 3.4259e-01,
        2.0530e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.940

[Epoch: 115, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7730e-02, 1.1885e-01, 2.0026e-02, 7.3756e-06, 4.5633e-07, 8.0374e-02,
        7.0301e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 115, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.7256e-08, 4.6037e-09, 1.8845e-06, 1.0000e+00, 2.4591e-08, 3.4831e-07,
        3.3981e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 115, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9991e-01, 4.7978e-03, 2.8199e-01, 2.5605e-05, 8.2845e-03, 4.9850e-03,
        3.8995e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 115, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0043, 0.0056, 0.9664, 0.0051, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 116, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8887e-01, 1.3361e-07, 3.6709e-01, 1.1637e-05, 8.7967e-07, 3.4403e-01,
        1.5149e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.944

[Epoch: 116, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7216e-02, 1.2847e-01, 1.9521e-02, 8.9774e-06, 4.0461e-07, 8.4013e-02,
        6.9077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 116, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.9543e-08, 2.3420e-09, 1.4995e-06, 1.0000e+00, 2.0913e-08, 2.6233e-07,
        1.9214e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 116, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2738e-01, 4.1557e-03, 2.5847e-01, 1.4326e-05, 5.4112e-03, 4.5572e-03,
        9.3621e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 116, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.0045, 0.0056, 0.9633, 0.0052, 0.0059, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 117, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6141e-01, 2.3077e-07, 3.8683e-01, 1.9166e-05, 1.3126e-06, 3.5174e-01,
        1.7448e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 117, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6776e-02, 1.1754e-01, 1.8817e-02, 5.1748e-06, 2.4850e-07, 7.7780e-02,
        6.9908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.073

[Epoch: 117, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.1201e-08, 4.4777e-09, 2.8045e-06, 1.0000e+00, 1.2455e-08, 3.2685e-07,
        2.6745e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 117, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5869e-01, 4.4974e-03, 3.2621e-01, 1.9258e-05, 5.5026e-03, 5.0822e-03,
        6.2275e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 117, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0047, 0.0056, 0.9641, 0.0050, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 118, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8638e-01, 2.1435e-07, 3.5751e-01, 1.5136e-05, 5.8798e-07, 3.5610e-01,
        1.4236e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.952

[Epoch: 118, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.1083e-02, 1.4507e-01, 1.8856e-02, 1.4097e-05, 4.3881e-07, 8.4071e-02,
        6.8090e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 118, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.0178e-08, 3.9925e-09, 2.2903e-06, 1.0000e+00, 1.3096e-08, 2.5341e-07,
        1.4155e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 118, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3407e-01, 4.8540e-03, 2.4673e-01, 1.6336e-05, 8.4763e-03, 5.8366e-03,
        1.0751e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 118, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0082, 0.0042, 0.0050, 0.9678, 0.0048, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 119, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6916e-01, 3.3334e-07, 3.7655e-01, 1.8876e-05, 1.1420e-06, 3.5427e-01,
        1.9754e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.916

[Epoch: 119, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.2746e-02, 1.1425e-01, 1.9035e-02, 8.5707e-06, 3.5486e-07, 8.3911e-02,
        7.0005e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 119, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.5965e-08, 1.9034e-09, 1.4817e-06, 1.0000e+00, 5.9977e-09, 2.5832e-07,
        2.4182e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 119, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6663e-01, 4.6767e-03, 3.1842e-01, 1.9830e-05, 5.5401e-03, 4.6969e-03,
        8.0436e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 119, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0040, 0.0049, 0.9671, 0.0050, 0.0056, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 120, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9728e-01, 2.7930e-07, 3.6497e-01, 1.4699e-05, 1.7452e-06, 3.3773e-01,
        2.1335e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 120, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.9238e-02, 1.2561e-01, 2.0579e-02, 1.3560e-05, 4.9715e-07, 8.0210e-02,
        6.9435e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 120, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3127e-07, 7.1030e-09, 1.9005e-06, 1.0000e+00, 7.2464e-08, 9.1937e-07,
        2.3775e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 120, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2162e-01, 4.5460e-03, 2.6090e-01, 3.4176e-05, 6.6555e-03, 6.2392e-03,
        4.3172e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 120, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0043, 0.0048, 0.9675, 0.0053, 0.0048, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 121, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6966e-01, 2.5873e-07, 3.7466e-01, 3.9683e-05, 9.8469e-07, 3.5564e-01,
        5.7405e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.967

[Epoch: 121, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3696e-02, 1.2626e-01, 1.7631e-02, 8.6530e-06, 6.0354e-07, 8.0068e-02,
        6.9233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 121, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.9368e-08, 3.9848e-09, 1.5462e-06, 1.0000e+00, 2.3628e-08, 5.0590e-07,
        9.3073e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.993

[Epoch: 121, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1341e-01, 4.4301e-03, 2.7066e-01, 2.3165e-05, 6.9460e-03, 4.5161e-03,
        7.3541e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 121, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0045, 0.0056, 0.9626, 0.0049, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 122, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9727e-01, 2.3589e-07, 3.7234e-01, 1.7246e-05, 1.8513e-06, 3.3038e-01,
        3.3846e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.944

[Epoch: 122, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.7513e-02, 1.1918e-01, 2.0054e-02, 8.6832e-06, 5.6048e-07, 8.2156e-02,
        7.1109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 122, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.9356e-08, 9.4168e-10, 1.4173e-06, 1.0000e+00, 3.8605e-08, 3.6732e-07,
        2.2722e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 122, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8562e-01, 5.5792e-03, 2.9915e-01, 4.3011e-05, 4.7684e-03, 4.8331e-03,
        4.1731e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.002

[Epoch: 122, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.0044, 0.0051, 0.9641, 0.0053, 0.0061, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 123, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5891e-01, 1.5491e-07, 3.8323e-01, 2.0943e-05, 9.2212e-07, 3.5784e-01,
        3.1059e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.943

[Epoch: 123, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3295e-02, 1.2025e-01, 1.7526e-02, 7.4872e-06, 5.3878e-07, 8.2966e-02,
        6.8596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 123, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.8639e-08, 6.0326e-09, 2.4032e-06, 1.0000e+00, 3.2010e-08, 4.4890e-07,
        5.7738e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 123, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2199e-01, 4.2643e-03, 2.6289e-01, 1.9260e-05, 5.6890e-03, 5.1430e-03,
        2.1959e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 123, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0044, 0.0046, 0.9659, 0.0047, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 124, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9367e-01, 1.7773e-07, 3.6697e-01, 1.4435e-05, 1.9147e-06, 3.3934e-01,
        3.4101e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 124, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.9006e-02, 1.3810e-01, 1.4138e-02, 3.7127e-06, 3.1821e-07, 7.2624e-02,
        7.0612e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 124, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.5326e-08, 1.4152e-09, 1.6410e-06, 1.0000e+00, 1.6159e-08, 2.5161e-07,
        2.1282e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 124, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6634e-01, 4.1721e-03, 3.1831e-01, 1.7017e-05, 6.4177e-03, 4.7375e-03,
        4.3989e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 124, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0047, 0.0047, 0.9636, 0.0053, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 125, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.5737e-01, 1.7697e-07, 3.8384e-01, 1.2741e-05, 8.8887e-07, 3.5878e-01,
        2.2646e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 125, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.3118e-02, 1.2320e-01, 2.2008e-02, 5.1424e-06, 7.3782e-07, 8.1060e-02,
        6.8061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 125, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.5185e-08, 3.7911e-09, 1.7414e-06, 1.0000e+00, 2.1358e-08, 3.6429e-07,
        2.8461e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 125, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3537e-01, 5.9239e-03, 2.4755e-01, 2.4335e-05, 6.3944e-03, 4.7231e-03,
        6.1813e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 125, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0042, 0.0049, 0.9667, 0.0050, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 126, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9697e-01, 1.4321e-07, 3.6948e-01, 1.6221e-05, 1.1124e-06, 3.3354e-01,
        1.9848e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.960

[Epoch: 126, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.0057e-02, 1.4250e-01, 1.8319e-02, 5.5774e-06, 5.4802e-07, 8.1836e-02,
        6.8728e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 126, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.8892e-08, 1.7666e-09, 2.1423e-06, 1.0000e+00, 1.5430e-08, 5.5919e-07,
        5.4322e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 126, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6273e-01, 3.9216e-03, 3.2246e-01, 1.6039e-05, 5.0266e-03, 5.8390e-03,
        9.2067e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 126, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0039, 0.0045, 0.9686, 0.0049, 0.0051, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 127, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8572e-01, 1.4339e-07, 3.7296e-01, 1.4713e-05, 7.8882e-07, 3.4131e-01,
        2.0822e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.921

[Epoch: 127, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0257e-02, 1.1833e-01, 1.9960e-02, 6.3354e-06, 6.2308e-07, 6.9429e-02,
        7.1202e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 127, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.5475e-08, 3.0726e-09, 1.4955e-06, 1.0000e+00, 9.6140e-09, 2.1763e-07,
        5.1419e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 127, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3310e-01, 5.8852e-03, 2.4970e-01, 2.0074e-05, 6.5057e-03, 4.7814e-03,
        3.5687e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 127, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0120, 0.0053, 0.0057, 0.9594, 0.0059, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 128, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7421e-01, 1.2960e-07, 3.7516e-01, 2.3762e-05, 1.6566e-06, 3.5060e-01,
        3.9108e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.949

[Epoch: 128, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.4084e-02, 1.4095e-01, 1.9007e-02, 8.7521e-06, 9.9746e-07, 9.5069e-02,
        6.5088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.073

[Epoch: 128, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.0439e-08, 1.6918e-09, 1.9650e-06, 1.0000e+00, 2.0451e-08, 4.0616e-07,
        3.1432e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 128, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6454e-01, 6.1665e-03, 3.1459e-01, 1.2981e-05, 8.7846e-03, 5.9079e-03,
        3.3261e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 128, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0044, 0.0054, 0.9641, 0.0054, 0.0058, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 129, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7297e-01, 1.1730e-07, 3.8441e-01, 1.0627e-05, 5.3616e-07, 3.4260e-01,
        2.3879e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.950

[Epoch: 129, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.0839e-02, 1.0068e-01, 1.6948e-02, 7.7719e-06, 8.5882e-07, 6.3095e-02,
        7.5843e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.054

[Epoch: 129, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.3033e-08, 5.7838e-09, 1.4651e-06, 1.0000e+00, 1.8002e-08, 3.7543e-07,
        1.0482e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 129, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1628e-01, 3.8773e-03, 2.7024e-01, 1.6447e-05, 4.6825e-03, 4.8997e-03,
        5.4390e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 129, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0106, 0.0043, 0.0054, 0.9638, 0.0054, 0.0054, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 130, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8105e-01, 1.3338e-07, 3.5844e-01, 1.1406e-05, 1.5421e-06, 3.6049e-01,
        2.0943e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.922

[Epoch: 130, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.5699e-02, 1.4744e-01, 2.0434e-02, 6.0594e-06, 4.5958e-07, 9.7632e-02,
        6.4879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 130, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.7649e-08, 5.8443e-09, 1.5072e-06, 1.0000e+00, 2.6830e-08, 2.2482e-07,
        6.7941e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 130, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9752e-01, 6.3476e-03, 2.8368e-01, 2.8466e-05, 6.3870e-03, 6.0227e-03,
        1.0248e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 130, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0047, 0.0054, 0.9635, 0.0059, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 131, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9039e-01, 1.4261e-07, 3.7742e-01, 1.3693e-05, 8.6983e-07, 3.3218e-01,
        4.0339e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 131, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.6324e-02, 1.2091e-01, 1.7338e-02, 5.8076e-06, 4.4408e-07, 7.7262e-02,
        6.8816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 131, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0007e-08, 1.9344e-09, 8.1508e-07, 1.0000e+00, 1.3056e-08, 2.2488e-07,
        1.1213e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 131, batch: 176/222] total loss per batch: 0.585
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9074e-01, 5.5421e-03, 2.8563e-01, 1.8439e-05, 1.1662e-02, 6.4026e-03,
        3.6053e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 131, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0052, 0.0046, 0.9656, 0.0054, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 132, batch: 44/222] total loss per batch: 0.622
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.4688e-01, 1.6841e-07, 3.6997e-01, 1.5862e-05, 1.1935e-06, 3.8313e-01,
        1.8942e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.895

[Epoch: 132, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.2779e-02, 1.3012e-01, 1.9858e-02, 9.4556e-06, 7.9037e-07, 9.7032e-02,
        6.9020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 132, batch: 132/222] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1394e-08, 8.7914e-09, 9.1424e-07, 1.0000e+00, 7.4545e-09, 6.0990e-07,
        1.7641e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 132, batch: 176/222] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9401e-01, 5.3954e-03, 2.9149e-01, 5.3835e-05, 5.0878e-03, 3.9655e-03,
        3.5757e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 132, batch: 220/222] total loss per batch: 0.596
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0045, 0.0053, 0.9648, 0.0059, 0.0060, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 133, batch: 44/222] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([3.0179e-01, 7.9100e-08, 3.6011e-01, 1.9054e-05, 1.0270e-06, 3.3808e-01,
        3.2325e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.951

[Epoch: 133, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3336e-02, 1.3947e-01, 1.9347e-02, 9.5759e-06, 6.0496e-07, 7.8494e-02,
        6.7935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.073

[Epoch: 133, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.4948e-08, 7.8167e-09, 7.5793e-07, 1.0000e+00, 6.6463e-09, 2.4094e-07,
        1.7438e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 133, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1591e-01, 5.0427e-03, 2.6816e-01, 3.4086e-05, 5.8598e-03, 4.9875e-03,
        5.6249e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 133, batch: 220/222] total loss per batch: 0.593
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0110, 0.0047, 0.0046, 0.9643, 0.0049, 0.0059, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 134, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 5
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8145e-01, 1.2613e-07, 3.5737e-01, 3.3892e-05, 2.8602e-07, 3.6115e-01,
        1.7526e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.966

[Epoch: 134, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0285e-02, 1.2757e-01, 1.6559e-02, 1.1795e-05, 1.1230e-06, 7.5404e-02,
        7.0017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 134, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.2175e-08, 3.0504e-09, 1.5831e-06, 1.0000e+00, 6.8846e-09, 3.1532e-07,
        5.0517e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 134, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9125e-01, 4.2905e-03, 2.9366e-01, 4.8949e-05, 5.5388e-03, 5.2077e-03,
        2.1020e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 134, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.0047, 0.0051, 0.9640, 0.0053, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 135, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8119e-01, 7.6948e-08, 3.7091e-01, 1.8347e-05, 5.2783e-07, 3.4788e-01,
        1.4459e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.954

[Epoch: 135, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3588e-02, 1.2664e-01, 1.8700e-02, 8.9986e-06, 7.0907e-07, 8.4775e-02,
        6.8629e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 135, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.4135e-08, 3.3205e-09, 1.2915e-06, 1.0000e+00, 9.8638e-09, 2.9839e-07,
        4.0788e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 135, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1503e-01, 4.4155e-03, 2.6897e-01, 3.3761e-05, 6.4585e-03, 5.0827e-03,
        2.3071e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.023

[Epoch: 135, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0042, 0.0051, 0.9661, 0.0052, 0.0052, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 136, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8206e-01, 9.5445e-08, 3.7756e-01, 1.4769e-05, 6.3481e-07, 3.4036e-01,
        2.4371e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 136, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1330e-02, 1.2907e-01, 1.7709e-02, 7.3869e-06, 6.1239e-07, 8.0059e-02,
        6.9183e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 136, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.6570e-08, 1.9856e-09, 9.4377e-07, 1.0000e+00, 6.6182e-09, 1.5484e-07,
        3.8106e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 136, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7016e-01, 5.2688e-03, 3.1316e-01, 2.9677e-05, 6.1618e-03, 5.2105e-03,
        2.7444e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 136, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0042, 0.0050, 0.9663, 0.0051, 0.0054, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 137, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8065e-01, 5.9206e-08, 3.6832e-01, 1.1992e-05, 5.3058e-07, 3.5101e-01,
        1.9129e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 137, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6580e-02, 1.2612e-01, 1.8209e-02, 6.2578e-06, 4.4625e-07, 7.9644e-02,
        6.9944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 137, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.8366e-08, 1.5496e-09, 8.2570e-07, 1.0000e+00, 4.7479e-09, 1.2328e-07,
        3.8567e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 137, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2173e-01, 4.9130e-03, 2.6238e-01, 2.3533e-05, 5.8306e-03, 5.1182e-03,
        2.6695e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 137, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0042, 0.0051, 0.9659, 0.0054, 0.0055, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 138, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8409e-01, 4.9721e-08, 3.6833e-01, 1.1674e-05, 4.8147e-07, 3.4756e-01,
        1.4835e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 138, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4217e-02, 1.2329e-01, 1.9043e-02, 5.2157e-06, 3.7989e-07, 8.1481e-02,
        6.9196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 138, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.5822e-08, 8.8601e-10, 4.6565e-07, 1.0000e+00, 2.7691e-09, 7.1680e-08,
        2.6497e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 138, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8141e-01, 5.2104e-03, 3.0309e-01, 2.1142e-05, 5.1971e-03, 5.0665e-03,
        2.7506e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 138, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0045, 0.0053, 0.9648, 0.0054, 0.0056, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 139, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7934e-01, 5.3049e-08, 3.7442e-01, 9.3225e-06, 5.4344e-07, 3.4623e-01,
        1.8774e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.933

[Epoch: 139, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.9790e-02, 1.2822e-01, 1.8881e-02, 4.3112e-06, 3.1417e-07, 7.7885e-02,
        6.9522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 139, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.3502e-08, 6.4221e-10, 4.1636e-07, 1.0000e+00, 2.7764e-09, 6.5989e-08,
        2.5363e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 139, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1785e-01, 4.7999e-03, 2.6678e-01, 1.5305e-05, 5.4979e-03, 5.0550e-03,
        1.9965e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 139, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0043, 0.0052, 0.9647, 0.0054, 0.0055, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 140, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7624e-01, 4.4611e-08, 3.7949e-01, 9.3242e-06, 3.8939e-07, 3.4426e-01,
        1.1297e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 140, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1136e-02, 1.2635e-01, 1.9025e-02, 4.8867e-06, 3.5259e-07, 8.2603e-02,
        6.9088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 140, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.5574e-08, 1.2570e-09, 5.6647e-07, 1.0000e+00, 2.8092e-09, 8.3368e-08,
        3.1994e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 140, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7013e-01, 5.0855e-03, 3.1350e-01, 1.5906e-05, 5.6571e-03, 5.6122e-03,
        2.7187e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 140, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0045, 0.0058, 0.9636, 0.0053, 0.0060, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 141, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8380e-01, 4.3998e-08, 3.6013e-01, 9.7343e-06, 3.7160e-07, 3.5605e-01,
        1.4046e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.941

[Epoch: 141, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6889e-02, 1.2411e-01, 1.8946e-02, 5.2694e-06, 3.6445e-07, 7.8742e-02,
        7.0131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 141, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3322e-08, 3.3560e-10, 5.3391e-07, 1.0000e+00, 1.2858e-09, 3.4072e-08,
        1.8490e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 141, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3791e-01, 4.6861e-03, 2.4591e-01, 2.2227e-05, 6.4030e-03, 5.0621e-03,
        1.5475e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 141, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0109, 0.0046, 0.0053, 0.9633, 0.0050, 0.0056, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 142, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7313e-01, 7.9693e-08, 3.8141e-01, 1.0702e-05, 6.4704e-07, 3.4545e-01,
        1.4846e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 142, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.9126e-02, 1.3583e-01, 2.2683e-02, 4.3706e-06, 1.9342e-07, 7.5148e-02,
        6.7721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 142, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.7663e-08, 1.0419e-09, 4.5455e-07, 1.0000e+00, 8.5144e-09, 1.0338e-07,
        6.8966e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 142, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7705e-01, 5.9740e-03, 3.0767e-01, 1.6214e-05, 4.2497e-03, 5.0353e-03,
        2.2458e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 142, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0045, 0.0055, 0.9638, 0.0058, 0.0057, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 143, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9057e-01, 5.2549e-08, 3.6597e-01, 1.8570e-05, 1.0547e-06, 3.4345e-01,
        1.5980e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 143, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.0811e-02, 1.0209e-01, 1.5069e-02, 7.4432e-06, 6.5918e-07, 8.8536e-02,
        7.2348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 143, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.2111e-08, 3.5435e-09, 9.9162e-07, 1.0000e+00, 7.8346e-09, 1.0438e-07,
        3.8251e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.992

[Epoch: 143, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0706e-01, 3.9415e-03, 2.7941e-01, 3.0610e-05, 4.0847e-03, 5.4695e-03,
        2.2803e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 143, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0048, 0.0052, 0.9651, 0.0053, 0.0057, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 144, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8885e-01, 2.1022e-07, 3.6572e-01, 1.5624e-05, 1.8827e-06, 3.4541e-01,
        4.2513e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.949

[Epoch: 144, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.3535e-02, 1.7275e-01, 2.0790e-02, 1.1919e-05, 1.0823e-06, 6.3855e-02,
        6.6906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 144, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.3785e-08, 3.9584e-09, 1.6190e-06, 1.0000e+00, 1.1385e-08, 3.6839e-07,
        5.0151e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 144, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2284e-01, 6.5730e-03, 2.5858e-01, 4.4827e-05, 6.5680e-03, 5.3911e-03,
        2.3412e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 144, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0047, 0.0050, 0.9644, 0.0049, 0.0053, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 145, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8802e-01, 1.0820e-07, 3.6834e-01, 2.8576e-05, 1.1667e-06, 3.4361e-01,
        2.4390e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 145, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6040e-02, 8.4210e-02, 1.7267e-02, 1.1195e-05, 4.9736e-07, 8.9076e-02,
        7.3340e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 145, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.7940e-08, 2.4984e-08, 2.1563e-06, 1.0000e+00, 2.3147e-08, 3.7268e-07,
        7.5604e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 145, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7409e-01, 5.0489e-03, 3.1090e-01, 8.0506e-05, 5.0966e-03, 4.7841e-03,
        4.2507e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 145, batch: 220/222] total loss per batch: 0.592
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0047, 0.0052, 0.9653, 0.0055, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 146, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8624e-01, 1.6543e-07, 3.7751e-01, 2.4073e-05, 1.0901e-06, 3.3622e-01,
        2.6187e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.926

[Epoch: 146, batch: 88/222] total loss per batch: 0.593
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.9245e-02, 1.6016e-01, 2.4456e-02, 1.0234e-05, 1.9956e-06, 8.7124e-02,
        6.3901e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.072

[Epoch: 146, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.2997e-08, 2.9626e-09, 8.3798e-07, 1.0000e+00, 1.8342e-08, 2.1028e-07,
        4.2694e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 146, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1051e-01, 4.7565e-03, 2.7389e-01, 4.4997e-05, 5.3165e-03, 5.4775e-03,
        2.0950e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 146, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0107, 0.0043, 0.0049, 0.9647, 0.0052, 0.0055, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 147, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6618e-01, 1.6724e-07, 3.8086e-01, 1.7527e-05, 1.7360e-06, 3.5295e-01,
        1.8347e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.921

[Epoch: 147, batch: 88/222] total loss per batch: 0.594
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.2144e-02, 1.0214e-01, 1.4046e-02, 1.1735e-05, 5.6434e-07, 7.9233e-02,
        7.4242e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.049

[Epoch: 147, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.9200e-08, 1.0502e-08, 9.9369e-07, 1.0000e+00, 1.8622e-08, 1.8048e-07,
        2.5824e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 147, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7848e-01, 5.7180e-03, 3.0583e-01, 4.0013e-05, 4.3749e-03, 5.5534e-03,
        4.5637e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 147, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0050, 0.0046, 0.9646, 0.0053, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 148, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8523e-01, 8.6331e-08, 3.6742e-01, 2.0263e-05, 9.2917e-07, 3.4733e-01,
        1.8278e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.955

[Epoch: 148, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.7864e-02, 1.3827e-01, 1.5834e-02, 1.0337e-05, 1.1372e-06, 7.0922e-02,
        6.7710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 148, batch: 132/222] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1609e-08, 1.1478e-08, 1.9820e-06, 1.0000e+00, 1.0707e-08, 2.6505e-07,
        7.5942e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 148, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9504e-01, 4.8147e-03, 2.8730e-01, 2.3295e-05, 6.3314e-03, 6.4893e-03,
        1.7510e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 148, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0048, 0.0048, 0.9643, 0.0051, 0.0058, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 149, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8722e-01, 1.5471e-07, 3.7075e-01, 8.3353e-06, 1.0505e-06, 3.4202e-01,
        9.5853e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 149, batch: 88/222] total loss per batch: 0.600
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5655e-02, 1.4723e-01, 2.0718e-02, 5.5539e-06, 4.4780e-07, 1.2062e-01,
        6.3577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.074

[Epoch: 149, batch: 132/222] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1282e-07, 6.6338e-08, 1.1599e-05, 9.9999e-01, 1.9833e-08, 3.5453e-07,
        1.0139e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.993

[Epoch: 149, batch: 176/222] total loss per batch: 0.584
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9106e-01, 6.2953e-03, 2.8752e-01, 6.2355e-05, 9.7323e-03, 5.3273e-03,
        4.6879e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 149, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0083, 0.0042, 0.0052, 0.9676, 0.0046, 0.0057, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.009

[Epoch: 150, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6843e-01, 9.9494e-08, 3.7092e-01, 3.7958e-05, 1.6978e-06, 3.6061e-01,
        1.7865e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.953

[Epoch: 150, batch: 88/222] total loss per batch: 0.603
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3252e-02, 1.0149e-01, 1.6109e-02, 5.3340e-06, 1.4726e-07, 6.2750e-02,
        7.3639e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 150, batch: 132/222] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3051e-07, 4.1959e-08, 3.1005e-06, 1.0000e+00, 4.8848e-08, 2.4664e-07,
        8.3212e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.989

[Epoch: 150, batch: 176/222] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3430e-01, 6.5153e-03, 2.4677e-01, 9.5762e-05, 7.6014e-03, 4.7144e-03,
        3.3632e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.005

[Epoch: 150, batch: 220/222] total loss per batch: 0.594
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0078, 0.0050, 0.0045, 0.9692, 0.0049, 0.0045, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 151, batch: 44/222] total loss per batch: 0.618
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8321e-01, 9.3917e-08, 3.7385e-01, 4.7950e-05, 1.3553e-06, 3.4289e-01,
        4.3124e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.937

[Epoch: 151, batch: 88/222] total loss per batch: 0.595
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1784e-02, 1.2489e-01, 1.8641e-02, 1.4504e-05, 3.6842e-07, 8.3486e-02,
        6.9118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.053

[Epoch: 151, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([5.8335e-08, 6.8761e-08, 3.9463e-06, 1.0000e+00, 3.8945e-08, 3.1897e-07,
        7.2910e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 151, batch: 176/222] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7270e-01, 5.2634e-03, 3.0872e-01, 4.6006e-05, 7.7835e-03, 5.4881e-03,
        2.3472e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 151, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0042, 0.0049, 0.9678, 0.0048, 0.0051, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 152, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8268e-01, 3.0534e-07, 3.7201e-01, 4.5029e-05, 2.5962e-06, 3.4527e-01,
        2.0443e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.918

[Epoch: 152, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6470e-02, 1.1637e-01, 1.8212e-02, 1.2910e-05, 5.9040e-07, 8.8479e-02,
        7.0046e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.048

[Epoch: 152, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.6596e-08, 4.3635e-08, 3.1936e-06, 1.0000e+00, 2.2659e-08, 2.1999e-07,
        5.9951e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 152, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0150e-01, 5.2862e-03, 2.8120e-01, 3.7379e-05, 7.0550e-03, 4.9138e-03,
        2.9692e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 152, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0044, 0.0047, 0.9680, 0.0048, 0.0051, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 153, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7677e-01, 1.7581e-07, 3.8016e-01, 3.4255e-05, 1.8950e-06, 3.4303e-01,
        1.8783e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.940

[Epoch: 153, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5899e-02, 1.2687e-01, 1.8043e-02, 1.1749e-05, 4.1620e-07, 8.9719e-02,
        6.8946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.053

[Epoch: 153, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0012e-08, 2.9817e-08, 2.4031e-06, 1.0000e+00, 1.3298e-08, 1.5857e-07,
        3.6254e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 153, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9560e-01, 5.4903e-03, 2.8736e-01, 3.4818e-05, 6.4675e-03, 5.0470e-03,
        2.2996e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 153, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0086, 0.0043, 0.0047, 0.9683, 0.0048, 0.0050, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 154, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7768e-01, 1.3460e-07, 3.7863e-01, 2.7114e-05, 1.5546e-06, 3.4365e-01,
        1.6444e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.937

[Epoch: 154, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7696e-02, 1.2314e-01, 1.7780e-02, 9.9199e-06, 3.3342e-07, 8.6752e-02,
        6.9462e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.054

[Epoch: 154, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.2985e-08, 2.3280e-08, 1.8298e-06, 1.0000e+00, 9.1476e-09, 1.2580e-07,
        2.9302e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 154, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9516e-01, 5.4487e-03, 2.8781e-01, 3.1550e-05, 6.3803e-03, 5.1711e-03,
        1.9241e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 154, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.0043, 0.0047, 0.9680, 0.0048, 0.0051, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 155, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7967e-01, 1.0778e-07, 3.7747e-01, 2.3049e-05, 1.2970e-06, 3.4283e-01,
        1.4234e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 155, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7642e-02, 1.2298e-01, 1.7843e-02, 8.6447e-06, 2.7124e-07, 8.5495e-02,
        6.9603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.056

[Epoch: 155, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.8539e-08, 1.8236e-08, 1.4566e-06, 1.0000e+00, 6.7898e-09, 1.0002e-07,
        2.4739e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 155, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9813e-01, 5.3821e-03, 2.8490e-01, 2.8077e-05, 6.2786e-03, 5.2741e-03,
        1.7708e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 155, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0045, 0.0048, 0.9673, 0.0048, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 156, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7903e-01, 8.9810e-08, 3.7735e-01, 1.9521e-05, 1.0899e-06, 3.4360e-01,
        1.1757e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 156, batch: 88/222] total loss per batch: 0.590
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7450e-02, 1.2271e-01, 1.7785e-02, 7.3218e-06, 2.1867e-07, 8.3979e-02,
        6.9806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 156, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.4561e-08, 1.4123e-08, 1.1648e-06, 1.0000e+00, 5.1259e-09, 8.2868e-08,
        1.9280e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 156, batch: 176/222] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9809e-01, 5.2490e-03, 2.8521e-01, 2.3553e-05, 6.1240e-03, 5.2992e-03,
        1.5129e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 156, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0044, 0.0048, 0.9672, 0.0048, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 157, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7886e-01, 7.3049e-08, 3.7712e-01, 1.6987e-05, 9.3899e-07, 3.4400e-01,
        1.1305e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.937

[Epoch: 157, batch: 88/222] total loss per batch: 0.590
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8603e-02, 1.2529e-01, 1.8050e-02, 6.5537e-06, 1.8064e-07, 8.3543e-02,
        6.9451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 157, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2418e-08, 1.1026e-08, 9.4934e-07, 1.0000e+00, 3.7977e-09, 6.7158e-08,
        1.6309e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 157, batch: 176/222] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9954e-01, 5.2182e-03, 2.8387e-01, 2.0846e-05, 6.0225e-03, 5.3272e-03,
        1.3704e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 157, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0046, 0.0049, 0.9665, 0.0049, 0.0053, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 158, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7941e-01, 6.0128e-08, 3.7616e-01, 1.4570e-05, 8.2561e-07, 3.4442e-01,
        8.6307e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 158, batch: 88/222] total loss per batch: 0.590
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7459e-02, 1.2261e-01, 1.8009e-02, 5.6389e-06, 1.5221e-07, 8.2299e-02,
        6.9962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 158, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([9.8395e-09, 8.6258e-09, 7.4791e-07, 1.0000e+00, 3.1115e-09, 5.5189e-08,
        1.2931e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 158, batch: 176/222] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9774e-01, 5.1733e-03, 2.8589e-01, 1.7490e-05, 5.9258e-03, 5.2512e-03,
        1.1818e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 158, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0045, 0.0049, 0.9664, 0.0049, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 159, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7824e-01, 5.2422e-08, 3.7694e-01, 1.2297e-05, 6.5004e-07, 3.4481e-01,
        7.9863e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 159, batch: 88/222] total loss per batch: 0.590
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0098e-02, 1.2553e-01, 1.8698e-02, 4.8135e-06, 1.2588e-07, 8.2286e-02,
        6.9338e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 159, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.4881e-09, 6.6068e-09, 6.1100e-07, 1.0000e+00, 2.2497e-09, 4.3718e-08,
        1.0620e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 159, batch: 176/222] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9949e-01, 5.1438e-03, 2.8447e-01, 1.5870e-05, 5.5578e-03, 5.3232e-03,
        1.0560e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 159, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0046, 0.0049, 0.9662, 0.0050, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 160, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8450e-01, 3.9996e-08, 3.7504e-01, 1.0692e-05, 6.0224e-07, 3.4045e-01,
        6.6999e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 160, batch: 88/222] total loss per batch: 0.590
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6551e-02, 1.2678e-01, 1.8673e-02, 4.6554e-06, 1.0891e-07, 8.1804e-02,
        6.9619e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 160, batch: 132/222] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([7.7610e-09, 6.2306e-09, 5.5968e-07, 1.0000e+00, 2.1794e-09, 4.3152e-08,
        8.8973e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 160, batch: 176/222] total loss per batch: 0.580
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9788e-01, 4.9216e-03, 2.8623e-01, 1.2713e-05, 5.7318e-03, 5.2176e-03,
        1.0421e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 160, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0046, 0.0049, 0.9665, 0.0049, 0.0053, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 161, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7461e-01, 4.0360e-08, 3.7448e-01, 1.0497e-05, 4.6034e-07, 3.5090e-01,
        5.7932e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 161, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3274e-02, 1.2156e-01, 1.8350e-02, 3.9232e-06, 1.0153e-07, 8.1888e-02,
        6.9492e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 161, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.5610e-09, 4.1396e-09, 4.8161e-07, 1.0000e+00, 1.6743e-09, 3.5411e-08,
        1.0330e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 161, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0275e-01, 5.4970e-03, 2.8076e-01, 1.4750e-05, 5.6155e-03, 5.3591e-03,
        9.4214e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 161, batch: 220/222] total loss per batch: 0.589
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0046, 0.0049, 0.9659, 0.0049, 0.0056, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 162, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8637e-01, 3.0401e-08, 3.7682e-01, 8.2391e-06, 5.0558e-07, 3.3680e-01,
        6.1078e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 162, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.2840e-02, 1.3459e-01, 1.9732e-02, 5.3775e-06, 1.0272e-07, 7.9538e-02,
        6.9329e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 162, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([8.9898e-09, 8.9770e-09, 4.4364e-07, 1.0000e+00, 2.4359e-09, 5.0488e-08,
        4.6846e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 162, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9511e-01, 4.6735e-03, 2.8947e-01, 1.1158e-05, 5.4190e-03, 5.3214e-03,
        1.2844e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 162, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0049, 0.0052, 0.9651, 0.0051, 0.0054, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 163, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7864e-01, 4.5006e-08, 3.7124e-01, 1.0296e-05, 5.2789e-07, 3.5011e-01,
        6.0818e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.923

[Epoch: 163, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.2670e-02, 1.2159e-01, 1.7678e-02, 4.4553e-06, 1.9078e-07, 8.5093e-02,
        6.8296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 163, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1367e-08, 5.8034e-09, 1.0654e-06, 1.0000e+00, 5.1126e-09, 6.1046e-08,
        1.6695e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 163, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0594e-01, 5.1686e-03, 2.7823e-01, 2.0241e-05, 5.3263e-03, 5.3154e-03,
        1.4445e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 163, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.0042, 0.0045, 0.9678, 0.0047, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 164, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7838e-01, 6.9765e-08, 3.8080e-01, 1.1764e-05, 5.3928e-07, 3.4082e-01,
        1.2820e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 164, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.8914e-02, 1.2900e-01, 2.0227e-02, 8.7433e-06, 1.2589e-07, 7.3700e-02,
        7.0815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 164, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.9337e-08, 4.3056e-08, 9.0385e-07, 1.0000e+00, 8.4998e-09, 2.1119e-07,
        8.3282e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 164, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8318e-01, 6.0033e-03, 2.9786e-01, 2.3709e-05, 6.8257e-03, 6.1111e-03,
        1.1175e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.009

[Epoch: 164, batch: 220/222] total loss per batch: 0.591
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0050, 0.0051, 0.9649, 0.0052, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 165, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9315e-01, 4.9297e-08, 3.6488e-01, 2.0923e-05, 5.9953e-07, 3.4195e-01,
        5.7940e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.930

[Epoch: 165, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0067e-02, 1.2925e-01, 1.9876e-02, 6.4142e-06, 2.1076e-07, 9.0189e-02,
        6.8061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 165, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1062e-08, 1.2316e-08, 1.5421e-06, 1.0000e+00, 1.0193e-08, 1.9402e-07,
        1.8193e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 165, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4108e-01, 4.5276e-03, 2.4473e-01, 1.9462e-05, 4.8281e-03, 4.8185e-03,
        1.1723e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 165, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0046, 0.0047, 0.9663, 0.0049, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 166, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7640e-01, 1.0050e-07, 3.8504e-01, 1.5096e-05, 1.1243e-06, 3.3855e-01,
        1.4991e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.942

[Epoch: 166, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3410e-02, 1.2499e-01, 2.0531e-02, 8.8622e-06, 1.9598e-07, 7.5551e-02,
        6.9551e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 166, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.9812e-08, 7.1008e-08, 1.6386e-06, 1.0000e+00, 8.5859e-09, 3.3186e-07,
        3.3262e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 166, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5910e-01, 5.3089e-03, 3.2483e-01, 1.9873e-05, 4.9601e-03, 5.7774e-03,
        1.5121e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 166, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0075, 0.0041, 0.0047, 0.9691, 0.0049, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 167, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8584e-01, 6.6828e-08, 3.6854e-01, 2.6651e-05, 7.1168e-07, 3.4559e-01,
        1.1251e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.933

[Epoch: 167, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.9766e-02, 1.2499e-01, 1.8411e-02, 6.0024e-06, 1.6724e-07, 7.5087e-02,
        7.0174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.055

[Epoch: 167, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.8452e-08, 1.1268e-08, 1.5284e-06, 1.0000e+00, 9.3115e-09, 1.7577e-07,
        1.3849e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 167, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3616e-01, 4.7361e-03, 2.4877e-01, 1.6330e-05, 5.3315e-03, 4.9782e-03,
        1.1225e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 167, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0047, 0.0049, 0.9655, 0.0051, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 168, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7647e-01, 8.5184e-08, 3.7617e-01, 1.9218e-05, 1.1464e-06, 3.4734e-01,
        9.8639e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.938

[Epoch: 168, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3907e-02, 1.2544e-01, 1.9654e-02, 6.8582e-06, 1.5804e-07, 8.8484e-02,
        6.8250e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 168, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6149e-08, 1.8243e-08, 1.1237e-06, 1.0000e+00, 4.6259e-09, 1.4110e-07,
        4.2630e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 168, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6070e-01, 5.6094e-03, 3.2271e-01, 2.0950e-05, 4.8373e-03, 6.1133e-03,
        1.1070e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 168, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0049, 0.0057, 0.9638, 0.0051, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 169, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7653e-01, 7.6044e-08, 3.7631e-01, 1.8344e-05, 7.8392e-07, 3.4714e-01,
        1.2362e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.934

[Epoch: 169, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.1419e-02, 1.2781e-01, 1.8627e-02, 6.6743e-06, 1.8423e-07, 7.5844e-02,
        7.0629e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 169, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.8346e-08, 1.9510e-08, 1.5878e-06, 1.0000e+00, 6.3943e-09, 2.0059e-07,
        3.3891e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.995

[Epoch: 169, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4423e-01, 4.5824e-03, 2.3976e-01, 1.4248e-05, 6.1359e-03, 5.2765e-03,
        1.5452e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.017

[Epoch: 169, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0046, 0.0048, 0.9656, 0.0053, 0.0056, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 170, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7973e-01, 6.7895e-08, 3.7686e-01, 1.3766e-05, 9.1581e-07, 3.4339e-01,
        1.1035e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.946

[Epoch: 170, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.8885e-02, 1.2396e-01, 1.8759e-02, 5.9085e-06, 1.2736e-07, 7.7853e-02,
        6.9053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 170, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6016e-08, 1.5011e-08, 7.8208e-07, 1.0000e+00, 4.4848e-09, 1.6571e-07,
        1.3746e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 170, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5239e-01, 5.8121e-03, 3.3199e-01, 1.6472e-05, 4.5854e-03, 5.2003e-03,
        2.3083e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 170, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0047, 0.0049, 0.9665, 0.0045, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 171, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7811e-01, 8.1067e-08, 3.7489e-01, 1.5228e-05, 1.0214e-06, 3.4698e-01,
        1.4131e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 171, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5175e-02, 1.2443e-01, 1.8408e-02, 7.0452e-06, 1.6754e-07, 8.5650e-02,
        6.9633e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 171, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.3325e-08, 1.9215e-08, 1.6066e-06, 1.0000e+00, 1.0008e-08, 1.5203e-07,
        4.1168e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 171, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3852e-01, 5.5372e-03, 2.4376e-01, 1.5294e-05, 6.3844e-03, 5.7866e-03,
        8.2948e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.008

[Epoch: 171, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.0046, 0.0049, 0.9655, 0.0049, 0.0053, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 172, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8272e-01, 8.7691e-08, 3.6626e-01, 1.2574e-05, 9.6531e-07, 3.5101e-01,
        7.8463e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 172, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.2723e-02, 1.2807e-01, 2.0184e-02, 6.8328e-06, 1.9970e-07, 7.5979e-02,
        6.9304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 172, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.0186e-08, 1.7944e-08, 1.0788e-06, 1.0000e+00, 8.8764e-09, 1.5062e-07,
        1.9224e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.994

[Epoch: 172, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0646e-01, 4.7551e-03, 2.7931e-01, 1.2968e-05, 4.8342e-03, 4.6222e-03,
        1.3253e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 172, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0052, 0.0053, 0.9636, 0.0056, 0.0055, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 173, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9159e-01, 5.5344e-08, 3.7477e-01, 1.2982e-05, 6.7524e-07, 3.3362e-01,
        1.2260e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.931

[Epoch: 173, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.6547e-02, 1.2652e-01, 2.0249e-02, 5.1871e-06, 1.8030e-07, 7.8323e-02,
        6.9835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.058

[Epoch: 173, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.6283e-08, 1.2216e-08, 1.3544e-06, 1.0000e+00, 3.9963e-09, 1.6645e-07,
        2.0644e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 173, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5706e-01, 5.2213e-03, 3.2754e-01, 2.0008e-05, 4.9267e-03, 5.2316e-03,
        8.7320e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.020

[Epoch: 173, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0042, 0.0046, 0.9679, 0.0050, 0.0049, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.018

[Epoch: 174, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7564e-01, 9.5267e-08, 3.8037e-01, 1.7900e-05, 1.1274e-06, 3.4396e-01,
        1.4946e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.930

[Epoch: 174, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7701e-02, 1.1398e-01, 1.9014e-02, 8.5825e-06, 3.1665e-07, 8.1052e-02,
        7.0824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 174, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.8895e-08, 9.0647e-09, 7.8620e-07, 1.0000e+00, 6.1778e-09, 1.7194e-07,
        9.0443e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 174, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5487e-01, 5.3496e-03, 2.3009e-01, 1.2330e-05, 5.1808e-03, 4.4908e-03,
        6.4065e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 174, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0051, 0.0049, 0.9639, 0.0056, 0.0055, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.010

[Epoch: 175, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7016e-01, 6.8614e-08, 3.7668e-01, 2.1574e-05, 4.7693e-07, 3.5313e-01,
        7.8886e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 175, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4638e-02, 1.4455e-01, 2.1746e-02, 8.5044e-06, 2.7242e-07, 8.3468e-02,
        6.6559e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 175, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.4078e-08, 6.0437e-08, 1.6297e-06, 1.0000e+00, 9.2952e-09, 3.4404e-07,
        2.7286e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 175, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7975e-01, 5.1125e-03, 3.0513e-01, 2.1193e-05, 4.9511e-03, 5.0270e-03,
        4.6380e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.003

[Epoch: 175, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.0054, 0.0058, 0.9605, 0.0055, 0.0058, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 176, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8579e-01, 1.3123e-07, 3.7356e-01, 1.3677e-05, 9.6571e-07, 3.4063e-01,
        1.6078e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 176, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7600e-02, 1.3122e-01, 2.1467e-02, 8.5297e-06, 2.2565e-07, 8.1583e-02,
        6.8812e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 176, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.1772e-08, 1.1510e-08, 1.4393e-06, 1.0000e+00, 9.5506e-09, 1.2192e-07,
        2.6320e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 176, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2517e-01, 5.7352e-03, 2.5770e-01, 1.6344e-05, 5.4110e-03, 5.9642e-03,
        9.1857e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.014

[Epoch: 176, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0047, 0.0057, 0.9636, 0.0049, 0.0052, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 177, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7955e-01, 1.1436e-07, 3.7254e-01, 1.5340e-05, 6.7920e-07, 3.4789e-01,
        1.2277e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.917

[Epoch: 177, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5263e-02, 1.0261e-01, 1.7975e-02, 8.8630e-06, 3.0613e-07, 7.9864e-02,
        7.2428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.055

[Epoch: 177, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0054e-08, 2.4944e-08, 1.3103e-06, 1.0000e+00, 6.4700e-09, 2.6472e-07,
        3.4528e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 177, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.5736e-01, 6.6301e-03, 3.2171e-01, 2.9634e-05, 7.1467e-03, 7.1230e-03,
        2.2449e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 177, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0047, 0.0049, 0.9656, 0.0053, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 178, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8024e-01, 8.7660e-08, 3.7889e-01, 1.1678e-05, 6.3210e-07, 3.4086e-01,
        1.1706e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.962

[Epoch: 178, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0338e-02, 1.3827e-01, 1.6741e-02, 7.8596e-06, 1.8021e-07, 7.8432e-02,
        6.8621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.064

[Epoch: 178, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4860e-08, 1.7318e-08, 8.4031e-07, 1.0000e+00, 6.9944e-09, 1.8572e-07,
        2.1054e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 178, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0795e-01, 5.6124e-03, 2.7266e-01, 1.7000e-05, 7.1220e-03, 6.6357e-03,
        4.1826e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.001

[Epoch: 178, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0046, 0.0052, 0.9659, 0.0052, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 179, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9296e-01, 9.3385e-08, 3.5883e-01, 1.1058e-05, 5.4890e-07, 3.4820e-01,
        2.1406e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.936

[Epoch: 179, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8096e-02, 1.3150e-01, 1.9315e-02, 7.3136e-06, 3.7765e-07, 8.2295e-02,
        6.8879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 179, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([4.6646e-08, 1.5685e-08, 1.1255e-06, 1.0000e+00, 7.4919e-09, 2.1215e-07,
        1.7072e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 179, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0125e-01, 5.4206e-03, 2.7973e-01, 2.5435e-05, 6.3237e-03, 7.2489e-03,
        2.7427e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.007

[Epoch: 179, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0049, 0.0056, 0.9641, 0.0048, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.017

[Epoch: 180, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6445e-01, 1.2044e-07, 3.8264e-01, 1.4002e-05, 8.3167e-07, 3.5289e-01,
        2.1286e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.932

[Epoch: 180, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.5306e-02, 1.1743e-01, 1.9244e-02, 6.4633e-06, 3.0810e-07, 8.1180e-02,
        6.9684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 180, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5978e-08, 6.2563e-09, 7.6864e-07, 1.0000e+00, 2.3168e-09, 1.1032e-07,
        1.3011e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 180, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7919e-01, 6.4602e-03, 3.0241e-01, 2.4653e-05, 5.8165e-03, 6.0940e-03,
        2.6078e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 180, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0049, 0.0053, 0.9647, 0.0051, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.011

[Epoch: 181, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8657e-01, 1.4175e-07, 3.7971e-01, 1.1155e-05, 7.3181e-07, 3.3371e-01,
        1.4221e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.940

[Epoch: 181, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7111e-02, 1.2959e-01, 1.9955e-02, 5.9515e-06, 2.8722e-07, 8.6649e-02,
        6.8669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 181, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4353e-08, 1.3474e-08, 1.1226e-06, 1.0000e+00, 7.6092e-09, 1.7876e-07,
        3.5155e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 181, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4163e-01, 4.3846e-03, 2.4267e-01, 2.0807e-05, 5.9940e-03, 5.3009e-03,
        2.1367e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.010

[Epoch: 181, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0044, 0.0052, 0.9666, 0.0047, 0.0042, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 182, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8216e-01, 1.1058e-07, 3.6724e-01, 1.1130e-05, 5.9120e-07, 3.5059e-01,
        1.1652e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.925

[Epoch: 182, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.7919e-02, 1.2086e-01, 1.7292e-02, 4.1363e-06, 1.7159e-07, 7.2285e-02,
        7.1164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 182, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4585e-08, 1.8190e-08, 8.7041e-07, 1.0000e+00, 6.2157e-09, 1.6673e-07,
        3.8861e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 182, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6187e-01, 4.4314e-03, 3.2406e-01, 1.6855e-05, 4.2560e-03, 5.3718e-03,
        1.4294e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 182, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.0043, 0.0044, 0.9691, 0.0047, 0.0046, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 183, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7177e-01, 8.0918e-08, 3.7840e-01, 9.2813e-06, 8.4829e-07, 3.4982e-01,
        1.5116e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.956

[Epoch: 183, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.5440e-02, 1.3566e-01, 2.2079e-02, 5.8135e-06, 2.6310e-07, 8.2550e-02,
        6.8426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 183, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.6437e-08, 2.0628e-08, 1.6940e-06, 1.0000e+00, 6.4089e-09, 2.0201e-07,
        2.8481e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 183, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2259e-01, 5.4597e-03, 2.5907e-01, 1.7385e-05, 7.4638e-03, 5.3924e-03,
        2.5506e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 183, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0037, 0.0042, 0.9717, 0.0044, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 184, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8070e-01, 1.0605e-07, 3.6915e-01, 9.2402e-06, 4.3665e-07, 3.5014e-01,
        8.7204e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.909

[Epoch: 184, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([9.1865e-02, 1.1867e-01, 1.6859e-02, 4.9740e-06, 2.4885e-07, 7.4042e-02,
        6.9856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 184, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.5929e-08, 1.6604e-08, 9.9675e-07, 1.0000e+00, 6.5286e-09, 1.2114e-07,
        4.9470e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 184, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9864e-01, 5.9537e-03, 2.8396e-01, 2.2079e-05, 5.9284e-03, 5.4936e-03,
        1.7205e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 184, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0043, 0.0048, 0.9697, 0.0045, 0.0048, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 185, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9611e-01, 5.2556e-08, 3.7449e-01, 6.1566e-06, 7.8555e-07, 3.2939e-01,
        6.4168e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.957

[Epoch: 185, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.6527e-02, 1.3158e-01, 2.2120e-02, 8.6978e-06, 4.4482e-07, 8.6553e-02,
        6.9321e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.057

[Epoch: 185, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0321e-08, 1.5769e-08, 9.8273e-07, 1.0000e+00, 8.2549e-09, 1.6950e-07,
        2.9965e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 185, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8204e-01, 3.6798e-03, 3.0357e-01, 2.1005e-05, 4.9942e-03, 5.6898e-03,
        1.8174e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 185, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0050, 0.0051, 0.9641, 0.0055, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 186, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7068e-01, 1.4630e-07, 3.7980e-01, 1.0619e-05, 4.6053e-07, 3.4951e-01,
        2.4960e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.961

[Epoch: 186, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.6530e-02, 1.2310e-01, 1.7514e-02, 5.8407e-06, 4.7814e-07, 6.7133e-02,
        7.0572e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 186, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([3.0715e-08, 1.4329e-08, 1.0901e-06, 1.0000e+00, 5.9040e-09, 1.8979e-07,
        4.6366e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 186, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1463e-01, 6.4025e-03, 2.6888e-01, 3.1146e-05, 4.5859e-03, 5.4717e-03,
        1.5590e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.016

[Epoch: 186, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0048, 0.0054, 0.9634, 0.0053, 0.0057, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.019

[Epoch: 187, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8406e-01, 8.8965e-08, 3.6890e-01, 5.8115e-06, 1.4570e-06, 3.4703e-01,
        2.4484e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 187, batch: 88/222] total loss per batch: 0.592
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.2673e-02, 1.1848e-01, 1.8062e-02, 1.0442e-05, 4.2352e-07, 9.1245e-02,
        6.9953e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.069

[Epoch: 187, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([6.1141e-08, 2.6194e-08, 1.1787e-06, 1.0000e+00, 7.4738e-07, 2.1197e-07,
        8.0927e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 187, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1492e-01, 4.7423e-03, 2.7120e-01, 1.4925e-05, 4.4224e-03, 4.6993e-03,
        2.0572e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 187, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.0052, 0.0060, 0.9623, 0.0057, 0.0058, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 188, batch: 44/222] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9568e-01, 2.5450e-07, 3.5898e-01, 1.6436e-05, 1.2375e-06, 3.4533e-01,
        3.4901e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.958

[Epoch: 188, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4392e-02, 1.2757e-01, 1.9561e-02, 7.0765e-06, 5.8274e-07, 7.3662e-02,
        6.9481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.065

[Epoch: 188, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4776e-08, 2.4154e-08, 9.7293e-07, 1.0000e+00, 2.2190e-07, 1.9846e-07,
        3.6942e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 188, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.8318e-01, 4.6297e-03, 3.0196e-01, 2.1274e-05, 5.3983e-03, 4.8094e-03,
        1.1268e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 188, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0050, 0.0057, 0.9626, 0.0054, 0.0056, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

[Epoch: 189, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.6719e-01, 1.4106e-07, 3.8496e-01, 9.8010e-06, 1.0483e-06, 3.4784e-01,
        2.0648e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 189, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.4647e-02, 1.3287e-01, 2.0180e-02, 7.9838e-06, 4.6058e-07, 7.9891e-02,
        6.8241e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 189, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4250e-08, 1.8955e-08, 8.1231e-07, 1.0000e+00, 8.8964e-08, 1.8748e-07,
        3.3692e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 189, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0629e-01, 5.6827e-03, 2.7709e-01, 1.9119e-05, 5.9137e-03, 4.9992e-03,
        2.3725e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 189, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0050, 0.0056, 0.9629, 0.0054, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 190, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8335e-01, 1.2316e-07, 3.6852e-01, 1.5725e-05, 6.7076e-07, 3.4811e-01,
        7.0444e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.950

[Epoch: 190, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.4676e-02, 1.1625e-01, 1.8711e-02, 4.5486e-06, 3.6552e-07, 7.8271e-02,
        7.1209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 190, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.5607e-08, 1.3339e-08, 7.2208e-07, 1.0000e+00, 5.8272e-08, 1.1337e-07,
        2.3380e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 190, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9634e-01, 6.1869e-03, 2.8664e-01, 2.0714e-05, 6.0632e-03, 4.7440e-03,
        1.7125e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 190, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.0048, 0.0055, 0.9639, 0.0052, 0.0055, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 191, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7850e-01, 7.7316e-08, 3.8161e-01, 7.3909e-06, 9.2633e-07, 3.3988e-01,
        7.7584e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.911

[Epoch: 191, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.1302e-02, 1.2329e-01, 1.8578e-02, 4.8816e-06, 2.6886e-07, 7.7173e-02,
        6.9965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 191, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.1604e-08, 1.0534e-08, 3.8296e-07, 1.0000e+00, 2.8120e-08, 1.3951e-07,
        3.9161e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 191, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9781e-01, 4.8346e-03, 2.8637e-01, 2.1495e-05, 5.5571e-03, 5.4043e-03,
        1.9962e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.021

[Epoch: 191, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.0050, 0.0052, 0.9633, 0.0054, 0.0050, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 192, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8108e-01, 7.4754e-08, 3.7226e-01, 1.1178e-05, 5.7833e-07, 3.4665e-01,
        1.3521e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.958

[Epoch: 192, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([6.9692e-02, 1.3810e-01, 1.6416e-02, 3.8830e-06, 1.7904e-07, 7.5109e-02,
        7.0068e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 192, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.6434e-08, 1.2151e-08, 9.7509e-07, 1.0000e+00, 3.8112e-08, 1.5214e-07,
        3.3239e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 192, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.0679e-01, 6.4466e-03, 2.7488e-01, 1.9027e-05, 6.0805e-03, 5.7830e-03,
        1.8943e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 192, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0048, 0.0048, 0.9668, 0.0047, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 193, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7735e-01, 6.7586e-08, 3.8189e-01, 6.9855e-06, 6.9470e-07, 3.4075e-01,
        4.4570e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.947

[Epoch: 193, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.8427e-02, 1.1864e-01, 1.8407e-02, 4.4758e-06, 2.6778e-07, 8.5065e-02,
        6.8945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.063

[Epoch: 193, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.2020e-08, 6.6322e-09, 6.5237e-07, 1.0000e+00, 5.3783e-08, 1.5784e-07,
        1.3370e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 193, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7889e-01, 4.9220e-03, 3.0546e-01, 1.3503e-05, 5.9906e-03, 4.7272e-03,
        2.2850e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.004

[Epoch: 193, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0045, 0.0053, 0.9668, 0.0048, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.013

[Epoch: 194, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8408e-01, 1.1103e-07, 3.6954e-01, 7.0811e-06, 6.7893e-07, 3.4637e-01,
        8.5400e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.932

[Epoch: 194, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0531e-02, 1.3797e-01, 2.0661e-02, 6.3496e-06, 2.6106e-07, 8.5747e-02,
        6.7508e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.062

[Epoch: 194, batch: 132/222] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1994e-08, 1.5083e-08, 8.8181e-07, 1.0000e+00, 3.8846e-08, 2.2209e-07,
        1.8206e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 194, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2484e-01, 4.8566e-03, 2.6072e-01, 1.4097e-05, 4.2425e-03, 5.3311e-03,
        1.0473e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 194, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0043, 0.0049, 0.9671, 0.0049, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 195, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7005e-01, 7.4199e-08, 3.8345e-01, 9.4538e-06, 5.3914e-07, 3.4649e-01,
        6.6673e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.935

[Epoch: 195, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.2653e-02, 1.3773e-01, 2.2605e-02, 7.9828e-06, 4.9063e-07, 7.7779e-02,
        6.7923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.060

[Epoch: 195, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.9453e-08, 2.4298e-08, 1.5495e-06, 1.0000e+00, 5.6388e-08, 2.2518e-07,
        6.6357e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 195, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7365e-01, 5.1379e-03, 3.1110e-01, 1.5021e-05, 4.4978e-03, 5.5944e-03,
        2.2127e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.013

[Epoch: 195, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.0048, 0.0052, 0.9652, 0.0051, 0.0049, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 196, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8800e-01, 8.4879e-08, 3.6618e-01, 1.2885e-05, 6.6863e-07, 3.4581e-01,
        8.8668e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.939

[Epoch: 196, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.9730e-02, 1.1745e-01, 2.1837e-02, 8.0367e-06, 5.8311e-07, 8.3877e-02,
        6.9709e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.061

[Epoch: 196, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.4697e-08, 2.1828e-08, 1.6550e-06, 1.0000e+00, 4.8003e-08, 2.3832e-07,
        4.5175e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.996

[Epoch: 196, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.3081e-01, 5.1146e-03, 2.5419e-01, 1.1737e-05, 4.9963e-03, 4.8683e-03,
        9.5951e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.015

[Epoch: 196, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0041, 0.0055, 0.9668, 0.0047, 0.0053, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.014

[Epoch: 197, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7621e-01, 1.1608e-07, 3.7406e-01, 1.0256e-05, 9.0715e-07, 3.4972e-01,
        9.3929e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.932

[Epoch: 197, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.8389e-02, 1.1659e-01, 1.9277e-02, 6.7440e-06, 3.3767e-07, 7.9408e-02,
        7.0633e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.066

[Epoch: 197, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.8563e-08, 2.0483e-08, 3.3529e-07, 1.0000e+00, 3.3005e-08, 1.1021e-07,
        3.5833e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.997

[Epoch: 197, batch: 176/222] total loss per batch: 0.581
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.7026e-01, 5.9841e-03, 3.1375e-01, 1.7707e-05, 4.6405e-03, 5.3409e-03,
        2.1175e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.011

[Epoch: 197, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.0048, 0.0048, 0.9658, 0.0051, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 198, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.8524e-01, 9.6181e-08, 3.6362e-01, 1.4557e-05, 1.1182e-06, 3.5113e-01,
        1.3365e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.930

[Epoch: 198, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([7.1647e-02, 1.2682e-01, 1.5973e-02, 6.5311e-06, 3.3099e-07, 7.2843e-02,
        7.1271e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 198, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.3618e-08, 2.2906e-08, 7.8201e-07, 1.0000e+00, 2.8295e-08, 1.5397e-07,
        4.4543e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.998

[Epoch: 198, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2918e-01, 5.0057e-03, 2.5459e-01, 2.1696e-05, 5.1925e-03, 6.0141e-03,
        1.3243e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.006

[Epoch: 198, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.0047, 0.0049, 0.9655, 0.0048, 0.0056, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.015

[Epoch: 199, batch: 44/222] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.9045e-01, 1.0610e-07, 3.6860e-01, 9.7382e-06, 7.7946e-07, 3.4094e-01,
        1.3042e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.955

[Epoch: 199, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.0119e-02, 1.1780e-01, 2.1172e-02, 5.2229e-06, 4.4758e-07, 7.8924e-02,
        7.0198e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.059

[Epoch: 199, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([1.7033e-08, 5.0954e-09, 3.5995e-07, 1.0000e+00, 2.7244e-08, 1.0880e-07,
        1.0837e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 199, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6907e-01, 6.0175e-03, 3.1407e-01, 2.0788e-05, 5.7467e-03, 5.0705e-03,
        1.2473e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 -0.012

[Epoch: 199, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.0048, 0.0049, 0.9652, 0.0049, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.012

[Epoch: 200, batch: 44/222] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.2800, 0.0000, 0.3750, 0.0000, 0.0000, 0.3450, 0.0000])
Policy pred: tensor([2.7976e-01, 1.1327e-07, 3.8118e-01, 1.0355e-05, 7.2432e-07, 3.3905e-01,
        1.0253e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.937 0.950

[Epoch: 200, batch: 88/222] total loss per batch: 0.591
Policy (actual, predicted): 6 6
Policy data: tensor([0.0800, 0.1250, 0.0200, 0.0000, 0.0000, 0.0800, 0.6950])
Policy pred: tensor([8.3207e-02, 1.3249e-01, 1.9701e-02, 4.5375e-06, 3.2902e-07, 8.7809e-02,
        6.7679e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.060 0.068

[Epoch: 200, batch: 132/222] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0., 0., 0., 1., 0., 0., 0.])
Policy pred: tensor([2.1917e-08, 1.5971e-08, 4.0792e-07, 1.0000e+00, 2.5246e-08, 1.8304e-07,
        9.8558e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -0.999

[Epoch: 200, batch: 176/222] total loss per batch: 0.582
Policy (actual, predicted): 0 0
Policy data: tensor([0.7000, 0.0050, 0.2850, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9846e-01, 5.8885e-03, 2.8336e-01, 1.6460e-05, 6.4257e-03, 5.8473e-03,
        1.8181e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.011 0.001

[Epoch: 200, batch: 220/222] total loss per batch: 0.590
Policy (actual, predicted): 3 3
Policy data: tensor([0.0100, 0.0050, 0.0050, 0.9650, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.0052, 0.0055, 0.9633, 0.0048, 0.0059, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.016

