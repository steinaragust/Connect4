Training set samples: 6414
Batch size: 32
[Epoch: 1, batch: 40/201] total loss per batch: 1.060
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1520e-01, 5.3678e-02, 2.2258e-01, 5.1902e-08, 1.8700e-01, 3.5636e-02,
        3.8592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 0.037

[Epoch: 1, batch: 80/201] total loss per batch: 1.075
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2680e-01, 6.3359e-01, 1.4330e-01, 2.9824e-10, 2.3219e-06, 2.0980e-07,
        9.6306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.548

[Epoch: 1, batch: 120/201] total loss per batch: 1.062
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([8.7971e-02, 2.4970e-01, 2.6174e-01, 1.6563e-08, 2.4824e-05, 1.8439e-01,
        2.1617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.246

[Epoch: 1, batch: 160/201] total loss per batch: 0.998
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.6188e-01, 1.0397e-01, 6.0809e-02, 8.8546e-06, 5.5957e-01, 3.9761e-02,
        7.3994e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.026

[Epoch: 1, batch: 200/201] total loss per batch: 1.029
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.5071e-02, 8.3320e-02, 4.6135e-02, 3.1933e-09, 7.0846e-01, 2.3430e-09,
        7.7017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 2, batch: 40/201] total loss per batch: 0.820
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7547e-01, 2.0536e-02, 2.3513e-01, 8.7591e-08, 1.3198e-01, 1.8670e-02,
        4.1822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 0.033

[Epoch: 2, batch: 80/201] total loss per batch: 0.813
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.7491e-02, 8.2098e-01, 4.6544e-02, 1.4958e-09, 4.6573e-07, 7.6937e-08,
        4.4984e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.680

[Epoch: 2, batch: 120/201] total loss per batch: 0.814
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.9753e-02, 2.8282e-01, 3.4501e-01, 1.4248e-08, 1.3728e-05, 1.2924e-01,
        1.9315e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.185

[Epoch: 2, batch: 160/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.6364e-01, 9.3872e-02, 7.2988e-02, 7.5551e-06, 5.7057e-01, 2.8360e-02,
        7.0558e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.051

[Epoch: 2, batch: 200/201] total loss per batch: 0.770
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.5415e-02, 6.8768e-02, 2.5167e-02, 7.1917e-09, 8.1084e-01, 2.4949e-09,
        3.9814e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.344

[Epoch: 3, batch: 40/201] total loss per batch: 0.667
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([5.5748e-02, 1.4957e-02, 4.5396e-01, 8.1636e-08, 5.2367e-02, 7.5138e-03,
        4.1545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.013

[Epoch: 3, batch: 80/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.9904e-02, 8.1542e-01, 2.2708e-02, 7.2906e-09, 2.1769e-07, 2.9248e-08,
        9.1968e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.761

[Epoch: 3, batch: 120/201] total loss per batch: 0.671
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([1.4489e-01, 2.9651e-01, 3.9024e-01, 3.6776e-08, 6.1076e-06, 7.9119e-02,
        8.9241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 3, batch: 160/201] total loss per batch: 0.639
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.2471e-01, 5.8872e-02, 3.9840e-02, 1.7260e-05, 6.8496e-01, 2.5707e-02,
        6.5896e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.020

[Epoch: 3, batch: 200/201] total loss per batch: 0.629
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([2.0460e-02, 3.2665e-02, 8.8966e-03, 1.9518e-08, 9.1562e-01, 1.4118e-09,
        2.2359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.373

[Epoch: 4, batch: 40/201] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([3.9859e-02, 9.3631e-03, 7.4486e-01, 1.1053e-07, 3.2724e-02, 4.1072e-03,
        1.6908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.046

[Epoch: 4, batch: 80/201] total loss per batch: 0.608
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([3.1313e-02, 9.2165e-01, 9.4160e-03, 1.1242e-08, 1.1651e-07, 1.0013e-08,
        3.7626e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.760

[Epoch: 4, batch: 120/201] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([2.5095e-01, 1.7422e-01, 4.6835e-01, 2.3333e-08, 5.0357e-06, 4.2191e-02,
        6.4277e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 4, batch: 160/201] total loss per batch: 0.596
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.1838e-02, 4.6310e-02, 3.3430e-02, 1.4117e-05, 7.6022e-01, 2.0224e-02,
        6.7961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.057

[Epoch: 4, batch: 200/201] total loss per batch: 0.587
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9577e-03, 2.0058e-02, 4.2762e-03, 6.0675e-08, 9.5693e-01, 1.8120e-09,
        1.0774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.303

[Epoch: 5, batch: 40/201] total loss per batch: 0.569
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.1503e-02, 1.4491e-02, 3.8419e-01, 8.6981e-08, 3.2825e-02, 4.3802e-03,
        5.4261e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.060

[Epoch: 5, batch: 80/201] total loss per batch: 0.588
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([2.4382e-02, 9.3945e-01, 7.0062e-03, 4.3755e-09, 9.4946e-08, 1.2984e-08,
        2.9159e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.804

[Epoch: 5, batch: 120/201] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.7761e-01, 7.0594e-02, 4.9974e-01, 2.3583e-08, 1.6572e-06, 2.9838e-02,
        2.2223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.071

[Epoch: 5, batch: 160/201] total loss per batch: 0.580
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.9146e-02, 4.9350e-02, 2.7954e-02, 1.8194e-05, 7.7818e-01, 2.5939e-02,
        5.9413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.038

[Epoch: 5, batch: 200/201] total loss per batch: 0.568
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([2.2939e-02, 3.2696e-02, 4.9522e-03, 2.5694e-08, 9.2349e-01, 2.7445e-09,
        1.5925e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.308

[Epoch: 6, batch: 40/201] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1914e-02, 7.4071e-03, 8.3492e-01, 1.4305e-07, 2.3015e-02, 1.8890e-03,
        1.2086e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.101

[Epoch: 6, batch: 80/201] total loss per batch: 0.575
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3985e-02, 9.5669e-01, 5.8796e-03, 7.0454e-09, 4.9000e-08, 1.5258e-08,
        2.3444e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.845

[Epoch: 6, batch: 120/201] total loss per batch: 0.579
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.2916e-01, 6.0348e-02, 5.7934e-01, 1.3193e-08, 2.0425e-06, 1.8328e-02,
        1.2829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.101

[Epoch: 6, batch: 160/201] total loss per batch: 0.566
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1506e-02, 3.5774e-02, 2.4317e-02, 1.7625e-05, 8.1021e-01, 2.3778e-02,
        5.4397e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.049

[Epoch: 6, batch: 200/201] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.1002e-02, 2.5579e-02, 5.3001e-03, 5.2315e-08, 9.4454e-01, 1.9573e-09,
        1.3584e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.383

[Epoch: 7, batch: 40/201] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0605e-02, 1.4615e-02, 4.9699e-01, 2.3094e-07, 4.7839e-02, 3.9685e-03,
        4.2598e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.121

[Epoch: 7, batch: 80/201] total loss per batch: 0.564
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.8352e-03, 9.6025e-01, 5.0904e-03, 1.0748e-08, 5.1737e-08, 4.5998e-08,
        2.5826e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.853

[Epoch: 7, batch: 120/201] total loss per batch: 0.568
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([5.2647e-01, 3.8398e-02, 3.8888e-01, 2.7229e-08, 2.5118e-06, 2.9273e-02,
        1.6973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.095

[Epoch: 7, batch: 160/201] total loss per batch: 0.557
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([9.3760e-02, 6.9226e-02, 3.2933e-02, 2.6550e-05, 6.9779e-01, 3.7800e-02,
        6.8470e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.071

[Epoch: 7, batch: 200/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.9187e-03, 1.7269e-02, 5.0562e-03, 5.2078e-08, 9.5877e-01, 9.2107e-10,
        8.9880e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.372

[Epoch: 8, batch: 40/201] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2683e-02, 1.5042e-02, 5.9077e-01, 2.4466e-07, 3.5448e-02, 3.2293e-03,
        3.4283e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.158

[Epoch: 8, batch: 80/201] total loss per batch: 0.559
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7491e-03, 9.7754e-01, 4.8797e-03, 1.6993e-09, 3.4375e-08, 1.1228e-08,
        7.8323e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.847

[Epoch: 8, batch: 120/201] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([1.5800e-01, 3.3354e-02, 7.9153e-01, 1.6006e-08, 1.7805e-06, 1.0648e-02,
        6.4660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.091

[Epoch: 8, batch: 160/201] total loss per batch: 0.550
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1142e-02, 4.1316e-02, 2.0377e-02, 1.9080e-05, 7.9215e-01, 4.0008e-02,
        5.4990e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.049

[Epoch: 8, batch: 200/201] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.6221e-03, 1.9919e-02, 5.8365e-03, 6.0424e-08, 9.5751e-01, 1.0371e-09,
        8.1087e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.296

[Epoch: 9, batch: 40/201] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([8.1315e-03, 8.3601e-03, 7.2317e-01, 3.0768e-07, 2.7105e-02, 3.2473e-03,
        2.2999e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.261

[Epoch: 9, batch: 80/201] total loss per batch: 0.557
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.7559e-03, 9.7150e-01, 4.6178e-03, 2.3189e-08, 5.0439e-08, 4.2336e-08,
        1.6126e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.887

[Epoch: 9, batch: 120/201] total loss per batch: 0.560
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([6.4812e-01, 6.0462e-02, 2.2400e-01, 5.4223e-08, 2.1012e-06, 3.5231e-02,
        3.2189e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.137

[Epoch: 9, batch: 160/201] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6703e-02, 5.5772e-02, 3.1090e-02, 1.8856e-05, 7.4914e-01, 4.8007e-02,
        5.9269e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.035

[Epoch: 9, batch: 200/201] total loss per batch: 0.528
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.0332e-03, 1.6168e-02, 5.5180e-03, 1.3002e-07, 9.6127e-01, 1.1567e-09,
        9.0149e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.379

[Epoch: 10, batch: 40/201] total loss per batch: 0.532
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2238e-02, 1.5296e-02, 4.1106e-01, 2.7128e-07, 4.3484e-02, 4.5344e-03,
        5.1339e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.311

[Epoch: 10, batch: 80/201] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.6351e-03, 9.5788e-01, 7.8754e-03, 1.3028e-08, 6.1152e-08, 3.8974e-08,
        2.5607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.864

[Epoch: 10, batch: 120/201] total loss per batch: 0.555
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.7027e-01, 3.6286e-02, 4.6484e-01, 2.4843e-08, 2.8200e-06, 2.3037e-02,
        5.5682e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.099

[Epoch: 10, batch: 160/201] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.9864e-02, 6.8496e-02, 2.0331e-02, 2.4346e-05, 7.4938e-01, 5.1933e-02,
        3.9968e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.053

[Epoch: 10, batch: 200/201] total loss per batch: 0.527
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.7060e-03, 1.5761e-02, 4.8067e-03, 1.7799e-07, 9.6381e-01, 2.8197e-09,
        8.9205e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.249

[Epoch: 11, batch: 40/201] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([5.5382e-03, 6.6810e-03, 7.7655e-01, 6.6120e-07, 1.5704e-02, 2.1447e-03,
        1.9338e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.373

[Epoch: 11, batch: 80/201] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.5668e-02, 9.6632e-01, 1.0499e-02, 3.6733e-08, 6.3170e-08, 4.1793e-08,
        7.5125e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.890

[Epoch: 11, batch: 120/201] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3298e-01, 3.7754e-02, 5.0401e-01, 6.8839e-08, 2.1963e-06, 1.8983e-02,
        6.2712e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.091

[Epoch: 11, batch: 160/201] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6920e-02, 4.7138e-02, 2.1172e-02, 2.6796e-05, 7.7143e-01, 4.9992e-02,
        6.3320e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.054

[Epoch: 11, batch: 200/201] total loss per batch: 0.526
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.8095e-03, 2.1078e-02, 6.5465e-03, 1.5034e-07, 9.4872e-01, 1.2977e-09,
        1.3841e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.364

[Epoch: 12, batch: 40/201] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([8.7295e-03, 1.2038e-02, 5.8839e-01, 1.6916e-07, 1.8035e-02, 3.4451e-03,
        3.6936e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.366

[Epoch: 12, batch: 80/201] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4489e-03, 9.7971e-01, 6.6651e-03, 9.7097e-09, 4.2533e-08, 1.3866e-08,
        4.1790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.883

[Epoch: 12, batch: 120/201] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0263e-01, 3.8520e-02, 5.2301e-01, 4.1152e-08, 3.1416e-06, 2.8994e-02,
        6.8474e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 12, batch: 160/201] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2815e-02, 4.0179e-02, 2.0766e-02, 2.1110e-05, 7.6971e-01, 6.2573e-02,
        4.3939e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.083

[Epoch: 12, batch: 200/201] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8436e-03, 2.0876e-02, 6.3248e-03, 9.2099e-08, 9.5076e-01, 2.3629e-10,
        1.5199e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.328

[Epoch: 13, batch: 40/201] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1004e-02, 1.0823e-02, 6.4281e-01, 2.4688e-07, 1.9223e-02, 2.1316e-03,
        3.1401e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.458

[Epoch: 13, batch: 80/201] total loss per batch: 0.549
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7246e-03, 9.6084e-01, 1.6255e-02, 7.4013e-08, 1.2800e-07, 2.4344e-08,
        1.3176e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.910

[Epoch: 13, batch: 120/201] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4082e-01, 2.6367e-02, 5.1412e-01, 3.3593e-08, 2.3336e-06, 1.4455e-02,
        4.2413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.095

[Epoch: 13, batch: 160/201] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0477e-02, 5.1197e-02, 2.1746e-02, 2.3729e-05, 7.4381e-01, 6.4312e-02,
        5.8431e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 13, batch: 200/201] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6356e-03, 1.8088e-02, 6.3843e-03, 1.3113e-07, 9.5856e-01, 1.4453e-09,
        1.0328e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.281

[Epoch: 14, batch: 40/201] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.3412e-03, 1.6703e-02, 5.4811e-01, 1.2071e-07, 1.5975e-02, 2.7406e-03,
        4.0713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.464

[Epoch: 14, batch: 80/201] total loss per batch: 0.548
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3504e-02, 9.6115e-01, 1.5773e-02, 1.6642e-08, 1.8429e-07, 1.4229e-08,
        9.5706e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.901

[Epoch: 14, batch: 120/201] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9977e-01, 3.9223e-02, 5.2602e-01, 3.1050e-08, 2.8888e-06, 2.5101e-02,
        9.8851e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 14, batch: 160/201] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2951e-02, 4.1467e-02, 2.1646e-02, 1.6203e-05, 7.6183e-01, 6.4963e-02,
        4.7128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.082

[Epoch: 14, batch: 200/201] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.6322e-03, 1.8499e-02, 5.8464e-03, 1.0687e-07, 9.5684e-01, 5.0689e-10,
        1.3183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.326

[Epoch: 15, batch: 40/201] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1555e-02, 9.6696e-03, 6.4484e-01, 1.9068e-07, 2.5292e-02, 3.0855e-03,
        3.0556e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.496

[Epoch: 15, batch: 80/201] total loss per batch: 0.548
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.5299e-03, 9.8185e-01, 5.9676e-03, 1.7474e-08, 6.8737e-08, 2.3225e-08,
        5.6503e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.905

[Epoch: 15, batch: 120/201] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1391e-01, 2.1608e-02, 5.4437e-01, 1.2782e-08, 2.3890e-06, 1.6103e-02,
        4.0068e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.094

[Epoch: 15, batch: 160/201] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.6016e-02, 3.7130e-02, 1.7088e-02, 9.7350e-06, 8.2190e-01, 3.8821e-02,
        4.9030e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 15, batch: 200/201] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.9531e-03, 2.2208e-02, 8.0799e-03, 4.6458e-08, 9.4539e-01, 2.6968e-10,
        1.5373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.334

[Epoch: 16, batch: 40/201] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.1941e-03, 1.3009e-02, 6.1097e-01, 5.0952e-08, 2.5867e-02, 2.8276e-03,
        3.3813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.453

[Epoch: 16, batch: 80/201] total loss per batch: 0.547
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.9588e-02, 9.3377e-01, 3.1864e-02, 5.5696e-08, 2.0752e-07, 3.9725e-08,
        1.4775e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.921

[Epoch: 16, batch: 120/201] total loss per batch: 0.549
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.6399e-01, 6.3185e-02, 4.3932e-01, 1.1054e-07, 3.3279e-06, 2.5558e-02,
        7.9473e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.148

[Epoch: 16, batch: 160/201] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.2573e-02, 6.4870e-02, 2.5846e-02, 4.8641e-05, 6.9886e-01, 7.9464e-02,
        5.8342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 16, batch: 200/201] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.6024e-03, 1.3918e-02, 6.8770e-03, 1.4335e-06, 9.5955e-01, 1.8823e-08,
        1.0052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.273

[Epoch: 17, batch: 40/201] total loss per batch: 0.529
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3684e-02, 8.3665e-03, 5.1161e-01, 1.1188e-06, 1.6450e-02, 2.2594e-03,
        4.4763e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.498

[Epoch: 17, batch: 80/201] total loss per batch: 0.548
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([3.7455e-03, 9.8456e-01, 7.5606e-03, 8.2087e-09, 4.0722e-08, 2.7303e-08,
        4.1383e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.904

[Epoch: 17, batch: 120/201] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.2104e-01, 3.0033e-02, 6.3242e-01, 2.8690e-08, 2.7388e-06, 1.3038e-02,
        3.4653e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 17, batch: 160/201] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.6118e-02, 4.4689e-02, 2.6180e-02, 2.9703e-05, 7.3968e-01, 7.3449e-02,
        4.9850e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.078

[Epoch: 17, batch: 200/201] total loss per batch: 0.527
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.0474e-02, 2.3339e-02, 9.1614e-03, 2.2667e-07, 9.4073e-01, 1.5615e-09,
        1.6296e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.395

[Epoch: 18, batch: 40/201] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3510e-02, 7.7542e-03, 7.2932e-01, 1.3495e-06, 1.3236e-02, 2.8961e-03,
        2.3329e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.433

[Epoch: 18, batch: 80/201] total loss per batch: 0.549
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([5.5456e-03, 9.8155e-01, 8.7869e-03, 6.6587e-09, 5.0265e-08, 1.1878e-07,
        4.1164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.920

[Epoch: 18, batch: 120/201] total loss per batch: 0.550
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([5.1767e-01, 4.8037e-02, 4.1448e-01, 4.4884e-08, 2.9280e-06, 1.5782e-02,
        4.0329e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.135

[Epoch: 18, batch: 160/201] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.0479e-02, 3.9754e-02, 1.2209e-02, 4.3386e-05, 8.3572e-01, 3.3483e-02,
        4.8315e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.068

[Epoch: 18, batch: 200/201] total loss per batch: 0.525
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6760e-03, 1.0768e-02, 1.3006e-02, 2.3926e-07, 9.5814e-01, 1.4662e-09,
        1.1414e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.295

[Epoch: 19, batch: 40/201] total loss per batch: 0.532
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.5825e-02, 1.3481e-02, 5.1675e-01, 1.2525e-07, 2.2708e-02, 3.8911e-03,
        4.1735e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.510

[Epoch: 19, batch: 80/201] total loss per batch: 0.552
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.9098e-02, 9.6728e-01, 8.3938e-03, 1.8106e-08, 1.0052e-07, 2.9573e-08,
        5.2301e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.941

[Epoch: 19, batch: 120/201] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9709e-01, 5.1601e-02, 5.2309e-01, 4.5386e-08, 3.7916e-06, 2.0646e-02,
        7.5673e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.098

[Epoch: 19, batch: 160/201] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.3182e-02, 6.2439e-02, 3.2729e-02, 2.0184e-05, 7.0120e-01, 8.3781e-02,
        4.6645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.116

[Epoch: 19, batch: 200/201] total loss per batch: 0.526
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8053e-03, 1.0301e-02, 5.9667e-03, 3.1007e-07, 9.6518e-01, 4.9256e-10,
        1.1748e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 20, batch: 40/201] total loss per batch: 0.533
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6762e-02, 1.2708e-02, 6.1206e-01, 1.1095e-07, 2.4816e-02, 3.6846e-03,
        3.2997e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.353

[Epoch: 20, batch: 80/201] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.5385e-02, 9.5972e-01, 1.6084e-02, 5.6566e-08, 4.7804e-07, 7.1370e-08,
        8.8072e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 20, batch: 120/201] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9256e-01, 3.9235e-02, 5.4045e-01, 1.0660e-08, 2.7625e-06, 2.1780e-02,
        5.9736e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 20, batch: 160/201] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.3306e-02, 6.6610e-02, 2.3392e-02, 1.4510e-05, 7.1312e-01, 7.0734e-02,
        5.2821e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.144

[Epoch: 20, batch: 200/201] total loss per batch: 0.526
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8362e-03, 1.5781e-02, 1.1097e-02, 9.2749e-08, 9.5525e-01, 3.8427e-10,
        1.1036e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.281

[Epoch: 21, batch: 40/201] total loss per batch: 0.531
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7856e-02, 7.1193e-03, 6.5691e-01, 4.1185e-07, 2.6701e-02, 2.3544e-03,
        2.8906e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.818

[Epoch: 21, batch: 80/201] total loss per batch: 0.554
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.2790e-03, 9.7566e-01, 8.9298e-03, 3.1507e-08, 1.7472e-07, 2.2337e-08,
        8.1303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.914

[Epoch: 21, batch: 120/201] total loss per batch: 0.553
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.9044e-01, 3.1245e-02, 4.4736e-01, 6.1202e-08, 7.2522e-06, 2.2029e-02,
        8.9178e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.152

[Epoch: 21, batch: 160/201] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.6169e-02, 3.1970e-02, 1.6111e-02, 2.2778e-05, 7.7515e-01, 5.9893e-02,
        8.0690e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.116

[Epoch: 21, batch: 200/201] total loss per batch: 0.530
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.2883e-03, 2.2626e-02, 6.5637e-03, 1.4453e-06, 9.3012e-01, 6.1215e-10,
        3.2398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.326

[Epoch: 22, batch: 40/201] total loss per batch: 0.534
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.3919e-02, 2.1862e-02, 4.8081e-01, 5.2296e-07, 3.2382e-02, 4.1601e-03,
        4.3687e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.312

[Epoch: 22, batch: 80/201] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5782e-03, 9.8008e-01, 5.8505e-03, 3.7568e-09, 8.3917e-08, 2.9635e-08,
        6.4905e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.903

[Epoch: 22, batch: 120/201] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.1713e-01, 3.2689e-02, 6.2441e-01, 6.9947e-09, 4.6286e-06, 2.1993e-02,
        3.7753e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.136

[Epoch: 22, batch: 160/201] total loss per batch: 0.548
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5671e-02, 5.1758e-02, 2.3993e-02, 1.0345e-04, 7.6685e-01, 6.9560e-02,
        3.2063e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 22, batch: 200/201] total loss per batch: 0.529
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.9201e-02, 2.4811e-02, 1.0665e-02, 1.2755e-06, 9.2575e-01, 3.5254e-10,
        1.9574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.196

[Epoch: 23, batch: 40/201] total loss per batch: 0.537
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2249e-02, 1.0706e-02, 7.3765e-01, 7.3368e-08, 1.1752e-02, 2.8516e-03,
        2.2479e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.399

[Epoch: 23, batch: 80/201] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([3.8644e-02, 9.4450e-01, 1.1238e-02, 3.0650e-08, 2.7292e-07, 1.6161e-08,
        5.6203e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.915

[Epoch: 23, batch: 120/201] total loss per batch: 0.551
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.7317e-01, 5.0367e-02, 4.3780e-01, 4.9132e-08, 4.3149e-06, 3.2703e-02,
        5.9508e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.099

[Epoch: 23, batch: 160/201] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0462e-02, 7.1483e-02, 2.1216e-02, 3.4675e-05, 7.5408e-01, 4.0947e-02,
        6.1779e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.137

[Epoch: 23, batch: 200/201] total loss per batch: 0.525
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.2753e-02, 2.5114e-02, 1.0071e-02, 1.5234e-07, 9.4051e-01, 4.4718e-11,
        1.1551e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.368

[Epoch: 24, batch: 40/201] total loss per batch: 0.532
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3462e-02, 8.3563e-03, 4.3889e-01, 8.4666e-08, 1.4398e-02, 1.9352e-03,
        5.2296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.552

[Epoch: 24, batch: 80/201] total loss per batch: 0.550
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([4.5445e-03, 9.8086e-01, 7.8482e-03, 7.1723e-09, 1.0396e-07, 6.3654e-09,
        6.7489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.922

[Epoch: 24, batch: 120/201] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.6267e-01, 3.7608e-02, 5.7936e-01, 2.3076e-08, 4.6184e-06, 1.6769e-02,
        3.5875e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 24, batch: 160/201] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.5819e-02, 5.2148e-02, 1.6785e-02, 6.5408e-05, 7.4810e-01, 6.6735e-02,
        7.0345e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 24, batch: 200/201] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5746e-03, 1.5528e-02, 7.4369e-03, 4.8292e-07, 9.6252e-01, 2.2956e-09,
        7.9409e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.344

[Epoch: 25, batch: 40/201] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.6862e-02, 1.3652e-02, 7.1090e-01, 4.8871e-07, 1.5122e-02, 4.8363e-03,
        2.2863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.508

[Epoch: 25, batch: 80/201] total loss per batch: 0.546
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.3762e-03, 9.7115e-01, 1.1311e-02, 2.5933e-08, 2.7076e-07, 5.6198e-08,
        9.1610e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.907

[Epoch: 25, batch: 120/201] total loss per batch: 0.547
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.9366e-01, 4.4027e-02, 4.2950e-01, 2.5930e-08, 2.8970e-06, 2.2460e-02,
        1.0352e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.139

[Epoch: 25, batch: 160/201] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0492e-02, 4.5484e-02, 2.6227e-02, 3.8809e-05, 7.7836e-01, 5.8816e-02,
        4.0584e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 25, batch: 200/201] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.7379e-03, 1.7984e-02, 8.5856e-03, 2.1933e-07, 9.5549e-01, 3.5319e-10,
        1.1206e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.344

[Epoch: 26, batch: 40/201] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.2332e-02, 1.8325e-02, 5.8281e-01, 6.6557e-07, 1.6741e-02, 5.7841e-03,
        3.5401e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.464

[Epoch: 26, batch: 80/201] total loss per batch: 0.544
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.8881e-03, 9.7468e-01, 1.1870e-02, 1.5361e-08, 2.0253e-07, 2.9859e-08,
        6.5626e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.914

[Epoch: 26, batch: 120/201] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9228e-01, 2.3368e-02, 5.6274e-01, 1.5988e-08, 2.6137e-06, 1.8416e-02,
        3.1921e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.110

[Epoch: 26, batch: 160/201] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2738e-02, 6.9814e-02, 1.8818e-02, 6.2009e-05, 7.1355e-01, 7.0062e-02,
        6.4960e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.117

[Epoch: 26, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.6399e-03, 1.3575e-02, 9.2691e-03, 3.0469e-07, 9.5699e-01, 8.9998e-10,
        1.1523e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.290

[Epoch: 27, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5276e-02, 1.3222e-02, 5.8461e-01, 1.1287e-07, 1.6214e-02, 3.9376e-03,
        3.6674e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.466

[Epoch: 27, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0741e-02, 9.7349e-01, 1.0891e-02, 2.3758e-08, 1.7045e-07, 2.0502e-08,
        4.8766e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.949

[Epoch: 27, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3072e-01, 3.5779e-02, 5.1369e-01, 1.1388e-08, 2.3655e-06, 1.6312e-02,
        3.4989e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.090

[Epoch: 27, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.5580e-02, 3.0878e-02, 1.5372e-02, 7.8217e-05, 8.4525e-01, 4.2554e-02,
        3.0291e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.114

[Epoch: 27, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8110e-03, 1.8566e-02, 7.3768e-03, 1.9331e-07, 9.5289e-01, 3.7809e-10,
        1.4359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 28, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0219e-02, 1.0913e-02, 6.5955e-01, 3.0232e-07, 1.1650e-02, 3.6558e-03,
        3.0401e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.506

[Epoch: 28, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.6098e-03, 9.7213e-01, 1.0463e-02, 1.7633e-08, 1.9132e-07, 3.9083e-08,
        9.7973e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.935

[Epoch: 28, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3471e-01, 3.7010e-02, 5.0306e-01, 1.3400e-08, 1.8511e-06, 2.0699e-02,
        4.5233e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.144

[Epoch: 28, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.1590e-02, 4.8695e-02, 2.0478e-02, 5.8564e-05, 7.0247e-01, 8.0285e-02,
        7.6427e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.125

[Epoch: 28, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.1531e-03, 1.4959e-02, 7.8081e-03, 2.9679e-07, 9.5992e-01, 2.8623e-10,
        1.2156e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.301

[Epoch: 29, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.9495e-02, 1.6516e-02, 4.9662e-01, 1.3028e-07, 1.9159e-02, 4.6887e-03,
        4.4352e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.526

[Epoch: 29, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2403e-02, 9.6772e-01, 1.3874e-02, 8.4328e-09, 1.0659e-07, 1.5644e-08,
        6.0080e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 29, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9459e-01, 3.2845e-02, 5.5422e-01, 6.9041e-09, 1.5476e-06, 1.4624e-02,
        3.7216e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.133

[Epoch: 29, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9490e-02, 5.0187e-02, 1.4647e-02, 4.5131e-05, 7.7955e-01, 6.1491e-02,
        4.4594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 29, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.5145e-03, 1.7254e-02, 1.0676e-02, 1.4790e-07, 9.4200e-01, 1.3829e-10,
        2.1558e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.378

[Epoch: 30, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.8337e-03, 7.2024e-03, 7.0489e-01, 8.6718e-08, 1.4238e-02, 3.2714e-03,
        2.6056e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.500

[Epoch: 30, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.5553e-03, 9.6813e-01, 1.5217e-02, 2.2645e-08, 1.8740e-07, 4.4359e-08,
        7.0951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.949

[Epoch: 30, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1446e-01, 3.8381e-02, 5.1806e-01, 1.2340e-08, 2.4337e-06, 2.4275e-02,
        4.8163e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 30, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4553e-02, 4.2922e-02, 1.7929e-02, 4.0982e-05, 7.7612e-01, 5.8472e-02,
        4.9962e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.103

[Epoch: 30, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([4.5796e-03, 1.2199e-02, 9.7579e-03, 2.4101e-07, 9.6066e-01, 2.1050e-10,
        1.2803e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.265

[Epoch: 31, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4889e-02, 1.1894e-02, 5.1237e-01, 6.0379e-08, 2.3124e-02, 5.3987e-03,
        4.3232e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.453

[Epoch: 31, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0061e-02, 9.7080e-01, 1.2697e-02, 1.4065e-08, 1.5416e-07, 1.1696e-08,
        6.4456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 31, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2843e-01, 2.5037e-02, 5.2514e-01, 5.6430e-09, 1.8601e-06, 1.8512e-02,
        2.8845e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.136

[Epoch: 31, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4871e-02, 4.8803e-02, 1.8349e-02, 6.9721e-05, 7.5200e-01, 6.5296e-02,
        6.0614e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.118

[Epoch: 31, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6728e-03, 1.3164e-02, 9.0576e-03, 2.7841e-07, 9.6204e-01, 1.0255e-10,
        9.0659e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.302

[Epoch: 32, batch: 40/201] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([7.9250e-03, 8.4783e-03, 6.6433e-01, 2.4414e-07, 1.0991e-02, 2.5094e-03,
        3.0577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.555

[Epoch: 32, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.5890e-03, 9.6911e-01, 1.0649e-02, 9.3987e-09, 1.4276e-07, 1.9956e-08,
        1.0654e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 32, batch: 120/201] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.6181e-01, 5.5642e-02, 5.6205e-01, 1.7002e-08, 2.5988e-06, 1.7655e-02,
        2.8401e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.119

[Epoch: 32, batch: 160/201] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0212e-02, 4.4723e-02, 1.9935e-02, 7.4265e-05, 7.5544e-01, 7.7626e-02,
        5.1987e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.135

[Epoch: 32, batch: 200/201] total loss per batch: 0.519
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.1009e-03, 1.6422e-02, 9.1891e-03, 1.2805e-07, 9.5492e-01, 6.3714e-11,
        1.4364e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 33, batch: 40/201] total loss per batch: 0.523
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8811e-02, 1.1488e-02, 6.2978e-01, 1.4318e-07, 1.4239e-02, 3.9330e-03,
        3.2175e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.446

[Epoch: 33, batch: 80/201] total loss per batch: 0.545
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4271e-02, 9.6150e-01, 1.5865e-02, 2.7122e-08, 3.8099e-07, 4.1467e-08,
        8.3615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 33, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.7065e-01, 1.8958e-02, 4.7893e-01, 1.7239e-08, 4.5167e-06, 2.5159e-02,
        6.2938e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.158

[Epoch: 33, batch: 160/201] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.4117e-02, 6.1018e-02, 1.5365e-02, 9.0617e-05, 7.3739e-01, 4.6195e-02,
        6.5829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 33, batch: 200/201] total loss per batch: 0.520
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.8563e-03, 1.0195e-02, 7.4681e-03, 7.3162e-07, 9.6388e-01, 8.5154e-10,
        1.2597e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 34, batch: 40/201] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.6184e-03, 1.3167e-02, 4.8403e-01, 5.3657e-07, 2.0973e-02, 4.2141e-03,
        4.6800e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.583

[Epoch: 34, batch: 80/201] total loss per batch: 0.546
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.3815e-03, 9.6829e-01, 1.2899e-02, 2.7824e-08, 3.1457e-07, 2.9247e-08,
        1.1429e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.927

[Epoch: 34, batch: 120/201] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0762e-01, 4.4569e-02, 5.2577e-01, 2.7041e-08, 1.6464e-06, 1.5609e-02,
        6.4298e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.142

[Epoch: 34, batch: 160/201] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.9113e-02, 3.6025e-02, 1.7104e-02, 5.9530e-05, 8.3244e-01, 5.0309e-02,
        2.4945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.087

[Epoch: 34, batch: 200/201] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6801e-03, 1.4038e-02, 1.0128e-02, 1.9749e-07, 9.5452e-01, 1.2776e-10,
        1.4630e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.287

[Epoch: 35, batch: 40/201] total loss per batch: 0.524
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0297e-02, 6.5028e-03, 6.6030e-01, 1.0164e-07, 1.6184e-02, 3.7276e-03,
        3.0298e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.461

[Epoch: 35, batch: 80/201] total loss per batch: 0.547
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5423e-03, 9.7639e-01, 1.0076e-02, 1.0571e-08, 1.3846e-07, 4.2171e-08,
        5.9932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.926

[Epoch: 35, batch: 120/201] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.6491e-01, 2.5570e-02, 5.9541e-01, 6.5420e-09, 1.2153e-06, 1.1265e-02,
        2.8447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 35, batch: 160/201] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.7907e-02, 5.4092e-02, 2.2432e-02, 1.2112e-04, 7.0544e-01, 8.0038e-02,
        6.9971e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.148

[Epoch: 35, batch: 200/201] total loss per batch: 0.523
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.4761e-02, 1.8778e-02, 1.9247e-02, 1.9157e-06, 9.2664e-01, 1.9151e-10,
        2.0573e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.471

[Epoch: 36, batch: 40/201] total loss per batch: 0.526
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3632e-02, 1.4744e-02, 5.8969e-01, 1.2276e-06, 1.1383e-02, 3.0042e-03,
        3.6754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.499

[Epoch: 36, batch: 80/201] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([3.4498e-02, 9.2223e-01, 3.5784e-02, 2.3963e-07, 4.7090e-07, 2.0387e-08,
        7.4909e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 36, batch: 120/201] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.5316e-01, 5.3413e-02, 4.5409e-01, 9.3183e-08, 1.0279e-05, 3.5423e-02,
        3.9047e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.018

[Epoch: 36, batch: 160/201] total loss per batch: 0.554
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2681e-02, 7.5578e-02, 2.2335e-02, 1.4908e-05, 6.8644e-01, 8.2696e-02,
        7.0252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.120

[Epoch: 36, batch: 200/201] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.3257e-02, 1.9973e-02, 1.0252e-02, 3.2764e-07, 9.3755e-01, 3.5849e-10,
        1.8965e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.225

[Epoch: 37, batch: 40/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8869e-02, 1.2720e-02, 5.9134e-01, 6.4173e-09, 2.4892e-02, 2.9400e-03,
        3.4924e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.399

[Epoch: 37, batch: 80/201] total loss per batch: 0.572
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.9748e-03, 9.7763e-01, 2.1166e-03, 2.5311e-08, 9.8904e-09, 4.2340e-07,
        1.8282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.870

[Epoch: 37, batch: 120/201] total loss per batch: 0.631
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([2.1639e-01, 1.5713e-01, 5.9378e-01, 1.3600e-08, 7.0711e-07, 1.8415e-02,
        1.4295e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.143

[Epoch: 37, batch: 160/201] total loss per batch: 0.626
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.0449e-02, 2.2638e-02, 9.2178e-03, 6.2757e-06, 8.7295e-01, 5.1307e-02,
        1.3430e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.094

[Epoch: 37, batch: 200/201] total loss per batch: 0.599
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.1809e-02, 1.0716e-02, 5.2880e-03, 3.3761e-08, 8.9920e-01, 4.2370e-09,
        7.2986e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.052

[Epoch: 38, batch: 40/201] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.4657e-02, 3.2249e-02, 4.6980e-01, 3.6245e-08, 1.4052e-02, 3.1321e-03,
        4.5611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.441

[Epoch: 38, batch: 80/201] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([2.9431e-02, 9.3568e-01, 2.9203e-02, 1.9235e-07, 3.5082e-07, 1.3369e-06,
        5.6832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.868

[Epoch: 38, batch: 120/201] total loss per batch: 0.640
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([6.7068e-01, 4.8386e-03, 2.8802e-01, 1.8714e-08, 3.1545e-06, 3.2027e-02,
        4.4307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 -0.053

[Epoch: 38, batch: 160/201] total loss per batch: 0.625
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5332e-02, 3.7382e-02, 1.7754e-02, 1.7938e-05, 7.9797e-01, 2.7262e-02,
        6.4279e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.211

[Epoch: 38, batch: 200/201] total loss per batch: 0.580
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9989e-03, 1.5310e-02, 4.7964e-03, 2.2471e-06, 9.6523e-01, 8.3328e-09,
        6.6617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.324

[Epoch: 39, batch: 40/201] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2660e-02, 4.4489e-02, 6.2789e-01, 3.1296e-07, 9.1936e-03, 6.3085e-03,
        2.9946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.363

[Epoch: 39, batch: 80/201] total loss per batch: 0.588
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0961e-02, 9.7505e-01, 3.4345e-03, 3.3693e-07, 4.4953e-07, 3.9907e-07,
        1.0558e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.917

[Epoch: 39, batch: 120/201] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([2.7402e-01, 7.2063e-02, 5.5564e-01, 1.7138e-07, 6.5535e-06, 7.7322e-02,
        2.0945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.107

[Epoch: 39, batch: 160/201] total loss per batch: 0.568
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.9713e-02, 4.9690e-02, 2.5928e-02, 1.7650e-04, 7.8389e-01, 2.4671e-02,
        7.5933e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.088

[Epoch: 39, batch: 200/201] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.4031e-02, 3.0379e-02, 1.2191e-02, 3.4453e-06, 9.2933e-01, 1.8931e-08,
        1.4066e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.343

[Epoch: 40, batch: 40/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([6.9027e-03, 2.3725e-02, 6.5133e-01, 3.7934e-07, 1.6327e-02, 2.8479e-03,
        2.9886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.447

[Epoch: 40, batch: 80/201] total loss per batch: 0.556
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7303e-03, 9.6775e-01, 4.9283e-03, 3.1920e-08, 6.5208e-08, 5.6609e-08,
        1.7588e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.900

[Epoch: 40, batch: 120/201] total loss per batch: 0.557
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([5.0402e-01, 9.2443e-03, 4.4583e-01, 5.3822e-08, 3.6062e-06, 3.0627e-02,
        1.0272e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.024

[Epoch: 40, batch: 160/201] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1103e-02, 3.4680e-02, 2.5354e-02, 1.3040e-04, 7.8459e-01, 4.8120e-02,
        5.6026e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 40, batch: 200/201] total loss per batch: 0.524
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.3803e-02, 1.8202e-02, 9.1776e-03, 5.3159e-07, 9.4948e-01, 6.3334e-09,
        9.3346e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 41, batch: 40/201] total loss per batch: 0.525
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5053e-02, 2.2609e-02, 5.3958e-01, 1.4761e-07, 2.9774e-02, 4.7928e-03,
        3.8819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.450

[Epoch: 41, batch: 80/201] total loss per batch: 0.546
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3915e-02, 9.6518e-01, 6.5404e-03, 5.5295e-08, 1.4834e-07, 4.6801e-08,
        1.4368e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.926

[Epoch: 41, batch: 120/201] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.6057e-01, 1.1708e-02, 5.9659e-01, 3.3314e-08, 3.0714e-06, 2.2323e-02,
        8.8025e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.064

[Epoch: 41, batch: 160/201] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3963e-02, 4.6780e-02, 2.4410e-02, 1.1511e-04, 7.6319e-01, 4.9610e-02,
        6.1934e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.103

[Epoch: 41, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.3987e-02, 2.0847e-02, 1.1319e-02, 6.5724e-07, 9.4389e-01, 6.3842e-09,
        9.9566e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.333

[Epoch: 42, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1143e-02, 1.2855e-02, 6.7533e-01, 1.0406e-07, 1.9971e-02, 4.5546e-03,
        2.7615e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.483

[Epoch: 42, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1621e-02, 9.6870e-01, 7.6097e-03, 4.5162e-08, 1.3225e-07, 3.9300e-08,
        1.2074e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.920

[Epoch: 42, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1971e-01, 1.6210e-02, 5.3793e-01, 2.4345e-08, 2.1659e-06, 1.9644e-02,
        6.4993e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.083

[Epoch: 42, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2329e-02, 4.4254e-02, 2.1924e-02, 7.4502e-05, 7.7072e-01, 5.2508e-02,
        5.8188e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 42, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.1720e-02, 1.7799e-02, 1.0357e-02, 4.2425e-07, 9.5081e-01, 3.1663e-09,
        9.3176e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 43, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1879e-02, 1.3236e-02, 5.0690e-01, 1.3577e-07, 1.8070e-02, 4.2380e-03,
        4.4567e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.480

[Epoch: 43, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0048e-02, 9.7198e-01, 8.0272e-03, 3.8856e-08, 1.3130e-07, 3.8902e-08,
        9.9421e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.930

[Epoch: 43, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1336e-01, 1.8648e-02, 5.4121e-01, 2.8032e-08, 2.2615e-06, 2.0962e-02,
        5.8125e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.106

[Epoch: 43, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5749e-02, 5.0657e-02, 2.2381e-02, 6.6654e-05, 7.4898e-01, 6.4943e-02,
        5.7226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 43, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.1036e-02, 1.6117e-02, 1.0020e-02, 3.2659e-07, 9.5257e-01, 2.1556e-09,
        1.0252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.328

[Epoch: 44, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1242e-02, 9.7557e-03, 7.2226e-01, 8.2231e-08, 1.7249e-02, 3.8078e-03,
        2.3568e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 44, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2069e-02, 9.6906e-01, 9.4459e-03, 3.1645e-08, 1.0285e-07, 3.0423e-08,
        9.4245e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.934

[Epoch: 44, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2511e-01, 2.1451e-02, 5.2937e-01, 1.5920e-08, 1.8195e-06, 1.8816e-02,
        5.2428e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.089

[Epoch: 44, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8049e-02, 4.3871e-02, 1.7847e-02, 5.7211e-05, 7.8363e-01, 5.8114e-02,
        4.8433e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 44, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.0071e-02, 1.5359e-02, 9.8848e-03, 2.8187e-07, 9.5534e-01, 1.8512e-09,
        9.3406e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.306

[Epoch: 45, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2781e-02, 1.2184e-02, 5.4844e-01, 7.0861e-08, 1.8034e-02, 4.0137e-03,
        4.0454e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.485

[Epoch: 45, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.3724e-03, 9.7355e-01, 9.0962e-03, 2.8871e-08, 1.0666e-07, 2.3293e-08,
        7.9781e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.940

[Epoch: 45, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1286e-01, 2.5375e-02, 5.3755e-01, 2.1982e-08, 1.9157e-06, 1.9723e-02,
        4.4976e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.107

[Epoch: 45, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.9279e-02, 5.1908e-02, 2.0108e-02, 5.3275e-05, 7.3826e-01, 6.8351e-02,
        6.2040e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 45, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.8321e-03, 1.4405e-02, 9.6272e-03, 1.8502e-07, 9.5531e-01, 1.0165e-09,
        1.0822e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.340

[Epoch: 46, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2721e-02, 1.0781e-02, 6.6120e-01, 4.5129e-08, 1.4877e-02, 3.2629e-03,
        2.9716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.518

[Epoch: 46, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1814e-02, 9.6998e-01, 1.0336e-02, 2.6541e-08, 1.0349e-07, 2.5844e-08,
        7.8730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.939

[Epoch: 46, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3192e-01, 2.7622e-02, 5.1782e-01, 1.4270e-08, 1.6872e-06, 1.8242e-02,
        4.3890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.109

[Epoch: 46, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6343e-02, 4.2803e-02, 1.6013e-02, 4.5274e-05, 7.8957e-01, 6.0706e-02,
        4.4516e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 46, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.8706e-03, 1.4108e-02, 9.0069e-03, 1.5517e-07, 9.5742e-01, 1.0530e-09,
        1.0595e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.295

[Epoch: 47, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2924e-02, 1.1691e-02, 5.8542e-01, 4.4661e-08, 1.7857e-02, 3.9033e-03,
        3.6820e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.493

[Epoch: 47, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0336e-02, 9.7164e-01, 1.0644e-02, 2.1023e-08, 9.2722e-08, 1.7682e-08,
        7.3783e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.942

[Epoch: 47, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1165e-01, 2.9555e-02, 5.3500e-01, 1.9397e-08, 1.7083e-06, 1.9297e-02,
        4.5013e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 47, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7943e-02, 4.9365e-02, 2.0518e-02, 4.8974e-05, 7.4335e-01, 6.4775e-02,
        6.4004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.113

[Epoch: 47, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.7555e-03, 1.4255e-02, 1.0226e-02, 1.3480e-07, 9.5471e-01, 6.7368e-10,
        1.1050e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.359

[Epoch: 48, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3185e-02, 1.2065e-02, 6.1317e-01, 5.3327e-08, 1.5434e-02, 3.6772e-03,
        3.4246e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.514

[Epoch: 48, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0520e-02, 9.7089e-01, 1.1447e-02, 1.9715e-08, 1.2224e-07, 2.1300e-08,
        7.1468e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.940

[Epoch: 48, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3624e-01, 3.4063e-02, 5.0904e-01, 1.1619e-08, 1.4102e-06, 1.6684e-02,
        3.9656e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 48, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5329e-02, 4.9637e-02, 1.5910e-02, 6.4161e-05, 7.6429e-01, 6.6062e-02,
        4.8705e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 48, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.7093e-03, 1.4694e-02, 8.6657e-03, 1.5677e-07, 9.5663e-01, 7.7607e-10,
        1.1304e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.274

[Epoch: 49, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2705e-02, 9.4832e-03, 6.4061e-01, 2.5838e-08, 1.8730e-02, 3.3030e-03,
        3.1517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.477

[Epoch: 49, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0445e-02, 9.7136e-01, 1.1370e-02, 1.3416e-08, 7.2804e-08, 1.2661e-08,
        6.8280e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.946

[Epoch: 49, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9625e-01, 2.8964e-02, 5.5070e-01, 1.2966e-08, 1.5548e-06, 1.9967e-02,
        4.1169e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 49, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8975e-02, 4.4283e-02, 1.8859e-02, 2.5307e-05, 7.8842e-01, 4.7774e-02,
        5.1667e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.114

[Epoch: 49, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.9466e-03, 1.6079e-02, 1.1076e-02, 1.1858e-07, 9.5105e-01, 4.8828e-10,
        1.2851e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.386

[Epoch: 50, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1508e-02, 1.3087e-02, 5.1240e-01, 7.9847e-08, 1.5874e-02, 4.4708e-03,
        4.4266e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.521

[Epoch: 50, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1851e-02, 9.6616e-01, 1.4176e-02, 3.0640e-08, 2.1521e-07, 2.7967e-08,
        7.8175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.946

[Epoch: 50, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4156e-01, 4.2903e-02, 4.9209e-01, 1.1756e-08, 1.7321e-06, 1.9237e-02,
        4.2069e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 50, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.7663e-02, 5.0466e-02, 2.2330e-02, 6.2121e-05, 7.0609e-01, 8.9532e-02,
        6.3854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 50, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.1457e-03, 1.1803e-02, 8.4435e-03, 8.1691e-08, 9.6026e-01, 4.1000e-10,
        1.1347e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.265

[Epoch: 51, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4908e-02, 9.2546e-03, 7.5119e-01, 2.5980e-08, 1.3850e-02, 3.4329e-03,
        2.0736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.489

[Epoch: 51, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.2866e-03, 9.7258e-01, 1.1494e-02, 1.8331e-08, 9.0417e-08, 1.9448e-08,
        6.6383e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.940

[Epoch: 51, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9210e-01, 2.7181e-02, 5.6063e-01, 6.9016e-09, 1.1142e-06, 1.6759e-02,
        3.3326e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.112

[Epoch: 51, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.4562e-02, 4.2969e-02, 1.7821e-02, 2.5327e-05, 8.1222e-01, 4.2458e-02,
        3.9941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 51, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.8604e-03, 1.3920e-02, 9.2912e-03, 1.0711e-07, 9.5766e-01, 4.2985e-10,
        1.1271e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.298

[Epoch: 52, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8460e-02, 1.7870e-02, 5.9656e-01, 3.1217e-08, 2.8314e-02, 6.2145e-03,
        3.3258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.462

[Epoch: 52, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4495e-03, 9.7056e-01, 1.2898e-02, 1.1687e-08, 1.4188e-07, 1.8039e-08,
        7.0907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 52, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1554e-01, 3.3945e-02, 5.2817e-01, 1.1999e-08, 2.0649e-06, 1.8362e-02,
        3.9740e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.116

[Epoch: 52, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5798e-02, 4.4265e-02, 1.8466e-02, 4.1424e-05, 7.5328e-01, 6.7846e-02,
        6.0304e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 52, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.4944e-03, 1.2838e-02, 9.5824e-03, 6.2376e-08, 9.5661e-01, 2.5655e-10,
        1.2474e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.343

[Epoch: 53, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5988e-02, 1.3604e-02, 6.1401e-01, 4.3792e-08, 1.7415e-02, 4.8066e-03,
        3.3418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.530

[Epoch: 53, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1744e-02, 9.6944e-01, 1.2248e-02, 9.5058e-09, 8.6334e-08, 1.1449e-08,
        6.5661e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.939

[Epoch: 53, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2612e-01, 3.7824e-02, 5.1638e-01, 7.9443e-09, 1.3625e-06, 1.5889e-02,
        3.7946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.116

[Epoch: 53, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6228e-02, 4.8566e-02, 2.0166e-02, 3.0946e-05, 7.6226e-01, 6.0322e-02,
        5.2426e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 53, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.0210e-03, 1.4761e-02, 9.2367e-03, 4.8325e-08, 9.5448e-01, 2.4160e-10,
        1.3503e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 54, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4211e-02, 1.0610e-02, 6.0246e-01, 1.9663e-08, 1.5261e-02, 5.6064e-03,
        3.5185e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.485

[Epoch: 54, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0008e-02, 9.6893e-01, 1.4587e-02, 1.2701e-08, 1.4112e-07, 1.5140e-08,
        6.4786e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.946

[Epoch: 54, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0948e-01, 3.1631e-02, 5.3675e-01, 6.2928e-09, 1.3292e-06, 1.8072e-02,
        4.0687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 54, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.5252e-02, 4.4641e-02, 1.4905e-02, 2.8540e-05, 7.8294e-01, 6.3432e-02,
        4.8805e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 54, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.7079e-03, 1.2961e-02, 1.1472e-02, 5.3243e-08, 9.5269e-01, 2.0652e-10,
        1.4169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.316

[Epoch: 55, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2003e-02, 9.7671e-03, 6.1497e-01, 6.3257e-08, 1.7348e-02, 3.6514e-03,
        3.4226e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.534

[Epoch: 55, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6559e-03, 9.7242e-01, 1.1888e-02, 1.4430e-08, 1.2053e-07, 1.4959e-08,
        6.0385e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.943

[Epoch: 55, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2146e-01, 4.1736e-02, 5.1662e-01, 1.3457e-08, 1.7896e-06, 1.6654e-02,
        3.5279e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.121

[Epoch: 55, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.1308e-02, 5.0612e-02, 2.1524e-02, 4.3411e-05, 7.2488e-01, 6.5863e-02,
        6.5768e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 55, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3637e-03, 1.3041e-02, 8.3333e-03, 5.2795e-08, 9.5904e-01, 4.1483e-10,
        1.2217e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.325

[Epoch: 56, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3473e-02, 1.1353e-02, 6.1885e-01, 1.5210e-08, 1.9230e-02, 5.2397e-03,
        3.3186e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.450

[Epoch: 56, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1844e-02, 9.6741e-01, 1.2965e-02, 7.1113e-09, 1.0358e-07, 6.0359e-09,
        7.7854e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 56, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1928e-01, 3.0050e-02, 5.3067e-01, 3.7633e-09, 9.6063e-07, 1.6221e-02,
        3.7769e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 56, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.3661e-02, 4.0862e-02, 1.3241e-02, 2.2160e-05, 8.1343e-01, 6.2802e-02,
        3.5985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 56, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.8808e-03, 1.5318e-02, 1.2717e-02, 3.4691e-08, 9.4821e-01, 1.9263e-10,
        1.3872e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 57, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4781e-02, 1.0030e-02, 5.8836e-01, 4.3769e-08, 1.6904e-02, 4.3092e-03,
        3.6562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.537

[Epoch: 57, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.1387e-03, 9.7207e-01, 1.3497e-02, 1.7102e-08, 1.6101e-07, 1.2743e-08,
        5.2921e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.943

[Epoch: 57, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3200e-01, 3.8943e-02, 5.1047e-01, 1.6346e-08, 1.7834e-06, 1.5844e-02,
        2.7356e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 57, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.9869e-02, 5.8499e-02, 1.6293e-02, 2.8828e-05, 7.3137e-01, 5.5370e-02,
        5.8571e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 57, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.1298e-03, 1.3932e-02, 1.0452e-02, 4.7240e-08, 9.5390e-01, 2.1265e-10,
        1.3582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.326

[Epoch: 58, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1250e-02, 9.5404e-03, 6.0903e-01, 1.3387e-08, 1.5245e-02, 3.5749e-03,
        3.5136e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.532

[Epoch: 58, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0248e-02, 9.6747e-01, 1.4106e-02, 1.2190e-08, 1.5461e-07, 8.1952e-09,
        8.1754e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.957

[Epoch: 58, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1632e-01, 3.2402e-02, 5.2812e-01, 5.0860e-09, 1.0801e-06, 1.8517e-02,
        4.6395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 58, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.9857e-02, 3.7834e-02, 1.7824e-02, 2.6570e-05, 7.6865e-01, 6.7350e-02,
        6.8458e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 58, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.1189e-03, 1.2358e-02, 8.0609e-03, 3.7485e-08, 9.6034e-01, 1.2997e-10,
        1.2122e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 59, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4176e-02, 1.3577e-02, 6.1717e-01, 6.3264e-08, 2.1436e-02, 4.6637e-03,
        3.2898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.488

[Epoch: 59, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2102e-02, 9.6483e-01, 1.6475e-02, 6.1992e-09, 7.2041e-08, 5.4687e-09,
        6.5886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 59, batch: 120/201] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4041e-01, 3.2871e-02, 5.0796e-01, 1.7465e-08, 2.5061e-06, 1.6181e-02,
        2.5805e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 59, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2542e-02, 5.9413e-02, 2.0618e-02, 4.4710e-05, 7.7560e-01, 6.0567e-02,
        3.1212e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.117

[Epoch: 59, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9178e-03, 1.0985e-02, 8.0796e-03, 7.2463e-08, 9.5919e-01, 4.7372e-10,
        1.4831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 60, batch: 40/201] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6637e-02, 8.9211e-03, 6.1248e-01, 6.1455e-08, 1.4488e-02, 4.9908e-03,
        3.4248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.452

[Epoch: 60, batch: 80/201] total loss per batch: 0.544
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.1899e-03, 9.7880e-01, 8.5564e-03, 2.2841e-08, 2.4665e-07, 1.5851e-08,
        5.4553e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.941

[Epoch: 60, batch: 120/201] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8361e-01, 3.1021e-02, 5.6579e-01, 2.5247e-08, 2.5656e-06, 1.6268e-02,
        3.3113e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.154

[Epoch: 60, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2790e-02, 2.6315e-02, 2.0350e-02, 2.5079e-05, 7.9927e-01, 5.0066e-02,
        5.1180e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 60, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.9208e-03, 1.7085e-02, 1.2305e-02, 8.8308e-08, 9.4691e-01, 2.4441e-10,
        1.3776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.369

[Epoch: 61, batch: 40/201] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0239e-02, 7.3149e-03, 6.0067e-01, 2.9454e-08, 1.5219e-02, 4.8440e-03,
        3.6172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.562

[Epoch: 61, batch: 80/201] total loss per batch: 0.545
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2021e-02, 9.5231e-01, 2.5979e-02, 3.7773e-08, 4.1024e-07, 4.5957e-08,
        9.6908e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.949

[Epoch: 61, batch: 120/201] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0775e-01, 4.7976e-02, 5.2303e-01, 1.2477e-08, 1.0477e-06, 1.7764e-02,
        3.4810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 61, batch: 160/201] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.0707e-01, 5.9957e-02, 2.9670e-02, 2.9466e-05, 6.3291e-01, 7.9050e-02,
        9.1312e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.118

[Epoch: 61, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4463e-03, 1.1537e-02, 8.0945e-03, 6.0597e-08, 9.6334e-01, 2.7885e-10,
        1.0579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.291

[Epoch: 62, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([7.7536e-03, 9.2926e-03, 5.9880e-01, 6.5796e-08, 2.8914e-02, 4.3995e-03,
        3.5084e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.435

[Epoch: 62, batch: 80/201] total loss per batch: 0.547
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1289e-02, 9.7572e-01, 5.6826e-03, 2.4967e-09, 2.9139e-08, 2.9062e-09,
        7.3044e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.934

[Epoch: 62, batch: 120/201] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1532e-01, 3.6932e-02, 5.2969e-01, 1.2721e-08, 1.6341e-06, 1.4099e-02,
        3.9639e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.074

[Epoch: 62, batch: 160/201] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.1654e-02, 1.9633e-02, 9.0059e-03, 4.4010e-05, 8.9795e-01, 4.0070e-02,
        2.1640e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.087

[Epoch: 62, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.6919e-03, 1.3300e-02, 1.2633e-02, 5.3171e-08, 9.4930e-01, 5.3816e-10,
        1.5075e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.301

[Epoch: 63, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7155e-02, 9.5004e-03, 6.7439e-01, 7.2699e-08, 1.7580e-02, 3.6429e-03,
        2.7773e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.542

[Epoch: 63, batch: 80/201] total loss per batch: 0.545
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.5185e-03, 9.7735e-01, 8.8696e-03, 9.1619e-09, 1.8113e-07, 2.3345e-08,
        7.2588e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.944

[Epoch: 63, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0125e-01, 3.0682e-02, 5.3625e-01, 4.0380e-08, 2.0036e-06, 2.4715e-02,
        7.1071e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.133

[Epoch: 63, batch: 160/201] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([8.4400e-02, 6.8286e-02, 3.4711e-02, 7.1603e-05, 6.6079e-01, 7.9092e-02,
        7.2653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.098

[Epoch: 63, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.0858e-03, 1.2741e-02, 9.7261e-03, 7.6143e-08, 9.5289e-01, 9.1777e-10,
        1.6557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.274

[Epoch: 64, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6781e-02, 1.7215e-02, 5.2070e-01, 4.7514e-08, 1.4103e-02, 6.1022e-03,
        4.2510e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.454

[Epoch: 64, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.2010e-03, 9.6602e-01, 1.8185e-02, 6.9677e-09, 2.7250e-07, 1.5292e-08,
        7.5901e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.946

[Epoch: 64, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2972e-01, 4.0977e-02, 4.9739e-01, 1.9178e-08, 1.8959e-06, 2.6056e-02,
        5.8551e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.155

[Epoch: 64, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.1024e-02, 4.1051e-02, 1.8429e-02, 3.5591e-05, 8.0264e-01, 5.0128e-02,
        4.6691e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 64, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3704e-03, 1.1718e-02, 1.1965e-02, 3.2804e-08, 9.5776e-01, 2.1398e-10,
        1.1183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.318

[Epoch: 65, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1577e-02, 6.5411e-03, 6.8650e-01, 7.0777e-08, 1.3292e-02, 4.1781e-03,
        2.7791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.541

[Epoch: 65, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4784e-02, 9.6518e-01, 1.3279e-02, 1.4343e-08, 2.7470e-07, 5.9078e-08,
        6.7544e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.942

[Epoch: 65, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0126e-01, 2.4020e-02, 5.6068e-01, 7.4535e-09, 1.8024e-06, 1.0054e-02,
        3.9832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.087

[Epoch: 65, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8001e-02, 4.1785e-02, 1.5257e-02, 4.8230e-05, 7.7634e-01, 6.2855e-02,
        5.5713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.095

[Epoch: 65, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.9825e-03, 1.3657e-02, 1.2585e-02, 1.4967e-08, 9.5182e-01, 1.1563e-10,
        1.2954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.334

[Epoch: 66, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5862e-02, 1.5376e-02, 5.7735e-01, 1.4210e-07, 3.3809e-02, 6.6257e-03,
        3.5097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.480

[Epoch: 66, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4445e-03, 9.6823e-01, 1.2660e-02, 1.5729e-08, 3.6091e-07, 1.6365e-08,
        9.6680e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.973

[Epoch: 66, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1214e-01, 5.1944e-02, 5.1460e-01, 2.5918e-08, 4.4395e-06, 1.7012e-02,
        4.2926e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 66, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7671e-02, 6.3126e-02, 1.9347e-02, 4.1142e-05, 7.1738e-01, 7.9726e-02,
        6.2709e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 66, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.2552e-03, 1.2586e-02, 1.1473e-02, 1.5196e-08, 9.5380e-01, 2.7235e-10,
        1.4887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 67, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7194e-02, 7.5731e-03, 6.3642e-01, 4.8505e-08, 1.1662e-02, 5.0083e-03,
        3.2214e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.534

[Epoch: 67, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7964e-03, 9.6799e-01, 1.5852e-02, 7.4355e-09, 1.8321e-07, 2.2577e-08,
        6.3607e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.942

[Epoch: 67, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2034e-01, 2.8694e-02, 5.3263e-01, 1.0823e-08, 1.7905e-06, 1.4109e-02,
        4.2324e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.105

[Epoch: 67, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6415e-02, 4.4008e-02, 1.4850e-02, 3.1552e-05, 7.8666e-01, 5.3926e-02,
        4.4107e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 67, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5987e-03, 1.2610e-02, 9.9624e-03, 1.7412e-08, 9.5523e-01, 1.3205e-10,
        1.4602e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.299

[Epoch: 68, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5609e-02, 1.0725e-02, 5.4361e-01, 5.7851e-08, 1.3283e-02, 4.4378e-03,
        4.1233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.531

[Epoch: 68, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6071e-03, 9.6877e-01, 1.5099e-02, 2.2334e-08, 5.7790e-07, 5.3708e-08,
        6.5266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 68, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1490e-01, 3.3798e-02, 5.2928e-01, 1.0099e-08, 2.4820e-06, 1.8114e-02,
        3.9026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 68, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1999e-02, 4.4926e-02, 2.1094e-02, 5.7601e-05, 7.6539e-01, 6.7143e-02,
        4.9391e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.119

[Epoch: 68, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5372e-03, 1.2506e-02, 1.0611e-02, 3.0057e-08, 9.5679e-01, 1.8506e-10,
        1.2551e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.343

[Epoch: 69, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.2934e-03, 8.6156e-03, 6.5210e-01, 4.7557e-08, 1.7387e-02, 6.8624e-03,
        3.0574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.396

[Epoch: 69, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.7620e-03, 9.7591e-01, 1.0944e-02, 7.6851e-09, 1.0541e-07, 1.1641e-08,
        5.3843e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.954

[Epoch: 69, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3626e-01, 5.1021e-02, 4.9388e-01, 8.3357e-09, 2.9397e-06, 1.4326e-02,
        4.5123e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 69, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.7468e-02, 5.6653e-02, 1.6793e-02, 3.9429e-05, 7.4798e-01, 6.6792e-02,
        6.4270e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.097

[Epoch: 69, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.2363e-03, 1.7026e-02, 1.0435e-02, 1.9637e-08, 9.4746e-01, 1.1221e-10,
        1.6839e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 70, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2875e-02, 8.8716e-03, 5.9388e-01, 6.1051e-08, 2.2802e-02, 3.8244e-03,
        3.5775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.586

[Epoch: 70, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1636e-02, 9.6867e-01, 1.2625e-02, 1.1107e-08, 2.8905e-07, 3.6861e-08,
        7.0730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.966

[Epoch: 70, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9144e-01, 1.9832e-02, 5.7117e-01, 1.6506e-08, 2.9215e-06, 1.3528e-02,
        4.0284e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.115

[Epoch: 70, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.8476e-02, 4.2994e-02, 1.6156e-02, 7.5648e-05, 7.7400e-01, 5.7777e-02,
        4.0525e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 70, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.1724e-03, 1.4311e-02, 9.7398e-03, 2.4119e-08, 9.5570e-01, 1.4785e-10,
        1.2077e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 71, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3033e-02, 1.1407e-02, 5.9854e-01, 2.3669e-08, 1.3119e-02, 3.8811e-03,
        3.6002e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.472

[Epoch: 71, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.4185e-03, 9.7191e-01, 1.3366e-02, 2.7655e-08, 5.0331e-07, 1.2697e-08,
        7.3045e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.947

[Epoch: 71, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9995e-01, 4.9375e-02, 5.2922e-01, 5.9703e-09, 1.9026e-06, 1.7451e-02,
        4.0085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.112

[Epoch: 71, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2982e-02, 5.3089e-02, 1.5757e-02, 6.2179e-05, 7.4853e-01, 7.2186e-02,
        5.7390e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.083

[Epoch: 71, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.2523e-03, 1.3117e-02, 7.7328e-03, 1.6402e-08, 9.6175e-01, 1.4314e-10,
        1.1149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.301

[Epoch: 72, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4494e-02, 1.1199e-02, 6.3493e-01, 6.4523e-08, 1.6731e-02, 3.4148e-03,
        3.1923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.559

[Epoch: 72, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.9479e-03, 9.7017e-01, 1.3924e-02, 1.0076e-08, 2.5637e-07, 8.7929e-09,
        7.9545e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 72, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1727e-01, 2.6818e-02, 5.3194e-01, 9.8808e-09, 3.9386e-06, 2.0006e-02,
        3.9554e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.141

[Epoch: 72, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.0358e-02, 4.0543e-02, 1.5745e-02, 4.9594e-05, 8.0817e-01, 4.5876e-02,
        4.9258e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.116

[Epoch: 72, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.6311e-03, 1.6552e-02, 1.1427e-02, 4.1539e-08, 9.4773e-01, 3.4710e-10,
        1.6664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.307

[Epoch: 73, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.9373e-02, 1.7600e-02, 4.6708e-01, 6.0678e-08, 3.0119e-02, 6.8676e-03,
        4.5896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.457

[Epoch: 73, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.5125e-02, 9.5974e-01, 1.5058e-02, 2.3310e-08, 6.5451e-07, 5.0957e-08,
        1.0074e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.928

[Epoch: 73, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8740e-01, 5.0896e-02, 5.3510e-01, 6.2395e-08, 9.2347e-06, 1.9279e-02,
        7.3219e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.136

[Epoch: 73, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.0258e-01, 6.0455e-02, 2.9535e-02, 5.8369e-05, 6.6371e-01, 8.2586e-02,
        6.1080e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.112

[Epoch: 73, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.5861e-03, 1.2926e-02, 1.1916e-02, 3.8366e-08, 9.5586e-01, 1.5852e-10,
        1.0716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.346

[Epoch: 74, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([7.2190e-03, 5.8432e-03, 8.1254e-01, 4.8417e-08, 9.3542e-03, 2.2258e-03,
        1.6282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.501

[Epoch: 74, batch: 80/201] total loss per batch: 0.544
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.6127e-02, 9.6863e-01, 1.2222e-02, 2.8833e-07, 7.0734e-07, 1.8181e-08,
        3.0157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.968

[Epoch: 74, batch: 120/201] total loss per batch: 0.546
Policy (actual, predicted): 2 0
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([5.1403e-01, 2.5024e-02, 4.3539e-01, 2.6443e-08, 2.2614e-06, 2.1715e-02,
        3.8311e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.041

[Epoch: 74, batch: 160/201] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([2.9248e-02, 5.0783e-02, 2.0170e-02, 5.9459e-05, 7.7542e-01, 6.0601e-02,
        6.3716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 74, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3424e-03, 1.3874e-02, 8.6854e-03, 2.0628e-08, 9.5864e-01, 1.3907e-10,
        1.1456e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.423

[Epoch: 75, batch: 40/201] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([4.6247e-02, 7.1103e-02, 4.2047e-01, 1.0590e-07, 3.3315e-02, 1.3248e-02,
        4.1562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.334

[Epoch: 75, batch: 80/201] total loss per batch: 0.544
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.3758e-03, 9.5496e-01, 2.5849e-02, 5.6111e-08, 1.4604e-06, 7.0831e-09,
        1.2818e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.957

[Epoch: 75, batch: 120/201] total loss per batch: 0.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([2.2899e-01, 3.6366e-02, 7.0628e-01, 1.5789e-08, 3.8972e-06, 2.0951e-02,
        7.4118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.143

[Epoch: 75, batch: 160/201] total loss per batch: 0.540
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9366e-02, 4.5264e-02, 1.4378e-02, 3.8836e-05, 8.0103e-01, 5.1468e-02,
        3.8454e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.097

[Epoch: 75, batch: 200/201] total loss per batch: 0.518
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.2121e-02, 1.9254e-02, 8.8924e-03, 1.8751e-08, 9.4250e-01, 1.4897e-10,
        1.7233e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.268

[Epoch: 76, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4966e-02, 6.8849e-03, 6.3803e-01, 3.4300e-07, 1.9605e-02, 5.6925e-03,
        3.1482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.602

[Epoch: 76, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.6423e-02, 9.7223e-01, 6.6391e-03, 1.7583e-08, 1.9921e-07, 3.5830e-09,
        4.7065e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 76, batch: 120/201] total loss per batch: 0.543
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0721e-01, 1.8625e-02, 5.5308e-01, 6.5266e-09, 2.5763e-06, 1.9029e-02,
        2.0507e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.052

[Epoch: 76, batch: 160/201] total loss per batch: 0.539
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6136e-02, 4.1342e-02, 2.1522e-02, 2.3440e-05, 7.6047e-01, 6.8844e-02,
        5.1665e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.096

[Epoch: 76, batch: 200/201] total loss per batch: 0.517
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.6043e-03, 1.2526e-02, 9.4436e-03, 5.4638e-08, 9.6020e-01, 1.8622e-10,
        1.0228e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.275

[Epoch: 77, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1990e-02, 6.1161e-03, 5.3844e-01, 1.5600e-07, 1.6613e-02, 7.0992e-03,
        4.1974e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.400

[Epoch: 77, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.5957e-03, 9.8039e-01, 7.7274e-03, 5.4777e-08, 4.3923e-07, 1.2597e-08,
        5.2874e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.930

[Epoch: 77, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2678e-01, 4.7981e-02, 4.9149e-01, 2.9239e-08, 3.6379e-06, 2.7483e-02,
        6.2578e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 77, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8643e-02, 5.5759e-02, 2.3178e-02, 7.6416e-05, 7.6104e-01, 6.0611e-02,
        5.0696e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.118

[Epoch: 77, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.3591e-03, 1.5453e-02, 1.2069e-02, 3.2765e-08, 9.5120e-01, 8.2087e-11,
        1.2915e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.285

[Epoch: 78, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.5621e-03, 7.0391e-03, 6.9015e-01, 3.8605e-08, 1.5929e-02, 3.2489e-03,
        2.7407e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.487

[Epoch: 78, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4848e-02, 9.7148e-01, 8.0286e-03, 2.0961e-08, 1.6857e-07, 3.9106e-09,
        5.6417e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 78, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2524e-01, 4.1273e-02, 5.0832e-01, 2.6954e-08, 4.1056e-06, 2.0784e-02,
        4.3784e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 78, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6831e-02, 4.3130e-02, 1.7457e-02, 5.5109e-05, 7.3604e-01, 6.8820e-02,
        7.7666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 78, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3228e-03, 1.3035e-02, 8.5558e-03, 2.4468e-08, 9.6340e-01, 3.6780e-10,
        7.6898e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 79, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3401e-02, 1.0662e-02, 6.2475e-01, 1.6737e-07, 1.2586e-02, 5.1229e-03,
        3.3348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.517

[Epoch: 79, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.0028e-03, 9.7378e-01, 1.1299e-02, 1.9669e-08, 1.9991e-07, 3.6093e-09,
        6.9148e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.951

[Epoch: 79, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8782e-01, 3.2966e-02, 5.5912e-01, 1.0344e-08, 2.6971e-06, 1.7413e-02,
        2.6802e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.141

[Epoch: 79, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.0660e-02, 4.1317e-02, 1.2740e-02, 4.0201e-05, 8.1424e-01, 6.1597e-02,
        2.9404e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 79, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.2742e-03, 1.0873e-02, 9.1981e-03, 1.1677e-08, 9.5830e-01, 1.7289e-10,
        1.5354e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 80, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3792e-02, 1.0029e-02, 5.5622e-01, 1.0264e-07, 1.7187e-02, 4.3235e-03,
        3.9845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.534

[Epoch: 80, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0870e-02, 9.6948e-01, 1.1734e-02, 3.1116e-08, 4.1193e-07, 1.2251e-08,
        7.9165e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 80, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0576e-01, 3.1714e-02, 5.4225e-01, 1.3717e-08, 4.7416e-06, 1.6544e-02,
        3.7333e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 80, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.9837e-02, 5.4996e-02, 2.1872e-02, 6.1660e-05, 7.2764e-01, 6.6657e-02,
        5.8933e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.122

[Epoch: 80, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3117e-03, 1.2820e-02, 1.0679e-02, 1.6018e-08, 9.5379e-01, 1.8828e-10,
        1.5394e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.317

[Epoch: 81, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2929e-02, 9.2427e-03, 6.0764e-01, 5.6256e-08, 1.6241e-02, 4.2205e-03,
        3.4973e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.475

[Epoch: 81, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.7669e-03, 9.7907e-01, 9.3621e-03, 2.4794e-08, 1.8201e-07, 7.0664e-09,
        4.7970e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.969

[Epoch: 81, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3190e-01, 4.0480e-02, 4.9658e-01, 2.2325e-08, 3.1775e-06, 2.6096e-02,
        4.9349e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.141

[Epoch: 81, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6925e-02, 4.8798e-02, 1.7434e-02, 3.8986e-05, 7.7799e-01, 5.4814e-02,
        5.3997e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 81, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.8110e-03, 2.0587e-02, 1.3175e-02, 9.3610e-09, 9.4371e-01, 1.0735e-10,
        1.2714e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.302

[Epoch: 82, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2709e-02, 9.0998e-03, 6.0762e-01, 1.0948e-07, 1.8915e-02, 3.4392e-03,
        3.4822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.519

[Epoch: 82, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3891e-02, 9.6696e-01, 1.2957e-02, 2.1820e-08, 1.7387e-07, 1.0463e-08,
        6.1890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.957

[Epoch: 82, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0737e-01, 3.9093e-02, 5.3821e-01, 3.6390e-08, 5.9318e-06, 1.1477e-02,
        3.8487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.099

[Epoch: 82, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.1373e-02, 4.7895e-02, 1.6936e-02, 1.0458e-04, 7.3450e-01, 7.9658e-02,
        5.9536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.112

[Epoch: 82, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5691e-03, 1.0943e-02, 1.0623e-02, 1.7362e-08, 9.5966e-01, 1.0451e-10,
        1.2209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 83, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.9247e-02, 1.2249e-02, 6.3533e-01, 2.8379e-08, 1.7986e-02, 4.8181e-03,
        3.1037e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.491

[Epoch: 83, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.5050e-03, 9.7091e-01, 1.2155e-02, 1.1938e-08, 1.1923e-07, 1.6301e-09,
        7.4337e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.963

[Epoch: 83, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3695e-01, 3.2030e-02, 5.1427e-01, 9.2566e-09, 1.8793e-06, 1.4291e-02,
        2.4574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 83, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.1743e-02, 3.9461e-02, 1.4972e-02, 3.4368e-05, 8.0327e-01, 5.7164e-02,
        4.3353e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.086

[Epoch: 83, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4793e-03, 1.0893e-02, 8.0057e-03, 5.5936e-09, 9.6115e-01, 2.4869e-10,
        1.3476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.333

[Epoch: 84, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3238e-02, 1.0061e-02, 5.7934e-01, 1.3374e-07, 1.3449e-02, 3.5288e-03,
        3.8038e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.514

[Epoch: 84, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4910e-03, 9.6982e-01, 1.3370e-02, 6.0953e-08, 3.5402e-07, 5.9030e-09,
        7.3161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.971

[Epoch: 84, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9244e-01, 3.4852e-02, 5.4687e-01, 1.5202e-08, 3.0742e-06, 2.1562e-02,
        4.2729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.147

[Epoch: 84, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5494e-02, 5.5419e-02, 2.0758e-02, 2.7280e-05, 7.5477e-01, 6.1368e-02,
        5.2168e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 84, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.6476e-03, 1.4237e-02, 1.0377e-02, 1.5985e-08, 9.5250e-01, 1.6925e-10,
        1.4235e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 85, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4419e-02, 6.5715e-03, 5.6590e-01, 8.1792e-08, 1.7462e-02, 3.3735e-03,
        3.9227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.537

[Epoch: 85, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0446e-02, 9.7066e-01, 1.4736e-02, 6.6008e-08, 2.8731e-07, 1.3164e-08,
        4.1553e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 85, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.6335e-01, 3.9370e-02, 4.8037e-01, 2.3230e-08, 3.0439e-06, 1.2445e-02,
        4.4574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.146

[Epoch: 85, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.6699e-02, 5.0255e-02, 1.9247e-02, 3.2807e-05, 7.3303e-01, 5.7142e-02,
        7.3590e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 85, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3573e-03, 1.4470e-02, 1.1678e-02, 1.8392e-08, 9.5236e-01, 1.5915e-10,
        1.4137e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.327

[Epoch: 86, batch: 40/201] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([7.6892e-03, 9.3875e-03, 6.5575e-01, 1.4308e-07, 2.1619e-02, 5.0596e-03,
        3.0049e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.493

[Epoch: 86, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0689e-02, 9.6986e-01, 1.2486e-02, 1.6558e-08, 8.2732e-08, 6.4667e-09,
        6.9676e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.957

[Epoch: 86, batch: 120/201] total loss per batch: 0.542
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.4547e-01, 2.7947e-02, 6.0165e-01, 1.0191e-07, 1.1052e-05, 2.1276e-02,
        3.6472e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.089

[Epoch: 86, batch: 160/201] total loss per batch: 0.537
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.4474e-02, 4.6557e-02, 1.9074e-02, 7.8997e-05, 7.6488e-01, 8.2793e-02,
        4.2139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 86, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.0153e-02, 1.4216e-02, 9.5475e-03, 1.4753e-08, 9.5067e-01, 1.5016e-10,
        1.5415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.321

[Epoch: 87, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7634e-02, 1.7504e-02, 5.8523e-01, 3.0180e-07, 1.5675e-02, 5.5837e-03,
        3.5838e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.465

[Epoch: 87, batch: 80/201] total loss per batch: 0.542
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1408e-02, 9.6620e-01, 1.5684e-02, 1.8517e-07, 7.6277e-07, 7.0399e-08,
        6.7079e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.976

[Epoch: 87, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2988e-01, 5.7725e-02, 4.9465e-01, 2.7117e-08, 1.9982e-06, 1.4546e-02,
        3.1912e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.100

[Epoch: 87, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7360e-02, 4.6778e-02, 2.3927e-02, 5.9576e-05, 7.6262e-01, 5.8535e-02,
        5.0726e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.096

[Epoch: 87, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9891e-03, 1.7988e-02, 8.7819e-03, 2.5443e-08, 9.5127e-01, 5.8449e-10,
        1.4970e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.282

[Epoch: 88, batch: 40/201] total loss per batch: 0.521
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.0224e-02, 1.2181e-02, 6.5765e-01, 5.4267e-08, 1.6392e-02, 6.0242e-03,
        2.8752e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.451

[Epoch: 88, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.1701e-03, 9.5971e-01, 1.8972e-02, 8.0637e-08, 5.5060e-07, 9.3677e-09,
        1.3145e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 88, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0935e-01, 2.7697e-02, 5.3598e-01, 1.3641e-08, 5.9329e-06, 2.1056e-02,
        5.9173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.155

[Epoch: 88, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8719e-02, 3.9350e-02, 1.2693e-02, 5.8044e-05, 8.2024e-01, 2.9909e-02,
        4.9036e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 88, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.2701e-03, 1.1723e-02, 9.7072e-03, 3.5053e-08, 9.5480e-01, 6.8344e-10,
        1.6498e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.337

[Epoch: 89, batch: 40/201] total loss per batch: 0.527
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.7338e-03, 1.0401e-02, 5.5019e-01, 2.7867e-07, 1.6272e-02, 4.1519e-03,
        4.0925e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.458

[Epoch: 89, batch: 80/201] total loss per batch: 0.551
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5225e-03, 9.7872e-01, 1.0350e-02, 4.7303e-08, 1.0804e-07, 6.6298e-09,
        3.4068e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.933

[Epoch: 89, batch: 120/201] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2106e-01, 3.2339e-02, 5.2295e-01, 2.9573e-08, 2.3028e-06, 1.8302e-02,
        5.3521e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.168

[Epoch: 89, batch: 160/201] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([1.0043e-01, 6.5627e-02, 2.1063e-02, 5.9954e-05, 6.4845e-01, 9.8370e-02,
        6.5996e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.120

[Epoch: 89, batch: 200/201] total loss per batch: 0.525
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.1677e-03, 1.7113e-02, 1.2671e-02, 5.6915e-08, 9.5070e-01, 6.8762e-10,
        1.0352e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.304

[Epoch: 90, batch: 40/201] total loss per batch: 0.535
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([6.7151e-03, 4.6702e-03, 7.1803e-01, 1.0784e-06, 1.0260e-02, 2.1209e-03,
        2.5821e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.531

[Epoch: 90, batch: 80/201] total loss per batch: 0.553
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2369e-02, 9.6619e-01, 1.3451e-02, 4.5786e-08, 1.0524e-07, 1.7715e-08,
        7.9897e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 90, batch: 120/201] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2505e-01, 1.7807e-02, 5.3691e-01, 2.8722e-09, 1.0948e-06, 1.6041e-02,
        4.1909e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 90, batch: 160/201] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6316e-02, 4.4750e-02, 1.3974e-02, 2.8715e-05, 7.6598e-01, 5.4160e-02,
        7.4787e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.084

[Epoch: 90, batch: 200/201] total loss per batch: 0.522
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.7466e-03, 1.3641e-02, 1.1456e-02, 3.6251e-08, 9.5502e-01, 5.6736e-11,
        1.1140e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.375

[Epoch: 91, batch: 40/201] total loss per batch: 0.526
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([2.5771e-02, 6.1367e-03, 4.6115e-01, 8.8349e-08, 1.0529e-02, 2.9332e-03,
        4.9348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.583

[Epoch: 91, batch: 80/201] total loss per batch: 0.543
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.6102e-02, 9.5326e-01, 1.3151e-02, 2.7854e-08, 1.0968e-07, 1.2062e-07,
        1.7487e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.926

[Epoch: 91, batch: 120/201] total loss per batch: 0.544
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4828e-01, 3.8516e-02, 4.8577e-01, 6.3232e-09, 2.2540e-06, 2.2927e-02,
        4.5093e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.133

[Epoch: 91, batch: 160/201] total loss per batch: 0.538
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0089e-02, 5.5608e-02, 2.1536e-02, 1.3720e-04, 7.2357e-01, 8.4429e-02,
        5.4634e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 91, batch: 200/201] total loss per batch: 0.516
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.1365e-03, 1.0706e-02, 1.2641e-02, 2.0492e-08, 9.5689e-01, 7.8316e-11,
        1.1627e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.342

[Epoch: 92, batch: 40/201] total loss per batch: 0.522
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2261e-02, 1.1385e-02, 6.7292e-01, 2.3213e-07, 1.9429e-02, 3.7559e-03,
        2.8024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.452

[Epoch: 92, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1459e-02, 9.6830e-01, 1.4747e-02, 6.4673e-08, 2.1080e-07, 5.1536e-08,
        5.4889e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.941

[Epoch: 92, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1130e-01, 3.6850e-02, 5.2782e-01, 1.0946e-08, 2.0210e-06, 1.9355e-02,
        4.6749e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 92, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2997e-02, 5.3269e-02, 1.5461e-02, 6.3782e-05, 7.6801e-01, 6.3527e-02,
        4.6675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 92, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8733e-03, 1.1417e-02, 1.1995e-02, 1.5433e-08, 9.5885e-01, 4.0778e-11,
        1.0868e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.307

[Epoch: 93, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8603e-02, 1.0429e-02, 6.2813e-01, 5.6106e-08, 2.5283e-02, 6.9018e-03,
        3.1065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.497

[Epoch: 93, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9468e-03, 9.7114e-01, 1.3488e-02, 5.7703e-08, 2.1586e-07, 9.0602e-08,
        5.4229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.939

[Epoch: 93, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1674e-01, 3.5743e-02, 5.2780e-01, 6.3745e-09, 1.9543e-06, 1.5383e-02,
        4.3331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 93, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6981e-02, 4.9445e-02, 1.5249e-02, 6.0572e-05, 7.7543e-01, 6.1745e-02,
        5.1092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 93, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.0394e-03, 1.2801e-02, 1.2044e-02, 1.5015e-08, 9.5689e-01, 5.0529e-11,
        1.1225e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 94, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3426e-02, 1.2182e-02, 6.1589e-01, 6.5572e-08, 2.3345e-02, 5.3359e-03,
        3.2982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.515

[Epoch: 94, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0685e-02, 9.7095e-01, 1.3167e-02, 3.3522e-08, 1.4177e-07, 5.9039e-08,
        5.1966e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.940

[Epoch: 94, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1703e-01, 3.5410e-02, 5.2707e-01, 5.4674e-09, 1.8086e-06, 1.6554e-02,
        3.9402e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.132

[Epoch: 94, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3185e-02, 4.7730e-02, 1.4950e-02, 4.9024e-05, 7.6555e-01, 6.3800e-02,
        5.4731e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 94, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5010e-03, 1.2550e-02, 1.2267e-02, 1.1672e-08, 9.5702e-01, 4.1817e-11,
        1.1664e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 95, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2639e-02, 1.0990e-02, 6.1202e-01, 4.9511e-08, 1.7847e-02, 4.3026e-03,
        3.4220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 95, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4743e-03, 9.7164e-01, 1.3137e-02, 2.9259e-08, 1.4285e-07, 4.8805e-08,
        5.7507e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.939

[Epoch: 95, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1864e-01, 3.7955e-02, 5.2371e-01, 4.8889e-09, 1.5044e-06, 1.5836e-02,
        3.8622e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.126

[Epoch: 95, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3170e-02, 4.8290e-02, 1.6015e-02, 4.4222e-05, 7.6576e-01, 6.5136e-02,
        5.1590e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 95, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8070e-03, 1.3306e-02, 1.1982e-02, 1.0753e-08, 9.5604e-01, 3.5119e-11,
        1.1861e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.318

[Epoch: 96, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3645e-02, 1.0020e-02, 6.0527e-01, 3.1949e-08, 1.6643e-02, 4.1256e-03,
        3.5030e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.511

[Epoch: 96, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9541e-03, 9.7187e-01, 1.2395e-02, 1.5338e-08, 9.3162e-08, 3.2163e-08,
        5.7838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 96, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1371e-01, 3.6845e-02, 5.2897e-01, 3.6151e-09, 1.3895e-06, 1.6787e-02,
        3.6954e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 96, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4319e-02, 4.6528e-02, 1.5265e-02, 4.1514e-05, 7.6659e-01, 6.2251e-02,
        5.5009e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 96, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5719e-03, 1.2749e-02, 1.1400e-02, 8.9886e-09, 9.5702e-01, 2.9839e-11,
        1.2254e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.314

[Epoch: 97, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2640e-02, 1.0137e-02, 6.0038e-01, 3.3126e-08, 1.8207e-02, 4.1234e-03,
        3.5451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.510

[Epoch: 97, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8974e-03, 9.7084e-01, 1.2458e-02, 1.5055e-08, 9.3134e-08, 3.3932e-08,
        6.8081e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 97, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1862e-01, 3.7277e-02, 5.2393e-01, 4.1140e-09, 1.4096e-06, 1.6220e-02,
        3.9500e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.121

[Epoch: 97, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2839e-02, 4.8092e-02, 1.5896e-02, 3.4919e-05, 7.6743e-01, 6.4066e-02,
        5.1645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 97, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.7098e-03, 1.4333e-02, 1.1741e-02, 8.5302e-09, 9.5309e-01, 2.9666e-11,
        1.4129e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 98, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3091e-02, 9.3452e-03, 6.1119e-01, 2.0887e-08, 1.6541e-02, 3.8175e-03,
        3.4602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.499

[Epoch: 98, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0822e-02, 9.6858e-01, 1.4370e-02, 1.4758e-08, 9.0867e-08, 3.6025e-08,
        6.2297e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 98, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1539e-01, 3.7668e-02, 5.2594e-01, 2.7935e-09, 1.0943e-06, 1.7434e-02,
        3.5688e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 98, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3526e-02, 4.6814e-02, 1.5804e-02, 3.8601e-05, 7.6522e-01, 6.4913e-02,
        5.3687e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 98, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4859e-03, 1.1814e-02, 1.0471e-02, 7.6504e-09, 9.5984e-01, 2.1367e-11,
        1.1386e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 99, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1661e-02, 9.2024e-03, 5.9229e-01, 2.5161e-08, 1.7686e-02, 4.1127e-03,
        3.6505e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.496

[Epoch: 99, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9318e-03, 9.6665e-01, 1.4901e-02, 1.3327e-08, 1.0251e-07, 3.5972e-08,
        8.5206e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 99, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1643e-01, 3.7124e-02, 5.2746e-01, 4.3887e-09, 1.5696e-06, 1.5580e-02,
        3.4026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 99, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1667e-02, 4.7461e-02, 1.5970e-02, 2.9505e-05, 7.6905e-01, 5.8861e-02,
        5.6959e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 99, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8363e-03, 1.4465e-02, 1.1253e-02, 7.2645e-09, 9.5351e-01, 2.2124e-11,
        1.3933e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.325

[Epoch: 100, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5466e-02, 9.6363e-03, 6.3102e-01, 1.8430e-08, 1.7249e-02, 3.6692e-03,
        3.2296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 100, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8599e-03, 9.7430e-01, 1.0464e-02, 9.5940e-09, 7.7084e-08, 3.5171e-08,
        5.3757e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.951

[Epoch: 100, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0070e-01, 3.2148e-02, 5.4404e-01, 1.6679e-09, 9.7496e-07, 1.9601e-02,
        3.5094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.133

[Epoch: 100, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2332e-02, 4.3352e-02, 1.6905e-02, 3.5100e-05, 7.8040e-01, 6.2030e-02,
        4.4950e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 100, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4455e-03, 1.2240e-02, 1.0485e-02, 7.7128e-09, 9.5852e-01, 2.4267e-11,
        1.2308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.333

[Epoch: 101, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3451e-02, 1.0957e-02, 5.9150e-01, 2.2144e-08, 1.9393e-02, 4.8294e-03,
        3.5987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 101, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8512e-03, 9.6919e-01, 1.4509e-02, 1.1308e-08, 8.9955e-08, 4.5893e-08,
        6.4523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 101, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3924e-01, 4.2120e-02, 5.0319e-01, 5.5149e-09, 1.5625e-06, 1.2203e-02,
        3.2466e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 101, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.8121e-02, 5.1567e-02, 1.6936e-02, 2.4546e-05, 7.4284e-01, 7.3450e-02,
        5.7057e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 101, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.7736e-03, 1.6091e-02, 1.1557e-02, 7.6545e-09, 9.4894e-01, 3.7337e-11,
        1.5639e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.309

[Epoch: 102, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0655e-02, 7.9987e-03, 6.0579e-01, 1.0856e-08, 1.4917e-02, 3.7201e-03,
        3.5692e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.505

[Epoch: 102, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8867e-03, 9.6780e-01, 1.5250e-02, 1.2194e-08, 1.0743e-07, 5.2015e-08,
        7.0660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.951

[Epoch: 102, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0061e-01, 3.8259e-02, 5.3614e-01, 2.8801e-09, 1.0926e-06, 2.0769e-02,
        4.2177e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.126

[Epoch: 102, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2811e-02, 4.8582e-02, 1.6508e-02, 2.9952e-05, 7.7612e-01, 5.3865e-02,
        5.2087e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 102, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.0271e-03, 1.2839e-02, 8.6216e-03, 5.4327e-09, 9.6236e-01, 1.5983e-11,
        9.1483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 103, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3399e-02, 1.1907e-02, 5.9191e-01, 3.2520e-08, 1.9883e-02, 3.8557e-03,
        3.5905e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.494

[Epoch: 103, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.1099e-03, 9.7300e-01, 1.2486e-02, 8.4498e-09, 7.3560e-08, 4.0288e-08,
        5.4066e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 103, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1568e-01, 3.7207e-02, 5.3005e-01, 3.3874e-09, 1.0107e-06, 1.4156e-02,
        2.9043e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.118

[Epoch: 103, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.3854e-02, 4.1285e-02, 1.5480e-02, 2.4667e-05, 7.6948e-01, 7.6898e-02,
        5.2983e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 103, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8910e-03, 1.3965e-02, 1.1275e-02, 8.5964e-09, 9.5366e-01, 3.5673e-11,
        1.4205e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.333

[Epoch: 104, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4235e-02, 9.4422e-03, 6.2179e-01, 2.0327e-08, 1.5844e-02, 4.1524e-03,
        3.3453e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.511

[Epoch: 104, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.3711e-03, 9.7321e-01, 1.1691e-02, 6.7189e-09, 6.3473e-08, 2.2263e-08,
        7.7236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.958

[Epoch: 104, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2003e-01, 3.5281e-02, 5.2207e-01, 3.7511e-09, 1.5066e-06, 1.8652e-02,
        3.9637e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.140

[Epoch: 104, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.7634e-02, 5.6661e-02, 1.9463e-02, 3.7321e-05, 7.2542e-01, 5.4008e-02,
        6.6778e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 104, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9899e-03, 1.3999e-02, 1.1164e-02, 6.0922e-09, 9.5157e-01, 2.5055e-11,
        1.5273e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.308

[Epoch: 105, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3322e-02, 1.0379e-02, 6.2238e-01, 9.8694e-09, 1.7291e-02, 3.4268e-03,
        3.3320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.486

[Epoch: 105, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.5755e-02, 9.6082e-01, 1.5617e-02, 9.6938e-09, 1.1852e-07, 3.4052e-08,
        7.8085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 105, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2468e-01, 3.4564e-02, 5.2249e-01, 4.2058e-09, 1.1869e-06, 1.4776e-02,
        3.4810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.130

[Epoch: 105, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([2.9556e-02, 3.1147e-02, 1.4621e-02, 2.0173e-05, 8.3438e-01, 5.8809e-02,
        3.1463e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 105, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8952e-03, 1.4110e-02, 1.0155e-02, 7.7601e-09, 9.5647e-01, 2.6388e-11,
        1.2374e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.329

[Epoch: 106, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3628e-02, 1.0627e-02, 5.5131e-01, 3.4667e-08, 1.8394e-02, 4.7334e-03,
        4.0131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 106, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.2677e-03, 9.7449e-01, 1.2865e-02, 5.0746e-09, 5.9693e-08, 1.4559e-08,
        5.3761e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 106, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0500e-01, 4.2893e-02, 5.3323e-01, 3.3320e-09, 1.0439e-06, 1.6070e-02,
        2.8034e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.114

[Epoch: 106, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([8.0092e-02, 5.7587e-02, 2.1667e-02, 3.6409e-05, 6.9728e-01, 7.4413e-02,
        6.8926e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.126

[Epoch: 106, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5116e-03, 1.2792e-02, 1.1338e-02, 6.2074e-09, 9.5666e-01, 5.8167e-11,
        1.1694e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.307

[Epoch: 107, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8602e-02, 1.0004e-02, 6.4248e-01, 1.4388e-08, 1.9126e-02, 3.7538e-03,
        3.0603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.526

[Epoch: 107, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.6142e-03, 9.7188e-01, 1.3575e-02, 4.9073e-09, 8.8675e-08, 1.5289e-08,
        5.9281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.969

[Epoch: 107, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0955e-01, 3.0148e-02, 5.3651e-01, 6.4326e-09, 2.3177e-06, 2.0163e-02,
        3.6276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 107, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.7743e-02, 4.7113e-02, 1.5388e-02, 2.4160e-05, 7.9143e-01, 6.0491e-02,
        4.7809e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 107, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([4.5804e-03, 1.2306e-02, 6.9259e-03, 5.0963e-09, 9.6567e-01, 2.7257e-11,
        1.0513e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.303

[Epoch: 108, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.0754e-03, 7.1303e-03, 6.4927e-01, 5.3727e-08, 1.1799e-02, 3.7850e-03,
        3.1894e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.498

[Epoch: 108, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.7079e-03, 9.6797e-01, 1.2810e-02, 1.0366e-08, 9.5647e-08, 3.7982e-08,
        1.0514e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.946

[Epoch: 108, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0655e-01, 3.8044e-02, 5.3212e-01, 4.5594e-09, 1.3837e-06, 1.9278e-02,
        4.0054e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.141

[Epoch: 108, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7228e-02, 4.0072e-02, 1.8798e-02, 6.0065e-05, 7.6933e-01, 5.9779e-02,
        5.4735e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 108, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5787e-03, 1.6175e-02, 1.0684e-02, 2.3346e-08, 9.4723e-01, 2.2475e-10,
        1.8328e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.337

[Epoch: 109, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6446e-02, 1.3562e-02, 5.3226e-01, 2.1925e-08, 1.8777e-02, 4.7018e-03,
        4.1426e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.475

[Epoch: 109, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.7846e-02, 9.6304e-01, 1.3768e-02, 8.8268e-09, 1.1133e-07, 7.5534e-09,
        5.3473e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 109, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4855e-01, 4.1781e-02, 4.9277e-01, 8.3722e-09, 9.9504e-07, 1.3886e-02,
        3.0146e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.105

[Epoch: 109, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.7479e-02, 5.2287e-02, 2.3376e-02, 3.4809e-05, 6.9964e-01, 8.2082e-02,
        7.5098e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.120

[Epoch: 109, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9824e-03, 1.3832e-02, 1.3452e-02, 4.5972e-09, 9.4924e-01, 4.3389e-11,
        1.5490e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.329

[Epoch: 110, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2519e-02, 1.3302e-02, 6.1961e-01, 7.4935e-08, 1.6235e-02, 4.1072e-03,
        3.3423e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.437

[Epoch: 110, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([4.4954e-03, 9.7812e-01, 1.1845e-02, 1.2162e-08, 6.3516e-08, 9.4461e-09,
        5.5435e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 110, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9085e-01, 4.1381e-02, 5.4234e-01, 1.9743e-08, 2.7212e-06, 2.1667e-02,
        3.7604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.158

[Epoch: 110, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.5983e-02, 4.4550e-02, 1.3412e-02, 3.4385e-05, 8.2730e-01, 5.0311e-02,
        2.8414e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 110, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.1187e-03, 1.0790e-02, 7.3956e-03, 5.7795e-09, 9.6698e-01, 3.0066e-11,
        8.7143e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 111, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1747e-02, 7.7326e-03, 6.4115e-01, 1.6801e-08, 1.8019e-02, 4.1877e-03,
        3.1716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.562

[Epoch: 111, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4810e-02, 9.6780e-01, 1.0879e-02, 4.6270e-09, 4.1634e-08, 2.5454e-08,
        6.5084e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.965

[Epoch: 111, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0637e-01, 2.3216e-02, 5.5066e-01, 7.9021e-09, 1.7397e-06, 1.6109e-02,
        3.6353e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 111, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0732e-02, 4.6044e-02, 1.8047e-02, 4.9735e-05, 7.6187e-01, 5.5888e-02,
        5.7369e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 111, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9732e-03, 1.7426e-02, 9.3192e-03, 8.7338e-09, 9.5130e-01, 1.2618e-10,
        1.4984e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.337

[Epoch: 112, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6593e-02, 1.2906e-02, 5.8945e-01, 1.0435e-07, 2.1833e-02, 5.0134e-03,
        3.5421e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 112, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.8587e-03, 9.7183e-01, 1.4409e-02, 2.4502e-08, 1.4044e-07, 4.3472e-08,
        5.8997e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.940

[Epoch: 112, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2000e-01, 5.3179e-02, 5.0441e-01, 3.3318e-08, 3.6806e-06, 1.8159e-02,
        4.2484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.132

[Epoch: 112, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5721e-02, 4.6006e-02, 1.7006e-02, 3.5524e-05, 7.4858e-01, 7.6573e-02,
        5.6074e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 112, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.7471e-03, 1.1853e-02, 1.0548e-02, 2.0386e-08, 9.5921e-01, 7.7155e-11,
        1.1646e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.277

[Epoch: 113, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0105e-02, 8.1913e-03, 5.9685e-01, 1.1851e-08, 9.9864e-03, 4.9362e-03,
        3.6993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.498

[Epoch: 113, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0338e-02, 9.6725e-01, 1.5517e-02, 9.3279e-09, 9.3199e-08, 2.8221e-08,
        6.8938e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 113, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2062e-01, 3.2933e-02, 5.2739e-01, 1.3864e-08, 2.2333e-06, 1.5883e-02,
        3.1757e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 113, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9682e-02, 5.1334e-02, 1.7300e-02, 5.0673e-05, 7.6411e-01, 6.3567e-02,
        5.3955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 113, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5399e-03, 1.5234e-02, 9.8040e-03, 7.5200e-09, 9.5312e-01, 8.1587e-11,
        1.4301e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.341

[Epoch: 114, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4859e-02, 9.3903e-03, 6.3060e-01, 2.5940e-08, 1.6201e-02, 4.1872e-03,
        3.2476e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.460

[Epoch: 114, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5743e-03, 9.7423e-01, 1.1950e-02, 3.7676e-09, 4.5009e-08, 6.5353e-09,
        6.2481e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.974

[Epoch: 114, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1333e-01, 3.7958e-02, 5.2871e-01, 8.0747e-09, 1.9163e-06, 1.6534e-02,
        3.4646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.116

[Epoch: 114, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0074e-02, 4.3494e-02, 1.8088e-02, 3.1690e-05, 7.7368e-01, 5.3781e-02,
        6.0848e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 114, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5780e-03, 1.3696e-02, 8.4343e-03, 1.6679e-08, 9.5704e-01, 5.6293e-11,
        1.4253e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 115, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3005e-02, 1.0555e-02, 5.8855e-01, 3.4202e-08, 2.3697e-02, 5.4535e-03,
        3.5874e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.531

[Epoch: 115, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.3729e-03, 9.6879e-01, 1.4996e-02, 1.1023e-08, 6.8097e-08, 2.4998e-08,
        6.8386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 115, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9648e-01, 3.5274e-02, 5.4729e-01, 7.8705e-09, 2.4988e-06, 1.6787e-02,
        4.1657e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.138

[Epoch: 115, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1021e-02, 4.6366e-02, 1.5720e-02, 3.7277e-05, 7.7901e-01, 6.8330e-02,
        3.9519e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.115

[Epoch: 115, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.3417e-03, 1.3563e-02, 1.0334e-02, 1.6606e-08, 9.5423e-01, 5.0171e-11,
        1.3528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.324

[Epoch: 116, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1056e-02, 1.0934e-02, 6.1200e-01, 2.8301e-08, 1.3197e-02, 3.9641e-03,
        3.4885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.565

[Epoch: 116, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2957e-02, 9.6638e-01, 1.4501e-02, 4.1620e-09, 1.0044e-07, 1.5485e-08,
        6.1587e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 116, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4924e-01, 4.3557e-02, 4.8350e-01, 8.2078e-09, 1.8097e-06, 1.8731e-02,
        4.9678e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.140

[Epoch: 116, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0789e-02, 4.6989e-02, 1.8193e-02, 2.4904e-05, 7.4517e-01, 7.2925e-02,
        5.5907e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 116, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.7105e-03, 1.4414e-02, 1.1567e-02, 8.1809e-09, 9.5389e-01, 3.3702e-11,
        1.2422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.329

[Epoch: 117, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4499e-02, 7.8380e-03, 6.1100e-01, 2.7584e-08, 1.4972e-02, 2.7880e-03,
        3.4890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.462

[Epoch: 117, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([5.8545e-03, 9.7810e-01, 1.0521e-02, 4.2812e-09, 6.2335e-08, 1.0315e-08,
        5.5282e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.951

[Epoch: 117, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9598e-01, 2.5502e-02, 5.6486e-01, 1.2071e-08, 1.9398e-06, 1.1617e-02,
        2.0418e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.080

[Epoch: 117, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6541e-02, 4.9669e-02, 1.7550e-02, 3.8792e-05, 7.5293e-01, 5.3290e-02,
        6.9982e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.097

[Epoch: 117, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([4.6700e-03, 9.3022e-03, 6.7673e-03, 1.2384e-08, 9.6980e-01, 3.3807e-11,
        9.4583e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.290

[Epoch: 118, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4769e-02, 1.2136e-02, 6.1394e-01, 7.5260e-09, 2.1863e-02, 4.4745e-03,
        3.3282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 118, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1767e-02, 9.6114e-01, 1.7176e-02, 1.1390e-08, 7.0486e-08, 1.5930e-08,
        9.9208e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 118, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2368e-01, 4.3548e-02, 5.0702e-01, 9.6296e-09, 2.6891e-06, 2.1763e-02,
        3.9863e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 118, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.1683e-02, 4.0732e-02, 1.5924e-02, 2.8758e-05, 8.0747e-01, 6.0224e-02,
        3.3941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 118, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9008e-03, 1.7814e-02, 1.0054e-02, 1.7192e-08, 9.4550e-01, 6.4591e-11,
        1.8736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.336

[Epoch: 119, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0777e-02, 1.0651e-02, 5.8461e-01, 7.3132e-08, 1.7140e-02, 4.8999e-03,
        3.7192e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.463

[Epoch: 119, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4692e-02, 9.6424e-01, 1.4352e-02, 9.2318e-09, 1.1090e-07, 4.9245e-08,
        6.7175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 119, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3339e-01, 4.6837e-02, 4.9621e-01, 2.6153e-08, 4.6582e-06, 1.6741e-02,
        6.8090e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.155

[Epoch: 119, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.1509e-02, 5.0334e-02, 2.2223e-02, 3.3419e-05, 7.3729e-01, 7.3562e-02,
        5.5050e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.098

[Epoch: 119, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([1.0492e-02, 1.4108e-02, 1.2881e-02, 1.1126e-08, 9.4952e-01, 7.2800e-11,
        1.2995e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.334

[Epoch: 120, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1944e-02, 8.1564e-03, 6.0816e-01, 1.2864e-08, 1.2519e-02, 3.6021e-03,
        3.5562e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.512

[Epoch: 120, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([4.3086e-03, 9.8086e-01, 1.0463e-02, 6.2293e-09, 6.3527e-08, 4.2582e-09,
        4.3637e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.970

[Epoch: 120, batch: 120/201] total loss per batch: 0.541
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9782e-01, 3.2419e-02, 5.5226e-01, 1.8657e-08, 2.0267e-06, 1.5246e-02,
        2.2541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 120, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2748e-02, 4.6357e-02, 1.5216e-02, 3.5110e-05, 7.4713e-01, 6.6110e-02,
        6.2399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 120, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([4.6755e-03, 1.0052e-02, 8.5348e-03, 5.5653e-09, 9.6698e-01, 2.5822e-11,
        9.7570e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.277

[Epoch: 121, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6876e-02, 1.1654e-02, 6.5743e-01, 3.9282e-08, 1.7883e-02, 2.9414e-03,
        2.9322e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.528

[Epoch: 121, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.7558e-03, 9.7460e-01, 1.0413e-02, 1.3591e-09, 3.5514e-08, 1.4060e-08,
        6.2327e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.933

[Epoch: 121, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2772e-01, 2.2471e-02, 5.3585e-01, 6.2208e-09, 1.6533e-06, 1.2181e-02,
        1.7733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.102

[Epoch: 121, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.6767e-02, 4.3437e-02, 1.7213e-02, 2.9875e-05, 8.0469e-01, 5.0746e-02,
        4.7117e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.091

[Epoch: 121, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.5235e-03, 1.8986e-02, 1.2047e-02, 7.2559e-09, 9.4380e-01, 7.6039e-11,
        1.6642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.317

[Epoch: 122, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3168e-02, 1.1139e-02, 5.6593e-01, 6.4589e-08, 1.5464e-02, 4.3836e-03,
        3.8992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.484

[Epoch: 122, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2733e-02, 9.5427e-01, 1.5167e-02, 2.9749e-09, 1.0171e-07, 6.9145e-09,
        1.7829e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.941

[Epoch: 122, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3128e-01, 4.9263e-02, 4.9600e-01, 2.6002e-08, 3.5572e-06, 1.8769e-02,
        4.6832e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.147

[Epoch: 122, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0307e-02, 5.0104e-02, 2.0142e-02, 1.8323e-05, 7.3950e-01, 7.6109e-02,
        5.3818e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.123

[Epoch: 122, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.1102e-03, 1.0888e-02, 8.0912e-03, 1.4011e-08, 9.6401e-01, 3.4318e-11,
        1.0897e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.344

[Epoch: 123, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1081e-02, 1.1805e-02, 5.7663e-01, 6.2670e-08, 1.8590e-02, 4.0078e-03,
        3.7788e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.477

[Epoch: 123, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.5281e-03, 9.7177e-01, 1.5116e-02, 3.0460e-09, 4.2001e-08, 3.9161e-09,
        3.5838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.973

[Epoch: 123, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1662e-01, 3.9759e-02, 5.1510e-01, 3.1678e-08, 3.6911e-06, 2.4350e-02,
        4.1673e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.145

[Epoch: 123, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4078e-02, 4.6724e-02, 1.9517e-02, 4.4976e-05, 7.4732e-01, 6.8326e-02,
        6.3988e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 123, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.4449e-03, 1.3007e-02, 9.6651e-03, 8.1309e-09, 9.5530e-01, 1.0265e-11,
        1.4586e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.346

[Epoch: 124, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6682e-02, 8.9274e-03, 6.5778e-01, 4.1325e-08, 1.4662e-02, 4.8145e-03,
        2.9713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.516

[Epoch: 124, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.5498e-03, 9.7866e-01, 8.7933e-03, 2.7148e-09, 4.2978e-08, 1.0410e-08,
        3.9927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.937

[Epoch: 124, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8772e-01, 2.7596e-02, 5.7089e-01, 1.7200e-08, 2.6876e-06, 1.1044e-02,
        2.7461e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 124, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4734e-02, 4.3219e-02, 1.3339e-02, 4.6783e-05, 7.7870e-01, 6.7247e-02,
        4.2716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 124, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.7774e-03, 1.2024e-02, 7.4104e-03, 6.5758e-09, 9.6324e-01, 3.8034e-11,
        1.1552e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.285

[Epoch: 125, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3872e-02, 8.1809e-03, 5.9646e-01, 2.6066e-08, 1.6908e-02, 4.5809e-03,
        3.6000e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.498

[Epoch: 125, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1439e-02, 9.6604e-01, 1.5081e-02, 1.3594e-08, 9.6345e-08, 2.6832e-08,
        7.4397e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.958

[Epoch: 125, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3418e-01, 4.4938e-02, 4.9994e-01, 2.1738e-08, 2.8645e-06, 1.7371e-02,
        3.5686e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.110

[Epoch: 125, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.2203e-02, 4.0641e-02, 1.5612e-02, 2.1372e-05, 7.9071e-01, 5.7563e-02,
        5.3254e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 125, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3445e-03, 1.1660e-02, 9.7280e-03, 6.2711e-08, 9.6006e-01, 6.3437e-11,
        1.1203e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.320

[Epoch: 126, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1991e-02, 1.0921e-02, 5.8305e-01, 2.8815e-08, 2.0438e-02, 3.9864e-03,
        3.6961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.526

[Epoch: 126, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.0194e-03, 9.7119e-01, 1.5821e-02, 8.6928e-09, 9.9808e-08, 2.1562e-08,
        4.9730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.933

[Epoch: 126, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2356e-01, 3.3705e-02, 5.2434e-01, 1.6385e-08, 2.4460e-06, 1.5142e-02,
        3.2497e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.159

[Epoch: 126, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([7.1946e-02, 5.6184e-02, 1.8358e-02, 2.8728e-05, 7.1598e-01, 7.3886e-02,
        6.3618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.117

[Epoch: 126, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.1101e-03, 1.4624e-02, 1.1475e-02, 1.2740e-08, 9.5348e-01, 1.0085e-11,
        1.3308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 127, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2226e-02, 1.0056e-02, 6.0441e-01, 4.3079e-08, 1.5466e-02, 4.5986e-03,
        3.5324e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.442

[Epoch: 127, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0651e-02, 9.7123e-01, 1.2089e-02, 4.3695e-09, 3.4827e-08, 7.9004e-09,
        6.0344e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.952

[Epoch: 127, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1766e-01, 3.8860e-02, 5.2255e-01, 1.9379e-08, 2.4761e-06, 1.7376e-02,
        3.5438e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 127, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8154e-02, 4.6637e-02, 1.8426e-02, 3.9197e-05, 7.6968e-01, 6.3738e-02,
        5.3324e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 127, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.8759e-03, 1.4215e-02, 1.0870e-02, 2.9138e-08, 9.5102e-01, 2.7161e-11,
        1.6021e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.314

[Epoch: 128, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3596e-02, 7.4815e-03, 6.4503e-01, 1.0097e-08, 1.6816e-02, 3.6345e-03,
        3.1344e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.563

[Epoch: 128, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.6827e-03, 9.7522e-01, 1.1726e-02, 2.5380e-09, 4.9277e-08, 1.4779e-08,
        5.3685e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 128, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0351e-01, 3.3311e-02, 5.4411e-01, 2.8752e-08, 3.0096e-06, 1.5603e-02,
        3.4665e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.144

[Epoch: 128, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.4102e-02, 3.9941e-02, 1.4507e-02, 4.2169e-05, 7.9327e-01, 5.9685e-02,
        4.8448e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 128, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4186e-03, 1.3881e-02, 9.3645e-03, 2.1314e-08, 9.5649e-01, 4.4025e-11,
        1.3842e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 129, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5354e-02, 1.5046e-02, 5.7368e-01, 6.4057e-08, 2.2823e-02, 5.8377e-03,
        3.6726e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.482

[Epoch: 129, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2490e-02, 9.6468e-01, 1.3874e-02, 6.1427e-09, 7.7297e-08, 1.7174e-08,
        8.9516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 129, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3594e-01, 3.9702e-02, 5.0140e-01, 9.2684e-09, 2.0728e-06, 1.9379e-02,
        3.5822e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 129, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6845e-02, 4.9172e-02, 1.5571e-02, 3.9943e-05, 7.6305e-01, 6.5103e-02,
        5.0219e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 129, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.5481e-03, 1.0464e-02, 7.6150e-03, 2.2693e-08, 9.6781e-01, 2.6281e-11,
        8.5663e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.310

[Epoch: 130, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1681e-02, 8.4151e-03, 5.9311e-01, 1.7015e-08, 1.3664e-02, 4.2154e-03,
        3.6892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.517

[Epoch: 130, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0817e-02, 9.7121e-01, 1.2908e-02, 3.0532e-09, 4.7660e-08, 1.3452e-08,
        5.0646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.965

[Epoch: 130, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1409e-01, 3.1798e-02, 5.3882e-01, 1.1722e-08, 2.1639e-06, 1.2343e-02,
        2.9537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 130, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0187e-02, 5.1512e-02, 2.0400e-02, 3.2198e-05, 7.3664e-01, 7.1195e-02,
        6.0038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.112

[Epoch: 130, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8405e-03, 1.5532e-02, 1.1222e-02, 2.2461e-08, 9.5439e-01, 2.1540e-11,
        1.2020e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.334

[Epoch: 131, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2079e-02, 8.7947e-03, 6.5612e-01, 2.5105e-08, 1.4188e-02, 3.1846e-03,
        3.0563e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.506

[Epoch: 131, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.7546e-03, 9.7382e-01, 1.2910e-02, 2.2285e-09, 3.6987e-08, 4.1380e-09,
        4.5162e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 131, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8768e-01, 2.8042e-02, 5.6330e-01, 1.1440e-08, 2.4430e-06, 1.8116e-02,
        2.8651e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 131, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.1891e-02, 4.1562e-02, 1.4154e-02, 5.8425e-05, 7.8730e-01, 5.8611e-02,
        5.6424e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 131, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([9.2813e-03, 1.7288e-02, 1.4128e-02, 2.1167e-08, 9.3739e-01, 2.7400e-11,
        2.1915e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.330

[Epoch: 132, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6492e-02, 1.2402e-02, 6.0658e-01, 1.4601e-08, 2.0614e-02, 4.5550e-03,
        3.3935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.464

[Epoch: 132, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.5663e-03, 9.7197e-01, 1.1421e-02, 9.6719e-10, 3.0715e-08, 7.7801e-09,
        8.0461e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.943

[Epoch: 132, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4312e-01, 5.4289e-02, 4.7818e-01, 2.4293e-08, 2.7569e-06, 2.0048e-02,
        4.3607e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 132, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.5804e-02, 5.3212e-02, 1.7433e-02, 4.1046e-05, 7.6750e-01, 5.0737e-02,
        4.5274e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 132, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4846e-03, 1.3079e-02, 8.5921e-03, 1.5768e-08, 9.5897e-01, 2.4327e-11,
        1.2873e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.308

[Epoch: 133, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1311e-02, 9.1121e-03, 5.2544e-01, 1.3135e-07, 1.6446e-02, 6.0364e-03,
        4.3165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.508

[Epoch: 133, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4159e-02, 9.6026e-01, 1.4422e-02, 7.0103e-09, 7.4887e-08, 2.9408e-08,
        1.1162e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 133, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0951e-01, 2.9229e-02, 5.4482e-01, 7.8228e-09, 1.8043e-06, 1.3133e-02,
        3.3092e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 133, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9026e-02, 4.6048e-02, 1.7031e-02, 8.4621e-05, 7.4551e-01, 7.8593e-02,
        6.3706e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 133, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.3894e-03, 1.0098e-02, 7.6943e-03, 6.3228e-08, 9.6640e-01, 5.9371e-11,
        9.4174e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.271

[Epoch: 134, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6061e-02, 1.0233e-02, 6.4214e-01, 2.0238e-08, 1.6559e-02, 2.6152e-03,
        3.1239e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.508

[Epoch: 134, batch: 80/201] total loss per batch: 0.541
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([5.8934e-03, 9.7805e-01, 1.2741e-02, 3.8975e-09, 7.0978e-08, 8.4721e-09,
        3.3198e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 134, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0962e-01, 3.5602e-02, 5.2284e-01, 3.2488e-08, 2.2055e-06, 2.8200e-02,
        3.7369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 134, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6409e-02, 4.8313e-02, 1.4872e-02, 5.4265e-05, 7.5854e-01, 6.3382e-02,
        5.8434e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 134, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.9904e-03, 1.6792e-02, 1.1509e-02, 3.6869e-08, 9.4769e-01, 1.4322e-11,
        1.6017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.354

[Epoch: 135, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3158e-02, 1.1686e-02, 6.3718e-01, 8.6262e-08, 1.5699e-02, 4.0843e-03,
        3.1819e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.495

[Epoch: 135, batch: 80/201] total loss per batch: 0.540
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0217e-02, 9.6734e-01, 1.6237e-02, 7.1311e-09, 1.0734e-07, 1.2604e-08,
        6.2023e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 135, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0242e-01, 3.3516e-02, 5.4978e-01, 2.2427e-08, 3.9258e-06, 1.1634e-02,
        2.6450e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.111

[Epoch: 135, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.2221e-02, 2.6805e-02, 1.1591e-02, 3.8980e-05, 8.5901e-01, 3.8742e-02,
        3.1597e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.099

[Epoch: 135, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5095e-03, 1.1437e-02, 9.0170e-03, 1.8979e-08, 9.6315e-01, 2.5060e-11,
        9.8861e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.305

[Epoch: 136, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2680e-02, 1.2594e-02, 5.8299e-01, 6.2107e-08, 1.7690e-02, 6.3427e-03,
        3.6770e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.480

[Epoch: 136, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4996e-02, 9.6604e-01, 1.0716e-02, 4.6219e-09, 4.1144e-08, 1.3360e-08,
        8.2460e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 136, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3993e-01, 4.3107e-02, 4.9577e-01, 1.2173e-08, 2.6617e-06, 1.6887e-02,
        4.3077e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.130

[Epoch: 136, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([8.9497e-02, 7.7832e-02, 3.6889e-02, 1.4142e-04, 6.2791e-01, 1.0365e-01,
        6.4084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 136, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.0670e-03, 1.1970e-02, 9.9142e-03, 4.0587e-08, 9.5603e-01, 6.8814e-11,
        1.5017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 137, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4503e-02, 7.7797e-03, 5.9088e-01, 4.1265e-08, 1.8576e-02, 3.6094e-03,
        3.6466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.551

[Epoch: 137, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.0599e-03, 9.7288e-01, 1.6070e-02, 1.4295e-08, 1.1078e-07, 9.9202e-09,
        4.9948e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 137, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1213e-01, 3.6099e-02, 5.2866e-01, 1.3157e-08, 3.0097e-06, 1.9207e-02,
        3.8927e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.130

[Epoch: 137, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7026e-02, 3.9718e-02, 1.5856e-02, 7.8698e-05, 7.7209e-01, 5.8057e-02,
        5.7178e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 137, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3178e-03, 1.4958e-02, 1.0940e-02, 2.6357e-08, 9.4976e-01, 1.3761e-11,
        1.7020e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.324

[Epoch: 138, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1596e-02, 1.0648e-02, 6.2227e-01, 4.2723e-08, 1.5292e-02, 4.6197e-03,
        3.3557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.524

[Epoch: 138, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0248e-02, 9.7252e-01, 1.1967e-02, 3.3370e-09, 4.4780e-08, 6.4157e-09,
        5.2654e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 138, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1371e-01, 3.9085e-02, 5.2642e-01, 1.2311e-08, 3.0105e-06, 1.6819e-02,
        3.9601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 138, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8960e-02, 4.2703e-02, 1.4570e-02, 7.1804e-05, 7.8639e-01, 5.8565e-02,
        4.8739e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 138, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4696e-03, 1.1943e-02, 7.9980e-03, 2.9993e-08, 9.6069e-01, 2.5361e-11,
        1.2902e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.327

[Epoch: 139, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2679e-02, 1.1011e-02, 6.0545e-01, 4.0871e-08, 1.7905e-02, 3.7138e-03,
        3.4924e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.508

[Epoch: 139, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0843e-02, 9.6737e-01, 1.4201e-02, 6.5511e-09, 6.8414e-08, 6.4776e-09,
        7.5837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 139, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2347e-01, 3.3212e-02, 5.2348e-01, 9.4993e-09, 1.8334e-06, 1.6634e-02,
        3.1971e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.137

[Epoch: 139, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0928e-02, 4.1340e-02, 1.9180e-02, 4.2077e-05, 7.6637e-01, 7.0559e-02,
        5.1579e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 139, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6484e-03, 1.3407e-02, 1.0702e-02, 2.2745e-08, 9.5651e-01, 1.5952e-11,
        1.2733e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 140, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4363e-02, 1.0725e-02, 5.9178e-01, 3.6872e-08, 1.6452e-02, 3.8226e-03,
        3.6285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.497

[Epoch: 140, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.8746e-03, 9.7131e-01, 1.2419e-02, 8.6569e-09, 6.4569e-08, 1.4988e-08,
        7.3959e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 140, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0474e-01, 3.8021e-02, 5.3604e-01, 2.0134e-08, 3.3025e-06, 1.7512e-02,
        3.6869e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 140, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.8541e-02, 4.8597e-02, 1.5865e-02, 5.8111e-05, 7.6229e-01, 5.7646e-02,
        5.7004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 140, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5995e-03, 1.2216e-02, 9.7522e-03, 2.0521e-08, 9.5846e-01, 2.6748e-11,
        1.2969e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.338

[Epoch: 141, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1579e-02, 7.2832e-03, 6.3137e-01, 1.3103e-08, 1.6430e-02, 3.5865e-03,
        3.2975e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 141, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1685e-02, 9.6848e-01, 1.3188e-02, 5.6777e-09, 6.0814e-08, 7.8615e-09,
        6.6474e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 141, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2257e-01, 3.8346e-02, 5.1790e-01, 2.3748e-08, 3.3660e-06, 1.7019e-02,
        4.1688e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.134

[Epoch: 141, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.1134e-02, 5.3605e-02, 2.0074e-02, 7.8989e-05, 7.4769e-01, 6.5825e-02,
        5.1594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 141, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.5801e-03, 1.2439e-02, 1.0389e-02, 2.6925e-08, 9.5556e-01, 2.4853e-11,
        1.6032e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.304

[Epoch: 142, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5563e-02, 1.1684e-02, 5.8868e-01, 7.0969e-08, 1.9746e-02, 3.8719e-03,
        3.6045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 142, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5782e-03, 9.7478e-01, 1.1736e-02, 3.6233e-09, 4.3166e-08, 8.6589e-09,
        5.9012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.963

[Epoch: 142, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0751e-01, 3.4333e-02, 5.3768e-01, 9.2306e-09, 2.1254e-06, 1.7471e-02,
        3.0062e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.109

[Epoch: 142, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.0774e-02, 4.2495e-02, 1.2761e-02, 5.0626e-05, 7.8371e-01, 6.7236e-02,
        5.2970e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 142, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.2362e-03, 1.2893e-02, 8.9627e-03, 1.6205e-08, 9.6150e-01, 1.3247e-11,
        1.0411e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 143, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2213e-02, 1.1324e-02, 6.0748e-01, 1.6525e-08, 1.6467e-02, 4.5340e-03,
        3.4798e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.512

[Epoch: 143, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1835e-02, 9.6113e-01, 1.9358e-02, 6.8844e-09, 7.1045e-08, 6.9596e-09,
        7.6765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.936

[Epoch: 143, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1643e-01, 2.9343e-02, 5.3849e-01, 6.3737e-09, 1.1039e-06, 1.2924e-02,
        2.8102e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.118

[Epoch: 143, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.9475e-02, 4.3586e-02, 1.5466e-02, 5.4045e-05, 7.7742e-01, 5.2587e-02,
        5.1415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.101

[Epoch: 143, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.2866e-03, 1.3121e-02, 1.0864e-02, 1.3769e-07, 9.5505e-01, 3.3517e-11,
        1.2679e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.296

[Epoch: 144, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1547e-02, 6.4505e-03, 6.0732e-01, 1.3824e-08, 1.2007e-02, 3.0280e-03,
        3.5965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.518

[Epoch: 144, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.8441e-03, 9.7856e-01, 8.5985e-03, 2.1381e-09, 5.2715e-08, 2.4272e-09,
        6.0024e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.941

[Epoch: 144, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3456e-01, 4.4342e-02, 4.9369e-01, 2.1696e-08, 3.4515e-06, 2.2671e-02,
        4.7328e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.136

[Epoch: 144, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2790e-02, 5.1921e-02, 1.8826e-02, 7.4234e-05, 7.4586e-01, 7.2303e-02,
        5.8230e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.117

[Epoch: 144, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9083e-03, 1.4799e-02, 1.0312e-02, 5.1850e-08, 9.5190e-01, 2.0959e-11,
        1.6083e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.309

[Epoch: 145, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5585e-02, 1.3322e-02, 6.3419e-01, 3.8918e-08, 1.5429e-02, 3.4125e-03,
        3.1806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.443

[Epoch: 145, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3987e-02, 9.6662e-01, 1.2604e-02, 1.6995e-09, 3.5452e-08, 4.3752e-09,
        6.7891e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.976

[Epoch: 145, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8065e-01, 3.2936e-02, 5.6045e-01, 1.8393e-08, 3.8190e-06, 2.0665e-02,
        5.2900e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.134

[Epoch: 145, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.3518e-02, 6.2513e-02, 2.2805e-02, 5.5921e-05, 7.2190e-01, 7.0285e-02,
        5.8926e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.116

[Epoch: 145, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.4071e-03, 1.3509e-02, 9.2860e-03, 3.9804e-08, 9.5505e-01, 2.1958e-11,
        1.4750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.336

[Epoch: 146, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.7285e-02, 9.5906e-03, 5.6719e-01, 1.4019e-07, 2.6346e-02, 5.2519e-03,
        3.7433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 146, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.0002e-03, 9.7193e-01, 1.4903e-02, 2.2940e-09, 3.2484e-08, 3.8162e-09,
        6.1668e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 146, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4061e-01, 3.9894e-02, 5.0299e-01, 3.5298e-09, 1.2690e-06, 1.2979e-02,
        3.5287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.135

[Epoch: 146, batch: 160/201] total loss per batch: 0.536
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.3708e-02, 3.3799e-02, 1.3092e-02, 4.4408e-05, 8.2845e-01, 4.2037e-02,
        3.8866e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.087

[Epoch: 146, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.8281e-03, 1.5391e-02, 9.7787e-03, 5.6240e-08, 9.5735e-01, 5.0636e-11,
        9.6560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.310

[Epoch: 147, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([7.8939e-03, 6.9224e-03, 6.4493e-01, 7.7449e-08, 1.5450e-02, 4.0821e-03,
        3.2072e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.544

[Epoch: 147, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3783e-02, 9.6521e-01, 1.2115e-02, 1.5197e-08, 1.0628e-07, 1.0529e-08,
        8.8931e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 147, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1282e-01, 3.3704e-02, 5.3539e-01, 1.2990e-08, 3.9882e-06, 1.4543e-02,
        3.5411e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 147, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4638e-02, 4.7695e-02, 1.8892e-02, 6.0809e-05, 7.1872e-01, 9.5693e-02,
        6.4301e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 147, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8996e-03, 1.1676e-02, 7.6365e-03, 3.1953e-08, 9.6179e-01, 1.8716e-11,
        1.2001e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.314

[Epoch: 148, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3107e-02, 1.1894e-02, 5.9131e-01, 2.1429e-08, 1.2123e-02, 3.6506e-03,
        3.6792e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.550

[Epoch: 148, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.0028e-03, 9.7348e-01, 1.2409e-02, 1.4543e-09, 3.9825e-08, 3.1172e-09,
        6.1093e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 148, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1367e-01, 3.8297e-02, 5.2582e-01, 2.2884e-08, 3.2248e-06, 1.8262e-02,
        3.9509e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.143

[Epoch: 148, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0275e-02, 4.8599e-02, 1.9301e-02, 4.3453e-05, 7.5600e-01, 6.5063e-02,
        6.0723e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.114

[Epoch: 148, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.2115e-03, 1.6545e-02, 1.1653e-02, 1.9741e-08, 9.4797e-01, 4.9453e-11,
        1.6618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.316

[Epoch: 149, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4966e-02, 1.0752e-02, 6.2445e-01, 3.0075e-08, 1.9192e-02, 3.4383e-03,
        3.2721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.463

[Epoch: 149, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.0649e-03, 9.7176e-01, 1.3453e-02, 6.8530e-09, 4.9435e-08, 4.6746e-09,
        6.7228e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.970

[Epoch: 149, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1434e-01, 3.6396e-02, 5.2946e-01, 1.1979e-08, 2.6344e-06, 1.5596e-02,
        4.2079e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.112

[Epoch: 149, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.8126e-02, 4.6153e-02, 1.8006e-02, 6.6819e-05, 7.7163e-01, 6.2202e-02,
        4.3818e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 149, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5551e-03, 1.2402e-02, 1.2017e-02, 3.1221e-08, 9.5380e-01, 3.2452e-11,
        1.4222e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.330

[Epoch: 150, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3272e-02, 9.4517e-03, 5.8360e-01, 1.6496e-08, 1.8509e-02, 3.7487e-03,
        3.7142e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.489

[Epoch: 150, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2738e-02, 9.6421e-01, 1.4763e-02, 4.0593e-09, 4.0516e-08, 2.7840e-09,
        8.2886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 150, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1052e-01, 4.4045e-02, 5.2545e-01, 3.5912e-08, 4.1633e-06, 1.5087e-02,
        4.8943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 150, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3973e-02, 4.5096e-02, 1.4995e-02, 5.1792e-05, 7.6210e-01, 6.6925e-02,
        5.6860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.110

[Epoch: 150, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.8764e-03, 1.2076e-02, 9.0559e-03, 1.5820e-08, 9.5991e-01, 4.1643e-11,
        1.3084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.317

[Epoch: 151, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2858e-02, 9.9685e-03, 6.0988e-01, 3.5841e-08, 1.5656e-02, 3.3696e-03,
        3.4827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.538

[Epoch: 151, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.4092e-03, 9.7641e-01, 1.0752e-02, 4.7758e-09, 3.9754e-08, 4.9360e-09,
        4.4329e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.963

[Epoch: 151, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3289e-01, 3.3514e-02, 5.1420e-01, 1.2341e-08, 2.4889e-06, 1.5558e-02,
        3.8359e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.115

[Epoch: 151, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9054e-02, 4.7746e-02, 1.4998e-02, 5.6182e-05, 7.7220e-01, 6.4894e-02,
        5.1048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 151, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.0218e-03, 1.2645e-02, 9.3090e-03, 1.5122e-08, 9.6012e-01, 2.4783e-11,
        1.1908e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.321

[Epoch: 152, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5125e-02, 1.0304e-02, 6.0520e-01, 2.3330e-08, 1.6351e-02, 3.7894e-03,
        3.4923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.505

[Epoch: 152, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.2358e-03, 9.7597e-01, 1.1093e-02, 4.0389e-09, 4.3704e-08, 7.3806e-09,
        5.7002e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.948

[Epoch: 152, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0945e-01, 3.3505e-02, 5.3785e-01, 1.5707e-08, 3.4633e-06, 1.6119e-02,
        3.0715e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 152, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3870e-02, 4.6140e-02, 1.7632e-02, 4.7155e-05, 7.6702e-01, 6.0385e-02,
        5.4906e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 152, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.1419e-03, 1.5263e-02, 1.0831e-02, 1.9085e-08, 9.5187e-01, 3.3653e-11,
        1.4897e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.321

[Epoch: 153, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2249e-02, 1.0623e-02, 6.1456e-01, 1.8953e-08, 1.6863e-02, 3.7782e-03,
        3.4192e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.492

[Epoch: 153, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.2112e-02, 9.6774e-01, 1.3578e-02, 4.8174e-09, 4.5081e-08, 6.2400e-09,
        6.5729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 153, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1959e-01, 3.4978e-02, 5.2418e-01, 1.5692e-08, 3.4537e-06, 1.7285e-02,
        3.9620e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 153, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3786e-02, 4.4249e-02, 1.5751e-02, 4.6883e-05, 7.7427e-01, 6.2459e-02,
        4.9443e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 153, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6571e-03, 1.3256e-02, 1.0660e-02, 1.7246e-08, 9.5581e-01, 3.1331e-11,
        1.3616e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.311

[Epoch: 154, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3378e-02, 9.4685e-03, 5.9040e-01, 2.0814e-08, 1.6787e-02, 4.0239e-03,
        3.6594e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.514

[Epoch: 154, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4059e-03, 9.7007e-01, 1.4162e-02, 3.3971e-09, 3.4974e-08, 4.6718e-09,
        6.3603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 154, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2248e-01, 3.9289e-02, 5.1708e-01, 8.3313e-09, 2.4100e-06, 1.7471e-02,
        3.6788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.134

[Epoch: 154, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3109e-02, 4.9321e-02, 1.7737e-02, 3.9988e-05, 7.5635e-01, 6.6074e-02,
        5.7373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 154, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.3324e-03, 1.2697e-02, 1.0058e-02, 1.0986e-08, 9.5801e-01, 1.8230e-11,
        1.2904e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.331

[Epoch: 155, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3966e-02, 9.8568e-03, 6.2366e-01, 2.6108e-08, 1.7224e-02, 3.2524e-03,
        3.3204e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.513

[Epoch: 155, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.3132e-03, 9.7080e-01, 1.3435e-02, 3.1270e-09, 3.2648e-08, 4.6250e-09,
        6.4535e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 155, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1660e-01, 3.6042e-02, 5.2794e-01, 1.0250e-08, 2.6435e-06, 1.6123e-02,
        3.2925e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 155, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4887e-02, 4.9417e-02, 1.6013e-02, 4.4170e-05, 7.6117e-01, 6.4522e-02,
        5.3947e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 155, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6011e-03, 1.3847e-02, 9.8083e-03, 1.1167e-08, 9.5508e-01, 2.2204e-11,
        1.4668e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.321

[Epoch: 156, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3170e-02, 9.6683e-03, 6.0544e-01, 1.3567e-08, 1.6490e-02, 3.2121e-03,
        3.5202e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 156, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0741e-02, 9.6955e-01, 1.3223e-02, 1.6275e-09, 2.6265e-08, 2.3785e-09,
        6.4872e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 156, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1673e-01, 3.5120e-02, 5.2940e-01, 7.1752e-09, 1.7128e-06, 1.5643e-02,
        3.0990e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 156, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0259e-02, 4.3878e-02, 1.5973e-02, 3.0218e-05, 7.7685e-01, 6.0706e-02,
        5.2300e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.109

[Epoch: 156, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.2758e-03, 1.2989e-02, 9.5262e-03, 9.0987e-09, 9.5829e-01, 1.8670e-11,
        1.2917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 157, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3244e-02, 1.0226e-02, 5.8795e-01, 1.1704e-08, 1.6042e-02, 3.1173e-03,
        3.6942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.507

[Epoch: 157, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0634e-02, 9.6948e-01, 1.2966e-02, 1.4082e-09, 2.7422e-08, 2.7998e-09,
        6.9212e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.960

[Epoch: 157, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1001e-01, 3.6945e-02, 5.3370e-01, 9.4138e-09, 2.3176e-06, 1.6211e-02,
        3.1360e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 157, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3719e-02, 4.6856e-02, 1.6795e-02, 2.7898e-05, 7.6943e-01, 6.3976e-02,
        4.9196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 157, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.3932e-03, 1.2913e-02, 9.8747e-03, 9.0093e-09, 9.5751e-01, 1.3781e-11,
        1.3306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.315

[Epoch: 158, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2588e-02, 1.1112e-02, 6.1011e-01, 2.0736e-08, 1.7028e-02, 3.1923e-03,
        3.4597e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.491

[Epoch: 158, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7733e-03, 9.6993e-01, 1.4031e-02, 1.8171e-09, 2.7708e-08, 3.2221e-09,
        6.2675e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 158, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3056e-01, 3.9210e-02, 5.0624e-01, 4.6428e-09, 1.4165e-06, 2.0298e-02,
        3.6916e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 158, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5494e-02, 4.4917e-02, 1.7411e-02, 2.2862e-05, 7.5816e-01, 6.3229e-02,
        6.0766e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 158, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.6524e-03, 1.2866e-02, 9.9656e-03, 5.2711e-09, 9.5934e-01, 8.1686e-12,
        1.1173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.331

[Epoch: 159, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4189e-02, 7.7178e-03, 6.1871e-01, 1.2087e-08, 1.6090e-02, 4.1331e-03,
        3.3916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.499

[Epoch: 159, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([8.1901e-03, 9.7382e-01, 1.1824e-02, 2.2061e-09, 2.3185e-08, 3.9723e-09,
        6.1704e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 159, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1087e-01, 3.3848e-02, 5.3798e-01, 7.8036e-09, 2.8068e-06, 1.4038e-02,
        3.2590e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 159, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0052e-02, 5.3810e-02, 1.9970e-02, 3.8863e-05, 7.4887e-01, 6.8095e-02,
        4.9165e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 159, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5877e-03, 1.5564e-02, 1.1355e-02, 6.3485e-09, 9.4902e-01, 3.1026e-11,
        1.6478e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.303

[Epoch: 160, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3356e-02, 1.0236e-02, 6.3717e-01, 6.3902e-09, 2.2229e-02, 3.5500e-03,
        3.1346e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.505

[Epoch: 160, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1823e-02, 9.6384e-01, 1.5584e-02, 1.8322e-09, 3.3499e-08, 1.8960e-09,
        8.7537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.951

[Epoch: 160, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1644e-01, 3.7153e-02, 5.2682e-01, 6.2924e-09, 1.5271e-06, 1.6083e-02,
        3.5088e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 160, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.5957e-02, 4.1907e-02, 1.3306e-02, 3.2786e-05, 7.8232e-01, 6.1018e-02,
        5.5461e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.111

[Epoch: 160, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.4410e-03, 1.2403e-02, 9.4323e-03, 1.5991e-08, 9.5636e-01, 2.5186e-11,
        1.4359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 161, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4177e-02, 1.1370e-02, 5.5663e-01, 3.7377e-08, 1.2516e-02, 3.3506e-03,
        4.0196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.517

[Epoch: 161, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1490e-02, 9.6279e-01, 1.8501e-02, 2.1162e-09, 3.8779e-08, 2.5564e-09,
        7.2174e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 161, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.3350e-01, 3.8150e-02, 5.0721e-01, 8.8183e-09, 1.7860e-06, 1.7642e-02,
        3.4934e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.111

[Epoch: 161, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([3.9828e-02, 4.1583e-02, 1.2556e-02, 3.8704e-05, 8.0041e-01, 6.2606e-02,
        4.2978e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.114

[Epoch: 161, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4314e-03, 1.1117e-02, 7.7491e-03, 1.5685e-08, 9.6509e-01, 2.4861e-11,
        9.6134e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.329

[Epoch: 162, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1805e-02, 1.1829e-02, 5.8681e-01, 2.0414e-08, 1.9245e-02, 5.3552e-03,
        3.6496e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 162, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.2493e-03, 9.8213e-01, 8.2462e-03, 8.1589e-09, 4.5902e-08, 7.7882e-09,
        3.3751e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.971

[Epoch: 162, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0508e-01, 3.2118e-02, 5.4192e-01, 4.8864e-09, 1.9139e-06, 1.7266e-02,
        3.6198e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.119

[Epoch: 162, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.5110e-02, 4.9690e-02, 1.8071e-02, 2.1385e-05, 7.4094e-01, 5.8937e-02,
        6.7235e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.089

[Epoch: 162, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.8818e-03, 1.7185e-02, 1.3779e-02, 1.2239e-08, 9.4825e-01, 4.5961e-11,
        1.2906e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.362

[Epoch: 163, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5227e-02, 9.8333e-03, 6.0926e-01, 1.3088e-07, 1.3313e-02, 3.0967e-03,
        3.4927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.471

[Epoch: 163, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1937e-02, 9.7106e-01, 1.0217e-02, 2.4614e-09, 2.5180e-08, 3.8540e-09,
        6.7893e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.971

[Epoch: 163, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1326e-01, 4.4175e-02, 5.1785e-01, 1.7562e-08, 2.2300e-06, 2.0645e-02,
        4.0675e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.153

[Epoch: 163, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2074e-02, 4.3846e-02, 1.8678e-02, 3.8455e-05, 7.4384e-01, 7.2472e-02,
        5.9054e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.120

[Epoch: 163, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.8039e-03, 1.5581e-02, 9.4424e-03, 2.0044e-08, 9.5105e-01, 2.7739e-11,
        1.6128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 164, batch: 40/201] total loss per batch: 0.519
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.6546e-02, 1.3743e-02, 6.3028e-01, 1.3974e-08, 2.1651e-02, 2.8016e-03,
        3.1498e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.555

[Epoch: 164, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.4657e-03, 9.7297e-01, 1.4830e-02, 1.0809e-08, 6.5410e-08, 1.6525e-08,
        5.7366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.966

[Epoch: 164, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1628e-01, 3.6781e-02, 5.2658e-01, 2.2718e-08, 2.2879e-06, 1.6526e-02,
        3.8260e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.135

[Epoch: 164, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.5970e-02, 4.7195e-02, 1.4442e-02, 6.3086e-05, 7.8044e-01, 5.5881e-02,
        4.6005e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.095

[Epoch: 164, batch: 200/201] total loss per batch: 0.515
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.0681e-03, 1.2477e-02, 1.0419e-02, 1.1073e-08, 9.5910e-01, 4.5991e-11,
        1.0934e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 165, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.8655e-02, 9.5759e-03, 6.0888e-01, 1.4996e-08, 1.7025e-02, 3.4676e-03,
        3.4240e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.520

[Epoch: 165, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0204e-02, 9.7095e-01, 1.1157e-02, 1.9174e-08, 6.9881e-08, 9.7410e-09,
        7.6891e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.973

[Epoch: 165, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2307e-01, 2.9341e-02, 5.2534e-01, 2.6942e-08, 1.7295e-06, 1.8120e-02,
        4.1267e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 165, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.5670e-02, 4.4968e-02, 1.9609e-02, 2.8467e-05, 7.6571e-01, 6.3508e-02,
        6.0508e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 165, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9517e-03, 1.3102e-02, 1.0644e-02, 1.3854e-08, 9.5632e-01, 3.6884e-11,
        1.2980e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.302

[Epoch: 166, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2755e-02, 1.1115e-02, 6.0731e-01, 3.3144e-08, 1.7876e-02, 4.0435e-03,
        3.4690e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.489

[Epoch: 166, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0549e-02, 9.6826e-01, 1.3078e-02, 1.9124e-08, 7.8062e-08, 1.3663e-08,
        8.1103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.966

[Epoch: 166, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0733e-01, 3.1828e-02, 5.3963e-01, 3.0266e-08, 1.4656e-06, 1.7508e-02,
        3.7038e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.112

[Epoch: 166, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.7678e-02, 4.6950e-02, 1.7661e-02, 4.3136e-05, 7.5911e-01, 6.2255e-02,
        5.6305e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.100

[Epoch: 166, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.0104e-03, 1.4334e-02, 1.0216e-02, 1.7178e-08, 9.5406e-01, 5.4735e-11,
        1.3384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 167, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2575e-02, 1.0153e-02, 6.1143e-01, 1.9811e-08, 1.6758e-02, 3.7091e-03,
        3.4538e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.487

[Epoch: 167, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0307e-02, 9.6962e-01, 1.3997e-02, 9.4087e-09, 6.3938e-08, 9.1692e-09,
        6.0792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 167, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1407e-01, 3.4373e-02, 5.2972e-01, 2.5232e-08, 1.5184e-06, 1.8113e-02,
        3.7221e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.124

[Epoch: 167, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2435e-02, 4.8687e-02, 1.6612e-02, 3.6717e-05, 7.6446e-01, 6.2879e-02,
        5.4885e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 167, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3884e-03, 1.4494e-02, 1.0312e-02, 1.5245e-08, 9.5405e-01, 5.0641e-11,
        1.3758e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 168, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3335e-02, 1.0108e-02, 6.0299e-01, 1.5493e-08, 1.7782e-02, 3.6331e-03,
        3.5215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 168, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.3697e-03, 9.7074e-01, 1.3386e-02, 6.7498e-09, 4.9327e-08, 6.7592e-09,
        6.5064e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 168, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1627e-01, 3.5854e-02, 5.2749e-01, 1.7809e-08, 1.1897e-06, 1.6674e-02,
        3.7119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 168, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2542e-02, 4.7812e-02, 1.6938e-02, 4.4646e-05, 7.6956e-01, 6.2882e-02,
        5.0218e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 168, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9608e-03, 1.4025e-02, 9.9807e-03, 1.2704e-08, 9.5615e-01, 4.3335e-11,
        1.2882e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 169, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2967e-02, 9.1964e-03, 6.0677e-01, 1.4083e-08, 1.6386e-02, 3.6237e-03,
        3.5105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 169, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9122e-03, 9.7019e-01, 1.3383e-02, 5.0122e-09, 4.2821e-08, 5.5384e-09,
        6.5139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 169, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1572e-01, 3.7362e-02, 5.2675e-01, 1.7083e-08, 1.2173e-06, 1.6520e-02,
        3.6486e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.126

[Epoch: 169, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3026e-02, 4.7010e-02, 1.6516e-02, 4.0425e-05, 7.6850e-01, 6.2565e-02,
        5.2343e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.104

[Epoch: 169, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8271e-03, 1.3726e-02, 9.9682e-03, 1.1316e-08, 9.5663e-01, 3.9929e-11,
        1.2847e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 170, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3372e-02, 9.7600e-03, 6.0802e-01, 1.2743e-08, 1.7599e-02, 3.4606e-03,
        3.4778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.507

[Epoch: 170, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0181e-02, 9.6939e-01, 1.3565e-02, 5.0604e-09, 4.3185e-08, 6.0037e-09,
        6.8665e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.960

[Epoch: 170, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1678e-01, 3.6721e-02, 5.2624e-01, 1.3344e-08, 1.0696e-06, 1.6597e-02,
        3.6607e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.126

[Epoch: 170, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3564e-02, 4.6833e-02, 1.6691e-02, 3.7618e-05, 7.6607e-01, 6.2911e-02,
        5.3897e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 170, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9210e-03, 1.3833e-02, 1.0123e-02, 9.7527e-09, 9.5608e-01, 3.0842e-11,
        1.3041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.320

[Epoch: 171, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3558e-02, 1.0082e-02, 6.0553e-01, 9.9955e-09, 1.6768e-02, 3.5529e-03,
        3.5050e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.507

[Epoch: 171, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8364e-03, 9.6971e-01, 1.3572e-02, 4.1174e-09, 3.8704e-08, 4.8946e-09,
        6.8792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 171, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1649e-01, 3.6614e-02, 5.2639e-01, 1.2461e-08, 1.0166e-06, 1.6843e-02,
        3.6637e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 171, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3167e-02, 4.6419e-02, 1.6412e-02, 3.2571e-05, 7.6740e-01, 6.3247e-02,
        5.3327e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 171, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8969e-03, 1.3735e-02, 1.0074e-02, 7.7418e-09, 9.5598e-01, 2.5040e-11,
        1.3317e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 172, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3236e-02, 9.9996e-03, 6.0707e-01, 9.8965e-09, 1.6454e-02, 3.4235e-03,
        3.4982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.507

[Epoch: 172, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0036e-02, 9.6946e-01, 1.3790e-02, 3.5283e-09, 3.5149e-08, 4.5265e-09,
        6.7154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 172, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1632e-01, 3.6606e-02, 5.2673e-01, 1.0492e-08, 9.2566e-07, 1.6710e-02,
        3.6346e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 172, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.4145e-02, 4.6379e-02, 1.6745e-02, 3.2531e-05, 7.6483e-01, 6.3992e-02,
        5.3874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 172, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9424e-03, 1.3826e-02, 1.0248e-02, 6.8002e-09, 9.5556e-01, 2.0070e-11,
        1.3422e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.324

[Epoch: 173, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3641e-02, 1.0209e-02, 6.0688e-01, 6.9413e-09, 1.6872e-02, 3.4499e-03,
        3.4895e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 173, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.8358e-03, 9.7003e-01, 1.3320e-02, 2.4071e-09, 2.7660e-08, 3.3338e-09,
        6.8118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 173, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1570e-01, 3.6560e-02, 5.2771e-01, 9.8363e-09, 8.8272e-07, 1.6494e-02,
        3.5324e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.126

[Epoch: 173, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2787e-02, 4.6353e-02, 1.6353e-02, 2.8816e-05, 7.6847e-01, 6.3324e-02,
        5.2681e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 173, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8120e-03, 1.3329e-02, 9.9864e-03, 5.7968e-09, 9.5631e-01, 1.8471e-11,
        1.3561e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 174, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2994e-02, 9.8714e-03, 6.0241e-01, 8.5034e-09, 1.7067e-02, 3.4791e-03,
        3.5418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 174, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.7736e-03, 9.7004e-01, 1.3465e-02, 2.2672e-09, 2.4710e-08, 3.1780e-09,
        6.7195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 174, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1701e-01, 3.6591e-02, 5.2600e-01, 7.3602e-09, 8.0939e-07, 1.6927e-02,
        3.4788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.129

[Epoch: 174, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3783e-02, 4.7233e-02, 1.6510e-02, 2.4744e-05, 7.6451e-01, 6.4658e-02,
        5.3285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 174, batch: 200/201] total loss per batch: 0.512
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9325e-03, 1.3739e-02, 1.0284e-02, 5.0179e-09, 9.5579e-01, 1.4120e-11,
        1.3252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.329

[Epoch: 175, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3516e-02, 9.9361e-03, 6.1364e-01, 4.6189e-09, 1.6299e-02, 3.2215e-03,
        3.4339e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 175, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0603e-02, 9.7028e-01, 1.2390e-02, 1.1677e-09, 1.7703e-08, 2.2491e-09,
        6.7302e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.962

[Epoch: 175, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2009e-01, 3.7114e-02, 5.2242e-01, 8.6796e-09, 7.7857e-07, 1.6744e-02,
        3.6393e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 175, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2614e-02, 4.6059e-02, 1.6548e-02, 2.6416e-05, 7.6574e-01, 6.4082e-02,
        5.4929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 175, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.4582e-03, 1.2921e-02, 9.5902e-03, 4.8960e-09, 9.5736e-01, 1.8264e-11,
        1.3666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 176, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2679e-02, 1.0116e-02, 5.9411e-01, 8.1242e-09, 1.6791e-02, 3.4984e-03,
        3.6280e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.499

[Epoch: 176, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6233e-03, 9.6835e-01, 1.4913e-02, 1.6443e-09, 2.2976e-08, 2.4833e-09,
        7.1143e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.960

[Epoch: 176, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0791e-01, 3.5485e-02, 5.3675e-01, 3.9819e-09, 5.3982e-07, 1.6736e-02,
        3.1239e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 176, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1518e-02, 4.4182e-02, 1.6417e-02, 1.7825e-05, 7.8158e-01, 6.0450e-02,
        4.5832e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.114

[Epoch: 176, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5512e-03, 1.3263e-02, 1.0604e-02, 4.5650e-09, 9.5717e-01, 1.1711e-11,
        1.2409e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.322

[Epoch: 177, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2713e-02, 9.7113e-03, 6.1606e-01, 3.1933e-09, 1.5857e-02, 3.2960e-03,
        3.4236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.519

[Epoch: 177, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9334e-03, 9.7191e-01, 1.2012e-02, 8.0200e-10, 1.7749e-08, 1.9553e-09,
        6.1483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.954

[Epoch: 177, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2564e-01, 3.7040e-02, 5.1819e-01, 8.1729e-09, 5.9952e-07, 1.5780e-02,
        3.3528e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.121

[Epoch: 177, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0337e-02, 5.8748e-02, 1.8819e-02, 3.7405e-05, 7.4330e-01, 6.2832e-02,
        5.5925e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.098

[Epoch: 177, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.7938e-03, 1.0926e-02, 8.0935e-03, 2.8634e-09, 9.6329e-01, 1.6507e-11,
        1.1896e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.304

[Epoch: 178, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2334e-02, 1.0437e-02, 6.2506e-01, 6.0775e-09, 1.8634e-02, 3.7712e-03,
        3.2976e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.482

[Epoch: 178, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0638e-02, 9.6746e-01, 1.4632e-02, 1.5930e-09, 1.8885e-08, 7.5440e-09,
        7.2692e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.960

[Epoch: 178, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0697e-01, 3.2140e-02, 5.4201e-01, 5.1932e-09, 1.0243e-06, 1.5715e-02,
        3.1718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.132

[Epoch: 178, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8068e-02, 3.2856e-02, 1.7078e-02, 1.9410e-05, 7.6639e-01, 6.3809e-02,
        7.1777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.129

[Epoch: 178, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.7350e-03, 1.7016e-02, 1.5687e-02, 8.0323e-09, 9.4080e-01, 1.7815e-11,
        1.7762e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.352

[Epoch: 179, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4862e-02, 1.0060e-02, 5.6384e-01, 1.4110e-08, 1.4480e-02, 3.3689e-03,
        3.9339e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.504

[Epoch: 179, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6279e-03, 9.7151e-01, 1.1919e-02, 2.0002e-09, 2.7263e-08, 2.7289e-09,
        6.9466e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 179, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1388e-01, 4.6626e-02, 5.1287e-01, 4.1959e-08, 1.5311e-06, 2.2373e-02,
        4.2541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.131

[Epoch: 179, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6906e-02, 4.6879e-02, 1.4322e-02, 2.5004e-05, 7.8923e-01, 6.3937e-02,
        3.8700e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.097

[Epoch: 179, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.2113e-03, 1.3358e-02, 8.3190e-03, 5.6594e-09, 9.5934e-01, 2.0611e-11,
        1.1776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.306

[Epoch: 180, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4355e-02, 9.4966e-03, 6.3317e-01, 6.7246e-09, 1.9058e-02, 4.1122e-03,
        3.1981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.493

[Epoch: 180, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.3542e-02, 9.6457e-01, 1.5152e-02, 3.8504e-09, 4.7624e-08, 1.1065e-08,
        6.7410e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.936

[Epoch: 180, batch: 120/201] total loss per batch: 0.540
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2768e-01, 3.5806e-02, 5.1873e-01, 6.9891e-09, 1.0821e-06, 1.3943e-02,
        3.8388e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.110

[Epoch: 180, batch: 160/201] total loss per batch: 0.535
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.2210e-02, 4.9348e-02, 2.2087e-02, 3.7115e-05, 7.4601e-01, 6.0546e-02,
        5.9763e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.113

[Epoch: 180, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.0350e-03, 1.1659e-02, 9.2775e-03, 5.0542e-09, 9.5997e-01, 5.3633e-11,
        1.2056e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 181, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2445e-02, 1.0888e-02, 5.9549e-01, 2.5091e-08, 1.5188e-02, 3.7312e-03,
        3.6226e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 181, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([6.9042e-03, 9.7416e-01, 1.2398e-02, 1.2923e-09, 3.1806e-08, 3.2837e-09,
        6.5354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.977

[Epoch: 181, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9374e-01, 3.2288e-02, 5.5821e-01, 1.9931e-08, 1.3582e-06, 1.3349e-02,
        2.4107e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 181, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.9096e-02, 4.7762e-02, 1.8523e-02, 3.7114e-05, 7.6309e-01, 6.5957e-02,
        5.5532e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 181, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3587e-03, 1.2790e-02, 9.8502e-03, 1.6058e-08, 9.5785e-01, 9.1013e-11,
        1.2147e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.310

[Epoch: 182, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3536e-02, 9.9800e-03, 6.1818e-01, 1.9018e-08, 1.9719e-02, 3.7427e-03,
        3.3485e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.521

[Epoch: 182, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.9215e-03, 9.6976e-01, 1.4041e-02, 3.3261e-09, 4.9090e-08, 3.9426e-09,
        6.2820e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 182, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.5594e-01, 4.3455e-02, 4.7417e-01, 3.4577e-08, 1.6044e-06, 2.2411e-02,
        4.0258e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.140

[Epoch: 182, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3371e-02, 4.3200e-02, 1.5465e-02, 3.1520e-05, 7.8377e-01, 5.8221e-02,
        4.5943e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.116

[Epoch: 182, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.2873e-03, 1.3833e-02, 1.0322e-02, 6.7832e-09, 9.5498e-01, 2.1789e-11,
        1.2582e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.333

[Epoch: 183, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3786e-02, 8.5164e-03, 5.8489e-01, 1.8884e-08, 1.7274e-02, 4.2853e-03,
        3.7125e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.510

[Epoch: 183, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6549e-03, 9.6880e-01, 1.5221e-02, 2.6189e-09, 5.9688e-08, 1.1314e-08,
        6.3285e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.969

[Epoch: 183, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9747e-01, 2.9757e-02, 5.5538e-01, 1.7093e-08, 1.5846e-06, 1.4627e-02,
        2.7640e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.119

[Epoch: 183, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2781e-02, 4.6015e-02, 1.7828e-02, 4.3366e-05, 7.6309e-01, 6.4093e-02,
        5.6147e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 183, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.4536e-03, 1.5862e-02, 1.0539e-02, 1.2171e-08, 9.5077e-01, 5.5877e-11,
        1.5380e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.330

[Epoch: 184, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3492e-02, 9.5812e-03, 6.3771e-01, 1.0831e-08, 1.4654e-02, 3.5585e-03,
        3.2101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.512

[Epoch: 184, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0344e-02, 9.6994e-01, 1.3582e-02, 2.1974e-09, 3.3913e-08, 4.8488e-09,
        6.1383e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 184, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1253e-01, 4.0863e-02, 5.2375e-01, 1.5366e-08, 1.7373e-06, 1.9054e-02,
        3.7948e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.130

[Epoch: 184, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.1726e-02, 4.9167e-02, 1.8445e-02, 3.4689e-05, 7.5015e-01, 6.7125e-02,
        5.3347e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.115

[Epoch: 184, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.5585e-03, 1.3223e-02, 1.0107e-02, 8.9388e-09, 9.5717e-01, 2.5805e-11,
        1.1937e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.315

[Epoch: 185, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.2405e-02, 1.1294e-02, 5.8327e-01, 1.2868e-08, 1.7326e-02, 3.8539e-03,
        3.7185e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.510

[Epoch: 185, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1294e-02, 9.6665e-01, 1.5078e-02, 1.7620e-09, 4.1481e-08, 6.4061e-09,
        6.9814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.961

[Epoch: 185, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1920e-01, 3.5105e-02, 5.2608e-01, 1.3132e-08, 1.3978e-06, 1.6360e-02,
        3.2558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 185, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.7848e-02, 4.5943e-02, 1.5915e-02, 2.3603e-05, 7.7556e-01, 5.9685e-02,
        5.5029e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.096

[Epoch: 185, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3886e-03, 1.3598e-02, 9.8600e-03, 5.3624e-09, 9.5473e-01, 2.7636e-11,
        1.4424e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 186, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3588e-02, 1.0470e-02, 6.1710e-01, 1.1539e-08, 1.6521e-02, 3.1429e-03,
        3.3917e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.510

[Epoch: 186, batch: 80/201] total loss per batch: 0.537
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.7710e-03, 9.7342e-01, 1.1258e-02, 9.2649e-10, 3.2941e-08, 4.2711e-09,
        7.5559e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.958

[Epoch: 186, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2286e-01, 3.2547e-02, 5.2558e-01, 1.1403e-08, 1.2433e-06, 1.5749e-02,
        3.2565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.123

[Epoch: 186, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.7563e-02, 4.1613e-02, 1.5033e-02, 2.1140e-05, 7.8500e-01, 6.0638e-02,
        5.0134e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.112

[Epoch: 186, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.0161e-03, 1.3178e-02, 9.4640e-03, 7.6155e-09, 9.5840e-01, 3.9304e-11,
        1.2945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.310

[Epoch: 187, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5098e-02, 9.6017e-03, 6.0012e-01, 8.4838e-09, 1.6842e-02, 3.0275e-03,
        3.5531e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.480

[Epoch: 187, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0873e-02, 9.7100e-01, 1.2433e-02, 1.1809e-09, 2.3791e-08, 2.5158e-09,
        5.6903e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.960

[Epoch: 187, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1076e-01, 4.1295e-02, 5.2742e-01, 1.7045e-08, 1.5915e-06, 1.7237e-02,
        3.2878e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 187, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.1189e-02, 5.4637e-02, 1.6944e-02, 3.0892e-05, 7.4874e-01, 6.5339e-02,
        5.3121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 187, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.0695e-03, 1.2013e-02, 8.8522e-03, 1.1294e-08, 9.6169e-01, 1.7355e-11,
        1.1373e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.324

[Epoch: 188, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1321e-02, 8.7450e-03, 6.1110e-01, 1.2114e-08, 1.6468e-02, 4.1231e-03,
        3.4824e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.501

[Epoch: 188, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0061e-02, 9.6878e-01, 1.4926e-02, 1.3852e-09, 4.0250e-08, 7.2852e-09,
        6.2296e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.950

[Epoch: 188, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1606e-01, 3.6309e-02, 5.2881e-01, 1.1374e-08, 9.7084e-07, 1.5491e-02,
        3.3315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 188, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.1030e-02, 4.2876e-02, 1.9263e-02, 2.0550e-05, 7.6210e-01, 6.9238e-02,
        5.5477e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.118

[Epoch: 188, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.2054e-03, 1.5081e-02, 1.1053e-02, 7.5520e-09, 9.5232e-01, 1.9871e-11,
        1.3346e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 189, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.3419e-02, 9.0235e-03, 6.1488e-01, 1.9130e-08, 1.7991e-02, 3.3893e-03,
        3.4129e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.494

[Epoch: 189, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4108e-03, 9.6551e-01, 1.6060e-02, 1.0616e-09, 3.8762e-08, 4.7864e-09,
        9.0238e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.945

[Epoch: 189, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0305e-01, 3.3577e-02, 5.4383e-01, 7.4710e-09, 9.8778e-07, 1.6672e-02,
        2.8761e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.135

[Epoch: 189, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3009e-02, 5.2745e-02, 1.5004e-02, 1.8107e-05, 7.5921e-01, 6.1980e-02,
        5.8035e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 189, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([8.6280e-03, 1.3584e-02, 1.1164e-02, 1.2763e-08, 9.4992e-01, 5.2091e-11,
        1.6708e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.302

[Epoch: 190, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.9342e-02, 1.3612e-02, 6.0228e-01, 1.0782e-08, 2.0804e-02, 3.5270e-03,
        3.4044e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.504

[Epoch: 190, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.6060e-03, 9.7388e-01, 1.2060e-02, 2.0376e-09, 3.0838e-08, 1.3419e-09,
        6.4586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.968

[Epoch: 190, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.4370e-01, 4.6754e-02, 4.8282e-01, 1.1238e-08, 1.0446e-06, 2.1728e-02,
        4.9990e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 190, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.8861e-02, 4.1419e-02, 1.5255e-02, 4.2691e-05, 7.8112e-01, 5.8989e-02,
        5.4319e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.090

[Epoch: 190, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.9952e-03, 1.3679e-02, 9.4687e-03, 1.1438e-08, 9.5646e-01, 2.0192e-11,
        1.4401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.336

[Epoch: 191, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1364e-02, 9.0226e-03, 5.9949e-01, 3.1614e-08, 1.7343e-02, 4.0198e-03,
        3.5876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.519

[Epoch: 191, batch: 80/201] total loss per batch: 0.539
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4649e-02, 9.6781e-01, 1.0612e-02, 2.5993e-09, 4.4672e-08, 1.3456e-08,
        6.9279e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 191, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.8938e-01, 2.9214e-02, 5.6331e-01, 2.6678e-08, 1.8146e-06, 1.5261e-02,
        2.8330e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.139

[Epoch: 191, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6931e-02, 5.0114e-02, 1.7606e-02, 4.2824e-05, 7.6706e-01, 6.3813e-02,
        4.4432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.105

[Epoch: 191, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.7233e-03, 1.2968e-02, 9.3755e-03, 9.8651e-09, 9.5953e-01, 3.0339e-11,
        1.2408e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.332

[Epoch: 192, batch: 40/201] total loss per batch: 0.518
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0345e-02, 9.5882e-03, 6.2586e-01, 7.9910e-09, 1.5621e-02, 2.7618e-03,
        3.3583e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.550

[Epoch: 192, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([7.5344e-03, 9.7116e-01, 1.5316e-02, 2.3964e-09, 5.8949e-08, 6.2564e-09,
        5.9868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.964

[Epoch: 192, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.5169e-01, 3.2576e-02, 4.9922e-01, 5.4030e-09, 6.3103e-07, 1.3499e-02,
        3.0082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.109

[Epoch: 192, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6437e-02, 4.0000e-02, 1.6408e-02, 2.0577e-05, 7.7576e-01, 6.3446e-02,
        4.7929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.117

[Epoch: 192, batch: 200/201] total loss per batch: 0.514
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.7547e-03, 1.3534e-02, 1.1823e-02, 1.3418e-08, 9.5211e-01, 5.2436e-11,
        1.4777e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.321

[Epoch: 193, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1875e-02, 1.0341e-02, 5.6202e-01, 1.8078e-08, 1.9369e-02, 4.0346e-03,
        3.9236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.502

[Epoch: 193, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.4618e-03, 9.7024e-01, 1.3532e-02, 1.6312e-09, 3.2876e-08, 1.8171e-09,
        6.7668e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.965

[Epoch: 193, batch: 120/201] total loss per batch: 0.539
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([3.9930e-01, 4.3382e-02, 5.3325e-01, 2.7407e-08, 2.2346e-06, 2.0005e-02,
        4.0603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.137

[Epoch: 193, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.0326e-02, 4.7604e-02, 1.5119e-02, 3.4104e-05, 7.5497e-01, 6.3727e-02,
        6.8221e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.096

[Epoch: 193, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.0895e-03, 1.3224e-02, 8.6722e-03, 1.0965e-08, 9.6170e-01, 3.0685e-11,
        1.0310e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.312

[Epoch: 194, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4810e-02, 1.2539e-02, 6.4738e-01, 2.2977e-08, 1.5579e-02, 4.1269e-03,
        3.0557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.481

[Epoch: 194, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0727e-02, 9.6947e-01, 1.2507e-02, 3.1744e-09, 4.6644e-08, 7.3506e-09,
        7.2902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.953

[Epoch: 194, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1699e-01, 3.4651e-02, 5.2912e-01, 9.7932e-09, 1.0978e-06, 1.5411e-02,
        3.8331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.125

[Epoch: 194, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([6.0680e-02, 5.1462e-02, 2.1669e-02, 3.1133e-05, 7.4029e-01, 6.9088e-02,
        5.6780e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.115

[Epoch: 194, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.8605e-03, 1.3992e-02, 9.5568e-03, 1.1200e-08, 9.5289e-01, 4.8579e-11,
        1.6697e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.326

[Epoch: 195, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.4814e-02, 1.1252e-02, 5.9122e-01, 1.7710e-08, 1.7554e-02, 4.5487e-03,
        3.6061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.503

[Epoch: 195, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.6142e-03, 9.6899e-01, 1.4287e-02, 1.1636e-09, 3.4984e-08, 3.9818e-09,
        7.1093e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.959

[Epoch: 195, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0517e-01, 3.4817e-02, 5.3971e-01, 1.3036e-08, 1.3226e-06, 1.6852e-02,
        3.4520e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.120

[Epoch: 195, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([4.6982e-02, 4.1377e-02, 1.5831e-02, 2.8836e-05, 7.9398e-01, 6.0903e-02,
        4.0901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.102

[Epoch: 195, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.9179e-03, 1.4169e-02, 1.0800e-02, 9.6394e-09, 9.5304e-01, 5.7778e-11,
        1.5077e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.319

[Epoch: 196, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.0711e-02, 8.9509e-03, 6.0052e-01, 9.8799e-09, 1.5107e-02, 3.6650e-03,
        3.6104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 196, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.1189e-02, 9.6989e-01, 1.3073e-02, 1.5678e-09, 4.2016e-08, 4.7451e-09,
        5.8429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.955

[Epoch: 196, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2588e-01, 3.7377e-02, 5.1366e-01, 1.3927e-08, 1.4159e-06, 1.9425e-02,
        3.6567e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.127

[Epoch: 196, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2290e-02, 4.7051e-02, 1.4751e-02, 1.9533e-05, 7.7254e-01, 5.9272e-02,
        5.4075e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 196, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5236e-03, 1.2645e-02, 9.6241e-03, 7.9577e-09, 9.6031e-01, 2.3659e-11,
        1.0901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.317

[Epoch: 197, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.1424e-02, 8.6897e-03, 6.0920e-01, 1.9904e-08, 1.6623e-02, 3.5233e-03,
        3.5054e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.495

[Epoch: 197, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([9.1014e-03, 9.7379e-01, 1.1082e-02, 1.7249e-09, 2.9003e-08, 5.7029e-09,
        6.0256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.956

[Epoch: 197, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.0905e-01, 3.6528e-02, 5.3701e-01, 1.2877e-08, 1.0707e-06, 1.4122e-02,
        3.2863e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.137

[Epoch: 197, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.6029e-02, 4.9773e-02, 1.6827e-02, 2.9727e-05, 7.4781e-01, 6.8652e-02,
        6.0877e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.106

[Epoch: 197, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([6.5737e-03, 1.3362e-02, 9.3090e-03, 1.0872e-08, 9.5887e-01, 2.7099e-11,
        1.1883e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.313

[Epoch: 198, batch: 40/201] total loss per batch: 0.516
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5556e-02, 1.1276e-02, 6.2023e-01, 1.3324e-08, 1.9013e-02, 3.5740e-03,
        3.3036e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.509

[Epoch: 198, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0362e-02, 9.6705e-01, 1.4388e-02, 1.1732e-09, 3.2340e-08, 2.5643e-09,
        8.1978e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.967

[Epoch: 198, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1285e-01, 3.8471e-02, 5.2479e-01, 1.0126e-08, 1.0844e-06, 1.9928e-02,
        3.9615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.128

[Epoch: 198, batch: 160/201] total loss per batch: 0.533
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3689e-02, 4.3086e-02, 1.7794e-02, 2.4462e-05, 7.6617e-01, 6.5471e-02,
        5.3767e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.108

[Epoch: 198, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.3313e-03, 1.4623e-02, 1.0022e-02, 7.7683e-09, 9.5045e-01, 3.7566e-11,
        1.7574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.323

[Epoch: 199, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([1.5660e-02, 1.1343e-02, 5.9956e-01, 1.1474e-08, 1.8569e-02, 3.7683e-03,
        3.5109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.512

[Epoch: 199, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.0149e-02, 9.6808e-01, 1.4337e-02, 1.4765e-09, 5.0588e-08, 5.5571e-09,
        7.4295e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.958

[Epoch: 199, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.2639e-01, 3.9138e-02, 5.1535e-01, 1.8970e-08, 1.5487e-06, 1.5133e-02,
        3.9895e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.122

[Epoch: 199, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.2019e-02, 4.7560e-02, 1.6847e-02, 2.8180e-05, 7.8235e-01, 5.5859e-02,
        4.5337e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.107

[Epoch: 199, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([7.6893e-03, 1.2573e-02, 1.0766e-02, 1.5858e-08, 9.5605e-01, 3.6025e-11,
        1.2920e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.334

[Epoch: 200, batch: 40/201] total loss per batch: 0.517
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0100, 0.6067, 0.0000, 0.0167, 0.0033, 0.3500])
Policy pred: tensor([9.6409e-03, 8.1453e-03, 5.9931e-01, 1.0461e-08, 1.3338e-02, 3.4215e-03,
        3.6614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.506 -0.516

[Epoch: 200, batch: 80/201] total loss per batch: 0.538
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9700, 0.0133, 0.0000, 0.0000, 0.0000, 0.0067])
Policy pred: tensor([1.4086e-02, 9.6823e-01, 1.2763e-02, 1.4901e-09, 3.4909e-08, 2.9811e-09,
        4.9229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.959 -0.963

[Epoch: 200, batch: 120/201] total loss per batch: 0.538
Policy (actual, predicted): 2 2
Policy data: tensor([0.4167, 0.0367, 0.5267, 0.0000, 0.0000, 0.0167, 0.0033])
Policy pred: tensor([4.1405e-01, 2.9472e-02, 5.4084e-01, 4.3855e-09, 8.6287e-07, 1.2814e-02,
        2.8195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.126 0.111

[Epoch: 200, batch: 160/201] total loss per batch: 0.534
Policy (actual, predicted): 4 4
Policy data: tensor([0.0533, 0.0467, 0.0167, 0.0000, 0.7667, 0.0633, 0.0533])
Policy pred: tensor([5.3377e-02, 5.0151e-02, 1.7515e-02, 1.3925e-05, 7.5653e-01, 6.6486e-02,
        5.5930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.108 0.118

[Epoch: 200, batch: 200/201] total loss per batch: 0.513
Policy (actual, predicted): 4 4
Policy data: tensor([0.0067, 0.0133, 0.0100, 0.0000, 0.9567, 0.0000, 0.0133])
Policy pred: tensor([5.3006e-03, 1.1273e-02, 8.2546e-03, 9.6460e-09, 9.6401e-01, 1.8586e-11,
        1.1159e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.321 0.328

