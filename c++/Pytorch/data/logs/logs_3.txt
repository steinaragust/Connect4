Training set samples: 6413
Batch size: 32
[Epoch: 1, batch: 40/201] total loss per batch: 1.738
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.0484, 0.1335, 0.4118, 0.1130, 0.1710, 0.0538, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.044

[Epoch: 1, batch: 80/201] total loss per batch: 1.610
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0927, 0.0410, 0.1012, 0.0887, 0.4429, 0.1557, 0.0777],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.035

[Epoch: 1, batch: 120/201] total loss per batch: 1.559
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0754, 0.3641, 0.1322, 0.0302, 0.0567, 0.1178, 0.2236],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.019

[Epoch: 1, batch: 160/201] total loss per batch: 1.600
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1680, 0.0344, 0.0894, 0.0561, 0.1457, 0.1666, 0.3397],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.013

[Epoch: 1, batch: 200/201] total loss per batch: 1.509
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0231, 0.0106, 0.2311, 0.2185, 0.1435, 0.2417, 0.1316],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.003

[Epoch: 2, batch: 40/201] total loss per batch: 1.299
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.0808, 0.0481, 0.5785, 0.0503, 0.1541, 0.0332, 0.0550],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 2, batch: 80/201] total loss per batch: 1.245
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0515, 0.0278, 0.0776, 0.0745, 0.6107, 0.1244, 0.0334],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.049

[Epoch: 2, batch: 120/201] total loss per batch: 1.206
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0159, 0.6026, 0.0488, 0.0073, 0.0237, 0.0853, 0.2165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 2, batch: 160/201] total loss per batch: 1.289
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1395, 0.0152, 0.0686, 0.0130, 0.1007, 0.2115, 0.4515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.010

[Epoch: 2, batch: 200/201] total loss per batch: 1.194
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0038, 0.0044, 0.0622, 0.0441, 0.1969, 0.6573, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.005

[Epoch: 3, batch: 40/201] total loss per batch: 1.029
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.1685, 0.0150, 0.5724, 0.0342, 0.1668, 0.0137, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 3, batch: 80/201] total loss per batch: 0.977
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0775, 0.0109, 0.0851, 0.0831, 0.6156, 0.0992, 0.0285],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.057

[Epoch: 3, batch: 120/201] total loss per batch: 0.944
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3491, 0.0173, 0.0024, 0.0106, 0.0553, 0.5602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.027

[Epoch: 3, batch: 160/201] total loss per batch: 1.007
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1036, 0.0051, 0.0231, 0.0098, 0.0417, 0.1019, 0.7148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 3, batch: 200/201] total loss per batch: 0.941
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([3.6396e-04, 2.3113e-03, 7.5664e-03, 1.0069e-02, 6.1233e-02, 9.1375e-01,
        4.7055e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 4, batch: 40/201] total loss per batch: 0.904
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3091, 0.0182, 0.2677, 0.1818, 0.1848, 0.0120, 0.0264],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.014

[Epoch: 4, batch: 80/201] total loss per batch: 0.885
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0412, 0.0104, 0.0943, 0.1123, 0.6521, 0.0759, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.058

[Epoch: 4, batch: 120/201] total loss per batch: 0.864
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0027, 0.3663, 0.0151, 0.0038, 0.0249, 0.0305, 0.5568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.026

[Epoch: 4, batch: 160/201] total loss per batch: 0.919
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1153, 0.0098, 0.1102, 0.0098, 0.0345, 0.5251, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 4, batch: 200/201] total loss per batch: 0.885
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([5.7489e-04, 1.6691e-03, 1.0221e-02, 1.1084e-02, 3.9305e-02, 9.3157e-01,
        5.5729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 5, batch: 40/201] total loss per batch: 0.884
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2148, 0.0129, 0.6035, 0.0212, 0.1307, 0.0039, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 5, batch: 80/201] total loss per batch: 0.873
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0437, 0.0043, 0.0580, 0.0783, 0.7516, 0.0471, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.043

[Epoch: 5, batch: 120/201] total loss per batch: 0.850
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0066, 0.3232, 0.0165, 0.0018, 0.0088, 0.0272, 0.6158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.013

[Epoch: 5, batch: 160/201] total loss per batch: 0.915
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1805, 0.0050, 0.0363, 0.0083, 0.0836, 0.2006, 0.4857],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 5, batch: 200/201] total loss per batch: 0.876
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([5.4188e-04, 2.3528e-03, 8.8849e-03, 3.2088e-03, 3.2849e-02, 9.4646e-01,
        5.7000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 6, batch: 40/201] total loss per batch: 0.870
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3918, 0.0396, 0.2486, 0.1388, 0.1568, 0.0101, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.034

[Epoch: 6, batch: 80/201] total loss per batch: 0.859
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0315, 0.0041, 0.0888, 0.1202, 0.6855, 0.0571, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.038

[Epoch: 6, batch: 120/201] total loss per batch: 0.830
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0040, 0.4825, 0.0122, 0.0043, 0.0283, 0.0548, 0.4139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.027

[Epoch: 6, batch: 160/201] total loss per batch: 0.887
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1698, 0.0070, 0.0406, 0.0095, 0.0499, 0.2827, 0.4406],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 6, batch: 200/201] total loss per batch: 0.846
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([6.8078e-04, 2.4045e-03, 1.2589e-02, 2.3699e-02, 3.8413e-02, 9.1165e-01,
        1.0561e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 7, batch: 40/201] total loss per batch: 0.846
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2086, 0.0103, 0.4840, 0.0569, 0.2056, 0.0111, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.035

[Epoch: 7, batch: 80/201] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0215, 0.0064, 0.0401, 0.1118, 0.7741, 0.0361, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.029

[Epoch: 7, batch: 120/201] total loss per batch: 0.815
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0063, 0.2527, 0.0104, 0.0068, 0.0218, 0.0240, 0.6780],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 7, batch: 160/201] total loss per batch: 0.865
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2226, 0.0042, 0.0559, 0.0127, 0.0944, 0.3558, 0.2544],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 7, batch: 200/201] total loss per batch: 0.833
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([5.8331e-04, 1.8942e-03, 9.2726e-03, 1.0397e-02, 1.8231e-02, 9.5206e-01,
        7.5603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 8, batch: 40/201] total loss per batch: 0.835
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3704, 0.0375, 0.2665, 0.0699, 0.2362, 0.0069, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.035

[Epoch: 8, batch: 80/201] total loss per batch: 0.821
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0177, 0.0039, 0.1093, 0.0818, 0.7272, 0.0449, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.029

[Epoch: 8, batch: 120/201] total loss per batch: 0.800
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0095, 0.3329, 0.0046, 0.0026, 0.0081, 0.0322, 0.6102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.010

[Epoch: 8, batch: 160/201] total loss per batch: 0.855
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.0949, 0.0063, 0.0125, 0.0041, 0.0329, 0.1886, 0.6607],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 8, batch: 200/201] total loss per batch: 0.819
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0011, 0.0027, 0.0170, 0.0080, 0.0474, 0.9104, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 9, batch: 40/201] total loss per batch: 0.828
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2862, 0.0088, 0.4040, 0.0633, 0.2186, 0.0054, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 9, batch: 80/201] total loss per batch: 0.810
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0147, 0.0060, 0.1312, 0.0988, 0.6953, 0.0474, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 9, batch: 120/201] total loss per batch: 0.793
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0060, 0.4890, 0.0052, 0.0056, 0.0206, 0.0170, 0.4566],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 9, batch: 160/201] total loss per batch: 0.841
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1621, 0.0053, 0.0298, 0.0055, 0.0612, 0.4190, 0.3170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 9, batch: 200/201] total loss per batch: 0.815
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([6.1934e-04, 3.1941e-03, 1.7321e-02, 1.3038e-02, 1.0115e-02, 9.5033e-01,
        5.3823e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 10, batch: 40/201] total loss per batch: 0.822
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2419, 0.0313, 0.4769, 0.0762, 0.1485, 0.0122, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 10, batch: 80/201] total loss per batch: 0.808
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0056, 0.0048, 0.0836, 0.0822, 0.7680, 0.0489, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.023

[Epoch: 10, batch: 120/201] total loss per batch: 0.788
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0082, 0.3486, 0.0062, 0.0029, 0.0051, 0.0162, 0.6128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 10, batch: 160/201] total loss per batch: 0.836
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1913, 0.0050, 0.0203, 0.0037, 0.0683, 0.3155, 0.3959],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 10, batch: 200/201] total loss per batch: 0.811
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([8.9101e-04, 2.6852e-03, 1.3824e-02, 7.6289e-03, 3.5845e-02, 9.2417e-01,
        1.4955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 11, batch: 40/201] total loss per batch: 0.817
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3930, 0.0067, 0.2525, 0.0840, 0.2477, 0.0023, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 11, batch: 80/201] total loss per batch: 0.804
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0089, 0.0100, 0.1184, 0.0923, 0.7028, 0.0581, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.026

[Epoch: 11, batch: 120/201] total loss per batch: 0.788
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0029, 0.2465, 0.0056, 0.0025, 0.0106, 0.0067, 0.7252],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.015

[Epoch: 11, batch: 160/201] total loss per batch: 0.835
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.0987, 0.0043, 0.0128, 0.0077, 0.0317, 0.2691, 0.5757],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 11, batch: 200/201] total loss per batch: 0.809
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([7.4787e-04, 3.2194e-03, 2.2691e-02, 1.5161e-02, 1.3132e-02, 9.3936e-01,
        5.6906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 12, batch: 40/201] total loss per batch: 0.812
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3175, 0.0061, 0.4523, 0.0524, 0.1536, 0.0065, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 12, batch: 80/201] total loss per batch: 0.801
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0146, 0.0058, 0.0633, 0.1119, 0.7204, 0.0764, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 12, batch: 120/201] total loss per batch: 0.788
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0059, 0.5206, 0.0081, 0.0033, 0.0058, 0.0198, 0.4366],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 12, batch: 160/201] total loss per batch: 0.834
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2709, 0.0058, 0.0225, 0.0037, 0.0803, 0.2615, 0.3554],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.004

[Epoch: 12, batch: 200/201] total loss per batch: 0.804
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0011, 0.0024, 0.0103, 0.0075, 0.0254, 0.9387, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 13, batch: 40/201] total loss per batch: 0.814
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3420, 0.0090, 0.4213, 0.0352, 0.1784, 0.0051, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 13, batch: 80/201] total loss per batch: 0.803
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0088, 0.0083, 0.0896, 0.0780, 0.7660, 0.0430, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 13, batch: 120/201] total loss per batch: 0.786
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0091, 0.2564, 0.0140, 0.0027, 0.0057, 0.0070, 0.7050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 13, batch: 160/201] total loss per batch: 0.832
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1263, 0.0048, 0.0129, 0.0091, 0.0618, 0.4077, 0.3773],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 13, batch: 200/201] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([5.7151e-04, 4.0995e-03, 9.0937e-03, 6.8973e-03, 1.1215e-02, 9.6410e-01,
        4.0247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 14, batch: 40/201] total loss per batch: 0.813
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3341, 0.0074, 0.2742, 0.1607, 0.2058, 0.0055, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 14, batch: 80/201] total loss per batch: 0.800
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0078, 0.0053, 0.0955, 0.0533, 0.7837, 0.0492, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 14, batch: 120/201] total loss per batch: 0.786
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0153, 0.5487, 0.0080, 0.0056, 0.0204, 0.0118, 0.3901],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 14, batch: 160/201] total loss per batch: 0.831
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1694, 0.0048, 0.0118, 0.0057, 0.0598, 0.2362, 0.5123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.002

[Epoch: 14, batch: 200/201] total loss per batch: 0.804
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0012, 0.0045, 0.0268, 0.0120, 0.0243, 0.9140, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.003

[Epoch: 15, batch: 40/201] total loss per batch: 0.813
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.1772, 0.0147, 0.6533, 0.0143, 0.1253, 0.0098, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 15, batch: 80/201] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0091, 0.0064, 0.0876, 0.1151, 0.7240, 0.0496, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 15, batch: 120/201] total loss per batch: 0.785
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0024, 0.1965, 0.0094, 0.0026, 0.0019, 0.0092, 0.7780],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.013

[Epoch: 15, batch: 160/201] total loss per batch: 0.831
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2907, 0.0112, 0.0540, 0.0064, 0.0758, 0.3378, 0.2242],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.002

[Epoch: 15, batch: 200/201] total loss per batch: 0.804
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([7.4981e-04, 1.9260e-03, 8.2025e-03, 4.1770e-03, 8.6149e-03, 9.7309e-01,
        3.2441e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 16, batch: 40/201] total loss per batch: 0.812
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4930, 0.0078, 0.0789, 0.0674, 0.3368, 0.0034, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 16, batch: 80/201] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0111, 0.0055, 0.1234, 0.0662, 0.7304, 0.0564, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 16, batch: 120/201] total loss per batch: 0.783
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0120, 0.5013, 0.0083, 0.0034, 0.0142, 0.0056, 0.4553],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 16, batch: 160/201] total loss per batch: 0.831
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.0604, 0.0043, 0.0074, 0.0034, 0.0474, 0.2516, 0.6255],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 16, batch: 200/201] total loss per batch: 0.805
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0013, 0.0045, 0.0382, 0.0174, 0.0238, 0.9073, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 17, batch: 40/201] total loss per batch: 0.811
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.1651, 0.0185, 0.6093, 0.1066, 0.0842, 0.0088, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 17, batch: 80/201] total loss per batch: 0.799
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0084, 0.0044, 0.0838, 0.1144, 0.7350, 0.0441, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 17, batch: 120/201] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0031, 0.2895, 0.0060, 0.0043, 0.0084, 0.0065, 0.6822],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.015

[Epoch: 17, batch: 160/201] total loss per batch: 0.827
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1646, 0.0075, 0.0216, 0.0056, 0.0333, 0.3710, 0.3964],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 17, batch: 200/201] total loss per batch: 0.803
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([5.7029e-04, 2.4085e-03, 6.9590e-03, 2.2514e-03, 1.4793e-02, 9.6918e-01,
        3.8390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 18, batch: 40/201] total loss per batch: 0.808
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3376, 0.0133, 0.3831, 0.0617, 0.1848, 0.0074, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 18, batch: 80/201] total loss per batch: 0.796
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0073, 0.0088, 0.1454, 0.0755, 0.7008, 0.0557, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 18, batch: 120/201] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0056, 0.3469, 0.0062, 0.0041, 0.0061, 0.0097, 0.6214],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 18, batch: 160/201] total loss per batch: 0.825
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1207, 0.0060, 0.0137, 0.0072, 0.0552, 0.2602, 0.5369],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 18, batch: 200/201] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([9.2283e-04, 3.8155e-03, 1.9034e-02, 1.4111e-02, 2.5306e-02, 9.2511e-01,
        1.1701e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 19, batch: 40/201] total loss per batch: 0.807
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3453, 0.0129, 0.3625, 0.0582, 0.2102, 0.0053, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 19, batch: 80/201] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0049, 0.0042, 0.0824, 0.0638, 0.7758, 0.0584, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 19, batch: 120/201] total loss per batch: 0.782
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0036, 0.4293, 0.0076, 0.0034, 0.0105, 0.0092, 0.5364],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.024

[Epoch: 19, batch: 160/201] total loss per batch: 0.825
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1945, 0.0083, 0.0192, 0.0087, 0.0883, 0.3639, 0.3171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 19, batch: 200/201] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0020, 0.0025, 0.0101, 0.0050, 0.0107, 0.9642, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 20, batch: 40/201] total loss per batch: 0.808
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2846, 0.0210, 0.3854, 0.0683, 0.2219, 0.0039, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 20, batch: 80/201] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0060, 0.0083, 0.1393, 0.0571, 0.7304, 0.0527, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 20, batch: 120/201] total loss per batch: 0.779
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0099, 0.3598, 0.0044, 0.0082, 0.0138, 0.0067, 0.5972],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.018

[Epoch: 20, batch: 160/201] total loss per batch: 0.825
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1514, 0.0032, 0.0109, 0.0019, 0.0490, 0.2143, 0.5694],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 20, batch: 200/201] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0017, 0.0062, 0.0179, 0.0094, 0.0221, 0.9365, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 21, batch: 40/201] total loss per batch: 0.807
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3559, 0.0101, 0.4278, 0.0886, 0.1061, 0.0048, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 21, batch: 80/201] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0134, 0.0065, 0.0714, 0.1202, 0.7179, 0.0601, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 21, batch: 120/201] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0033, 0.3504, 0.0104, 0.0065, 0.0045, 0.0116, 0.6133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.020

[Epoch: 21, batch: 160/201] total loss per batch: 0.824
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1746, 0.0044, 0.0257, 0.0060, 0.0518, 0.4552, 0.2824],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 21, batch: 200/201] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0022, 0.0021, 0.0116, 0.0047, 0.0057, 0.9698, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.028

[Epoch: 22, batch: 40/201] total loss per batch: 0.807
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2992, 0.0083, 0.3880, 0.0290, 0.2629, 0.0030, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 22, batch: 80/201] total loss per batch: 0.794
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0106, 0.0085, 0.0809, 0.0664, 0.7518, 0.0740, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 22, batch: 120/201] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0081, 0.3725, 0.0119, 0.0076, 0.0153, 0.0071, 0.5775],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.013

[Epoch: 22, batch: 160/201] total loss per batch: 0.823
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1802, 0.0043, 0.0103, 0.0050, 0.0753, 0.2472, 0.4777],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 22, batch: 200/201] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0026, 0.0055, 0.0207, 0.0088, 0.0167, 0.9396, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 23, batch: 40/201] total loss per batch: 0.805
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3169, 0.0116, 0.4040, 0.1314, 0.1226, 0.0057, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 23, batch: 80/201] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0016, 0.0044, 0.1469, 0.0625, 0.7372, 0.0423, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.002

[Epoch: 23, batch: 120/201] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0026, 0.3293, 0.0034, 0.0034, 0.0063, 0.0067, 0.6482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 23, batch: 160/201] total loss per batch: 0.822
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1474, 0.0036, 0.0091, 0.0031, 0.0486, 0.2788, 0.5095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 23, batch: 200/201] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([7.3814e-04, 2.5534e-03, 2.0784e-02, 5.4970e-03, 7.4634e-03, 9.5909e-01,
        3.8695e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 24, batch: 40/201] total loss per batch: 0.804
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4414, 0.0062, 0.3041, 0.0210, 0.2205, 0.0021, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 24, batch: 80/201] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0111, 0.0061, 0.0587, 0.0704, 0.7899, 0.0567, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 24, batch: 120/201] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0068, 0.4086, 0.0082, 0.0051, 0.0075, 0.0059, 0.5580],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.013

[Epoch: 24, batch: 160/201] total loss per batch: 0.822
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2260, 0.0089, 0.0107, 0.0039, 0.0726, 0.4824, 0.1954],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 24, batch: 200/201] total loss per batch: 0.796
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0066, 0.0048, 0.0269, 0.0135, 0.0161, 0.9205, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 25, batch: 40/201] total loss per batch: 0.804
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2041, 0.0144, 0.4455, 0.1135, 0.1967, 0.0071, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 25, batch: 80/201] total loss per batch: 0.792
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.0944, 0.1014, 0.7649, 0.0243, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 25, batch: 120/201] total loss per batch: 0.776
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0032, 0.3208, 0.0055, 0.0039, 0.0103, 0.0071, 0.6492],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 25, batch: 160/201] total loss per batch: 0.821
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.0955, 0.0036, 0.0087, 0.0054, 0.0610, 0.1733, 0.6525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 25, batch: 200/201] total loss per batch: 0.795
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0018, 0.0046, 0.0163, 0.0065, 0.0127, 0.9522, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.027

[Epoch: 26, batch: 40/201] total loss per batch: 0.804
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3164, 0.0064, 0.4059, 0.0271, 0.2343, 0.0017, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 26, batch: 80/201] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0080, 0.0080, 0.1282, 0.1097, 0.6362, 0.1006, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.006

[Epoch: 26, batch: 120/201] total loss per batch: 0.775
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0042, 0.3739, 0.0057, 0.0024, 0.0050, 0.0077, 0.6011],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.032

[Epoch: 26, batch: 160/201] total loss per batch: 0.821
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1911, 0.0038, 0.0223, 0.0038, 0.0602, 0.3630, 0.3557],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 26, batch: 200/201] total loss per batch: 0.795
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0027, 0.0039, 0.0136, 0.0105, 0.0107, 0.9544, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.030

[Epoch: 27, batch: 40/201] total loss per batch: 0.805
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3708, 0.0074, 0.3149, 0.1436, 0.1495, 0.0034, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.008

[Epoch: 27, batch: 80/201] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0079, 0.0061, 0.1072, 0.0971, 0.7449, 0.0320, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 27, batch: 120/201] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0037, 0.3628, 0.0065, 0.0079, 0.0087, 0.0060, 0.6044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.029

[Epoch: 27, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1476, 0.0083, 0.0117, 0.0054, 0.0708, 0.3824, 0.3737],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 27, batch: 200/201] total loss per batch: 0.795
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0029, 0.0036, 0.0382, 0.0095, 0.0124, 0.9242, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 28, batch: 40/201] total loss per batch: 0.804
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2980, 0.0061, 0.4750, 0.0298, 0.1795, 0.0029, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 28, batch: 80/201] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.1189, 0.0563, 0.7308, 0.0775, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 28, batch: 120/201] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0112, 0.4498, 0.0035, 0.0045, 0.0116, 0.0076, 0.5117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 28, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2199, 0.0020, 0.0122, 0.0030, 0.0655, 0.2312, 0.4663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 28, batch: 200/201] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0031, 0.0120, 0.0088, 0.0061, 0.0113, 0.9551, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 29, batch: 40/201] total loss per batch: 0.802
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2944, 0.0098, 0.3992, 0.0419, 0.2469, 0.0036, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 29, batch: 80/201] total loss per batch: 0.791
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0031, 0.0068, 0.0894, 0.0895, 0.7570, 0.0459, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.006

[Epoch: 29, batch: 120/201] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0047, 0.3302, 0.0100, 0.0055, 0.0107, 0.0041, 0.6348],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 29, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1147, 0.0055, 0.0075, 0.0029, 0.0576, 0.3502, 0.4616],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 29, batch: 200/201] total loss per batch: 0.794
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0023, 0.0035, 0.0159, 0.0049, 0.0089, 0.9565, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 30, batch: 40/201] total loss per batch: 0.802
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3182, 0.0103, 0.3774, 0.1129, 0.1698, 0.0029, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 30, batch: 80/201] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0099, 0.0062, 0.0801, 0.0814, 0.7625, 0.0517, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 30, batch: 120/201] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0118, 0.3875, 0.0074, 0.0038, 0.0118, 0.0025, 0.5751],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.020

[Epoch: 30, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1454, 0.0035, 0.0133, 0.0071, 0.0549, 0.4035, 0.3723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 30, batch: 200/201] total loss per batch: 0.793
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0025, 0.0045, 0.0166, 0.0048, 0.0179, 0.9420, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 31, batch: 40/201] total loss per batch: 0.803
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4292, 0.0113, 0.3281, 0.0323, 0.1889, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 31, batch: 80/201] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0025, 0.0032, 0.1160, 0.1303, 0.6962, 0.0445, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 31, batch: 120/201] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0059, 0.3933, 0.0071, 0.0071, 0.0096, 0.0085, 0.5685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.022

[Epoch: 31, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1493, 0.0033, 0.0070, 0.0037, 0.0484, 0.2281, 0.5602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.024

[Epoch: 31, batch: 200/201] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0032, 0.0049, 0.0205, 0.0076, 0.0069, 0.9488, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.038

[Epoch: 32, batch: 40/201] total loss per batch: 0.803
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2426, 0.0053, 0.5036, 0.0767, 0.1573, 0.0044, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 32, batch: 80/201] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0079, 0.0110, 0.0874, 0.1020, 0.7130, 0.0687, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 32, batch: 120/201] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0047, 0.2830, 0.0087, 0.0039, 0.0108, 0.0026, 0.6863],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 32, batch: 160/201] total loss per batch: 0.820
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2233, 0.0040, 0.0173, 0.0049, 0.0782, 0.3765, 0.2959],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 32, batch: 200/201] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0031, 0.0022, 0.0376, 0.0103, 0.0247, 0.9129, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 33, batch: 40/201] total loss per batch: 0.803
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3867, 0.0127, 0.2970, 0.0637, 0.2258, 0.0046, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 33, batch: 80/201] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0042, 0.0045, 0.0800, 0.0929, 0.7687, 0.0462, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.003

[Epoch: 33, batch: 120/201] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0036, 0.4121, 0.0046, 0.0072, 0.0084, 0.0125, 0.5515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.015

[Epoch: 33, batch: 160/201] total loss per batch: 0.822
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1402, 0.0070, 0.0101, 0.0055, 0.0566, 0.2696, 0.5110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 33, batch: 200/201] total loss per batch: 0.793
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0040, 0.0035, 0.0066, 0.0042, 0.0051, 0.9686, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 34, batch: 40/201] total loss per batch: 0.803
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2012, 0.0079, 0.5432, 0.0836, 0.1567, 0.0046, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 34, batch: 80/201] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0051, 0.0066, 0.1588, 0.0687, 0.6932, 0.0595, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.006

[Epoch: 34, batch: 120/201] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.4029, 0.0053, 0.0039, 0.0054, 0.0059, 0.5716],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.022

[Epoch: 34, batch: 160/201] total loss per batch: 0.819
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2089, 0.0057, 0.0143, 0.0037, 0.0811, 0.3797, 0.3067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 34, batch: 200/201] total loss per batch: 0.793
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0092, 0.0041, 0.0329, 0.0092, 0.0130, 0.9240, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 35, batch: 40/201] total loss per batch: 0.802
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4847, 0.0177, 0.2412, 0.0528, 0.1922, 0.0037, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 35, batch: 80/201] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0029, 0.0046, 0.0940, 0.0859, 0.7498, 0.0568, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 35, batch: 120/201] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0054, 0.3030, 0.0086, 0.0051, 0.0063, 0.0039, 0.6677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.024

[Epoch: 35, batch: 160/201] total loss per batch: 0.816
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1732, 0.0053, 0.0106, 0.0025, 0.0538, 0.2202, 0.5344],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.035

[Epoch: 35, batch: 200/201] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0044, 0.0034, 0.0130, 0.0076, 0.0068, 0.9553, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.030

[Epoch: 36, batch: 40/201] total loss per batch: 0.800
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.1850, 0.0076, 0.4256, 0.0627, 0.2969, 0.0062, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 36, batch: 80/201] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0023, 0.0052, 0.0622, 0.0858, 0.7898, 0.0494, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 36, batch: 120/201] total loss per batch: 0.772
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0037, 0.4699, 0.0061, 0.0041, 0.0074, 0.0077, 0.5011],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 36, batch: 160/201] total loss per batch: 0.815
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1865, 0.0034, 0.0100, 0.0059, 0.0659, 0.3815, 0.3469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 36, batch: 200/201] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0044, 0.0042, 0.0123, 0.0056, 0.0165, 0.9509, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 37, batch: 40/201] total loss per batch: 0.799
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4483, 0.0061, 0.3508, 0.0687, 0.1149, 0.0046, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 37, batch: 80/201] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.1582, 0.0897, 0.6738, 0.0561, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 37, batch: 120/201] total loss per batch: 0.770
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0048, 0.3402, 0.0060, 0.0078, 0.0091, 0.0061, 0.6261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.024

[Epoch: 37, batch: 160/201] total loss per batch: 0.815
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1415, 0.0043, 0.0098, 0.0054, 0.0608, 0.2709, 0.5073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 37, batch: 200/201] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0028, 0.0083, 0.0183, 0.0108, 0.0086, 0.9478, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.029

[Epoch: 38, batch: 40/201] total loss per batch: 0.798
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2434, 0.0079, 0.4224, 0.0626, 0.2501, 0.0049, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 38, batch: 80/201] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.1419, 0.0768, 0.7007, 0.0663, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 38, batch: 120/201] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3589, 0.0075, 0.0054, 0.0045, 0.0059, 0.6129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.025

[Epoch: 38, batch: 160/201] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2102, 0.0050, 0.0143, 0.0056, 0.1036, 0.2805, 0.3808],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 38, batch: 200/201] total loss per batch: 0.788
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0049, 0.0075, 0.0251, 0.0065, 0.0119, 0.9375, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 39, batch: 40/201] total loss per batch: 0.797
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3485, 0.0063, 0.3700, 0.0722, 0.1934, 0.0039, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 39, batch: 80/201] total loss per batch: 0.785
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0045, 0.0059, 0.0893, 0.0956, 0.7480, 0.0511, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 39, batch: 120/201] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0057, 0.3486, 0.0049, 0.0043, 0.0061, 0.0040, 0.6264],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 39, batch: 160/201] total loss per batch: 0.813
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1410, 0.0074, 0.0076, 0.0047, 0.0489, 0.3382, 0.4522],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.032

[Epoch: 39, batch: 200/201] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0051, 0.0057, 0.0174, 0.0055, 0.0078, 0.9552, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 40, batch: 40/201] total loss per batch: 0.796
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3222, 0.0074, 0.3771, 0.0651, 0.2113, 0.0059, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 40, batch: 80/201] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0043, 0.0084, 0.0927, 0.0591, 0.7874, 0.0426, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 40, batch: 120/201] total loss per batch: 0.769
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0061, 0.4274, 0.0048, 0.0080, 0.0057, 0.0063, 0.5419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.028

[Epoch: 40, batch: 160/201] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1992, 0.0035, 0.0105, 0.0042, 0.0599, 0.3248, 0.3978],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 40, batch: 200/201] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0026, 0.0061, 0.0189, 0.0063, 0.0090, 0.9508, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 41, batch: 40/201] total loss per batch: 0.795
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3097, 0.0068, 0.4358, 0.0605, 0.1793, 0.0046, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 41, batch: 80/201] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0056, 0.0047, 0.1347, 0.0979, 0.6916, 0.0591, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 41, batch: 120/201] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0057, 0.3014, 0.0082, 0.0044, 0.0047, 0.0043, 0.6713],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.026

[Epoch: 41, batch: 160/201] total loss per batch: 0.810
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1487, 0.0073, 0.0110, 0.0060, 0.1011, 0.3022, 0.4237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 41, batch: 200/201] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0041, 0.0033, 0.0142, 0.0080, 0.0079, 0.9578, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.040

[Epoch: 42, batch: 40/201] total loss per batch: 0.794
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3383, 0.0061, 0.3284, 0.1172, 0.2036, 0.0029, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 42, batch: 80/201] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0049, 0.0040, 0.0729, 0.1009, 0.7523, 0.0604, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 42, batch: 120/201] total loss per batch: 0.767
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0056, 0.4241, 0.0051, 0.0046, 0.0060, 0.0047, 0.5499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.020

[Epoch: 42, batch: 160/201] total loss per batch: 0.810
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1878, 0.0027, 0.0093, 0.0045, 0.0560, 0.3507, 0.3890],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 42, batch: 200/201] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0047, 0.0038, 0.0305, 0.0051, 0.0062, 0.9450, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 43, batch: 40/201] total loss per batch: 0.794
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3310, 0.0135, 0.4382, 0.0217, 0.1845, 0.0039, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 43, batch: 80/201] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.0921, 0.0892, 0.7509, 0.0497, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.006

[Epoch: 43, batch: 120/201] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0091, 0.3691, 0.0065, 0.0047, 0.0081, 0.0060, 0.5966],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.022

[Epoch: 43, batch: 160/201] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1560, 0.0036, 0.0084, 0.0044, 0.0442, 0.3331, 0.4504],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 43, batch: 200/201] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0029, 0.0056, 0.0106, 0.0063, 0.0096, 0.9587, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.027

[Epoch: 44, batch: 40/201] total loss per batch: 0.796
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2834, 0.0072, 0.3951, 0.0955, 0.2078, 0.0068, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 44, batch: 80/201] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0045, 0.0079, 0.1458, 0.0888, 0.6905, 0.0580, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 44, batch: 120/201] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0021, 0.2721, 0.0066, 0.0064, 0.0043, 0.0044, 0.7041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.017

[Epoch: 44, batch: 160/201] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2114, 0.0031, 0.0094, 0.0035, 0.0638, 0.2975, 0.4113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 44, batch: 200/201] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0021, 0.0025, 0.0123, 0.0059, 0.0073, 0.9663, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 45, batch: 40/201] total loss per batch: 0.878
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4117, 0.0042, 0.3816, 0.0308, 0.1579, 0.0067, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.043

[Epoch: 45, batch: 80/201] total loss per batch: 0.910
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0109, 0.0043, 0.0569, 0.0450, 0.8208, 0.0524, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.033

[Epoch: 45, batch: 120/201] total loss per batch: 0.889
Policy (actual, predicted): 6 1
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0033, 0.6381, 0.0080, 0.0031, 0.0406, 0.0075, 0.2995],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.006

[Epoch: 45, batch: 160/201] total loss per batch: 0.924
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1022, 0.0114, 0.0328, 0.0154, 0.0981, 0.6001, 0.1400],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.012

[Epoch: 45, batch: 200/201] total loss per batch: 0.881
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([7.7138e-04, 3.8108e-03, 2.9388e-02, 6.6956e-03, 9.2599e-02, 8.5294e-01,
        1.3794e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 46, batch: 40/201] total loss per batch: 0.899
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2052, 0.0146, 0.5096, 0.0940, 0.1571, 0.0018, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 46, batch: 80/201] total loss per batch: 0.856
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0064, 0.0853, 0.0745, 0.7031, 0.1168, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 46, batch: 120/201] total loss per batch: 0.828
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0165, 0.1541, 0.0073, 0.0070, 0.0102, 0.0585, 0.7465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 46, batch: 160/201] total loss per batch: 0.878
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1165, 0.0037, 0.0223, 0.0032, 0.0459, 0.0683, 0.7401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 46, batch: 200/201] total loss per batch: 0.841
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([2.1747e-03, 9.4782e-04, 1.7968e-02, 1.2205e-02, 7.9630e-03, 9.5457e-01,
        4.1725e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.054

[Epoch: 47, batch: 40/201] total loss per batch: 0.843
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4364, 0.0037, 0.2274, 0.1360, 0.1879, 0.0033, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 47, batch: 80/201] total loss per batch: 0.807
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0088, 0.0045, 0.0878, 0.1060, 0.7234, 0.0554, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 47, batch: 120/201] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0203, 0.3817, 0.0042, 0.0105, 0.0108, 0.0188, 0.5538],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 47, batch: 160/201] total loss per batch: 0.829
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1725, 0.0073, 0.0238, 0.0179, 0.0795, 0.3469, 0.3520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 47, batch: 200/201] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0033, 0.0042, 0.0555, 0.0144, 0.0258, 0.8865, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.032

[Epoch: 48, batch: 40/201] total loss per batch: 0.804
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2279, 0.0069, 0.4164, 0.0523, 0.2857, 0.0029, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 48, batch: 80/201] total loss per batch: 0.785
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0036, 0.0051, 0.1012, 0.0882, 0.7474, 0.0457, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 48, batch: 120/201] total loss per batch: 0.766
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0172, 0.3837, 0.0047, 0.0066, 0.0099, 0.0158, 0.5622],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 48, batch: 160/201] total loss per batch: 0.811
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1780, 0.0070, 0.0179, 0.0091, 0.0552, 0.3089, 0.4240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 48, batch: 200/201] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0019, 0.0028, 0.0131, 0.0101, 0.0138, 0.9518, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.037

[Epoch: 49, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3767, 0.0058, 0.3739, 0.0665, 0.1690, 0.0028, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.021

[Epoch: 49, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0039, 0.0047, 0.1014, 0.0725, 0.7521, 0.0569, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 49, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0119, 0.3260, 0.0053, 0.0049, 0.0067, 0.0116, 0.6336],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 49, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1657, 0.0056, 0.0134, 0.0079, 0.0680, 0.3121, 0.4273],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 49, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0021, 0.0028, 0.0146, 0.0077, 0.0141, 0.9523, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 50, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3045, 0.0051, 0.3796, 0.0927, 0.2102, 0.0030, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 50, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0042, 0.0049, 0.0984, 0.0968, 0.7341, 0.0549, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 50, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0105, 0.3815, 0.0050, 0.0058, 0.0071, 0.0101, 0.5800],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 50, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1886, 0.0056, 0.0137, 0.0059, 0.0599, 0.3589, 0.3673],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 50, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0021, 0.0039, 0.0192, 0.0067, 0.0138, 0.9487, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 51, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3598, 0.0050, 0.3876, 0.0527, 0.1862, 0.0032, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 51, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0040, 0.0052, 0.0960, 0.0855, 0.7447, 0.0574, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 51, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0086, 0.3554, 0.0049, 0.0051, 0.0062, 0.0102, 0.6097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 51, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1586, 0.0052, 0.0098, 0.0055, 0.0650, 0.3075, 0.4482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 51, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0022, 0.0033, 0.0192, 0.0058, 0.0105, 0.9533, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 52, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3197, 0.0047, 0.3863, 0.0751, 0.2054, 0.0033, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 52, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0050, 0.1052, 0.0860, 0.7362, 0.0574, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 52, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0072, 0.3671, 0.0047, 0.0051, 0.0056, 0.0084, 0.6019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 52, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1765, 0.0054, 0.0103, 0.0056, 0.0660, 0.3313, 0.4050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 52, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0022, 0.0035, 0.0198, 0.0061, 0.0095, 0.9541, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 53, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3271, 0.0045, 0.3942, 0.0738, 0.1915, 0.0034, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 53, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0050, 0.1022, 0.0834, 0.7437, 0.0557, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 53, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0065, 0.3766, 0.0049, 0.0052, 0.0058, 0.0096, 0.5914],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 53, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1691, 0.0055, 0.0090, 0.0052, 0.0688, 0.3145, 0.4279],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 53, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0025, 0.0035, 0.0192, 0.0057, 0.0083, 0.9555, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 54, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3330, 0.0048, 0.3906, 0.0653, 0.1969, 0.0035, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 54, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0037, 0.0049, 0.1001, 0.0833, 0.7468, 0.0547, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 54, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0062, 0.3603, 0.0047, 0.0048, 0.0050, 0.0076, 0.6115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 54, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1755, 0.0051, 0.0087, 0.0049, 0.0597, 0.3351, 0.4109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 54, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0024, 0.0038, 0.0199, 0.0054, 0.0084, 0.9553, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 55, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3264, 0.0049, 0.3796, 0.0843, 0.1959, 0.0037, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 55, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0050, 0.1103, 0.0868, 0.7336, 0.0545, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 55, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0055, 0.3921, 0.0045, 0.0053, 0.0056, 0.0080, 0.5789],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 55, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1670, 0.0052, 0.0078, 0.0051, 0.0709, 0.3052, 0.4388],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 55, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0029, 0.0042, 0.0209, 0.0056, 0.0067, 0.9546, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 56, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3368, 0.0048, 0.4085, 0.0540, 0.1864, 0.0038, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 56, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0039, 0.0050, 0.0952, 0.0859, 0.7514, 0.0522, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 56, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0056, 0.3518, 0.0049, 0.0044, 0.0046, 0.0076, 0.6212],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 56, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1843, 0.0051, 0.0082, 0.0044, 0.0596, 0.3535, 0.3849],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 56, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0031, 0.0045, 0.0205, 0.0058, 0.0081, 0.9524, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 57, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3107, 0.0054, 0.3655, 0.0992, 0.2095, 0.0039, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 57, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0055, 0.1221, 0.0877, 0.7198, 0.0555, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 57, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0048, 0.3851, 0.0048, 0.0049, 0.0048, 0.0073, 0.5883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 57, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1506, 0.0044, 0.0062, 0.0046, 0.0713, 0.2863, 0.4767],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 57, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0035, 0.0051, 0.0186, 0.0049, 0.0054, 0.9579, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 58, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3711, 0.0040, 0.4130, 0.0361, 0.1674, 0.0034, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 58, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.0887, 0.0850, 0.7582, 0.0512, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 58, batch: 120/201] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0057, 0.3738, 0.0046, 0.0051, 0.0075, 0.0068, 0.5965],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 58, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2073, 0.0049, 0.0093, 0.0041, 0.0607, 0.3671, 0.3466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 58, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0039, 0.0050, 0.0227, 0.0077, 0.0106, 0.9438, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.034

[Epoch: 59, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2642, 0.0066, 0.3542, 0.1124, 0.2490, 0.0061, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 59, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0042, 0.0048, 0.1060, 0.0732, 0.7505, 0.0565, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 59, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0051, 0.3784, 0.0065, 0.0051, 0.0033, 0.0072, 0.5943],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 59, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1554, 0.0024, 0.0069, 0.0048, 0.0620, 0.2901, 0.4783],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 59, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0037, 0.0048, 0.0173, 0.0056, 0.0068, 0.9556, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 60, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4016, 0.0036, 0.3924, 0.0264, 0.1693, 0.0028, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 60, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0057, 0.1207, 0.0890, 0.7261, 0.0445, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 60, batch: 120/201] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0040, 0.3296, 0.0059, 0.0057, 0.0065, 0.0042, 0.6439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 60, batch: 160/201] total loss per batch: 0.810
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1967, 0.0044, 0.0079, 0.0030, 0.0527, 0.3522, 0.3831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 60, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0052, 0.0069, 0.0299, 0.0063, 0.0098, 0.9345, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 61, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2624, 0.0069, 0.4063, 0.1266, 0.1863, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 61, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0044, 0.0040, 0.0801, 0.1001, 0.7541, 0.0517, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 61, batch: 120/201] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0085, 0.4268, 0.0045, 0.0061, 0.0035, 0.0077, 0.5428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 61, batch: 160/201] total loss per batch: 0.809
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1708, 0.0053, 0.0075, 0.0068, 0.0757, 0.3064, 0.4275],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 61, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0040, 0.0044, 0.0131, 0.0049, 0.0062, 0.9628, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.037

[Epoch: 62, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3971, 0.0046, 0.3590, 0.0234, 0.2068, 0.0039, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 62, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0086, 0.0053, 0.1278, 0.0747, 0.7243, 0.0510, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 62, batch: 120/201] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0037, 0.2983, 0.0069, 0.0034, 0.0049, 0.0050, 0.6777],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 62, batch: 160/201] total loss per batch: 0.809
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1751, 0.0028, 0.0057, 0.0028, 0.0577, 0.3233, 0.4326],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.007

[Epoch: 62, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0034, 0.0044, 0.0207, 0.0066, 0.0064, 0.9529, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 63, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2644, 0.0043, 0.4331, 0.0815, 0.2089, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 63, batch: 80/201] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0047, 0.0038, 0.1002, 0.0696, 0.7737, 0.0431, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 63, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0068, 0.4027, 0.0074, 0.0116, 0.0061, 0.0073, 0.5582],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 63, batch: 160/201] total loss per batch: 0.808
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1722, 0.0043, 0.0077, 0.0074, 0.0723, 0.2880, 0.4481],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 63, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0045, 0.0051, 0.0143, 0.0047, 0.0069, 0.9597, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.030

[Epoch: 64, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4141, 0.0078, 0.3140, 0.0842, 0.1695, 0.0045, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 64, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0060, 0.0044, 0.1110, 0.0894, 0.7330, 0.0505, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 64, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0049, 0.3699, 0.0085, 0.0055, 0.0071, 0.0058, 0.5983],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 64, batch: 160/201] total loss per batch: 0.808
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1909, 0.0038, 0.0082, 0.0047, 0.0625, 0.3890, 0.3409],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 64, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0039, 0.0074, 0.0192, 0.0065, 0.0088, 0.9474, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.036

[Epoch: 65, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2874, 0.0065, 0.4272, 0.0388, 0.2230, 0.0087, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 65, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0068, 0.0893, 0.0761, 0.7552, 0.0602, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 65, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0070, 0.3840, 0.0082, 0.0058, 0.0055, 0.0056, 0.5839],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 65, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1370, 0.0063, 0.0055, 0.0059, 0.0612, 0.2694, 0.5147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 65, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0063, 0.0048, 0.0201, 0.0044, 0.0109, 0.9482, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 66, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3415, 0.0053, 0.3459, 0.1143, 0.1861, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 66, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.1174, 0.0806, 0.7287, 0.0585, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 66, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0036, 0.3566, 0.0054, 0.0061, 0.0075, 0.0051, 0.6158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 66, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1876, 0.0059, 0.0094, 0.0063, 0.0920, 0.3330, 0.3657],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 66, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0036, 0.0088, 0.0204, 0.0111, 0.0067, 0.9456, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 67, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3240, 0.0055, 0.4074, 0.0527, 0.1980, 0.0069, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 67, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.1122, 0.0954, 0.7244, 0.0539, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 67, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0066, 0.3699, 0.0071, 0.0052, 0.0042, 0.0067, 0.6003],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 67, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1869, 0.0053, 0.0078, 0.0062, 0.0402, 0.3416, 0.4120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 67, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.0147, 0.0058, 0.0085, 0.9542, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 68, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2812, 0.0038, 0.4584, 0.0519, 0.1970, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 68, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0059, 0.0053, 0.0836, 0.0686, 0.7725, 0.0569, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.005

[Epoch: 68, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0033, 0.3896, 0.0064, 0.0053, 0.0060, 0.0047, 0.5848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 68, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1373, 0.0044, 0.0070, 0.0043, 0.0699, 0.2686, 0.5085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 68, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0043, 0.0050, 0.0235, 0.0040, 0.0059, 0.9542, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 69, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4334, 0.0052, 0.3025, 0.0709, 0.1790, 0.0034, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 69, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0066, 0.0039, 0.1197, 0.1009, 0.7203, 0.0453, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.002

[Epoch: 69, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0049, 0.3443, 0.0058, 0.0055, 0.0081, 0.0050, 0.6263],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 69, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2405, 0.0042, 0.0078, 0.0042, 0.0586, 0.3543, 0.3305],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 69, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0031, 0.0039, 0.0091, 0.0059, 0.0064, 0.9674, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.033

[Epoch: 70, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2419, 0.0069, 0.4441, 0.0941, 0.2008, 0.0056, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 70, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0985, 0.0881, 0.7474, 0.0510, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.004

[Epoch: 70, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0055, 0.4118, 0.0063, 0.0057, 0.0048, 0.0073, 0.5585],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 70, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1395, 0.0051, 0.0084, 0.0047, 0.0682, 0.2743, 0.4999],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 70, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0062, 0.0046, 0.0232, 0.0034, 0.0051, 0.9528, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.028

[Epoch: 71, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3357, 0.0057, 0.3878, 0.0544, 0.2061, 0.0062, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 71, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0036, 0.0050, 0.1206, 0.0591, 0.7339, 0.0723, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 71, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0059, 0.3572, 0.0077, 0.0067, 0.0088, 0.0050, 0.6088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 71, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1815, 0.0048, 0.0111, 0.0072, 0.0576, 0.3518, 0.3859],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 71, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0045, 0.0046, 0.0248, 0.0060, 0.0060, 0.9496, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 72, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3340, 0.0088, 0.3716, 0.0791, 0.1953, 0.0073, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 72, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0057, 0.0051, 0.0884, 0.1129, 0.7432, 0.0394, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 72, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0049, 0.3531, 0.0057, 0.0056, 0.0033, 0.0053, 0.6220],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.009

[Epoch: 72, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1775, 0.0053, 0.0092, 0.0077, 0.0813, 0.3481, 0.3709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 72, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0052, 0.0061, 0.0168, 0.0071, 0.0100, 0.9462, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.024

[Epoch: 73, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3039, 0.0051, 0.3961, 0.0599, 0.2198, 0.0072, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 73, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0081, 0.0066, 0.1141, 0.0917, 0.7186, 0.0542, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 73, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3454, 0.0040, 0.0055, 0.0065, 0.0035, 0.6300],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 73, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1547, 0.0043, 0.0076, 0.0050, 0.0441, 0.2583, 0.5261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 73, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0056, 0.0048, 0.0183, 0.0070, 0.0099, 0.9464, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 74, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3180, 0.0057, 0.4210, 0.0764, 0.1679, 0.0060, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 74, batch: 80/201] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0035, 0.0067, 0.1242, 0.0543, 0.7396, 0.0663, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.003

[Epoch: 74, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0076, 0.4220, 0.0072, 0.0069, 0.0051, 0.0070, 0.5441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.011

[Epoch: 74, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1834, 0.0044, 0.0073, 0.0030, 0.0726, 0.3794, 0.3499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 74, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.0239, 0.0046, 0.0069, 0.9502, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 75, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3460, 0.0064, 0.3960, 0.0526, 0.1908, 0.0045, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 75, batch: 80/201] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0952, 0.1120, 0.7120, 0.0638, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 75, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0030, 0.3451, 0.0040, 0.0043, 0.0053, 0.0026, 0.6357],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.008

[Epoch: 75, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1592, 0.0046, 0.0082, 0.0055, 0.0695, 0.2888, 0.4642],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 75, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0036, 0.0050, 0.0139, 0.0078, 0.0078, 0.9576, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.027

[Epoch: 76, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2990, 0.0026, 0.3756, 0.0933, 0.2215, 0.0038, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 76, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0025, 0.0043, 0.0978, 0.0529, 0.7858, 0.0529, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 76, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0062, 0.3986, 0.0056, 0.0058, 0.0049, 0.0058, 0.5730],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 76, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1916, 0.0062, 0.0078, 0.0056, 0.0575, 0.3101, 0.4211],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 76, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0034, 0.0044, 0.0245, 0.0052, 0.0050, 0.9543, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 77, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3676, 0.0044, 0.4560, 0.0356, 0.1288, 0.0043, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 77, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0068, 0.0052, 0.0959, 0.0973, 0.7464, 0.0432, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 77, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0048, 0.3561, 0.0046, 0.0071, 0.0038, 0.0033, 0.6202],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 77, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1762, 0.0038, 0.0076, 0.0050, 0.0651, 0.3858, 0.3565],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 77, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0075, 0.0088, 0.0225, 0.0072, 0.0088, 0.9384, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 78, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2798, 0.0065, 0.3272, 0.1299, 0.2478, 0.0052, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 78, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0059, 0.0057, 0.1204, 0.0911, 0.7168, 0.0549, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 78, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0051, 0.3397, 0.0068, 0.0057, 0.0038, 0.0047, 0.6342],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 78, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1756, 0.0050, 0.0076, 0.0054, 0.0654, 0.2599, 0.4811],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 78, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0043, 0.0044, 0.0218, 0.0046, 0.0085, 0.9505, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 79, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3664, 0.0051, 0.3659, 0.0368, 0.2176, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 79, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0038, 0.0056, 0.1123, 0.0755, 0.7321, 0.0661, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 79, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0070, 0.4110, 0.0049, 0.0061, 0.0095, 0.0052, 0.5564],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 79, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1645, 0.0065, 0.0085, 0.0060, 0.0633, 0.3204, 0.4308],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 79, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0058, 0.0050, 0.0204, 0.0045, 0.0059, 0.9536, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.032

[Epoch: 80, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3248, 0.0049, 0.3962, 0.0592, 0.2021, 0.0061, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 80, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0064, 0.0067, 0.0832, 0.0847, 0.7670, 0.0478, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 80, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0051, 0.3811, 0.0044, 0.0042, 0.0022, 0.0066, 0.5966],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.012

[Epoch: 80, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1793, 0.0050, 0.0070, 0.0041, 0.0697, 0.3799, 0.3550],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 80, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0038, 0.0040, 0.0179, 0.0071, 0.0077, 0.9546, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 81, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3143, 0.0070, 0.4120, 0.0573, 0.2003, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 81, batch: 80/201] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0101, 0.0052, 0.1379, 0.0673, 0.7278, 0.0454, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 81, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0058, 0.3209, 0.0058, 0.0068, 0.0073, 0.0034, 0.6501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 81, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1592, 0.0033, 0.0065, 0.0034, 0.0459, 0.2797, 0.5020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 81, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0061, 0.0067, 0.0197, 0.0046, 0.0082, 0.9492, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.029

[Epoch: 82, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3427, 0.0068, 0.3644, 0.0829, 0.1912, 0.0045, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.011

[Epoch: 82, batch: 80/201] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0032, 0.0064, 0.1004, 0.0971, 0.7275, 0.0591, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 82, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0040, 0.4089, 0.0076, 0.0048, 0.0052, 0.0048, 0.5647],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 82, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1805, 0.0041, 0.0059, 0.0050, 0.0677, 0.3714, 0.3655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 82, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0060, 0.0041, 0.0177, 0.0061, 0.0060, 0.9540, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 83, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3230, 0.0046, 0.4324, 0.0664, 0.1662, 0.0038, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 83, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0034, 0.0039, 0.1108, 0.1033, 0.7218, 0.0526, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 83, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0032, 0.3339, 0.0039, 0.0045, 0.0042, 0.0042, 0.6462],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 83, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1703, 0.0030, 0.0051, 0.0023, 0.0567, 0.3059, 0.4568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 83, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0060, 0.0063, 0.0187, 0.0049, 0.0091, 0.9499, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.030

[Epoch: 84, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3439, 0.0087, 0.3395, 0.0545, 0.2418, 0.0048, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 84, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0042, 0.0047, 0.0805, 0.0571, 0.7837, 0.0650, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 84, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0083, 0.4037, 0.0069, 0.0078, 0.0079, 0.0089, 0.5565],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 84, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1818, 0.0069, 0.0080, 0.0041, 0.0772, 0.3248, 0.3972],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 84, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0043, 0.0035, 0.0255, 0.0077, 0.0048, 0.9476, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 85, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3035, 0.0041, 0.4314, 0.1012, 0.1511, 0.0042, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 85, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0063, 0.0047, 0.1130, 0.1013, 0.7164, 0.0541, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 85, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0043, 0.3732, 0.0061, 0.0053, 0.0046, 0.0049, 0.6016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 85, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1718, 0.0069, 0.0082, 0.0059, 0.0807, 0.3244, 0.4023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 85, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0075, 0.0041, 0.0196, 0.0035, 0.0074, 0.9522, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 86, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3092, 0.0075, 0.3816, 0.0576, 0.2316, 0.0072, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 86, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0046, 0.0071, 0.0955, 0.0800, 0.7405, 0.0635, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.028

[Epoch: 86, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0085, 0.3358, 0.0075, 0.0067, 0.0067, 0.0081, 0.6267],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.014

[Epoch: 86, batch: 160/201] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1623, 0.0042, 0.0059, 0.0035, 0.0611, 0.2932, 0.4698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 86, batch: 200/201] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0031, 0.0057, 0.0123, 0.0076, 0.0065, 0.9613, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.031

[Epoch: 87, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3937, 0.0024, 0.3392, 0.0610, 0.1948, 0.0040, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 87, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0080, 0.0058, 0.1157, 0.0941, 0.7298, 0.0418, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 87, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0056, 0.3878, 0.0049, 0.0039, 0.0071, 0.0037, 0.5870],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 87, batch: 160/201] total loss per batch: 0.818
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2200, 0.0053, 0.0093, 0.0040, 0.0522, 0.3664, 0.3429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 87, batch: 200/201] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0091, 0.0056, 0.0285, 0.0078, 0.0094, 0.9231, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 88, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2967, 0.0047, 0.4108, 0.1033, 0.1756, 0.0049, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 88, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0079, 0.0042, 0.0948, 0.0838, 0.7534, 0.0517, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.002

[Epoch: 88, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0042, 0.4000, 0.0051, 0.0042, 0.0044, 0.0051, 0.5770],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 88, batch: 160/201] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1360, 0.0063, 0.0072, 0.0052, 0.0654, 0.2814, 0.4985],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 88, batch: 200/201] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0018, 0.0031, 0.0238, 0.0039, 0.0052, 0.9608, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 89, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3055, 0.0044, 0.4014, 0.0644, 0.2126, 0.0053, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 89, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0051, 0.0039, 0.1242, 0.0666, 0.7550, 0.0405, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.007

[Epoch: 89, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0033, 0.3632, 0.0069, 0.0061, 0.0063, 0.0041, 0.6102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 89, batch: 160/201] total loss per batch: 0.809
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1919, 0.0041, 0.0060, 0.0037, 0.0510, 0.3758, 0.3674],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 89, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0036, 0.0058, 0.0158, 0.0069, 0.0061, 0.9580, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.028

[Epoch: 90, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3262, 0.0056, 0.3797, 0.0497, 0.2264, 0.0050, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 90, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0053, 0.0038, 0.0762, 0.0785, 0.7667, 0.0655, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 90, batch: 120/201] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0043, 0.3782, 0.0039, 0.0061, 0.0086, 0.0036, 0.5952],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 90, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1539, 0.0072, 0.0074, 0.0051, 0.0705, 0.2778, 0.4781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 90, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0029, 0.0041, 0.0275, 0.0058, 0.0065, 0.9488, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 91, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3416, 0.0037, 0.4051, 0.0798, 0.1622, 0.0035, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 91, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0108, 0.0063, 0.1417, 0.0971, 0.6791, 0.0589, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 91, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0041, 0.3511, 0.0059, 0.0055, 0.0068, 0.0047, 0.6220],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 91, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1972, 0.0063, 0.0076, 0.0062, 0.0606, 0.3250, 0.3971],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 91, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0042, 0.0041, 0.0150, 0.0068, 0.0049, 0.9613, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 92, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3356, 0.0053, 0.3777, 0.0613, 0.2103, 0.0045, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 92, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0029, 0.0047, 0.0912, 0.0664, 0.7686, 0.0610, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 92, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0059, 0.3924, 0.0051, 0.0044, 0.0057, 0.0072, 0.5793],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 92, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1606, 0.0062, 0.0072, 0.0054, 0.0715, 0.3286, 0.4205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 92, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0047, 0.0040, 0.0196, 0.0072, 0.0094, 0.9507, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 93, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3277, 0.0054, 0.3871, 0.0827, 0.1856, 0.0053, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 93, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.0879, 0.0859, 0.7632, 0.0476, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 93, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0031, 0.3980, 0.0045, 0.0044, 0.0044, 0.0053, 0.5804],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 93, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1701, 0.0056, 0.0066, 0.0060, 0.0648, 0.3174, 0.4295],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 93, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.0164, 0.0062, 0.0067, 0.9562, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 94, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3236, 0.0044, 0.3880, 0.0602, 0.2139, 0.0042, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 94, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0051, 0.0045, 0.1315, 0.0896, 0.7064, 0.0563, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 94, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3097, 0.0062, 0.0055, 0.0045, 0.0058, 0.6633],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 94, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1817, 0.0039, 0.0070, 0.0040, 0.0644, 0.3241, 0.4148],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 94, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0049, 0.0035, 0.0327, 0.0052, 0.0055, 0.9445, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 95, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3137, 0.0069, 0.3944, 0.0882, 0.1854, 0.0050, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 95, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0071, 0.0059, 0.1100, 0.0985, 0.7059, 0.0660, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 95, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0062, 0.4340, 0.0047, 0.0047, 0.0061, 0.0044, 0.5400],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 95, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1701, 0.0038, 0.0050, 0.0043, 0.0622, 0.3191, 0.4354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 95, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0043, 0.0043, 0.0095, 0.0038, 0.0055, 0.9671, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 96, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3767, 0.0040, 0.3512, 0.0530, 0.2064, 0.0038, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 96, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0034, 0.0063, 0.0756, 0.0927, 0.7706, 0.0453, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 96, batch: 120/201] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0081, 0.3325, 0.0066, 0.0059, 0.0051, 0.0081, 0.6337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.005

[Epoch: 96, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1930, 0.0066, 0.0079, 0.0043, 0.0747, 0.3203, 0.3933],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.007

[Epoch: 96, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0060, 0.0027, 0.0137, 0.0043, 0.0040, 0.9662, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 97, batch: 40/201] total loss per batch: 0.790
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2571, 0.0044, 0.4497, 0.0769, 0.2022, 0.0044, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.013

[Epoch: 97, batch: 80/201] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0037, 0.0048, 0.0965, 0.0693, 0.7669, 0.0533, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.010

[Epoch: 97, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0038, 0.3770, 0.0051, 0.0053, 0.0052, 0.0043, 0.5993],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.010

[Epoch: 97, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1650, 0.0039, 0.0063, 0.0062, 0.0655, 0.3110, 0.4421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 97, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0065, 0.0050, 0.0222, 0.0056, 0.0052, 0.9504, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.026

[Epoch: 98, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.4360, 0.0053, 0.3249, 0.0605, 0.1659, 0.0032, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 98, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0077, 0.0047, 0.1205, 0.0557, 0.7561, 0.0514, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 98, batch: 120/201] total loss per batch: 0.763
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3871, 0.0028, 0.0049, 0.0038, 0.0056, 0.5908],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 98, batch: 160/201] total loss per batch: 0.806
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1349, 0.0032, 0.0066, 0.0051, 0.0311, 0.3872, 0.4320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 98, batch: 200/201] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0070, 0.0081, 0.0207, 0.0073, 0.0084, 0.9446, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 99, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2635, 0.0070, 0.4772, 0.0793, 0.1551, 0.0106, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.037

[Epoch: 99, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0041, 0.0058, 0.1072, 0.1025, 0.7244, 0.0489, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 99, batch: 120/201] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0088, 0.3885, 0.0098, 0.0057, 0.0077, 0.0066, 0.5731],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.015

[Epoch: 99, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1402, 0.0061, 0.0067, 0.0070, 0.1103, 0.2171, 0.5127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 99, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0058, 0.0058, 0.0246, 0.0091, 0.0079, 0.9386, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 100, batch: 40/201] total loss per batch: 0.793
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3269, 0.0092, 0.3767, 0.0581, 0.2165, 0.0070, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 100, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0052, 0.0060, 0.1214, 0.0964, 0.7255, 0.0405, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.008

[Epoch: 100, batch: 120/201] total loss per batch: 0.764
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0036, 0.3441, 0.0056, 0.0050, 0.0050, 0.0081, 0.6286],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.003

[Epoch: 100, batch: 160/201] total loss per batch: 0.807
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.2320, 0.0058, 0.0098, 0.0037, 0.0630, 0.3609, 0.3248],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 100, batch: 200/201] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0030, 0.0028, 0.0124, 0.0067, 0.0063, 0.9640, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 101, batch: 40/201] total loss per batch: 0.791
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3545, 0.0080, 0.3771, 0.0578, 0.1953, 0.0038, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 101, batch: 80/201] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0076, 0.0064, 0.1000, 0.0996, 0.7151, 0.0663, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 101, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0067, 0.4246, 0.0046, 0.0042, 0.0049, 0.0056, 0.5493],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 101, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1889, 0.0062, 0.0060, 0.0059, 0.0692, 0.3244, 0.3994],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 101, batch: 200/201] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0033, 0.0059, 0.0241, 0.0047, 0.0063, 0.9496, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 102, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3029, 0.0061, 0.3981, 0.0871, 0.1981, 0.0038, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 102, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0081, 0.0045, 0.0903, 0.0733, 0.7569, 0.0617, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 102, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0045, 0.3176, 0.0054, 0.0041, 0.0038, 0.0049, 0.6598],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 102, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1696, 0.0047, 0.0063, 0.0053, 0.0621, 0.3116, 0.4405],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 102, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0036, 0.0047, 0.0166, 0.0054, 0.0063, 0.9591, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 103, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 0
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3701, 0.0069, 0.3624, 0.0541, 0.1982, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 103, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.1023, 0.0895, 0.7385, 0.0552, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.013

[Epoch: 103, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0055, 0.4206, 0.0054, 0.0040, 0.0056, 0.0053, 0.5535],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 103, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1764, 0.0046, 0.0057, 0.0051, 0.0654, 0.3175, 0.4254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 103, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0036, 0.0038, 0.0177, 0.0042, 0.0043, 0.9618, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 104, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3184, 0.0065, 0.4020, 0.0842, 0.1796, 0.0044, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 104, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0065, 0.0051, 0.1001, 0.0885, 0.7296, 0.0645, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 104, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0053, 0.3524, 0.0068, 0.0045, 0.0067, 0.0059, 0.6183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 104, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1729, 0.0052, 0.0070, 0.0067, 0.0686, 0.3304, 0.4092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 104, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0256, 0.0057, 0.0066, 0.9469, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 105, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3227, 0.0064, 0.4013, 0.0566, 0.2047, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 105, batch: 80/201] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0052, 0.0056, 0.1069, 0.0836, 0.7476, 0.0459, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.014

[Epoch: 105, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.3777, 0.0063, 0.0048, 0.0065, 0.0050, 0.5947],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 105, batch: 160/201] total loss per batch: 0.800
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1747, 0.0053, 0.0064, 0.0055, 0.0647, 0.3067, 0.4366],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 105, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0162, 0.0060, 0.0060, 0.9566, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 106, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3392, 0.0071, 0.3716, 0.0796, 0.1915, 0.0054, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 106, batch: 80/201] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0056, 0.1032, 0.0838, 0.7348, 0.0617, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.018

[Epoch: 106, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0054, 0.3798, 0.0050, 0.0053, 0.0057, 0.0047, 0.5941],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 106, batch: 160/201] total loss per batch: 0.800
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1815, 0.0049, 0.0066, 0.0059, 0.0696, 0.3359, 0.3956],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 106, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0188, 0.0047, 0.0058, 0.9564, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 107, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3232, 0.0058, 0.3992, 0.0656, 0.1970, 0.0042, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 107, batch: 80/201] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.1078, 0.0817, 0.7481, 0.0490, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 107, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0051, 0.3676, 0.0052, 0.0051, 0.0051, 0.0053, 0.6067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 107, batch: 160/201] total loss per batch: 0.800
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1692, 0.0044, 0.0058, 0.0049, 0.0631, 0.3088, 0.4438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 107, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0042, 0.0047, 0.0221, 0.0049, 0.0050, 0.9543, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 108, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3373, 0.0052, 0.3727, 0.0747, 0.2001, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 108, batch: 80/201] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0045, 0.0048, 0.1041, 0.0771, 0.7455, 0.0586, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 108, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0048, 0.4005, 0.0051, 0.0047, 0.0047, 0.0047, 0.5755],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 108, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1861, 0.0043, 0.0057, 0.0051, 0.0661, 0.3248, 0.4079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 108, batch: 200/201] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0046, 0.0041, 0.0174, 0.0042, 0.0052, 0.9600, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 109, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3304, 0.0054, 0.4022, 0.0665, 0.1863, 0.0046, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 109, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0998, 0.0854, 0.7489, 0.0516, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.012

[Epoch: 109, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0048, 0.3242, 0.0052, 0.0052, 0.0054, 0.0057, 0.6496],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.004

[Epoch: 109, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1622, 0.0044, 0.0053, 0.0048, 0.0599, 0.3542, 0.4091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 109, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0234, 0.0048, 0.0058, 0.9503, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 110, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3254, 0.0064, 0.3960, 0.0715, 0.1921, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 110, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0044, 0.0045, 0.1137, 0.0861, 0.7390, 0.0478, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 110, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0050, 0.4411, 0.0048, 0.0045, 0.0062, 0.0057, 0.5326],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 110, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1702, 0.0059, 0.0064, 0.0062, 0.0749, 0.2871, 0.4491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 110, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0054, 0.0069, 0.0176, 0.0080, 0.0062, 0.9502, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 111, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3124, 0.0048, 0.4257, 0.0702, 0.1779, 0.0050, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 111, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0058, 0.0941, 0.0864, 0.7504, 0.0527, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 111, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0049, 0.2945, 0.0078, 0.0072, 0.0059, 0.0054, 0.6743],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 111, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1902, 0.0039, 0.0055, 0.0050, 0.0553, 0.3503, 0.3899],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 111, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0056, 0.0052, 0.0205, 0.0036, 0.0060, 0.9544, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 112, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3406, 0.0054, 0.3703, 0.0759, 0.1973, 0.0049, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 112, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0056, 0.0076, 0.0964, 0.0881, 0.7363, 0.0612, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 112, batch: 120/201] total loss per batch: 0.760
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0063, 0.4521, 0.0051, 0.0056, 0.0047, 0.0089, 0.5174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 112, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1654, 0.0065, 0.0066, 0.0055, 0.0719, 0.3131, 0.4310],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 112, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0055, 0.0073, 0.0194, 0.0083, 0.0094, 0.9398, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 113, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3474, 0.0053, 0.3655, 0.0536, 0.2172, 0.0044, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 113, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0088, 0.0059, 0.1237, 0.0903, 0.7040, 0.0621, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.026

[Epoch: 113, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0062, 0.3529, 0.0061, 0.0088, 0.0052, 0.0056, 0.6152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.016

[Epoch: 113, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1907, 0.0038, 0.0057, 0.0050, 0.0718, 0.3103, 0.4126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 113, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0041, 0.0037, 0.0169, 0.0036, 0.0058, 0.9627, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 114, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2799, 0.0047, 0.4412, 0.0739, 0.1909, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 114, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0069, 0.0072, 0.0988, 0.0895, 0.7431, 0.0497, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.022

[Epoch: 114, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0039, 0.3801, 0.0042, 0.0036, 0.0035, 0.0039, 0.6008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.006

[Epoch: 114, batch: 160/201] total loss per batch: 0.805
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1643, 0.0040, 0.0051, 0.0032, 0.0569, 0.3243, 0.4421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 114, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.0187, 0.0030, 0.0050, 0.9577, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 115, batch: 40/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3598, 0.0041, 0.3816, 0.0706, 0.1761, 0.0040, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 115, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0036, 0.0063, 0.1002, 0.0650, 0.7627, 0.0565, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 115, batch: 120/201] total loss per batch: 0.761
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0035, 0.3381, 0.0045, 0.0046, 0.0037, 0.0034, 0.6422],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 115, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1890, 0.0040, 0.0056, 0.0041, 0.0529, 0.3127, 0.4319],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 115, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0028, 0.0050, 0.0281, 0.0043, 0.0057, 0.9491, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 116, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3348, 0.0064, 0.3649, 0.0702, 0.2129, 0.0047, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 116, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0060, 0.0045, 0.1121, 0.0937, 0.7386, 0.0412, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.017

[Epoch: 116, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0029, 0.4555, 0.0041, 0.0056, 0.0074, 0.0038, 0.5207],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.007

[Epoch: 116, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1671, 0.0050, 0.0062, 0.0049, 0.0752, 0.3315, 0.4101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 116, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0061, 0.0071, 0.0134, 0.0058, 0.0066, 0.9563, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 117, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3449, 0.0059, 0.3940, 0.0639, 0.1816, 0.0053, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 117, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0051, 0.0046, 0.0978, 0.0925, 0.7344, 0.0611, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 117, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0091, 0.2325, 0.0059, 0.0072, 0.0084, 0.0048, 0.7321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.006

[Epoch: 117, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1864, 0.0066, 0.0067, 0.0092, 0.0714, 0.3283, 0.3913],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 117, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0053, 0.0036, 0.0162, 0.0051, 0.0068, 0.9585, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 118, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2948, 0.0073, 0.3818, 0.0911, 0.2136, 0.0057, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 118, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.0950, 0.0770, 0.7670, 0.0444, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.024

[Epoch: 118, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0078, 0.4556, 0.0062, 0.0080, 0.0070, 0.0076, 0.5078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 118, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1676, 0.0049, 0.0061, 0.0048, 0.0615, 0.3003, 0.4548],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 118, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0043, 0.0038, 0.0235, 0.0054, 0.0081, 0.9491, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 119, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3541, 0.0046, 0.3912, 0.0512, 0.1885, 0.0048, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 119, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0040, 0.0044, 0.1269, 0.0747, 0.7147, 0.0705, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 119, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0037, 0.3881, 0.0064, 0.0051, 0.0043, 0.0053, 0.5872],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 119, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1736, 0.0020, 0.0044, 0.0026, 0.0552, 0.3459, 0.4163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 119, batch: 200/201] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0042, 0.0083, 0.0156, 0.0043, 0.0053, 0.9582, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 120, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3359, 0.0079, 0.3875, 0.0841, 0.1741, 0.0049, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 120, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0041, 0.0038, 0.0957, 0.0783, 0.7652, 0.0488, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.021

[Epoch: 120, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0051, 0.3619, 0.0045, 0.0048, 0.0044, 0.0052, 0.6140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 120, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1805, 0.0038, 0.0049, 0.0036, 0.0643, 0.3188, 0.4240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 120, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0037, 0.0046, 0.0167, 0.0042, 0.0043, 0.9627, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 121, batch: 40/201] total loss per batch: 0.786
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3104, 0.0060, 0.3737, 0.0655, 0.2337, 0.0045, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 121, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0075, 0.0058, 0.0959, 0.0995, 0.7409, 0.0457, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.009

[Epoch: 121, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0055, 0.3939, 0.0051, 0.0056, 0.0058, 0.0064, 0.5777],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.007

[Epoch: 121, batch: 160/201] total loss per batch: 0.801
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1761, 0.0054, 0.0056, 0.0051, 0.0692, 0.3290, 0.4096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 121, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0086, 0.0059, 0.0327, 0.0079, 0.0075, 0.9307, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 122, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3375, 0.0061, 0.3971, 0.0768, 0.1735, 0.0050, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 122, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0063, 0.0068, 0.1063, 0.0848, 0.7164, 0.0728, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.016

[Epoch: 122, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0069, 0.3549, 0.0057, 0.0069, 0.0050, 0.0048, 0.6158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 122, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1737, 0.0055, 0.0064, 0.0063, 0.0679, 0.3081, 0.4321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 122, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0044, 0.0059, 0.0104, 0.0064, 0.0061, 0.9615, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 123, batch: 40/201] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3247, 0.0038, 0.4078, 0.0553, 0.2002, 0.0039, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 123, batch: 80/201] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.1184, 0.0763, 0.7342, 0.0541, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.019

[Epoch: 123, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0046, 0.3849, 0.0049, 0.0050, 0.0039, 0.0049, 0.5919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 123, batch: 160/201] total loss per batch: 0.802
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1806, 0.0040, 0.0052, 0.0076, 0.0672, 0.3387, 0.3966],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 123, batch: 200/201] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0060, 0.0048, 0.0190, 0.0053, 0.0089, 0.9485, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 124, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3343, 0.0048, 0.3753, 0.0855, 0.1894, 0.0045, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 124, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.1093, 0.0834, 0.7381, 0.0534, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.020

[Epoch: 124, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0033, 0.3416, 0.0037, 0.0030, 0.0029, 0.0030, 0.6424],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.008

[Epoch: 124, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1627, 0.0034, 0.0043, 0.0029, 0.0509, 0.2815, 0.4944],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 124, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0032, 0.0058, 0.0164, 0.0044, 0.0042, 0.9621, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 125, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3156, 0.0036, 0.4010, 0.0589, 0.2132, 0.0035, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 125, batch: 80/201] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0069, 0.0057, 0.1188, 0.0948, 0.7187, 0.0496, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.015

[Epoch: 125, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0037, 0.4042, 0.0050, 0.0035, 0.0040, 0.0040, 0.5755],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 125, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 5
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1982, 0.0030, 0.0068, 0.0034, 0.0671, 0.4200, 0.3014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 125, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0027, 0.0033, 0.0249, 0.0040, 0.0060, 0.9520, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 126, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3594, 0.0055, 0.3920, 0.0558, 0.1792, 0.0042, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 126, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0874, 0.0986, 0.7358, 0.0625, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.025

[Epoch: 126, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0057, 0.3648, 0.0049, 0.0063, 0.0081, 0.0058, 0.6045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.003

[Epoch: 126, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1541, 0.0069, 0.0065, 0.0090, 0.0641, 0.2698, 0.4896],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 126, batch: 200/201] total loss per batch: 0.779
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0048, 0.0039, 0.0230, 0.0053, 0.0042, 0.9565, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 127, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.2822, 0.0054, 0.3924, 0.1075, 0.2012, 0.0062, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.008

[Epoch: 127, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0037, 0.0061, 0.0930, 0.0803, 0.7645, 0.0474, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.011

[Epoch: 127, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0075, 0.4079, 0.0057, 0.0059, 0.0076, 0.0065, 0.5589],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.018

[Epoch: 127, batch: 160/201] total loss per batch: 0.804
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1814, 0.0057, 0.0080, 0.0058, 0.0626, 0.3412, 0.3953],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 127, batch: 200/201] total loss per batch: 0.780
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0200, 0.0050, 0.0050, 0.9550, 0.0050])
Policy pred: tensor([0.0071, 0.0073, 0.0144, 0.0079, 0.0077, 0.9472, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 128, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 2 2
Policy data: tensor([0.3300, 0.0050, 0.3900, 0.0700, 0.1950, 0.0050, 0.0050])
Policy pred: tensor([0.3614, 0.0037, 0.3636, 0.0558, 0.2053, 0.0044, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 128, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.1050, 0.0850, 0.7400, 0.0550, 0.0050])
Policy pred: tensor([0.0065, 0.0071, 0.0819, 0.0753, 0.7668, 0.0566, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.027

[Epoch: 128, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.3750, 0.0050, 0.0050, 0.0050, 0.0050, 0.6000])
Policy pred: tensor([0.0052, 0.3583, 0.0063, 0.0049, 0.0050, 0.0052, 0.6151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 128, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 6 6
Policy data: tensor([0.1750, 0.0050, 0.0050, 0.0050, 0.0650, 0.3250, 0.4200])
Policy pred: tensor([0.1717, 0.0035, 0.0062, 0.0055, 0.0615, 0.3161, 0.4355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

