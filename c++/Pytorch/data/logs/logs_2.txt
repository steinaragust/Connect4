Training set samples: 6598
Batch size: 32
[Epoch: 1, batch: 41/207] total loss per batch: 1.628
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.2187e-01, 1.4465e-01, 1.2267e-01, 5.1851e-05, 1.0174e-04, 9.8890e-02,
        5.1176e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.271

[Epoch: 1, batch: 82/207] total loss per batch: 1.507
Policy (actual, predicted): 5 2
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1615e-04, 2.2306e-01, 3.2558e-01, 3.0806e-05, 2.9592e-01, 1.5517e-01,
        1.1866e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.268

[Epoch: 1, batch: 123/207] total loss per batch: 1.414
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0202, 0.7638, 0.0263, 0.0551, 0.0427, 0.0709, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.023

[Epoch: 1, batch: 164/207] total loss per batch: 1.501
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([1.4898e-01, 3.0779e-01, 4.1964e-01, 3.4267e-04, 3.8148e-02, 1.5219e-04,
        8.4940e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.011

[Epoch: 1, batch: 205/207] total loss per batch: 1.430
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1911e-01, 3.9475e-05, 1.6310e-01, 1.1666e-01, 8.7285e-04, 4.5065e-05,
        1.7294e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.080

[Epoch: 2, batch: 41/207] total loss per batch: 1.216
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([6.5489e-02, 4.4956e-02, 5.1039e-02, 1.7601e-05, 2.0559e-05, 4.5755e-02,
        7.9272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.354

[Epoch: 2, batch: 82/207] total loss per batch: 1.140
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.7149e-05, 3.5816e-02, 3.2887e-01, 1.4180e-05, 1.9133e-01, 4.4392e-01,
        2.1927e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.598

[Epoch: 2, batch: 123/207] total loss per batch: 1.046
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0109, 0.8938, 0.0076, 0.0236, 0.0182, 0.0410, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 2, batch: 164/207] total loss per batch: 1.138
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.7822e-02, 9.4973e-02, 7.8290e-01, 5.0712e-05, 2.7680e-02, 4.4939e-05,
        1.6534e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.181

[Epoch: 2, batch: 205/207] total loss per batch: 1.020
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.6081e-01, 2.5178e-07, 3.1244e-02, 7.9099e-03, 2.7699e-05, 2.4517e-06,
        5.1328e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.020

[Epoch: 3, batch: 41/207] total loss per batch: 0.945
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7013e-02, 5.6431e-03, 2.4957e-02, 2.7145e-06, 1.0774e-06, 2.3978e-02,
        9.1841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.413

[Epoch: 3, batch: 82/207] total loss per batch: 0.908
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.7216e-06, 2.6440e-03, 1.0106e-01, 2.5619e-06, 2.8059e-01, 6.1569e-01,
        8.5187e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.704

[Epoch: 3, batch: 123/207] total loss per batch: 0.856
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0289, 0.7152, 0.0090, 0.0232, 0.0235, 0.1964, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 3, batch: 164/207] total loss per batch: 0.916
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([8.7909e-02, 9.6098e-02, 7.7128e-01, 8.1838e-05, 3.9554e-02, 4.0123e-05,
        5.0422e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.168

[Epoch: 3, batch: 205/207] total loss per batch: 0.838
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3900e-01, 2.2387e-07, 5.8953e-02, 2.0347e-03, 1.4674e-05, 1.2791e-06,
        1.0369e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.156

[Epoch: 4, batch: 41/207] total loss per batch: 0.835
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4054e-02, 9.6230e-03, 4.3843e-02, 8.4698e-07, 4.8295e-07, 3.2917e-02,
        8.8956e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.351

[Epoch: 4, batch: 82/207] total loss per batch: 0.848
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.5338e-06, 1.2242e-03, 3.4347e-02, 5.4671e-07, 7.9702e-02, 8.8472e-01,
        5.3168e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.770

[Epoch: 4, batch: 123/207] total loss per batch: 0.811
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0148, 0.5728, 0.0111, 0.0238, 0.0213, 0.3529, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 4, batch: 164/207] total loss per batch: 0.848
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.4328e-02, 8.4051e-02, 8.4263e-01, 9.3516e-05, 2.4601e-02, 4.2402e-05,
        4.2531e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 4, batch: 205/207] total loss per batch: 0.801
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.7863e-01, 2.2365e-08, 2.0508e-02, 8.4235e-04, 1.5227e-05, 3.8692e-07,
        1.3431e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.089

[Epoch: 5, batch: 41/207] total loss per batch: 0.814
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4050e-02, 2.6884e-03, 9.0816e-03, 8.3382e-07, 4.1752e-07, 4.3321e-03,
        9.5985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.383

[Epoch: 5, batch: 82/207] total loss per batch: 0.833
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7052e-06, 2.9948e-03, 2.0023e-02, 1.2815e-06, 2.8795e-02, 9.4818e-01,
        4.8626e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.811

[Epoch: 5, batch: 123/207] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0174, 0.5407, 0.0174, 0.0187, 0.0185, 0.3831, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 5, batch: 164/207] total loss per batch: 0.826
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7324e-02, 2.7628e-02, 8.8376e-01, 1.1547e-04, 2.5999e-02, 3.7060e-05,
        5.1338e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.010

[Epoch: 5, batch: 205/207] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9287e-01, 5.4826e-07, 9.5429e-02, 1.1607e-02, 8.4057e-05, 2.0483e-06,
        5.4090e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.151

[Epoch: 6, batch: 41/207] total loss per batch: 0.794
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.7810e-02, 4.8041e-03, 2.0394e-02, 3.4281e-06, 6.4701e-07, 8.6756e-03,
        9.1831e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.367

[Epoch: 6, batch: 82/207] total loss per batch: 0.811
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.6370e-06, 3.3038e-03, 1.7481e-02, 2.5278e-06, 4.6922e-02, 9.3228e-01,
        8.6544e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.802

[Epoch: 6, batch: 123/207] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0084, 0.9051, 0.0103, 0.0094, 0.0095, 0.0551, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 6, batch: 164/207] total loss per batch: 0.815
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.8497e-02, 5.8708e-02, 8.3579e-01, 5.1260e-05, 2.2071e-02, 8.3055e-05,
        4.7977e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.098

[Epoch: 6, batch: 205/207] total loss per batch: 0.751
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9182e-01, 7.7012e-08, 1.0328e-01, 4.8713e-03, 1.8730e-05, 1.9837e-06,
        3.9309e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.136

[Epoch: 7, batch: 41/207] total loss per batch: 0.774
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3555e-02, 9.7110e-03, 3.1528e-02, 1.2826e-06, 4.4102e-06, 1.4413e-02,
        9.1079e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.386

[Epoch: 7, batch: 82/207] total loss per batch: 0.795
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.9049e-06, 5.6039e-03, 2.5489e-02, 1.1437e-06, 2.3060e-02, 9.4584e-01,
        2.6944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.783

[Epoch: 7, batch: 123/207] total loss per batch: 0.757
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.4483, 0.0113, 0.0141, 0.0136, 0.5005, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 7, batch: 164/207] total loss per batch: 0.800
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.3541e-02, 1.5845e-02, 9.2448e-01, 5.8932e-05, 1.1670e-02, 2.9617e-05,
        4.3788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.275

[Epoch: 7, batch: 205/207] total loss per batch: 0.731
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0924e-01, 6.0089e-07, 1.7816e-01, 1.2562e-02, 2.3509e-05, 2.6288e-06,
        1.1224e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.216

[Epoch: 8, batch: 41/207] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([6.3286e-02, 3.1834e-03, 1.7936e-02, 2.6728e-06, 1.4544e-06, 6.9204e-03,
        9.0867e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.343

[Epoch: 8, batch: 82/207] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.4381e-06, 1.3932e-03, 2.5787e-02, 3.8477e-06, 2.3898e-02, 9.4890e-01,
        5.6766e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.794

[Epoch: 8, batch: 123/207] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0163, 0.7488, 0.0080, 0.0089, 0.0173, 0.1974, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.000

[Epoch: 8, batch: 164/207] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.5436e-02, 2.4874e-02, 8.9265e-01, 1.0396e-04, 9.1808e-03, 6.2388e-05,
        7.6924e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.080

[Epoch: 8, batch: 205/207] total loss per batch: 0.730
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4783e-01, 2.2966e-07, 1.4628e-01, 5.8716e-03, 1.1049e-05, 1.0551e-06,
        3.0534e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.252

[Epoch: 9, batch: 41/207] total loss per batch: 0.759
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4084e-02, 4.9807e-03, 6.0112e-02, 2.6606e-06, 2.4935e-06, 8.5458e-03,
        8.9227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.367

[Epoch: 9, batch: 82/207] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.1408e-06, 5.8961e-03, 1.8278e-02, 2.5795e-06, 1.8208e-02, 9.5761e-01,
        1.6242e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.781

[Epoch: 9, batch: 123/207] total loss per batch: 0.744
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0151, 0.4552, 0.0208, 0.0191, 0.0220, 0.4652, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 9, batch: 164/207] total loss per batch: 0.787
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9519e-02, 2.1186e-02, 8.9415e-01, 8.8828e-05, 1.1933e-02, 2.6033e-05,
        3.0940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.212

[Epoch: 9, batch: 205/207] total loss per batch: 0.731
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6070e-01, 3.7769e-07, 1.2914e-01, 1.0095e-02, 4.0042e-05, 2.0922e-06,
        2.4216e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.412

[Epoch: 10, batch: 41/207] total loss per batch: 0.756
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2977e-02, 3.8826e-03, 1.3583e-02, 7.6921e-07, 1.0499e-06, 3.8384e-03,
        9.5572e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.360

[Epoch: 10, batch: 82/207] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.6308e-06, 7.4144e-04, 2.8130e-02, 2.4148e-06, 1.7749e-02, 9.5337e-01,
        3.1030e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.876

[Epoch: 10, batch: 123/207] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0114, 0.8279, 0.0120, 0.0059, 0.0136, 0.1265, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.003

[Epoch: 10, batch: 164/207] total loss per batch: 0.783
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9040e-02, 4.9635e-03, 9.1429e-01, 6.6308e-05, 7.8276e-03, 3.2235e-05,
        3.7781e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.086

[Epoch: 10, batch: 205/207] total loss per batch: 0.723
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1925e-01, 4.9171e-07, 1.7182e-01, 8.9234e-03, 5.3438e-06, 2.2204e-06,
        3.0477e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.277

[Epoch: 11, batch: 41/207] total loss per batch: 0.755
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8451e-02, 2.5613e-03, 3.3017e-02, 1.1794e-06, 1.3819e-06, 9.5393e-03,
        9.2643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.402

[Epoch: 11, batch: 82/207] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.1566e-06, 1.9493e-03, 1.4925e-02, 5.4229e-06, 2.7317e-02, 9.5579e-01,
        7.5480e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.809

[Epoch: 11, batch: 123/207] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0232, 0.5805, 0.0120, 0.0169, 0.0210, 0.3433, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.003

[Epoch: 11, batch: 164/207] total loss per batch: 0.780
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([9.2391e-02, 1.1376e-02, 8.7551e-01, 7.5665e-05, 1.5130e-02, 5.0596e-05,
        5.4633e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.219

[Epoch: 11, batch: 205/207] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.3187e-01, 6.6168e-07, 2.5721e-01, 1.0851e-02, 7.0033e-05, 1.6260e-06,
        3.8001e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.390

[Epoch: 12, batch: 41/207] total loss per batch: 0.753
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6752e-02, 1.0415e-02, 2.5937e-02, 2.5607e-06, 1.5948e-06, 3.5213e-03,
        9.3337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.297

[Epoch: 12, batch: 82/207] total loss per batch: 0.776
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.0965e-06, 7.4815e-03, 1.5440e-02, 2.7298e-06, 1.3888e-02, 9.6318e-01,
        1.5902e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.890

[Epoch: 12, batch: 123/207] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0150, 0.7539, 0.0124, 0.0097, 0.0131, 0.1930, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 12, batch: 164/207] total loss per batch: 0.777
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0663e-02, 9.7475e-03, 9.0705e-01, 1.3373e-04, 1.8319e-02, 2.1088e-05,
        4.0644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.076

[Epoch: 12, batch: 205/207] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4800e-01, 5.6847e-07, 4.4253e-02, 7.7376e-03, 3.5191e-06, 1.7757e-06,
        1.5937e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.311

[Epoch: 13, batch: 41/207] total loss per batch: 0.751
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2687e-02, 1.5855e-02, 3.9649e-02, 1.5290e-06, 5.2511e-06, 6.8008e-03,
        9.1500e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 13, batch: 82/207] total loss per batch: 0.775
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.7868e-05, 1.2663e-03, 1.7575e-02, 5.9382e-06, 2.8681e-02, 9.5245e-01,
        6.5167e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.845

[Epoch: 13, batch: 123/207] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.5330, 0.0075, 0.0084, 0.0106, 0.4269, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.002

[Epoch: 13, batch: 164/207] total loss per batch: 0.776
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7438e-02, 8.7801e-03, 9.1932e-01, 1.1082e-04, 1.0911e-02, 3.4462e-05,
        3.4074e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.155

[Epoch: 13, batch: 205/207] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6747e-01, 1.4004e-06, 2.0476e-01, 2.7740e-02, 1.9304e-05, 2.5844e-06,
        1.5849e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.317

[Epoch: 14, batch: 41/207] total loss per batch: 0.748
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.1119e-02, 7.7755e-03, 5.1460e-02, 4.4628e-06, 2.2650e-06, 7.1542e-03,
        8.9248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.397

[Epoch: 14, batch: 82/207] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.4418e-06, 4.2442e-03, 8.8554e-03, 3.0532e-06, 2.3776e-02, 9.6311e-01,
        2.4464e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.898

[Epoch: 14, batch: 123/207] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0181, 0.7218, 0.0074, 0.0148, 0.0116, 0.2194, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.006

[Epoch: 14, batch: 164/207] total loss per batch: 0.774
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1704e-02, 7.8104e-03, 9.2640e-01, 1.2518e-04, 9.1715e-03, 4.0629e-05,
        4.7451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.184

[Epoch: 14, batch: 205/207] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1431e-01, 1.2862e-07, 8.3563e-02, 2.1144e-03, 4.1343e-06, 2.1370e-06,
        1.4770e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.370

[Epoch: 15, batch: 41/207] total loss per batch: 0.746
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.4922e-02, 5.1784e-03, 4.8011e-02, 1.4890e-06, 6.2609e-06, 8.6119e-03,
        8.9327e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.476

[Epoch: 15, batch: 82/207] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.8274e-06, 1.4425e-03, 3.9707e-02, 6.7737e-06, 9.9651e-03, 9.4887e-01,
        4.2392e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.849

[Epoch: 15, batch: 123/207] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0124, 0.5749, 0.0092, 0.0115, 0.0117, 0.3771, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 15, batch: 164/207] total loss per batch: 0.774
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.6805e-02, 3.8798e-03, 9.4020e-01, 5.5355e-05, 6.5743e-03, 1.3609e-05,
        2.4715e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.054

[Epoch: 15, batch: 205/207] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0277e-01, 7.7454e-07, 1.8078e-01, 1.6414e-02, 1.0282e-05, 3.1287e-06,
        1.9058e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.371

[Epoch: 16, batch: 41/207] total loss per batch: 0.743
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.1508e-02, 3.8777e-03, 2.7367e-02, 3.0176e-06, 1.2092e-06, 2.2908e-03,
        9.5495e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 16, batch: 82/207] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.2279e-05, 5.0109e-03, 1.2286e-02, 2.9442e-06, 9.6266e-03, 9.7306e-01,
        4.3835e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.869

[Epoch: 16, batch: 123/207] total loss per batch: 0.734
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0132, 0.6747, 0.0178, 0.0179, 0.0130, 0.2606, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 16, batch: 164/207] total loss per batch: 0.771
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0499e-02, 5.9214e-03, 9.2531e-01, 1.1393e-04, 1.1272e-02, 2.4392e-05,
        6.8564e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.184

[Epoch: 16, batch: 205/207] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2955e-01, 6.3159e-07, 5.6801e-02, 1.3632e-02, 5.6488e-06, 3.0155e-06,
        4.7580e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.423

[Epoch: 17, batch: 41/207] total loss per batch: 0.740
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5790e-02, 5.4659e-03, 6.5112e-02, 1.0150e-06, 5.0672e-06, 7.0689e-03,
        8.8656e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.382

[Epoch: 17, batch: 82/207] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3520e-06, 5.9526e-03, 4.3642e-03, 5.9655e-06, 2.1018e-02, 9.6865e-01,
        7.7053e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.821

[Epoch: 17, batch: 123/207] total loss per batch: 0.731
Policy (actual, predicted): 1 5
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0169, 0.4377, 0.0104, 0.0158, 0.0137, 0.5016, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.001

[Epoch: 17, batch: 164/207] total loss per batch: 0.770
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([1.2135e-01, 5.2401e-03, 8.5675e-01, 1.1715e-04, 1.0916e-02, 4.1316e-05,
        5.5869e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.156

[Epoch: 17, batch: 205/207] total loss per batch: 0.712
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1779e-01, 2.5638e-06, 2.4282e-01, 3.9357e-02, 1.0789e-05, 5.5799e-06,
        1.2734e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.362

[Epoch: 18, batch: 41/207] total loss per batch: 0.738
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([5.1740e-02, 1.8288e-02, 7.2782e-02, 5.5206e-06, 1.2489e-05, 8.6924e-03,
        8.4848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.353

[Epoch: 18, batch: 82/207] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.1831e-06, 4.4356e-03, 6.5924e-03, 3.0269e-06, 6.5120e-03, 9.8245e-01,
        1.3030e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.906

[Epoch: 18, batch: 123/207] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0088, 0.8984, 0.0073, 0.0065, 0.0105, 0.0669, 0.0015],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 18, batch: 164/207] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4090e-02, 5.5712e-03, 9.3105e-01, 4.9099e-05, 4.9826e-03, 1.7345e-05,
        4.2358e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.112

[Epoch: 18, batch: 205/207] total loss per batch: 0.709
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4471e-01, 9.8656e-07, 1.3407e-01, 2.1182e-02, 1.1013e-05, 3.0949e-06,
        2.5308e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.228

[Epoch: 19, batch: 41/207] total loss per batch: 0.739
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.0009e-02, 1.4558e-03, 2.0498e-02, 1.1288e-06, 2.3360e-06, 2.3392e-03,
        9.5569e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.386

[Epoch: 19, batch: 82/207] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.3664e-06, 9.4798e-03, 8.7962e-03, 4.0205e-06, 1.6228e-02, 9.6548e-01,
        5.0084e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.858

[Epoch: 19, batch: 123/207] total loss per batch: 0.733
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0106, 0.7300, 0.0082, 0.0101, 0.0077, 0.2304, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.007

[Epoch: 19, batch: 164/207] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([9.7082e-02, 6.2504e-02, 8.2186e-01, 1.5891e-04, 1.0979e-02, 7.7155e-05,
        7.3349e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 19, batch: 205/207] total loss per batch: 0.709
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3407e-01, 2.6802e-06, 1.4293e-01, 2.2958e-02, 1.2395e-05, 7.4536e-06,
        1.5714e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.212

[Epoch: 20, batch: 41/207] total loss per batch: 0.739
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8139e-02, 6.2701e-03, 8.9579e-02, 1.9711e-06, 1.0307e-05, 7.4674e-03,
        8.6853e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.415

[Epoch: 20, batch: 82/207] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5604e-05, 3.0727e-03, 6.1557e-03, 4.8300e-06, 8.2871e-03, 9.8246e-01,
        2.3401e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 20, batch: 123/207] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0197, 0.5091, 0.0108, 0.0161, 0.0207, 0.4185, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 20, batch: 164/207] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.1784e-02, 1.1040e-03, 9.4936e-01, 3.4438e-05, 3.8560e-03, 1.4623e-05,
        3.8489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.172

[Epoch: 20, batch: 205/207] total loss per batch: 0.708
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9777e-01, 6.2478e-07, 8.8759e-02, 1.3441e-02, 1.6621e-05, 3.0097e-06,
        6.3100e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.273

[Epoch: 21, batch: 41/207] total loss per batch: 0.740
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.0067e-02, 1.7757e-03, 1.8282e-02, 1.1711e-06, 1.0532e-06, 3.0468e-03,
        9.5683e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.463

[Epoch: 21, batch: 82/207] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0374e-05, 2.8241e-03, 2.5865e-03, 1.9333e-06, 1.3851e-02, 9.8073e-01,
        3.2310e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.900

[Epoch: 21, batch: 123/207] total loss per batch: 0.731
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0296, 0.5578, 0.0084, 0.0134, 0.0132, 0.3725, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 21, batch: 164/207] total loss per batch: 0.767
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([3.6979e-02, 2.5994e-03, 9.5124e-01, 5.0998e-05, 7.5204e-03, 1.7558e-05,
        1.5939e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.123

[Epoch: 21, batch: 205/207] total loss per batch: 0.708
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0497e-01, 7.8384e-07, 2.6875e-01, 2.6234e-02, 1.8820e-05, 5.0411e-06,
        1.6352e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.280

[Epoch: 22, batch: 41/207] total loss per batch: 0.740
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.3870e-02, 5.2745e-03, 9.6899e-02, 3.0328e-06, 2.1817e-05, 3.0008e-03,
        8.7093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.431

[Epoch: 22, batch: 82/207] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([9.6026e-06, 3.9022e-03, 4.9799e-03, 7.9222e-06, 1.8503e-02, 9.7260e-01,
        2.1260e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.839

[Epoch: 22, batch: 123/207] total loss per batch: 0.730
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0080, 0.7018, 0.0117, 0.0118, 0.0123, 0.2527, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.000

[Epoch: 22, batch: 164/207] total loss per batch: 0.766
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1176e-02, 6.2327e-03, 9.3235e-01, 6.7829e-05, 6.9419e-03, 2.4243e-05,
        3.2033e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.069

[Epoch: 22, batch: 205/207] total loss per batch: 0.709
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0141e-01, 1.0502e-06, 8.8174e-02, 1.0404e-02, 6.8876e-06, 2.0135e-06,
        4.8630e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.265

[Epoch: 23, batch: 41/207] total loss per batch: 0.739
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.2135e-02, 8.2367e-03, 5.7705e-02, 3.9855e-06, 5.3133e-06, 4.8725e-03,
        8.8704e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.326

[Epoch: 23, batch: 82/207] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.8316e-06, 2.1238e-03, 3.1647e-03, 6.5738e-06, 6.4250e-03, 9.8827e-01,
        7.6406e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.915

[Epoch: 23, batch: 123/207] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0140, 0.6447, 0.0089, 0.0102, 0.0132, 0.3011, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.003

[Epoch: 23, batch: 164/207] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5760e-02, 4.7456e-03, 9.3074e-01, 7.4368e-05, 5.2137e-03, 2.0427e-05,
        3.4501e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.177

[Epoch: 23, batch: 205/207] total loss per batch: 0.707
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0392e-01, 1.9357e-06, 2.6456e-01, 3.1469e-02, 2.4013e-05, 7.0362e-06,
        1.5235e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.291

[Epoch: 24, batch: 41/207] total loss per batch: 0.738
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.6436e-02, 5.8533e-03, 3.4665e-02, 2.5885e-06, 2.9497e-06, 5.4089e-03,
        9.0763e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.443

[Epoch: 24, batch: 82/207] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3742e-06, 6.2789e-03, 2.7701e-03, 3.2994e-06, 1.9689e-02, 9.7125e-01,
        7.6946e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.912

[Epoch: 24, batch: 123/207] total loss per batch: 0.727
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0127, 0.6584, 0.0120, 0.0107, 0.0097, 0.2917, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.001

[Epoch: 24, batch: 164/207] total loss per batch: 0.766
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.9458e-02, 3.8456e-03, 9.0342e-01, 1.0425e-04, 5.2213e-03, 8.0559e-05,
        7.8658e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.094

[Epoch: 24, batch: 205/207] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7179e-01, 6.1681e-07, 1.1117e-01, 1.7017e-02, 8.7494e-06, 3.6334e-06,
        9.3944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.397

[Epoch: 25, batch: 41/207] total loss per batch: 0.736
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.1975e-02, 3.0605e-03, 4.4562e-02, 1.6906e-06, 1.3918e-06, 3.0429e-03,
        9.2736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.435

[Epoch: 25, batch: 82/207] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9704e-05, 5.7959e-03, 1.2177e-02, 1.6947e-05, 3.2020e-02, 9.4996e-01,
        2.4369e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 25, batch: 123/207] total loss per batch: 0.728
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0074, 0.6886, 0.0090, 0.0074, 0.0115, 0.2735, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.001

[Epoch: 25, batch: 164/207] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8244e-02, 4.5996e-03, 9.1673e-01, 2.0465e-04, 8.0313e-03, 2.7869e-05,
        2.1618e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.156

[Epoch: 25, batch: 205/207] total loss per batch: 0.704
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0938e-01, 1.1365e-06, 1.5939e-01, 3.1213e-02, 6.3447e-06, 2.4832e-06,
        1.1334e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.395

[Epoch: 26, batch: 41/207] total loss per batch: 0.735
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.6035e-02, 1.0753e-02, 1.3617e-01, 1.6502e-06, 4.7430e-06, 8.2745e-03,
        8.0876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.422

[Epoch: 26, batch: 82/207] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.4492e-06, 3.5784e-03, 6.2370e-03, 2.2163e-06, 2.0251e-02, 9.6992e-01,
        8.9860e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.905

[Epoch: 26, batch: 123/207] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0140, 0.6272, 0.0091, 0.0104, 0.0081, 0.3270, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 26, batch: 164/207] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([3.1135e-02, 2.9943e-03, 9.5471e-01, 7.2034e-05, 6.8538e-03, 2.2679e-05,
        4.2114e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.113

[Epoch: 26, batch: 205/207] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7582e-01, 8.6255e-07, 1.1032e-01, 1.3827e-02, 1.8865e-05, 2.2088e-06,
        1.3233e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.241

[Epoch: 27, batch: 41/207] total loss per batch: 0.732
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.5761e-02, 7.8646e-03, 3.3529e-02, 2.9939e-06, 8.3748e-07, 1.8959e-03,
        9.3095e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.423

[Epoch: 27, batch: 82/207] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.6665e-05, 8.4195e-03, 9.3516e-03, 4.8409e-06, 6.6964e-03, 9.7551e-01,
        1.5280e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.881

[Epoch: 27, batch: 123/207] total loss per batch: 0.725
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0177, 0.5732, 0.0066, 0.0113, 0.0225, 0.3651, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 27, batch: 164/207] total loss per batch: 0.763
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4594e-02, 4.1559e-03, 9.3501e-01, 7.0030e-05, 3.4556e-03, 3.1998e-05,
        2.6803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 27, batch: 205/207] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8296e-01, 5.6813e-07, 2.0109e-01, 1.5903e-02, 2.1404e-05, 4.5998e-06,
        2.2804e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 28, batch: 41/207] total loss per batch: 0.731
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8148e-02, 6.9038e-03, 1.0277e-01, 4.4553e-06, 8.5250e-06, 5.2722e-03,
        8.5689e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.372

[Epoch: 28, batch: 82/207] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.5473e-06, 9.3261e-03, 9.0339e-03, 5.9385e-06, 2.0033e-02, 9.6159e-01,
        2.7038e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.936

[Epoch: 28, batch: 123/207] total loss per batch: 0.725
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0092, 0.6509, 0.0090, 0.0094, 0.0093, 0.3083, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 28, batch: 164/207] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([3.0674e-02, 1.3170e-03, 9.5421e-01, 6.3050e-05, 9.5520e-03, 2.3495e-05,
        4.1650e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.127

[Epoch: 28, batch: 205/207] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0727e-01, 6.8672e-07, 1.7221e-01, 2.0479e-02, 1.4426e-05, 2.3710e-06,
        2.3860e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.284

[Epoch: 29, batch: 41/207] total loss per batch: 0.731
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4386e-02, 5.1646e-03, 1.8321e-02, 1.5213e-06, 1.9054e-06, 1.0991e-03,
        9.5103e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 29, batch: 82/207] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9475e-05, 5.1773e-03, 2.0059e-02, 1.8987e-05, 1.9901e-02, 9.5481e-01,
        7.1631e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.862

[Epoch: 29, batch: 123/207] total loss per batch: 0.724
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0124, 0.7358, 0.0058, 0.0078, 0.0140, 0.2228, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 29, batch: 164/207] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4994e-02, 5.3215e-03, 9.1553e-01, 6.8746e-05, 9.9478e-03, 2.9733e-05,
        4.1050e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.199

[Epoch: 29, batch: 205/207] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9649e-01, 1.9117e-06, 9.1724e-02, 1.1772e-02, 5.2359e-06, 1.3535e-06,
        1.0094e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 30, batch: 41/207] total loss per batch: 0.732
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.8333e-02, 1.5726e-02, 8.4832e-02, 4.4175e-06, 1.0895e-05, 7.3192e-03,
        8.4377e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.430

[Epoch: 30, batch: 82/207] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3310e-06, 3.2075e-03, 6.7871e-03, 3.4161e-06, 9.5832e-03, 9.8041e-01,
        5.0160e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 30, batch: 123/207] total loss per batch: 0.724
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0220, 0.5291, 0.0103, 0.0144, 0.0186, 0.4003, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 30, batch: 164/207] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([8.0859e-02, 2.1509e-03, 9.0694e-01, 5.1665e-05, 5.9913e-03, 1.4462e-05,
        3.9907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.129

[Epoch: 30, batch: 205/207] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4524e-01, 2.2994e-06, 3.2283e-01, 3.1887e-02, 2.4350e-05, 2.7195e-06,
        1.1523e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.364

[Epoch: 31, batch: 41/207] total loss per batch: 0.731
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.5528e-02, 4.7419e-03, 6.8877e-02, 2.9306e-06, 3.6468e-06, 8.5123e-03,
        8.7233e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.366

[Epoch: 31, batch: 82/207] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.3630e-06, 1.9828e-03, 5.4797e-03, 1.4835e-05, 9.5822e-03, 9.8294e-01,
        4.7300e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 31, batch: 123/207] total loss per batch: 0.726
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0119, 0.6838, 0.0139, 0.0089, 0.0148, 0.2622, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 31, batch: 164/207] total loss per batch: 0.764
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1045e-02, 6.2565e-03, 9.2511e-01, 9.6587e-05, 4.8796e-03, 3.8785e-05,
        2.5748e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.049

[Epoch: 31, batch: 205/207] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.5501e-01, 7.1786e-07, 3.6961e-02, 8.0223e-03, 3.0833e-06, 1.6933e-06,
        5.7052e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.304

[Epoch: 32, batch: 41/207] total loss per batch: 0.735
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.4558e-02, 3.2586e-03, 3.5743e-02, 3.0156e-06, 1.5766e-06, 1.2811e-03,
        9.4516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.460

[Epoch: 32, batch: 82/207] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.0109e-05, 1.1463e-02, 9.1346e-03, 1.6455e-05, 1.0295e-02, 9.6907e-01,
        9.3808e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.953

[Epoch: 32, batch: 123/207] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0179, 0.5856, 0.0206, 0.0114, 0.0125, 0.3478, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.000

[Epoch: 32, batch: 164/207] total loss per batch: 0.766
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.1236e-02, 9.5690e-03, 9.4027e-01, 6.7314e-05, 5.8750e-03, 1.4177e-05,
        2.9700e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.187

[Epoch: 32, batch: 205/207] total loss per batch: 0.707
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4910e-01, 2.1868e-06, 1.3935e-01, 1.1509e-02, 1.7340e-05, 1.6839e-06,
        2.4597e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.259

[Epoch: 33, batch: 41/207] total loss per batch: 0.750
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([5.7873e-03, 2.7499e-03, 2.4203e-02, 5.0425e-06, 4.6636e-06, 1.0671e-03,
        9.6618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.295

[Epoch: 33, batch: 82/207] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3634e-05, 1.1238e-02, 6.7226e-02, 6.6840e-05, 6.4311e-02, 8.5712e-01,
        9.6966e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.962

[Epoch: 33, batch: 123/207] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0110, 0.7887, 0.0049, 0.0053, 0.0095, 0.1719, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 33, batch: 164/207] total loss per batch: 0.812
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.5100e-02, 5.4797e-03, 9.3642e-01, 1.0829e-04, 9.0048e-03, 4.0411e-05,
        3.8506e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.054

[Epoch: 33, batch: 205/207] total loss per batch: 0.768
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7524e-01, 4.3519e-06, 6.9544e-02, 5.4633e-02, 5.3725e-06, 1.5400e-05,
        5.6161e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.139

[Epoch: 34, batch: 41/207] total loss per batch: 0.792
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.5769e-02, 1.4148e-03, 2.0342e-02, 5.7470e-06, 1.6636e-05, 5.5390e-03,
        9.5691e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.221

[Epoch: 34, batch: 82/207] total loss per batch: 0.807
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3413e-05, 5.2402e-03, 3.0472e-03, 7.0303e-05, 2.2098e-02, 9.6952e-01,
        3.3449e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.807

[Epoch: 34, batch: 123/207] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0098, 0.6244, 0.0165, 0.0170, 0.0139, 0.3115, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.005

[Epoch: 34, batch: 164/207] total loss per batch: 0.809
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8827e-02, 5.9961e-02, 8.5717e-01, 5.0201e-04, 4.5013e-03, 5.1033e-05,
        8.9868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.106

[Epoch: 34, batch: 205/207] total loss per batch: 0.742
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5980e-01, 1.2108e-06, 1.3160e-01, 8.5820e-03, 4.9888e-06, 1.0311e-05,
        1.6925e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.309

[Epoch: 35, batch: 41/207] total loss per batch: 0.777
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.2749e-02, 4.9524e-03, 2.4822e-02, 8.7690e-07, 5.6620e-06, 5.3816e-03,
        9.5209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.381

[Epoch: 35, batch: 82/207] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.9270e-05, 1.7171e-02, 3.1419e-02, 2.7059e-05, 1.6176e-02, 9.3519e-01,
        6.2960e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.846

[Epoch: 35, batch: 123/207] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0040, 0.8037, 0.0052, 0.0096, 0.0120, 0.1629, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 35, batch: 164/207] total loss per batch: 0.784
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.2989e-02, 3.9780e-03, 9.4494e-01, 3.4286e-04, 5.5157e-03, 2.0359e-05,
        2.2093e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 35, batch: 205/207] total loss per batch: 0.716
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5185e-01, 3.9091e-06, 1.2560e-01, 2.2516e-02, 2.3001e-06, 1.8452e-05,
        6.7327e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.276

[Epoch: 36, batch: 41/207] total loss per batch: 0.745
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.5536e-02, 3.3739e-03, 6.5993e-02, 2.0182e-06, 1.0225e-05, 5.2419e-03,
        9.0984e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 36, batch: 82/207] total loss per batch: 0.763
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5862e-05, 3.3356e-03, 3.3164e-03, 3.5293e-05, 1.2355e-02, 9.8094e-01,
        3.4307e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.912

[Epoch: 36, batch: 123/207] total loss per batch: 0.727
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0148, 0.4910, 0.0096, 0.0115, 0.0183, 0.4501, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 36, batch: 164/207] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([8.2166e-02, 1.4281e-02, 8.8776e-01, 4.4481e-04, 1.1340e-02, 4.1573e-05,
        3.9689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.146

[Epoch: 36, batch: 205/207] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6759e-01, 2.3932e-06, 2.2159e-01, 1.0795e-02, 4.1007e-06, 1.0148e-05,
        2.9805e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.319

[Epoch: 37, batch: 41/207] total loss per batch: 0.727
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6241e-02, 4.9890e-03, 5.7766e-02, 4.3249e-06, 9.0437e-06, 5.0363e-03,
        9.0596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.383

[Epoch: 37, batch: 82/207] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0721e-05, 1.5072e-03, 1.9517e-03, 4.4142e-05, 1.0253e-02, 9.8623e-01,
        4.6319e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.897

[Epoch: 37, batch: 123/207] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0120, 0.6418, 0.0089, 0.0105, 0.0147, 0.3090, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 37, batch: 164/207] total loss per batch: 0.751
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.7826e-02, 3.8781e-03, 9.3753e-01, 4.9291e-04, 7.2500e-03, 2.7996e-05,
        2.9973e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 37, batch: 205/207] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4321e-01, 1.8568e-06, 4.6683e-02, 1.0095e-02, 6.5836e-07, 4.0085e-06,
        1.7220e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.353

[Epoch: 38, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2227e-02, 7.1599e-03, 5.5255e-02, 1.3643e-06, 6.1755e-06, 4.0405e-03,
        9.0131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.397

[Epoch: 38, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1846e-05, 3.0669e-03, 4.4411e-03, 3.5574e-05, 1.0063e-02, 9.8238e-01,
        5.8933e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.886

[Epoch: 38, batch: 123/207] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0120, 0.6397, 0.0106, 0.0089, 0.0124, 0.3129, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 38, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.1894e-02, 4.4850e-03, 9.4352e-01, 2.5205e-04, 6.7271e-03, 3.1226e-05,
        3.0949e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.127

[Epoch: 38, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1803e-01, 9.8283e-07, 1.6082e-01, 2.1137e-02, 1.2331e-06, 3.6696e-06,
        2.4895e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.327

[Epoch: 39, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2646e-02, 7.5582e-03, 6.4783e-02, 1.7876e-06, 3.8435e-06, 3.1811e-03,
        9.0183e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.392

[Epoch: 39, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0328e-05, 2.1268e-03, 3.0541e-03, 3.8639e-05, 9.1456e-03, 9.8562e-01,
        5.0379e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.907

[Epoch: 39, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0121, 0.6315, 0.0103, 0.0107, 0.0143, 0.3182, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 39, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5073e-02, 3.6852e-03, 9.3183e-01, 2.6020e-04, 6.3273e-03, 1.4610e-05,
        2.8053e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 39, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4581e-01, 1.6232e-06, 1.3969e-01, 1.4488e-02, 1.4179e-06, 4.5280e-06,
        1.4875e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.360

[Epoch: 40, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2104e-02, 6.0051e-03, 5.9068e-02, 1.4238e-06, 4.0865e-06, 3.9151e-03,
        8.9890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.397

[Epoch: 40, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([9.4804e-06, 3.0251e-03, 2.3589e-03, 2.3661e-05, 8.3957e-03, 9.8619e-01,
        4.4003e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 40, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0115, 0.6024, 0.0092, 0.0105, 0.0126, 0.3505, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 40, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.9480e-02, 2.3226e-03, 9.4059e-01, 1.7634e-04, 4.5359e-03, 1.7686e-05,
        2.8735e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 40, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3354e-01, 8.8587e-07, 1.4209e-01, 2.4360e-02, 1.1655e-06, 2.3348e-06,
        2.8771e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.308

[Epoch: 41, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7211e-02, 5.6618e-03, 6.7538e-02, 1.1704e-06, 2.6530e-06, 2.9528e-03,
        8.9663e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 41, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1150e-05, 3.9398e-03, 4.3118e-03, 3.9251e-05, 1.2732e-02, 9.7897e-01,
        5.2976e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.912

[Epoch: 41, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0113, 0.6472, 0.0099, 0.0073, 0.0130, 0.3082, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 41, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7072e-02, 4.1659e-03, 9.3101e-01, 1.6620e-04, 4.5380e-03, 1.0937e-05,
        3.0413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.153

[Epoch: 41, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3443e-01, 1.2590e-06, 1.5296e-01, 1.2594e-02, 1.4098e-06, 3.3133e-06,
        1.2275e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.337

[Epoch: 42, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3656e-02, 6.7271e-03, 4.7269e-02, 1.5920e-06, 3.9281e-06, 3.7832e-03,
        9.0856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.405

[Epoch: 42, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3688e-06, 1.9140e-03, 2.0116e-03, 1.4307e-05, 6.9546e-03, 9.8910e-01,
        2.8235e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 42, batch: 123/207] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0089, 0.6316, 0.0094, 0.0086, 0.0111, 0.3276, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 42, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8872e-02, 3.8716e-03, 9.2960e-01, 1.3058e-04, 4.4295e-03, 1.6674e-05,
        3.0841e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.110

[Epoch: 42, batch: 205/207] total loss per batch: 0.689
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1845e-01, 1.8510e-06, 1.5519e-01, 2.6345e-02, 1.7333e-06, 3.7356e-06,
        3.4424e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 43, batch: 41/207] total loss per batch: 0.722
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2180e-02, 3.9310e-03, 5.4369e-02, 1.0655e-06, 2.4367e-06, 2.1936e-03,
        9.1732e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 43, batch: 82/207] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1092e-05, 3.8845e-03, 2.6858e-03, 4.1950e-05, 1.0381e-02, 9.8299e-01,
        1.0613e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.918

[Epoch: 43, batch: 123/207] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0117, 0.6504, 0.0065, 0.0065, 0.0114, 0.3098, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.003

[Epoch: 43, batch: 164/207] total loss per batch: 0.751
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2359e-02, 3.1914e-03, 9.3489e-01, 9.5292e-05, 6.5257e-03, 1.1418e-05,
        2.9228e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.157

[Epoch: 43, batch: 205/207] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0671e-01, 3.1805e-06, 1.6793e-01, 2.5351e-02, 2.9044e-06, 4.5163e-06,
        2.4375e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.339

[Epoch: 44, batch: 41/207] total loss per batch: 0.724
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1174e-02, 8.3654e-03, 7.1265e-02, 3.9872e-06, 8.7737e-06, 2.7951e-03,
        8.8639e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.401

[Epoch: 44, batch: 82/207] total loss per batch: 0.752
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.4281e-06, 2.2852e-03, 3.9447e-03, 2.4486e-05, 1.0259e-02, 9.8348e-01,
        4.0540e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.908

[Epoch: 44, batch: 123/207] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0108, 0.6280, 0.0140, 0.0062, 0.0113, 0.3262, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 44, batch: 164/207] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8887e-02, 4.3667e-03, 9.1613e-01, 1.4100e-04, 6.0172e-03, 2.4235e-05,
        4.4362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 44, batch: 205/207] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7397e-01, 1.3864e-06, 1.1169e-01, 1.4321e-02, 1.7218e-06, 3.9443e-06,
        3.3099e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 45, batch: 41/207] total loss per batch: 0.727
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7935e-02, 3.6728e-03, 4.0985e-02, 1.1758e-06, 2.6367e-06, 3.4837e-03,
        9.2392e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 45, batch: 82/207] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.6204e-05, 3.1994e-03, 3.9422e-03, 2.3200e-05, 1.2609e-02, 9.8021e-01,
        1.0239e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.908

[Epoch: 45, batch: 123/207] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0125, 0.6187, 0.0075, 0.0166, 0.0118, 0.3307, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 45, batch: 164/207] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.4954e-02, 4.2349e-03, 9.0891e-01, 2.0011e-04, 6.6040e-03, 2.2765e-05,
        5.0755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.136

[Epoch: 45, batch: 205/207] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.5309e-01, 2.8373e-06, 2.2179e-01, 2.5103e-02, 7.4081e-06, 3.7306e-06,
        4.2343e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.338

[Epoch: 46, batch: 41/207] total loss per batch: 0.728
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7129e-02, 9.0928e-03, 8.5735e-02, 5.1936e-06, 1.4206e-05, 3.8023e-03,
        8.6422e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.392

[Epoch: 46, batch: 82/207] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.6171e-05, 5.4339e-03, 3.7377e-03, 1.4176e-05, 7.4266e-03, 9.8337e-01,
        8.2223e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.898

[Epoch: 46, batch: 123/207] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0136, 0.6800, 0.0109, 0.0087, 0.0068, 0.2763, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 46, batch: 164/207] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([8.7385e-02, 6.2532e-03, 8.9656e-01, 1.2337e-04, 4.8762e-03, 1.9276e-05,
        4.7800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.153

[Epoch: 46, batch: 205/207] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8290e-01, 2.6068e-06, 1.0353e-01, 1.3562e-02, 3.3762e-06, 5.2238e-06,
        3.9653e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.351

[Epoch: 47, batch: 41/207] total loss per batch: 0.728
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4297e-02, 4.1753e-03, 3.8452e-02, 2.6544e-06, 5.7555e-06, 1.9602e-03,
        9.2111e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 47, batch: 82/207] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0179e-05, 2.2217e-03, 3.0741e-03, 1.8194e-05, 9.0382e-03, 9.8564e-01,
        5.2064e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 47, batch: 123/207] total loss per batch: 0.720
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0107, 0.5884, 0.0141, 0.0103, 0.0138, 0.3586, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 47, batch: 164/207] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.0230e-02, 3.7131e-03, 9.4762e-01, 9.7548e-05, 4.6106e-03, 1.8128e-05,
        3.7152e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 47, batch: 205/207] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9606e-01, 3.3603e-06, 1.7656e-01, 2.7366e-02, 3.3812e-06, 1.9794e-06,
        3.2540e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.363

[Epoch: 48, batch: 41/207] total loss per batch: 0.728
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7017e-02, 4.7253e-03, 5.2281e-02, 1.7924e-06, 4.5355e-06, 3.8944e-03,
        9.1208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.422

[Epoch: 48, batch: 82/207] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0470e-05, 5.7891e-03, 3.1662e-03, 3.4694e-05, 2.0017e-02, 9.7097e-01,
        8.5603e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.887

[Epoch: 48, batch: 123/207] total loss per batch: 0.720
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0092, 0.6363, 0.0112, 0.0074, 0.0087, 0.3233, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 48, batch: 164/207] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.3133e-02, 4.7558e-03, 9.4034e-01, 1.4825e-04, 4.1256e-03, 1.4683e-05,
        7.4796e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.156

[Epoch: 48, batch: 205/207] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5621e-01, 2.0549e-06, 1.2603e-01, 1.7748e-02, 4.9161e-06, 3.5819e-06,
        2.5860e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.242

[Epoch: 49, batch: 41/207] total loss per batch: 0.728
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7340e-02, 4.1657e-03, 5.4775e-02, 5.1570e-06, 8.4588e-06, 3.3380e-03,
        9.1037e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.448

[Epoch: 49, batch: 82/207] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.2827e-05, 4.9953e-03, 5.2587e-03, 1.3053e-05, 4.5366e-03, 9.8518e-01,
        1.1208e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.896

[Epoch: 49, batch: 123/207] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0136, 0.6514, 0.0124, 0.0085, 0.0116, 0.2969, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 49, batch: 164/207] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([3.9118e-02, 5.3354e-03, 9.4746e-01, 5.4919e-05, 4.1330e-03, 1.3682e-05,
        3.8869e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.111

[Epoch: 49, batch: 205/207] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1553e-01, 2.2700e-06, 1.5831e-01, 2.6136e-02, 4.4648e-06, 3.8892e-06,
        9.3897e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.401

[Epoch: 50, batch: 41/207] total loss per batch: 0.728
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4319e-02, 5.1733e-03, 6.6080e-02, 3.5159e-06, 1.6005e-05, 4.5342e-03,
        8.8987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 50, batch: 82/207] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.6441e-05, 5.7201e-03, 1.6592e-03, 6.3281e-05, 1.6811e-02, 9.7573e-01,
        9.6818e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.895

[Epoch: 50, batch: 123/207] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0084, 0.6586, 0.0059, 0.0084, 0.0084, 0.3075, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 50, batch: 164/207] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.0943e-02, 4.2586e-03, 9.4758e-01, 1.1672e-04, 3.5634e-03, 2.0437e-05,
        3.5173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 50, batch: 205/207] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4447e-01, 5.9042e-06, 1.3990e-01, 1.5617e-02, 5.2550e-06, 5.0440e-06,
        4.9075e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.283

[Epoch: 51, batch: 41/207] total loss per batch: 0.727
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.0350e-02, 5.4030e-03, 7.7519e-02, 3.3504e-06, 1.2785e-05, 4.4917e-03,
        8.7222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 51, batch: 82/207] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.7281e-05, 4.4748e-03, 4.2429e-03, 2.8736e-05, 8.2765e-03, 9.8296e-01,
        3.1219e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.879

[Epoch: 51, batch: 123/207] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.7344, 0.0130, 0.0063, 0.0074, 0.2257, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 51, batch: 164/207] total loss per batch: 0.752
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8165e-02, 2.3757e-03, 9.2180e-01, 8.7488e-05, 5.6101e-03, 1.3529e-05,
        1.9497e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.157

[Epoch: 51, batch: 205/207] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4479e-01, 3.9643e-06, 2.3285e-01, 2.2350e-02, 5.1566e-06, 1.8469e-06,
        3.3499e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.356

[Epoch: 52, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5489e-02, 5.3318e-03, 7.2129e-02, 3.5936e-06, 8.6947e-06, 4.2796e-03,
        8.8276e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.423

[Epoch: 52, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1197e-05, 3.6379e-03, 5.6201e-03, 2.8836e-05, 1.3312e-02, 9.7739e-01,
        2.3336e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.901

[Epoch: 52, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0120, 0.5211, 0.0152, 0.0120, 0.0130, 0.4222, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 52, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.8561e-02, 2.3898e-03, 9.4150e-01, 1.0719e-04, 4.1862e-03, 1.1846e-05,
        3.2448e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 52, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9470e-01, 4.6437e-06, 9.0189e-02, 1.5099e-02, 3.3855e-06, 2.6355e-06,
        4.8115e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.269

[Epoch: 53, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6126e-02, 5.0585e-03, 4.3779e-02, 2.2502e-06, 9.3017e-06, 3.2566e-03,
        9.2177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 53, batch: 82/207] total loss per batch: 0.743
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0203e-05, 2.9791e-03, 2.6138e-03, 1.9587e-05, 8.0594e-03, 9.8632e-01,
        1.3790e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.907

[Epoch: 53, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0077, 0.6665, 0.0094, 0.0062, 0.0070, 0.2996, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 53, batch: 164/207] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8380e-02, 3.9516e-03, 9.1994e-01, 9.1375e-05, 4.9867e-03, 1.2362e-05,
        2.6423e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 53, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2535e-01, 2.7295e-06, 1.4731e-01, 2.7335e-02, 2.9077e-06, 1.3916e-06,
        4.2726e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.321

[Epoch: 54, batch: 41/207] total loss per batch: 0.717
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4066e-02, 3.2690e-03, 6.1018e-02, 1.2426e-06, 4.3635e-06, 2.3064e-03,
        9.0933e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.397

[Epoch: 54, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5553e-05, 3.9729e-03, 3.7405e-03, 1.6171e-05, 9.6006e-03, 9.8265e-01,
        2.0285e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.916

[Epoch: 54, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0075, 0.6626, 0.0106, 0.0064, 0.0083, 0.3007, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 54, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6255e-02, 2.1684e-03, 9.3471e-01, 9.3248e-05, 3.6030e-03, 1.3600e-05,
        3.1533e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.121

[Epoch: 54, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2359e-01, 2.4982e-06, 1.6194e-01, 1.4458e-02, 2.0142e-06, 1.3182e-06,
        3.4019e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.296

[Epoch: 55, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.3612e-02, 3.5113e-03, 4.6218e-02, 6.3555e-07, 2.8362e-06, 2.1070e-03,
        9.2455e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.417

[Epoch: 55, batch: 82/207] total loss per batch: 0.743
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.5118e-06, 2.3733e-03, 2.8659e-03, 8.8425e-06, 9.8542e-03, 9.8489e-01,
        8.3960e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.921

[Epoch: 55, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0090, 0.6578, 0.0067, 0.0068, 0.0076, 0.3085, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 55, batch: 164/207] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4025e-02, 2.3437e-03, 9.2621e-01, 6.1132e-05, 4.6388e-03, 1.1962e-05,
        2.7138e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 55, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2447e-01, 2.7105e-06, 1.5071e-01, 2.4813e-02, 2.4108e-06, 2.6951e-06,
        4.4184e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.326

[Epoch: 56, batch: 41/207] total loss per batch: 0.717
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.8823e-02, 5.6148e-03, 8.5000e-02, 1.4542e-06, 5.2824e-06, 2.6590e-03,
        8.6790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.391

[Epoch: 56, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.5150e-06, 3.7945e-03, 2.9141e-03, 9.8407e-06, 8.9582e-03, 9.8431e-01,
        1.2877e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 56, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0079, 0.6405, 0.0100, 0.0058, 0.0069, 0.3259, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 56, batch: 164/207] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0990e-02, 1.7907e-03, 9.4028e-01, 5.7434e-05, 4.0632e-03, 8.8267e-06,
        2.8073e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 56, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5548e-01, 2.2112e-06, 1.3221e-01, 1.2295e-02, 2.3294e-06, 2.7490e-06,
        6.1000e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.361

[Epoch: 57, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4142e-02, 7.1191e-03, 7.6664e-02, 1.5095e-06, 1.2620e-05, 4.7197e-03,
        8.7734e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.406

[Epoch: 57, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.9618e-06, 4.3483e-03, 3.1141e-03, 1.6222e-05, 9.4402e-03, 9.8308e-01,
        1.2631e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.921

[Epoch: 57, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0086, 0.6727, 0.0084, 0.0051, 0.0070, 0.2960, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 57, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.7854e-02, 2.2974e-03, 9.2303e-01, 4.7167e-05, 3.9181e-03, 5.6229e-06,
        2.8469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.127

[Epoch: 57, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2829e-01, 2.0568e-06, 1.5399e-01, 1.7711e-02, 1.9535e-06, 1.6872e-06,
        2.4527e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.348

[Epoch: 58, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6747e-02, 3.8950e-03, 6.4191e-02, 3.6027e-06, 4.7689e-06, 3.7146e-03,
        9.0144e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.398

[Epoch: 58, batch: 82/207] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0668e-05, 5.1672e-03, 4.3906e-03, 2.9838e-05, 1.0252e-02, 9.8015e-01,
        1.9566e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.920

[Epoch: 58, batch: 123/207] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0147, 0.5451, 0.0157, 0.0108, 0.0135, 0.3910, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 58, batch: 164/207] total loss per batch: 0.751
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.3631e-02, 8.5285e-03, 9.3886e-01, 1.0597e-04, 4.1513e-03, 1.3678e-05,
        4.7128e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.120

[Epoch: 58, batch: 205/207] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0276e-01, 2.2932e-06, 1.7889e-01, 1.8335e-02, 3.7544e-06, 1.9339e-06,
        8.6233e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.294

[Epoch: 59, batch: 41/207] total loss per batch: 0.725
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4702e-02, 4.7879e-03, 2.8722e-02, 1.8192e-06, 5.5750e-06, 3.5564e-03,
        9.2822e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.421

[Epoch: 59, batch: 82/207] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7538e-06, 4.6352e-03, 2.6991e-03, 1.9061e-05, 2.4724e-02, 9.6792e-01,
        2.1290e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 59, batch: 123/207] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0054, 0.7418, 0.0077, 0.0061, 0.0079, 0.2290, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 59, batch: 164/207] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([8.0882e-02, 5.8367e-03, 9.0421e-01, 6.3910e-05, 5.0189e-03, 9.4107e-06,
        3.9798e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 59, batch: 205/207] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4289e-01, 3.2990e-06, 1.3294e-01, 2.4159e-02, 2.6527e-06, 3.3098e-06,
        5.0657e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.299

[Epoch: 60, batch: 41/207] total loss per batch: 0.726
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.7350e-02, 3.2161e-03, 5.8584e-02, 2.0061e-06, 2.9589e-06, 1.9477e-03,
        9.1890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.438

[Epoch: 60, batch: 82/207] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.7025e-05, 1.9646e-03, 1.3297e-03, 1.0832e-05, 6.0080e-03, 9.9067e-01,
        9.5541e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.918

[Epoch: 60, batch: 123/207] total loss per batch: 0.717
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6620, 0.0086, 0.0111, 0.0089, 0.2951, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 60, batch: 164/207] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.4647e-02, 5.0111e-03, 9.3818e-01, 2.0193e-04, 5.1005e-03, 5.9736e-05,
        6.8033e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.107

[Epoch: 60, batch: 205/207] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5073e-01, 4.6349e-06, 1.4019e-01, 9.0441e-03, 1.8409e-05, 3.5770e-06,
        7.8433e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.396

[Epoch: 61, batch: 41/207] total loss per batch: 0.725
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([5.8947e-02, 4.5734e-03, 9.0946e-02, 6.8209e-06, 4.5396e-06, 2.8185e-03,
        8.4270e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.411

[Epoch: 61, batch: 82/207] total loss per batch: 0.751
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.0184e-06, 3.4661e-03, 2.1207e-03, 8.7056e-06, 1.2442e-02, 9.8195e-01,
        1.3951e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 61, batch: 123/207] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0076, 0.5916, 0.0067, 0.0092, 0.0077, 0.3737, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 61, batch: 164/207] total loss per batch: 0.753
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0830e-02, 1.7070e-03, 9.3994e-01, 8.2809e-05, 4.3921e-03, 1.4199e-05,
        3.0294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.149

[Epoch: 61, batch: 205/207] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0040e-01, 2.3027e-06, 1.7437e-01, 2.5184e-02, 1.0138e-05, 1.8506e-06,
        3.4344e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.265

[Epoch: 62, batch: 41/207] total loss per batch: 0.722
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0275e-02, 5.4469e-03, 6.9805e-02, 6.1841e-06, 5.5584e-06, 5.1258e-03,
        8.8934e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.390

[Epoch: 62, batch: 82/207] total loss per batch: 0.749
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.6116e-05, 4.8015e-03, 5.8065e-03, 1.9696e-05, 8.9872e-03, 9.8037e-01,
        3.7825e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.911

[Epoch: 62, batch: 123/207] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0094, 0.6448, 0.0085, 0.0076, 0.0073, 0.3199, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 62, batch: 164/207] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3750e-02, 2.3573e-03, 9.2685e-01, 5.0364e-05, 3.8123e-03, 9.4933e-06,
        3.1747e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 62, batch: 205/207] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4085e-01, 3.6524e-06, 1.3519e-01, 2.3942e-02, 2.9541e-06, 1.7199e-06,
        1.0897e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.301

[Epoch: 63, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8068e-02, 4.0321e-03, 4.9196e-02, 3.0684e-06, 1.8218e-06, 2.5215e-03,
        9.1618e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.424

[Epoch: 63, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5950e-05, 2.8528e-03, 2.6395e-03, 1.7796e-05, 8.4858e-03, 9.8599e-01,
        1.6893e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 63, batch: 123/207] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0117, 0.6340, 0.0099, 0.0088, 0.0085, 0.3232, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.000

[Epoch: 63, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1679e-02, 4.0121e-03, 9.2372e-01, 8.3122e-05, 6.2637e-03, 9.0548e-06,
        4.2328e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.149

[Epoch: 63, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1923e-01, 2.3629e-06, 1.5779e-01, 2.2961e-02, 4.9485e-06, 3.0224e-06,
        7.2398e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.352

[Epoch: 64, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8761e-02, 3.7359e-03, 5.5397e-02, 1.7927e-06, 7.1218e-06, 3.4869e-03,
        9.0861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.415

[Epoch: 64, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.5071e-06, 3.4659e-03, 4.3696e-03, 9.0034e-06, 8.4699e-03, 9.8368e-01,
        9.8465e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.911

[Epoch: 64, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0127, 0.6460, 0.0113, 0.0069, 0.0082, 0.3108, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 64, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.5941e-02, 2.5074e-03, 9.2416e-01, 5.6246e-05, 4.8924e-03, 1.4016e-05,
        2.4299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.130

[Epoch: 64, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3001e-01, 1.3490e-06, 1.4371e-01, 2.6270e-02, 2.3875e-06, 2.0255e-06,
        5.9729e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.318

[Epoch: 65, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2964e-02, 6.6575e-03, 4.8863e-02, 1.9040e-06, 2.0483e-06, 2.2527e-03,
        9.1926e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.427

[Epoch: 65, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([9.5974e-06, 7.6162e-03, 2.9322e-03, 1.4923e-05, 1.3115e-02, 9.7631e-01,
        1.7627e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.909

[Epoch: 65, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0103, 0.6726, 0.0125, 0.0061, 0.0071, 0.2878, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 65, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0788e-02, 2.5180e-03, 9.4028e-01, 7.9838e-05, 3.3184e-03, 1.4375e-05,
        3.0007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.118

[Epoch: 65, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2001e-01, 5.7413e-06, 1.5320e-01, 2.6769e-02, 5.6017e-06, 2.7783e-06,
        6.8372e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 66, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2049e-02, 3.8124e-03, 6.4520e-02, 1.1355e-06, 4.0278e-06, 3.5747e-03,
        8.9604e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.442

[Epoch: 66, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5556e-05, 4.2892e-03, 5.2087e-03, 1.6106e-05, 9.3973e-03, 9.8107e-01,
        1.7741e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.896

[Epoch: 66, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0126, 0.6188, 0.0093, 0.0090, 0.0066, 0.3399, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 66, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.7105e-02, 1.5205e-03, 9.4720e-01, 3.9292e-05, 2.4017e-03, 5.9127e-06,
        1.7259e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.146

[Epoch: 66, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6247e-01, 1.8793e-06, 1.2025e-01, 1.7262e-02, 3.9460e-06, 2.6727e-06,
        1.0777e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.364

[Epoch: 67, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.8269e-02, 9.4991e-03, 6.8374e-02, 2.6463e-06, 5.7285e-06, 3.5378e-03,
        8.7031e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.385

[Epoch: 67, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.9059e-06, 3.1127e-03, 2.7692e-03, 1.0535e-05, 1.2665e-02, 9.8143e-01,
        1.7404e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.921

[Epoch: 67, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6478, 0.0128, 0.0055, 0.0072, 0.3120, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 67, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.8001e-02, 2.5925e-03, 9.4244e-01, 3.0204e-05, 3.6108e-03, 5.5557e-06,
        3.3223e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 67, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7780e-01, 1.8130e-06, 1.9625e-01, 2.5941e-02, 4.7160e-06, 1.2956e-06,
        8.5076e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.339

[Epoch: 68, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6082e-02, 3.7967e-03, 9.0765e-02, 2.5320e-06, 6.3250e-06, 4.0700e-03,
        8.7528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.427

[Epoch: 68, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.3676e-06, 1.8765e-03, 4.2117e-03, 1.2383e-05, 1.7948e-02, 9.7594e-01,
        2.2803e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 68, batch: 123/207] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0134, 0.6298, 0.0081, 0.0060, 0.0082, 0.3311, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 68, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3643e-02, 2.3106e-03, 9.3680e-01, 3.9752e-05, 4.3220e-03, 6.2406e-06,
        2.8837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.116

[Epoch: 68, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5771e-01, 1.6079e-06, 1.2837e-01, 1.3908e-02, 4.2226e-06, 1.5348e-06,
        1.0034e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.337

[Epoch: 69, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.0063e-02, 3.4801e-03, 3.1846e-02, 3.1017e-06, 2.3740e-06, 2.5572e-03,
        9.4205e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.449

[Epoch: 69, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.8722e-06, 4.2635e-03, 2.4248e-03, 9.9835e-06, 6.6306e-03, 9.8666e-01,
        3.7920e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.914

[Epoch: 69, batch: 123/207] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0089, 0.6230, 0.0101, 0.0051, 0.0052, 0.3440, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.000

[Epoch: 69, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5184e-02, 3.2100e-03, 9.3424e-01, 4.1488e-05, 3.7115e-03, 1.1091e-05,
        3.5997e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 69, batch: 205/207] total loss per batch: 0.689
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2968e-01, 1.3273e-06, 1.4407e-01, 2.6234e-02, 5.0149e-06, 3.7736e-06,
        7.7706e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.285

[Epoch: 70, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.6103e-02, 3.7015e-03, 6.6532e-02, 1.1245e-06, 2.4821e-06, 1.9768e-03,
        8.9168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.398

[Epoch: 70, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.0220e-06, 2.5560e-03, 5.6310e-03, 1.0868e-05, 8.8382e-03, 9.8295e-01,
        1.2409e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.914

[Epoch: 70, batch: 123/207] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0106, 0.6901, 0.0136, 0.0091, 0.0084, 0.2645, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 70, batch: 164/207] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8699e-02, 1.8079e-03, 9.3147e-01, 6.6271e-05, 4.0428e-03, 1.1494e-05,
        3.8998e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.119

[Epoch: 70, batch: 205/207] total loss per batch: 0.689
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0378e-01, 4.1696e-06, 1.7857e-01, 1.7627e-02, 6.0084e-06, 2.9741e-06,
        1.0853e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.313

[Epoch: 71, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5228e-02, 4.6728e-03, 5.0806e-02, 5.7895e-06, 2.6460e-06, 2.8510e-03,
        9.0643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 71, batch: 82/207] total loss per batch: 0.748
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.0864e-05, 5.1240e-03, 2.2669e-03, 1.4167e-05, 9.3774e-03, 9.8320e-01,
        9.7155e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.903

[Epoch: 71, batch: 123/207] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0072, 0.6656, 0.0095, 0.0063, 0.0063, 0.3007, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.000

[Epoch: 71, batch: 164/207] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4030e-02, 3.7507e-03, 9.2348e-01, 4.4205e-05, 5.3658e-03, 8.6162e-06,
        3.3232e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 71, batch: 205/207] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5028e-01, 3.2609e-06, 1.3212e-01, 1.7569e-02, 1.0401e-05, 1.0304e-05,
        8.0240e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.372

[Epoch: 72, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4428e-02, 4.3936e-03, 1.0598e-01, 2.8175e-06, 8.2285e-06, 2.8908e-03,
        8.5230e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.399

[Epoch: 72, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.6984e-06, 1.5178e-03, 2.3551e-03, 9.7628e-06, 8.1451e-03, 9.8797e-01,
        1.4603e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 72, batch: 123/207] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0103, 0.6277, 0.0081, 0.0045, 0.0067, 0.3382, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 72, batch: 164/207] total loss per batch: 0.750
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([9.0190e-02, 2.8977e-03, 8.9728e-01, 3.4616e-05, 4.4820e-03, 7.5048e-06,
        5.1050e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.113

[Epoch: 72, batch: 205/207] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0973e-01, 1.3719e-06, 1.7436e-01, 1.5893e-02, 6.5130e-06, 1.5112e-06,
        1.0778e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.330

[Epoch: 73, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.8075e-02, 4.4974e-03, 4.5892e-02, 4.5161e-06, 4.6380e-06, 3.4503e-03,
        8.9808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 73, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5417e-05, 4.6387e-03, 4.2528e-03, 1.0296e-05, 1.5526e-02, 9.7556e-01,
        1.2285e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 73, batch: 123/207] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0131, 0.5737, 0.0126, 0.0123, 0.0121, 0.3703, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 73, batch: 164/207] total loss per batch: 0.750
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([9.3711e-02, 5.7623e-03, 8.9036e-01, 5.2566e-05, 4.8420e-03, 1.1036e-05,
        5.2642e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.150

[Epoch: 73, batch: 205/207] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7764e-01, 2.4978e-06, 1.0346e-01, 1.8880e-02, 5.6140e-06, 2.5548e-06,
        2.9736e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.290

[Epoch: 74, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.5437e-02, 3.8868e-03, 5.7016e-02, 1.8091e-06, 2.0567e-06, 3.0026e-03,
        9.1065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 74, batch: 82/207] total loss per batch: 0.747
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.4229e-06, 2.4248e-03, 5.2004e-03, 1.7549e-05, 8.7205e-03, 9.8362e-01,
        3.4702e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 74, batch: 123/207] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6696, 0.0113, 0.0068, 0.0081, 0.2902, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 74, batch: 164/207] total loss per batch: 0.750
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5640e-02, 3.2357e-03, 9.2991e-01, 1.2601e-04, 5.3807e-03, 1.5659e-05,
        5.6930e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.121

[Epoch: 74, batch: 205/207] total loss per batch: 0.689
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7919e-01, 1.3932e-06, 2.0210e-01, 1.8692e-02, 6.8341e-06, 3.3056e-06,
        1.0928e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.294

[Epoch: 75, batch: 41/207] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2753e-02, 3.7623e-03, 3.3208e-02, 1.2572e-06, 6.9225e-07, 2.3059e-03,
        9.3797e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.385

[Epoch: 75, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.7932e-05, 4.2541e-03, 5.6792e-03, 8.0343e-06, 1.2279e-02, 9.7776e-01,
        1.8126e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.920

[Epoch: 75, batch: 123/207] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0106, 0.6069, 0.0108, 0.0085, 0.0070, 0.3519, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.003

[Epoch: 75, batch: 164/207] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2386e-02, 1.0958e-03, 9.3982e-01, 5.6366e-05, 3.4061e-03, 2.3711e-05,
        3.2126e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.121

[Epoch: 75, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5001e-01, 2.8222e-06, 1.3129e-01, 1.8673e-02, 7.5674e-06, 5.0243e-06,
        5.1674e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.355

[Epoch: 76, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5826e-02, 3.7401e-03, 8.8169e-02, 4.7564e-06, 2.4393e-06, 3.1691e-03,
        8.6909e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.421

[Epoch: 76, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.2115e-05, 1.5771e-03, 8.2900e-03, 1.4409e-05, 1.8026e-02, 9.7207e-01,
        3.1796e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.916

[Epoch: 76, batch: 123/207] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0080, 0.6932, 0.0067, 0.0062, 0.0063, 0.2758, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.000

[Epoch: 76, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.6278e-02, 2.2771e-03, 9.4419e-01, 4.3958e-05, 3.4441e-03, 1.2442e-05,
        3.7574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.143

[Epoch: 76, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5472e-01, 1.4407e-06, 1.2946e-01, 1.5800e-02, 8.2128e-06, 5.3157e-06,
        1.1073e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.338

[Epoch: 77, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.5398e-02, 7.7927e-03, 7.3139e-02, 7.6159e-06, 6.8111e-06, 5.7569e-03,
        8.8790e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.437

[Epoch: 77, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.9876e-05, 3.6664e-03, 3.5395e-03, 1.1550e-05, 1.8806e-02, 9.7395e-01,
        2.5284e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.891

[Epoch: 77, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0091, 0.6439, 0.0096, 0.0056, 0.0054, 0.3232, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 77, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.2116e-02, 2.4532e-03, 9.1689e-01, 2.9527e-05, 4.4017e-03, 5.0553e-06,
        4.1015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 77, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0997e-01, 1.1264e-06, 1.6407e-01, 2.5937e-02, 3.1075e-06, 1.6095e-06,
        1.0135e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.309

[Epoch: 78, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.6617e-02, 3.7746e-03, 7.0773e-02, 3.6533e-06, 2.5801e-06, 3.0220e-03,
        8.8581e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.427

[Epoch: 78, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.7823e-06, 5.7276e-03, 3.2871e-03, 1.5074e-05, 5.5166e-03, 9.8544e-01,
        2.1111e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.916

[Epoch: 78, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0112, 0.5793, 0.0116, 0.0084, 0.0068, 0.3790, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 78, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3939e-02, 2.7513e-03, 9.2606e-01, 4.7534e-05, 3.9723e-03, 7.0650e-06,
        3.2267e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.121

[Epoch: 78, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3471e-01, 9.4496e-07, 1.4979e-01, 1.5485e-02, 4.4224e-06, 2.1087e-06,
        7.2693e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.376

[Epoch: 79, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.1110e-02, 2.2859e-03, 4.3189e-02, 2.8733e-06, 1.7574e-06, 3.1618e-03,
        9.3025e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.415

[Epoch: 79, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0650e-05, 3.8581e-03, 4.7303e-03, 4.4486e-06, 7.3044e-03, 9.8409e-01,
        1.8380e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 79, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0077, 0.6627, 0.0082, 0.0044, 0.0059, 0.3076, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.001

[Epoch: 79, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6025e-02, 2.2136e-03, 9.3513e-01, 4.8366e-05, 3.1962e-03, 1.0605e-05,
        3.3775e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 79, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3616e-01, 9.2480e-07, 1.3727e-01, 2.6551e-02, 4.0080e-06, 3.0171e-06,
        1.1158e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.311

[Epoch: 80, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.0190e-02, 3.0057e-03, 4.5807e-02, 1.4361e-06, 4.2714e-07, 1.5800e-03,
        9.2942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.403

[Epoch: 80, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.3537e-06, 2.0872e-03, 2.0915e-03, 7.3161e-06, 6.8527e-03, 9.8895e-01,
        1.3667e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.920

[Epoch: 80, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0077, 0.6787, 0.0102, 0.0070, 0.0068, 0.2866, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 80, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7787e-02, 1.9380e-03, 9.3454e-01, 4.8412e-05, 3.0796e-03, 9.8546e-06,
        2.6020e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.119

[Epoch: 80, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8197e-01, 4.0195e-06, 1.9819e-01, 1.9810e-02, 1.2052e-05, 4.5722e-06,
        9.6056e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.295

[Epoch: 81, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.0586e-02, 3.4902e-03, 6.1349e-02, 4.4490e-06, 1.7516e-06, 2.7130e-03,
        8.9186e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.394

[Epoch: 81, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.6202e-06, 3.6174e-03, 2.4891e-03, 7.7698e-06, 8.6365e-03, 9.8524e-01,
        9.5137e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.895

[Epoch: 81, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0091, 0.6089, 0.0061, 0.0039, 0.0050, 0.3635, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 81, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9325e-02, 3.2684e-03, 9.3067e-01, 6.0033e-05, 3.3524e-03, 6.6549e-06,
        3.3184e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.152

[Epoch: 81, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8005e-01, 2.2564e-06, 1.0526e-01, 1.4667e-02, 3.6640e-06, 3.4994e-06,
        1.0536e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.356

[Epoch: 82, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0684e-02, 6.0103e-03, 7.4252e-02, 8.0807e-06, 2.4451e-06, 4.5415e-03,
        8.8450e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 82, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.9572e-06, 3.1546e-03, 6.2175e-03, 7.3344e-06, 1.2761e-02, 9.7785e-01,
        1.0897e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 82, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0077, 0.6773, 0.0114, 0.0062, 0.0097, 0.2838, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 82, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.5572e-02, 3.8503e-03, 9.2358e-01, 4.9592e-05, 2.8709e-03, 9.2305e-06,
        4.0729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 82, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1241e-01, 2.5651e-06, 1.6061e-01, 2.6956e-02, 4.7439e-06, 2.9199e-06,
        1.3477e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.314

[Epoch: 83, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.6359e-02, 2.8071e-03, 6.2778e-02, 2.9082e-06, 1.5873e-06, 2.9133e-03,
        8.9514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.390

[Epoch: 83, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.2236e-06, 8.3422e-03, 4.3181e-03, 1.4757e-05, 2.3980e-02, 9.6333e-01,
        2.6877e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.920

[Epoch: 83, batch: 123/207] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0136, 0.6381, 0.0105, 0.0072, 0.0087, 0.3170, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 83, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4186e-02, 3.2869e-03, 9.2387e-01, 7.2558e-05, 3.9927e-03, 1.4688e-05,
        4.5809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 83, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1974e-01, 1.9313e-06, 1.5813e-01, 2.2094e-02, 5.8694e-06, 5.2656e-06,
        2.7420e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.402

[Epoch: 84, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2816e-02, 2.5260e-03, 3.4062e-02, 1.8624e-06, 8.4615e-07, 2.4484e-03,
        9.3815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.427

[Epoch: 84, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.1226e-06, 1.1417e-03, 2.6259e-03, 4.6436e-06, 4.4747e-03, 9.9175e-01,
        1.0650e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.913

[Epoch: 84, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0126, 0.6127, 0.0162, 0.0089, 0.0084, 0.3346, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.001

[Epoch: 84, batch: 164/207] total loss per batch: 0.749
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4943e-02, 1.8024e-03, 9.3702e-01, 6.3544e-05, 2.7359e-03, 7.3953e-06,
        3.4236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.119

[Epoch: 84, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2270e-01, 2.2997e-06, 1.4847e-01, 2.8813e-02, 5.3305e-06, 8.6573e-06,
        8.9585e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.317

[Epoch: 85, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.9195e-02, 3.7964e-03, 9.3023e-02, 1.5364e-06, 1.1728e-06, 2.5910e-03,
        8.6139e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.437

[Epoch: 85, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.6448e-06, 5.4450e-03, 3.7712e-03, 7.1540e-06, 7.3765e-03, 9.8339e-01,
        1.2611e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.924

[Epoch: 85, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0074, 0.6951, 0.0084, 0.0049, 0.0062, 0.2743, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 85, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5337e-02, 2.1305e-03, 9.3715e-01, 6.2340e-05, 2.8622e-03, 1.1433e-05,
        2.4434e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.122

[Epoch: 85, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6242e-01, 1.9425e-06, 1.2301e-01, 1.4553e-02, 3.9278e-06, 2.5857e-06,
        5.8356e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.351

[Epoch: 86, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7490e-02, 3.8784e-03, 7.3178e-02, 3.1894e-06, 4.6995e-06, 3.7740e-03,
        8.9167e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.378

[Epoch: 86, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.0263e-05, 4.9501e-03, 3.4624e-03, 7.9436e-06, 9.7877e-03, 9.8178e-01,
        2.7545e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 86, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0105, 0.6483, 0.0080, 0.0053, 0.0054, 0.3200, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 86, batch: 164/207] total loss per batch: 0.748
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1088e-02, 2.0297e-03, 9.4056e-01, 5.8090e-05, 4.2638e-03, 4.8082e-06,
        1.9932e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.163

[Epoch: 86, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3854e-01, 1.3676e-06, 1.4605e-01, 1.5394e-02, 5.1432e-06, 1.3209e-06,
        7.0556e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.317

[Epoch: 87, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([5.3843e-02, 4.7448e-03, 8.4455e-02, 2.8459e-06, 2.9657e-06, 6.5049e-03,
        8.5045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 87, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.3981e-06, 3.7413e-03, 5.9335e-03, 1.4859e-05, 1.0708e-02, 9.7959e-01,
        1.8925e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 87, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0094, 0.6258, 0.0123, 0.0068, 0.0072, 0.3349, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 -0.002

[Epoch: 87, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8919e-02, 3.6909e-03, 9.1891e-01, 8.6839e-05, 4.4254e-03, 8.3968e-06,
        3.9572e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 87, batch: 205/207] total loss per batch: 0.688
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2943e-01, 1.2270e-06, 1.4803e-01, 2.2525e-02, 2.6952e-06, 2.3631e-06,
        5.5889e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.379

[Epoch: 88, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.6926e-02, 1.4246e-03, 5.0907e-02, 1.8715e-06, 8.5659e-07, 2.1123e-03,
        9.2863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.396

[Epoch: 88, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3327e-06, 2.9890e-03, 2.4665e-03, 6.9390e-06, 1.0591e-02, 9.8394e-01,
        4.7503e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.931

[Epoch: 88, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0088, 0.6391, 0.0123, 0.0081, 0.0051, 0.3239, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 88, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1729e-02, 2.3484e-03, 9.4024e-01, 5.0694e-05, 2.4232e-03, 1.4565e-05,
        3.1907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 88, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9528e-01, 1.0966e-06, 1.9122e-01, 1.3488e-02, 4.1958e-06, 5.1861e-06,
        8.1843e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.304

[Epoch: 89, batch: 41/207] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1308e-02, 2.0533e-03, 5.5309e-02, 1.8645e-06, 1.7481e-06, 2.5887e-03,
        9.0874e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.417

[Epoch: 89, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1658e-05, 4.9432e-03, 5.6604e-03, 1.1709e-05, 1.0694e-02, 9.7868e-01,
        2.1138e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.904

[Epoch: 89, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0077, 0.6607, 0.0087, 0.0056, 0.0064, 0.3079, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 89, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2272e-02, 2.6100e-03, 9.3866e-01, 9.6776e-05, 3.1153e-03, 1.4801e-05,
        3.2283e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 89, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3082e-01, 7.0255e-06, 1.4064e-01, 2.8509e-02, 5.1097e-06, 6.1724e-06,
        1.3065e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.314

[Epoch: 90, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8648e-02, 4.2544e-03, 5.5923e-02, 6.2192e-06, 1.9100e-06, 2.8445e-03,
        9.0832e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.400

[Epoch: 90, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.8145e-06, 3.6036e-03, 2.0477e-03, 7.9504e-06, 1.3256e-02, 9.8108e-01,
        2.5891e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.911

[Epoch: 90, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0087, 0.6486, 0.0067, 0.0036, 0.0047, 0.3240, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.000

[Epoch: 90, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2070e-02, 2.7106e-03, 9.2724e-01, 8.8259e-05, 3.8794e-03, 1.2234e-05,
        3.9979e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 90, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2754e-01, 3.6489e-06, 1.5584e-01, 1.6578e-02, 6.8626e-06, 6.8288e-06,
        2.0775e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.347

[Epoch: 91, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0292e-02, 7.9934e-03, 7.5378e-02, 1.1329e-05, 3.6215e-06, 5.9578e-03,
        8.8036e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.387

[Epoch: 91, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.3858e-06, 4.1384e-03, 4.2773e-03, 1.2318e-05, 1.0254e-02, 9.8130e-01,
        4.8704e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.921

[Epoch: 91, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0113, 0.6224, 0.0100, 0.0055, 0.0063, 0.3417, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.004

[Epoch: 91, batch: 164/207] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.1084e-02, 4.3662e-03, 9.1738e-01, 7.7075e-05, 2.8047e-03, 7.1049e-06,
        4.2771e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.128

[Epoch: 91, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8228e-01, 1.1723e-06, 1.0087e-01, 1.6832e-02, 2.4385e-06, 1.4656e-06,
        1.2394e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 92, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([6.1843e-02, 4.4463e-03, 7.1653e-02, 4.8998e-06, 2.0287e-06, 3.7468e-03,
        8.5830e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.423

[Epoch: 92, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.7178e-06, 3.2877e-03, 3.7477e-03, 1.3871e-05, 1.0243e-02, 9.8270e-01,
        1.2827e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.930

[Epoch: 92, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0100, 0.6279, 0.0143, 0.0089, 0.0084, 0.3257, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 92, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8432e-02, 4.5427e-03, 9.2732e-01, 1.1907e-04, 4.4725e-03, 1.2123e-05,
        5.1064e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 92, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6158e-01, 2.6012e-06, 2.1331e-01, 2.5072e-02, 4.5502e-06, 9.4429e-06,
        1.4166e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.305

[Epoch: 93, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4071e-02, 3.6613e-03, 7.5787e-02, 5.6001e-06, 8.7391e-07, 3.2692e-03,
        8.9320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.432

[Epoch: 93, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8553e-06, 2.8157e-03, 2.7264e-03, 7.2325e-06, 1.1721e-02, 9.8272e-01,
        1.9315e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 93, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0104, 0.6501, 0.0129, 0.0076, 0.0068, 0.3084, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.001

[Epoch: 93, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.5005e-02, 1.9613e-03, 9.4757e-01, 8.7053e-05, 3.1795e-03, 1.3848e-05,
        2.1789e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.116

[Epoch: 93, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7488e-01, 2.1817e-06, 1.0953e-01, 1.5566e-02, 4.0051e-06, 2.7371e-06,
        1.4617e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.384

[Epoch: 94, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6002e-02, 3.2880e-03, 5.5931e-02, 4.2579e-06, 2.1883e-06, 3.1225e-03,
        9.1165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.394

[Epoch: 94, batch: 82/207] total loss per batch: 0.746
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.3422e-05, 2.6472e-03, 2.8666e-03, 4.7841e-06, 6.7426e-03, 9.8772e-01,
        1.0317e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.916

[Epoch: 94, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0081, 0.6563, 0.0067, 0.0046, 0.0053, 0.3157, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 94, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.1441e-02, 2.0774e-03, 9.2031e-01, 8.0082e-05, 3.5172e-03, 1.2929e-05,
        2.5583e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.116

[Epoch: 94, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2284e-01, 2.7561e-06, 1.5547e-01, 2.1676e-02, 2.8125e-06, 3.8822e-06,
        6.9014e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 95, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2195e-02, 2.4674e-03, 3.5382e-02, 2.2089e-06, 9.4439e-07, 2.1901e-03,
        9.3776e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.424

[Epoch: 95, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.4711e-06, 3.2569e-03, 7.2587e-03, 1.0995e-05, 6.8138e-03, 9.8265e-01,
        3.8184e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 95, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0125, 0.6521, 0.0101, 0.0049, 0.0057, 0.3123, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 95, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3922e-02, 1.1854e-03, 9.3788e-01, 3.6894e-05, 4.0467e-03, 7.8710e-06,
        2.9199e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 95, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5188e-01, 1.8835e-06, 1.3743e-01, 1.0671e-02, 4.5200e-06, 4.0684e-06,
        9.9698e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.297

[Epoch: 96, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.5906e-02, 2.1702e-03, 6.4461e-02, 4.5473e-06, 1.3231e-06, 3.3296e-03,
        9.0413e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.434

[Epoch: 96, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([8.7434e-06, 6.0039e-03, 4.9058e-03, 1.3225e-05, 1.7151e-02, 9.7192e-01,
        1.0893e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.923

[Epoch: 96, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0110, 0.6469, 0.0096, 0.0075, 0.0071, 0.3131, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 96, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1657e-02, 3.0367e-03, 9.3949e-01, 3.8648e-05, 2.2654e-03, 6.5656e-06,
        3.5017e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 96, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9343e-01, 3.4007e-06, 1.7236e-01, 3.4178e-02, 5.4784e-06, 2.9445e-06,
        1.9914e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.323

[Epoch: 97, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1746e-02, 4.4987e-03, 8.7603e-02, 1.8249e-06, 1.5542e-06, 3.8319e-03,
        8.7232e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.390

[Epoch: 97, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5388e-05, 3.5101e-03, 3.0871e-03, 1.5294e-05, 8.4848e-03, 9.8489e-01,
        1.6005e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.945

[Epoch: 97, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0094, 0.6458, 0.0094, 0.0052, 0.0065, 0.3202, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 97, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9267e-02, 3.8766e-03, 9.1777e-01, 7.8165e-05, 6.0490e-03, 1.2504e-05,
        2.9465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.120

[Epoch: 97, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2668e-01, 3.1813e-06, 1.5850e-01, 1.4797e-02, 3.3474e-06, 8.4127e-06,
        8.2639e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.352

[Epoch: 98, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7488e-02, 2.5308e-03, 5.2963e-02, 2.2471e-06, 1.0154e-06, 2.6931e-03,
        9.0432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.439

[Epoch: 98, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1203e-05, 6.8789e-03, 6.4064e-03, 7.5168e-06, 1.7470e-02, 9.6922e-01,
        5.1105e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.910

[Epoch: 98, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0085, 0.6561, 0.0085, 0.0054, 0.0048, 0.3137, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 98, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9727e-02, 3.7662e-03, 9.1807e-01, 6.8544e-05, 3.5009e-03, 1.3467e-05,
        4.8508e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 98, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3821e-01, 6.6950e-06, 1.3544e-01, 2.6317e-02, 4.9257e-06, 6.0241e-06,
        1.7072e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.278

[Epoch: 99, batch: 41/207] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3496e-02, 5.7515e-03, 6.4709e-02, 7.9698e-06, 1.8631e-06, 3.9569e-03,
        8.9208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.393

[Epoch: 99, batch: 82/207] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.1980e-05, 4.0124e-03, 4.8698e-03, 1.0308e-05, 5.4664e-03, 9.8563e-01,
        2.3968e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.907

[Epoch: 99, batch: 123/207] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0074, 0.6262, 0.0091, 0.0035, 0.0035, 0.3481, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.002

[Epoch: 99, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.6839e-02, 3.0586e-03, 9.2153e-01, 6.8085e-05, 3.5513e-03, 1.1277e-05,
        4.9374e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 99, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3315e-01, 3.8506e-06, 1.5196e-01, 1.4864e-02, 4.2468e-06, 5.2558e-06,
        1.2667e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.406

[Epoch: 100, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6215e-02, 4.8742e-03, 6.4334e-02, 1.1336e-05, 2.5255e-06, 5.6490e-03,
        8.9891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.423

[Epoch: 100, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7605e-06, 3.5942e-03, 2.0232e-03, 4.7819e-06, 9.0541e-03, 9.8532e-01,
        9.4603e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 100, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0164, 0.6006, 0.0121, 0.0066, 0.0091, 0.3517, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 100, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3273e-02, 3.6042e-03, 9.2660e-01, 4.8280e-05, 3.5361e-03, 6.2495e-06,
        2.9273e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.124

[Epoch: 100, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3934e-01, 3.4533e-06, 1.4098e-01, 1.9663e-02, 2.5519e-06, 2.7212e-06,
        6.1807e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.290

[Epoch: 101, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4060e-02, 2.5316e-03, 4.1979e-02, 2.0009e-06, 8.0711e-07, 2.5430e-03,
        9.2888e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.436

[Epoch: 101, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.6900e-06, 3.7314e-03, 3.5484e-03, 8.3531e-06, 1.0441e-02, 9.8226e-01,
        9.0806e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 101, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0082, 0.6801, 0.0099, 0.0050, 0.0069, 0.2869, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 101, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9605e-02, 3.2554e-03, 9.2954e-01, 4.6647e-05, 3.7025e-03, 1.0459e-05,
        3.8387e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.123

[Epoch: 101, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2858e-01, 2.0839e-06, 1.5245e-01, 1.8951e-02, 6.3082e-06, 3.5901e-06,
        1.4224e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.330

[Epoch: 102, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1841e-02, 2.4154e-03, 7.1464e-02, 2.3536e-06, 6.9660e-07, 2.1210e-03,
        8.9216e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.408

[Epoch: 102, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.6212e-06, 2.4481e-03, 2.2128e-03, 2.9784e-06, 7.4924e-03, 9.8784e-01,
        6.7178e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 102, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.6616, 0.0098, 0.0044, 0.0057, 0.3043, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 102, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0714e-02, 3.1350e-03, 9.2908e-01, 5.9471e-05, 3.4681e-03, 1.2762e-05,
        3.5287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.123

[Epoch: 102, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1442e-01, 3.5546e-06, 1.6225e-01, 2.3301e-02, 3.7203e-06, 6.3021e-06,
        9.6686e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 103, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5027e-02, 4.1173e-03, 6.9296e-02, 3.5106e-06, 1.4121e-06, 3.2835e-03,
        8.8827e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 103, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.8571e-06, 4.4054e-03, 4.0795e-03, 4.5613e-06, 1.1554e-02, 9.7995e-01,
        9.1153e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 103, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0118, 0.6395, 0.0110, 0.0048, 0.0059, 0.3231, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 103, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3266e-02, 3.3966e-03, 9.3678e-01, 3.2402e-05, 3.7005e-03, 6.0760e-06,
        2.8205e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 103, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3190e-01, 2.4661e-06, 1.4741e-01, 2.0672e-02, 2.3386e-06, 4.3270e-06,
        7.7296e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.350

[Epoch: 104, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7905e-02, 3.8358e-03, 7.0361e-02, 3.2916e-06, 1.1601e-06, 2.8791e-03,
        8.9501e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.400

[Epoch: 104, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.4132e-06, 3.4184e-03, 3.5399e-03, 6.7438e-06, 1.1059e-02, 9.8197e-01,
        1.0808e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 104, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.6433, 0.0102, 0.0051, 0.0055, 0.3224, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 104, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4538e-02, 3.2478e-03, 9.2511e-01, 2.8414e-05, 3.8037e-03, 6.8595e-06,
        3.2647e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.130

[Epoch: 104, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4776e-01, 1.5269e-06, 1.3268e-01, 1.9551e-02, 1.3895e-06, 2.3096e-06,
        6.3986e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 105, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8576e-02, 3.2168e-03, 5.7716e-02, 2.4870e-06, 1.0193e-06, 3.5216e-03,
        9.0697e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.409

[Epoch: 105, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.3951e-06, 3.7962e-03, 3.0762e-03, 3.8774e-06, 1.0634e-02, 9.8249e-01,
        6.9556e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 105, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0112, 0.6328, 0.0096, 0.0046, 0.0051, 0.3331, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 105, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7727e-02, 3.7992e-03, 9.3119e-01, 3.8067e-05, 3.7859e-03, 6.7568e-06,
        3.4481e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 105, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1335e-01, 1.5434e-06, 1.6602e-01, 2.0629e-02, 1.7135e-06, 2.2206e-06,
        4.5766e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.325

[Epoch: 106, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7987e-02, 2.7727e-03, 6.2104e-02, 2.0553e-06, 5.2067e-07, 2.7983e-03,
        9.0433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 106, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.6658e-06, 2.6000e-03, 3.2252e-03, 2.8894e-06, 7.9544e-03, 9.8621e-01,
        7.0427e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 106, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0078, 0.6685, 0.0077, 0.0039, 0.0039, 0.3059, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 106, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2784e-02, 2.0871e-03, 9.2894e-01, 4.0549e-05, 2.9801e-03, 6.3144e-06,
        3.1656e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 106, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4359e-01, 1.6037e-06, 1.3782e-01, 1.8582e-02, 1.7895e-06, 2.0591e-06,
        4.5542e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.338

[Epoch: 107, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4353e-02, 4.0246e-03, 6.9043e-02, 3.0161e-06, 1.2638e-06, 3.8601e-03,
        8.8872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.396

[Epoch: 107, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.5674e-06, 4.1365e-03, 2.8098e-03, 2.0229e-06, 9.9903e-03, 9.8306e-01,
        5.4120e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.936

[Epoch: 107, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6341, 0.0089, 0.0035, 0.0038, 0.3374, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 107, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5320e-02, 3.4416e-03, 9.3485e-01, 2.4151e-05, 3.1120e-03, 4.6970e-06,
        3.2469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.136

[Epoch: 107, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1149e-01, 2.3695e-06, 1.6552e-01, 2.2972e-02, 2.6601e-06, 1.9446e-06,
        4.9099e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.336

[Epoch: 108, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2305e-02, 4.3675e-03, 6.1848e-02, 5.9353e-06, 1.5378e-06, 3.1423e-03,
        8.9833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.402

[Epoch: 108, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.6930e-06, 4.0275e-03, 3.5492e-03, 2.6454e-06, 7.7443e-03, 9.8467e-01,
        3.7146e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.914

[Epoch: 108, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0098, 0.6174, 0.0106, 0.0056, 0.0049, 0.3481, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 108, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.5193e-02, 3.2425e-03, 9.1391e-01, 6.1393e-05, 3.3272e-03, 4.9779e-06,
        4.2655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 108, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4181e-01, 1.6734e-06, 1.3721e-01, 2.0970e-02, 2.1726e-06, 2.9787e-06,
        9.0352e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.314

[Epoch: 109, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0332e-02, 3.1940e-03, 6.7968e-02, 2.8396e-06, 7.8596e-07, 2.9150e-03,
        8.9559e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 109, batch: 82/207] total loss per batch: 0.743
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9180e-06, 2.1963e-03, 1.9816e-03, 3.8822e-06, 1.2161e-02, 9.8365e-01,
        4.3351e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.945

[Epoch: 109, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0111, 0.6571, 0.0139, 0.0047, 0.0063, 0.3023, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 109, batch: 164/207] total loss per batch: 0.746
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4139e-02, 4.2993e-03, 9.3347e-01, 4.5500e-05, 3.6165e-03, 7.7210e-06,
        4.4199e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 109, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2618e-01, 2.7577e-06, 1.4997e-01, 2.3821e-02, 4.9244e-06, 6.4268e-06,
        1.2786e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.328

[Epoch: 110, batch: 41/207] total loss per batch: 0.717
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1356e-02, 3.0889e-03, 4.1950e-02, 2.1609e-06, 4.8790e-07, 1.9374e-03,
        9.2167e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.428

[Epoch: 110, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9253e-06, 4.6814e-03, 5.7344e-03, 4.3373e-06, 8.8420e-03, 9.8073e-01,
        1.2847e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 110, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0086, 0.6495, 0.0123, 0.0063, 0.0068, 0.3116, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 110, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.9534e-02, 3.1814e-03, 9.3976e-01, 5.2981e-05, 4.5616e-03, 1.9132e-05,
        2.8917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.111

[Epoch: 110, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5007e-01, 1.6214e-06, 1.4054e-01, 9.3736e-03, 3.0774e-06, 5.2686e-06,
        5.5875e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.338

[Epoch: 111, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1908e-02, 3.2158e-03, 6.6141e-02, 4.2134e-06, 2.4476e-06, 2.9288e-03,
        8.9580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.451

[Epoch: 111, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.8582e-06, 1.5507e-03, 2.4442e-03, 4.8745e-06, 1.6248e-02, 9.7975e-01,
        1.0394e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 111, batch: 123/207] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0081, 0.6896, 0.0084, 0.0030, 0.0045, 0.2843, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 111, batch: 164/207] total loss per batch: 0.747
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7091e-02, 3.0806e-03, 9.3491e-01, 3.6354e-05, 2.7795e-03, 7.3204e-06,
        2.0924e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 111, batch: 205/207] total loss per batch: 0.687
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3737e-01, 2.6752e-06, 1.4205e-01, 2.0549e-02, 2.7886e-06, 3.3012e-06,
        1.8302e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.359

[Epoch: 112, batch: 41/207] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8803e-02, 3.7497e-03, 7.4708e-02, 2.3277e-06, 1.6228e-06, 4.3238e-03,
        8.8841e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.429

[Epoch: 112, batch: 82/207] total loss per batch: 0.744
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.3729e-06, 6.9234e-03, 4.1505e-03, 6.8112e-06, 1.0081e-02, 9.7883e-01,
        1.1334e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.933

[Epoch: 112, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0118, 0.5939, 0.0116, 0.0057, 0.0052, 0.3681, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 112, batch: 164/207] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.8707e-02, 2.1753e-03, 9.2319e-01, 4.0792e-05, 3.4376e-03, 5.6716e-06,
        2.4407e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.150

[Epoch: 112, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1825e-01, 1.4465e-06, 1.6627e-01, 1.5471e-02, 1.8456e-06, 3.5507e-06,
        6.5595e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.334

[Epoch: 113, batch: 41/207] total loss per batch: 0.717
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0867e-02, 2.6000e-03, 4.8801e-02, 2.3430e-06, 1.2893e-06, 2.6215e-03,
        9.1511e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.405

[Epoch: 113, batch: 82/207] total loss per batch: 0.743
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.7479e-06, 4.1901e-03, 3.6066e-03, 8.6991e-06, 1.3508e-02, 9.7868e-01,
        1.1567e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 113, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0085, 0.6910, 0.0070, 0.0038, 0.0055, 0.2804, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 113, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9650e-02, 3.2789e-03, 9.2002e-01, 4.2872e-05, 4.1250e-03, 7.3096e-06,
        2.8796e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.149

[Epoch: 113, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2674e-01, 3.1462e-06, 1.4397e-01, 2.9272e-02, 3.7127e-06, 5.6104e-06,
        1.2453e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.295

[Epoch: 114, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6891e-02, 3.6741e-03, 6.6089e-02, 2.3710e-06, 1.5237e-06, 3.3467e-03,
        8.9999e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.390

[Epoch: 114, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.0765e-06, 2.7808e-03, 3.7015e-03, 3.9694e-06, 7.6879e-03, 9.8582e-01,
        2.0994e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.920

[Epoch: 114, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0086, 0.6104, 0.0106, 0.0044, 0.0039, 0.3591, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 114, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.1513e-02, 3.4853e-03, 9.1610e-01, 4.4601e-05, 4.3128e-03, 8.6006e-06,
        4.5386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.142

[Epoch: 114, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9840e-01, 3.6846e-06, 1.7573e-01, 2.5841e-02, 8.1832e-06, 5.5474e-06,
        1.2131e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.371

[Epoch: 115, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7558e-02, 4.5614e-03, 6.0181e-02, 5.9330e-06, 2.1508e-06, 3.1191e-03,
        8.9457e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.383

[Epoch: 115, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7648e-06, 3.8893e-03, 3.2314e-03, 6.3164e-06, 9.5293e-03, 9.8334e-01,
        7.2109e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.912

[Epoch: 115, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0081, 0.6715, 0.0088, 0.0044, 0.0041, 0.2994, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 115, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9389e-02, 3.1799e-03, 9.3118e-01, 3.5664e-05, 2.8458e-03, 6.4403e-06,
        3.3601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 115, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6886e-01, 2.6556e-06, 1.1134e-01, 1.9768e-02, 5.6144e-06, 8.0958e-06,
        1.3866e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.311

[Epoch: 116, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6813e-02, 4.6506e-03, 5.7435e-02, 3.0771e-06, 1.6662e-06, 3.6807e-03,
        9.0742e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.427

[Epoch: 116, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.3411e-06, 3.1681e-03, 2.8398e-03, 4.0099e-06, 1.1523e-02, 9.8246e-01,
        8.6381e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 116, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0103, 0.6112, 0.0138, 0.0054, 0.0057, 0.3500, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 116, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9000e-02, 3.3417e-03, 9.2950e-01, 4.4854e-05, 3.5601e-03, 7.0218e-06,
        4.5441e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.129

[Epoch: 116, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1277e-01, 1.8444e-06, 1.6588e-01, 2.1330e-02, 3.7767e-06, 4.7420e-06,
        9.6599e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 117, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.8617e-02, 3.9801e-03, 7.3989e-02, 3.3025e-06, 1.1868e-06, 2.7953e-03,
        8.8061e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.433

[Epoch: 117, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.6949e-06, 2.1680e-03, 2.9175e-03, 2.8930e-06, 6.9737e-03, 9.8793e-01,
        5.0602e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 117, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0103, 0.6506, 0.0115, 0.0052, 0.0052, 0.3127, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 117, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2726e-02, 2.7950e-03, 9.3905e-01, 3.3653e-05, 2.5984e-03, 6.5291e-06,
        2.7934e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 117, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4786e-01, 1.5572e-06, 1.3400e-01, 1.8123e-02, 3.2072e-06, 3.3982e-06,
        1.0763e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.345

[Epoch: 118, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.1576e-02, 2.1260e-03, 5.0398e-02, 1.0353e-06, 3.2943e-07, 2.6651e-03,
        9.2323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.428

[Epoch: 118, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8485e-06, 3.5301e-03, 3.7341e-03, 3.4230e-06, 9.5537e-03, 9.8318e-01,
        6.8186e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 118, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0099, 0.6558, 0.0124, 0.0053, 0.0055, 0.3072, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 118, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7659e-02, 2.3470e-03, 9.3447e-01, 2.0868e-05, 3.3578e-03, 4.5250e-06,
        2.1354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.147

[Epoch: 118, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0767e-01, 2.5835e-06, 1.6941e-01, 2.2901e-02, 3.5679e-06, 2.6347e-06,
        1.0971e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.304

[Epoch: 119, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7797e-02, 2.6024e-03, 6.3930e-02, 1.3104e-06, 8.7152e-07, 2.2419e-03,
        9.0343e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.399

[Epoch: 119, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.8678e-06, 5.0895e-03, 5.7697e-03, 5.5343e-06, 1.3039e-02, 9.7609e-01,
        1.0807e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 119, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0080, 0.6594, 0.0084, 0.0042, 0.0036, 0.3132, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 119, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8888e-02, 2.0897e-03, 9.3320e-01, 1.7356e-05, 2.8525e-03, 4.3534e-06,
        2.9516e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 119, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5617e-01, 1.5044e-06, 1.2659e-01, 1.7224e-02, 2.2358e-06, 1.9475e-06,
        5.4749e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.331

[Epoch: 120, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5666e-02, 4.2929e-03, 7.1929e-02, 1.7839e-06, 1.3118e-06, 3.0138e-03,
        8.8510e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.402

[Epoch: 120, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.0531e-06, 4.8588e-03, 2.7311e-03, 4.5179e-06, 1.6200e-02, 9.7620e-01,
        1.1238e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 120, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0117, 0.6234, 0.0078, 0.0050, 0.0038, 0.3453, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 120, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.5823e-02, 2.6579e-03, 9.2344e-01, 1.8676e-05, 4.1272e-03, 5.0170e-06,
        3.9281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.145

[Epoch: 120, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9635e-01, 3.0246e-06, 1.7976e-01, 2.3861e-02, 3.5755e-06, 2.7973e-06,
        1.2863e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.345

[Epoch: 121, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2354e-02, 4.1942e-03, 6.9194e-02, 3.2430e-06, 1.2217e-06, 3.8102e-03,
        8.9044e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.410

[Epoch: 121, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.2462e-06, 3.4586e-03, 4.5990e-03, 8.3725e-06, 9.9039e-03, 9.8202e-01,
        9.7446e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.930

[Epoch: 121, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6141, 0.0093, 0.0032, 0.0053, 0.3547, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.016

[Epoch: 121, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7543e-02, 4.3180e-03, 9.3264e-01, 3.3181e-05, 2.7222e-03, 7.2860e-06,
        2.7390e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.142

[Epoch: 121, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3199e-01, 3.5546e-06, 1.4572e-01, 2.2261e-02, 5.4266e-06, 4.9998e-06,
        1.4846e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.268

[Epoch: 122, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.1164e-02, 3.5481e-03, 4.7043e-02, 2.1925e-06, 1.0713e-06, 3.5897e-03,
        9.2465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 122, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.9590e-06, 2.2974e-03, 2.1584e-03, 4.5200e-06, 4.4404e-03, 9.9109e-01,
        5.5758e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.933

[Epoch: 122, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0095, 0.6580, 0.0102, 0.0043, 0.0041, 0.3110, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 122, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0951e-02, 4.0093e-03, 9.2458e-01, 5.6627e-05, 4.3481e-03, 1.3390e-05,
        6.0388e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.118

[Epoch: 122, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4089e-01, 2.8314e-06, 1.3848e-01, 2.0616e-02, 5.5727e-06, 3.2945e-06,
        1.1070e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.376

[Epoch: 123, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1766e-02, 6.1645e-03, 7.2137e-02, 4.3728e-06, 1.8546e-06, 4.6499e-03,
        8.8528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 123, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.2071e-06, 2.5500e-03, 2.9224e-03, 3.8486e-06, 6.2218e-03, 9.8830e-01,
        1.1545e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.919

[Epoch: 123, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0074, 0.6785, 0.0078, 0.0034, 0.0043, 0.2950, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 123, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9481e-02, 4.3925e-03, 9.2988e-01, 3.5591e-05, 2.8395e-03, 9.6215e-06,
        3.3637e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 123, batch: 205/207] total loss per batch: 0.686
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2001e-01, 2.8200e-06, 1.6007e-01, 1.9881e-02, 5.5380e-06, 4.3428e-06,
        2.1120e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.340

[Epoch: 124, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8165e-02, 2.9983e-03, 5.7636e-02, 1.2656e-06, 1.2568e-06, 3.8250e-03,
        9.0737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 124, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.2886e-06, 3.2675e-03, 2.9942e-03, 2.5894e-06, 1.0616e-02, 9.8312e-01,
        3.5580e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.941

[Epoch: 124, batch: 123/207] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6220, 0.0110, 0.0049, 0.0046, 0.3438, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 124, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.8702e-02, 2.5536e-03, 9.4282e-01, 2.5178e-05, 3.2425e-03, 6.2654e-06,
        2.6549e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 124, batch: 205/207] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6816e-01, 1.7298e-06, 1.1377e-01, 1.8053e-02, 3.0346e-06, 1.9820e-06,
        1.0353e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.334

[Epoch: 125, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7999e-02, 2.6263e-03, 8.0881e-02, 2.0064e-06, 1.9296e-06, 2.8105e-03,
        8.7568e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.389

[Epoch: 125, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.8381e-06, 3.7933e-03, 4.0035e-03, 6.4683e-06, 1.1196e-02, 9.8100e-01,
        8.1006e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 125, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6362, 0.0131, 0.0062, 0.0065, 0.3235, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 125, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.6079e-02, 2.4674e-03, 9.4497e-01, 4.3904e-05, 2.7726e-03, 1.1601e-05,
        3.6548e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 125, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0598e-01, 3.6798e-06, 1.7169e-01, 2.2298e-02, 6.2582e-06, 3.1896e-06,
        1.2060e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.301

[Epoch: 126, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.9934e-02, 1.4830e-03, 4.0569e-02, 8.5267e-07, 8.8830e-07, 1.9688e-03,
        9.3604e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 126, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.5886e-06, 3.7724e-03, 4.9427e-03, 8.2529e-06, 1.1242e-02, 9.8003e-01,
        1.0269e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 126, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0091, 0.6551, 0.0108, 0.0047, 0.0045, 0.3125, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 126, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3877e-02, 2.8018e-03, 9.2470e-01, 1.9337e-05, 5.4219e-03, 6.2870e-06,
        3.1756e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.121

[Epoch: 126, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3049e-01, 2.5870e-06, 1.4917e-01, 2.0324e-02, 3.9264e-06, 3.8908e-06,
        1.0274e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.357

[Epoch: 127, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.0853e-02, 6.0277e-03, 8.6219e-02, 3.4198e-06, 2.6131e-06, 4.3101e-03,
        8.6258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.400

[Epoch: 127, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.3842e-06, 4.9652e-03, 4.2714e-03, 8.8392e-06, 1.1638e-02, 9.7911e-01,
        1.7488e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.930

[Epoch: 127, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0081, 0.6636, 0.0100, 0.0037, 0.0053, 0.3064, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 127, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4428e-02, 2.2531e-03, 9.2691e-01, 2.1449e-05, 3.5152e-03, 4.5129e-06,
        2.8711e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.152

[Epoch: 127, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4011e-01, 2.3050e-06, 1.4153e-01, 1.8335e-02, 2.9625e-06, 2.3816e-06,
        1.1126e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 128, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.2244e-02, 4.5818e-03, 7.5754e-02, 3.0643e-06, 2.4297e-06, 3.3163e-03,
        8.7410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.421

[Epoch: 128, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.1441e-06, 4.5867e-03, 5.4117e-03, 1.0160e-05, 1.3596e-02, 9.7639e-01,
        2.0048e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 128, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0099, 0.6193, 0.0097, 0.0046, 0.0047, 0.3480, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 128, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.0760e-02, 3.9384e-03, 9.1773e-01, 2.8121e-05, 4.0435e-03, 6.6722e-06,
        3.4926e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.123

[Epoch: 128, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0638e-01, 4.6053e-06, 1.6841e-01, 2.5186e-02, 3.5366e-06, 3.2822e-06,
        1.0015e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.324

[Epoch: 129, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.1805e-02, 3.0508e-03, 5.3461e-02, 3.1967e-06, 1.3013e-06, 3.9144e-03,
        9.1776e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 129, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.4718e-06, 2.8885e-03, 1.3607e-03, 3.3761e-06, 6.5792e-03, 9.8916e-01,
        4.5558e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.924

[Epoch: 129, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6499, 0.0096, 0.0040, 0.0041, 0.3197, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 129, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.9079e-02, 2.8645e-03, 9.4047e-01, 3.4702e-05, 3.2828e-03, 5.2661e-06,
        4.2635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 129, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4220e-01, 3.4584e-06, 1.3985e-01, 1.7920e-02, 5.5085e-06, 2.8745e-06,
        1.0818e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.319

[Epoch: 130, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1391e-02, 2.8467e-03, 4.9225e-02, 1.3508e-06, 9.7786e-07, 2.8982e-03,
        9.1364e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.423

[Epoch: 130, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.0968e-06, 1.8905e-03, 3.9620e-03, 3.1318e-06, 7.3578e-03, 9.8678e-01,
        6.8455e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.947

[Epoch: 130, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0122, 0.6344, 0.0129, 0.0052, 0.0057, 0.3252, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 130, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6388e-02, 3.4303e-03, 9.3456e-01, 3.0605e-05, 2.7611e-03, 6.8889e-06,
        2.8258e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 130, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0118e-01, 4.9728e-06, 1.7351e-01, 2.5280e-02, 5.4132e-06, 3.7405e-06,
        1.3392e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.338

[Epoch: 131, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.2693e-02, 3.5624e-03, 5.3841e-02, 2.1776e-06, 1.0830e-06, 3.3131e-03,
        9.1659e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.394

[Epoch: 131, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.2151e-06, 3.3951e-03, 3.8452e-03, 5.4349e-06, 7.7400e-03, 9.8501e-01,
        5.5087e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 131, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0076, 0.6696, 0.0089, 0.0034, 0.0050, 0.3018, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.005

[Epoch: 131, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3676e-02, 3.1555e-03, 9.3486e-01, 5.1471e-05, 3.9353e-03, 8.0570e-06,
        4.3115e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.136

[Epoch: 131, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5131e-01, 2.0862e-06, 1.3386e-01, 1.4822e-02, 3.7697e-06, 3.3304e-06,
        4.7637e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 132, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7053e-02, 3.5592e-03, 7.8300e-02, 1.6510e-06, 1.0973e-06, 2.9540e-03,
        8.7813e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.438

[Epoch: 132, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.7292e-06, 2.5016e-03, 3.8910e-03, 4.5547e-06, 1.5126e-02, 9.7847e-01,
        1.1617e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.943

[Epoch: 132, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0106, 0.6183, 0.0123, 0.0062, 0.0059, 0.3426, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 132, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.5444e-02, 3.4439e-03, 9.1393e-01, 2.2687e-05, 3.7855e-03, 6.2672e-06,
        3.3672e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 132, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4391e-01, 1.6531e-06, 1.3680e-01, 1.9275e-02, 5.1211e-06, 1.8585e-06,
        8.8307e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.320

[Epoch: 133, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7841e-02, 2.6366e-03, 6.6638e-02, 1.3471e-06, 1.4725e-06, 2.7508e-03,
        8.9013e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.422

[Epoch: 133, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.6149e-06, 3.6178e-03, 6.1908e-03, 6.8874e-06, 9.8653e-03, 9.8031e-01,
        1.6113e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 133, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0119, 0.6408, 0.0118, 0.0052, 0.0058, 0.3196, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 133, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.8575e-02, 2.7542e-03, 9.4164e-01, 5.0194e-05, 4.0100e-03, 8.6083e-06,
        2.9656e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 133, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2671e-01, 2.6627e-06, 1.5132e-01, 2.1953e-02, 3.3885e-06, 2.9126e-06,
        1.1262e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.325

[Epoch: 134, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4201e-02, 2.0482e-03, 5.9069e-02, 1.1887e-06, 1.2398e-06, 2.9064e-03,
        9.1177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.443

[Epoch: 134, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([7.2407e-06, 4.4587e-03, 5.1695e-03, 9.4773e-06, 1.4589e-02, 9.7576e-01,
        2.8546e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 134, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0079, 0.6793, 0.0082, 0.0039, 0.0042, 0.2941, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 134, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2671e-02, 3.6777e-03, 9.2564e-01, 2.6620e-05, 4.5726e-03, 1.2171e-05,
        3.4038e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.129

[Epoch: 134, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2875e-01, 1.5575e-06, 1.5307e-01, 1.8165e-02, 3.4649e-06, 2.5378e-06,
        6.9553e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 135, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7219e-02, 2.7723e-03, 6.6336e-02, 1.6450e-06, 1.7297e-06, 2.9728e-03,
        9.0070e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.393

[Epoch: 135, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.5144e-06, 3.4010e-03, 3.6720e-03, 6.5657e-06, 7.3686e-03, 9.8555e-01,
        1.1870e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.916

[Epoch: 135, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0091, 0.6296, 0.0094, 0.0041, 0.0036, 0.3412, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 135, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7453e-02, 2.3553e-03, 9.3535e-01, 1.5821e-05, 2.2351e-03, 4.3918e-06,
        2.5816e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.148

[Epoch: 135, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2405e-01, 3.0345e-06, 1.5117e-01, 2.4754e-02, 3.6931e-06, 3.4630e-06,
        1.6727e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.359

[Epoch: 136, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.8902e-02, 4.9779e-03, 8.7847e-02, 5.4298e-06, 3.2765e-06, 3.5395e-03,
        8.6473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.403

[Epoch: 136, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([9.6615e-06, 4.4269e-03, 3.3103e-03, 8.9184e-06, 1.1088e-02, 9.8115e-01,
        1.2178e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 136, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0108, 0.6506, 0.0100, 0.0032, 0.0039, 0.3190, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 136, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.2174e-02, 3.7518e-03, 9.1351e-01, 4.5084e-05, 5.0513e-03, 1.0267e-05,
        5.4601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.127

[Epoch: 136, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2130e-01, 3.5125e-06, 1.5766e-01, 2.1016e-02, 7.0910e-06, 3.2736e-06,
        1.0007e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.307

[Epoch: 137, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4426e-02, 3.8403e-03, 5.0072e-02, 3.0761e-06, 2.1919e-06, 3.1575e-03,
        9.1850e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 137, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5402e-06, 2.3636e-03, 2.4821e-03, 2.8957e-06, 1.1172e-02, 9.8398e-01,
        8.4093e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 137, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6303, 0.0097, 0.0039, 0.0044, 0.3391, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 137, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3655e-02, 4.1662e-03, 9.2473e-01, 4.7483e-05, 3.5343e-03, 1.0466e-05,
        3.8584e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.123

[Epoch: 137, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5788e-01, 2.2218e-06, 1.2370e-01, 1.8399e-02, 4.6864e-06, 3.3936e-06,
        1.0805e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.340

[Epoch: 138, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8947e-02, 2.5034e-03, 4.9847e-02, 2.8168e-06, 8.7787e-07, 2.8922e-03,
        9.1581e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.425

[Epoch: 138, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.7041e-06, 2.1327e-03, 2.6394e-03, 3.1960e-06, 7.5766e-03, 9.8764e-01,
        4.2278e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 138, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0111, 0.6266, 0.0116, 0.0043, 0.0046, 0.3364, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.013

[Epoch: 138, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.7487e-02, 4.1275e-03, 9.1861e-01, 5.4494e-05, 4.2455e-03, 1.3530e-05,
        5.4635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 138, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9180e-01, 4.1580e-06, 1.8559e-01, 2.2585e-02, 5.3453e-06, 4.4005e-06,
        1.5004e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.347

[Epoch: 139, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7114e-02, 2.6978e-03, 5.7932e-02, 2.6400e-06, 1.3656e-06, 2.8369e-03,
        9.0942e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.413

[Epoch: 139, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.8996e-06, 3.1170e-03, 4.7123e-03, 5.3892e-06, 1.1423e-02, 9.8074e-01,
        1.1146e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.941

[Epoch: 139, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0097, 0.6844, 0.0108, 0.0049, 0.0047, 0.2821, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 139, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([4.8528e-02, 2.2122e-03, 9.4390e-01, 3.2055e-05, 3.2705e-03, 8.7939e-06,
        2.0475e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 139, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3898e-01, 2.8245e-06, 1.4201e-01, 1.8985e-02, 4.6230e-06, 4.3779e-06,
        1.1316e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.303

[Epoch: 140, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1206e-02, 3.6825e-03, 6.6782e-02, 2.7070e-06, 1.7570e-06, 2.6090e-03,
        8.9572e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.414

[Epoch: 140, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.5934e-06, 3.8755e-03, 3.7963e-03, 7.8277e-06, 1.3594e-02, 9.7872e-01,
        1.6668e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 140, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0117, 0.6173, 0.0120, 0.0053, 0.0060, 0.3434, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 140, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3199e-02, 2.5411e-03, 9.3905e-01, 1.6730e-05, 2.7549e-03, 3.8003e-06,
        2.4359e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.128

[Epoch: 140, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4945e-01, 1.6040e-06, 1.3101e-01, 1.9527e-02, 2.0673e-06, 1.8967e-06,
        6.6014e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.356

[Epoch: 141, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1592e-02, 2.4658e-03, 6.6676e-02, 1.4263e-06, 8.5283e-07, 3.6759e-03,
        8.9559e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.443

[Epoch: 141, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.1507e-06, 4.4570e-03, 5.2430e-03, 7.9634e-06, 1.0221e-02, 9.8007e-01,
        9.0289e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 141, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0098, 0.6415, 0.0092, 0.0035, 0.0039, 0.3293, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 141, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9956e-02, 2.9748e-03, 9.3113e-01, 2.3371e-05, 3.7923e-03, 7.4407e-06,
        2.1131e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 141, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9380e-01, 2.7167e-06, 1.8209e-01, 2.4094e-02, 2.4401e-06, 2.6203e-06,
        1.1887e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.315

[Epoch: 142, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.3064e-02, 2.7196e-03, 5.3379e-02, 2.0392e-06, 1.3717e-06, 2.7154e-03,
        9.1812e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.428

[Epoch: 142, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3914e-06, 5.0536e-03, 5.1342e-03, 7.4559e-06, 1.2222e-02, 9.7758e-01,
        1.7067e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 142, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0081, 0.6938, 0.0108, 0.0045, 0.0051, 0.2734, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 142, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8352e-02, 2.7741e-03, 9.3303e-01, 2.9190e-05, 2.9824e-03, 6.7667e-06,
        2.8252e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 142, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5511e-01, 2.7157e-06, 1.2637e-01, 1.8508e-02, 1.9175e-06, 2.1400e-06,
        6.4634e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 143, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3890e-02, 5.2583e-03, 7.3515e-02, 1.6938e-06, 1.8756e-06, 3.3387e-03,
        8.8399e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.376

[Epoch: 143, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.1102e-06, 3.2672e-03, 2.3227e-03, 6.4369e-06, 1.0305e-02, 9.8409e-01,
        1.0120e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.936

[Epoch: 143, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.5953, 0.0084, 0.0035, 0.0030, 0.3775, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 143, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.5877e-02, 3.1268e-03, 9.1323e-01, 3.3502e-05, 3.6346e-03, 4.8402e-06,
        4.0976e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 143, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0924e-01, 3.5713e-06, 1.6825e-01, 2.2490e-02, 3.8537e-06, 3.2967e-06,
        1.0408e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.305

[Epoch: 144, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8589e-02, 2.9076e-03, 5.1439e-02, 3.9057e-06, 1.5684e-06, 3.4656e-03,
        9.1359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 144, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.5036e-06, 3.6257e-03, 2.5399e-03, 7.3651e-06, 6.8852e-03, 9.8694e-01,
        6.8536e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.923

[Epoch: 144, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0109, 0.6270, 0.0075, 0.0040, 0.0039, 0.3415, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 144, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.1343e-02, 3.3996e-03, 9.3641e-01, 4.4506e-05, 4.3047e-03, 1.2802e-05,
        4.4862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.116

[Epoch: 144, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2484e-01, 3.3178e-06, 1.5299e-01, 2.2142e-02, 4.1577e-06, 5.2740e-06,
        2.1270e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.348

[Epoch: 145, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9442e-02, 4.8854e-03, 7.3693e-02, 5.3466e-06, 2.0884e-06, 4.0182e-03,
        8.8795e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 145, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.5831e-06, 2.9543e-03, 2.1547e-03, 4.3392e-06, 1.1424e-02, 9.8346e-01,
        4.7697e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.912

[Epoch: 145, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0103, 0.6767, 0.0113, 0.0034, 0.0047, 0.2904, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 145, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.6049e-02, 5.2286e-03, 9.1962e-01, 4.7110e-05, 3.7878e-03, 1.1998e-05,
        5.2534e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.128

[Epoch: 145, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4271e-01, 2.3993e-06, 1.3880e-01, 1.8466e-02, 4.1454e-06, 4.6003e-06,
        7.6768e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.311

[Epoch: 146, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([4.8771e-02, 2.9987e-03, 6.2167e-02, 1.5794e-06, 1.0338e-06, 3.2402e-03,
        8.8282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 146, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.5097e-06, 2.6918e-03, 2.3397e-03, 3.6259e-06, 6.1358e-03, 9.8883e-01,
        3.7168e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 146, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0126, 0.6216, 0.0107, 0.0059, 0.0054, 0.3386, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 146, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3322e-02, 4.5721e-03, 9.2463e-01, 3.8774e-05, 4.1870e-03, 1.3189e-05,
        3.2404e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 146, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4461e-01, 2.6139e-06, 1.3334e-01, 2.2035e-02, 2.7884e-06, 4.3232e-06,
        8.2927e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.323

[Epoch: 147, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.0986e-02, 2.9622e-03, 5.6749e-02, 1.7872e-06, 1.5450e-06, 2.9238e-03,
        9.1638e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 147, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.0604e-06, 3.4692e-03, 4.9012e-03, 5.6039e-06, 9.8433e-03, 9.8178e-01,
        5.6993e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.944

[Epoch: 147, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0082, 0.6739, 0.0108, 0.0045, 0.0046, 0.2946, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 147, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6266e-02, 2.5914e-03, 9.3432e-01, 3.1854e-05, 4.2155e-03, 1.0353e-05,
        2.5651e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 147, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2549e-01, 2.4466e-06, 1.5917e-01, 1.5329e-02, 2.1962e-06, 4.0276e-06,
        5.7113e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.351

[Epoch: 148, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3670e-02, 2.3688e-03, 7.1396e-02, 1.6300e-06, 1.8606e-06, 3.3850e-03,
        8.8918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.398

[Epoch: 148, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.6759e-06, 4.9162e-03, 3.8953e-03, 7.8735e-06, 1.5029e-02, 9.7615e-01,
        1.0130e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 148, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0108, 0.6328, 0.0119, 0.0053, 0.0040, 0.3320, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.007

[Epoch: 148, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0768e-02, 2.1075e-03, 9.3181e-01, 1.7554e-05, 3.1567e-03, 7.8214e-06,
        2.1319e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.142

[Epoch: 148, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2956e-01, 3.2480e-06, 1.4541e-01, 2.5009e-02, 3.3510e-06, 2.7650e-06,
        7.5845e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.327

[Epoch: 149, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.9664e-02, 6.2115e-03, 8.1171e-02, 1.6943e-06, 1.6171e-06, 4.0429e-03,
        8.6891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.425

[Epoch: 149, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([6.6921e-06, 5.8342e-03, 3.5083e-03, 1.0105e-05, 9.5518e-03, 9.8109e-01,
        9.2921e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.922

[Epoch: 149, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.6451, 0.0094, 0.0039, 0.0053, 0.3230, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.013

[Epoch: 149, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.0505e-02, 2.4808e-03, 9.2153e-01, 1.5904e-05, 2.8441e-03, 4.4912e-06,
        2.6170e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 149, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2586e-01, 2.8324e-06, 1.5131e-01, 2.2813e-02, 2.6149e-06, 3.9487e-06,
        7.8471e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.323

[Epoch: 150, batch: 41/207] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.9859e-02, 2.8461e-03, 5.5857e-02, 2.4266e-06, 1.8756e-06, 3.5213e-03,
        9.1791e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 150, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.6294e-06, 2.3670e-03, 3.6364e-03, 4.8421e-06, 1.0472e-02, 9.8352e-01,
        5.1069e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 150, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0108, 0.6691, 0.0108, 0.0050, 0.0040, 0.2974, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 150, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9390e-02, 3.9902e-03, 9.2920e-01, 4.7891e-05, 3.3600e-03, 8.7152e-06,
        4.0058e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 150, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1364e-01, 3.5991e-06, 1.6160e-01, 2.4743e-02, 3.1684e-06, 3.7744e-06,
        1.1185e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.346

[Epoch: 151, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6613e-02, 3.3146e-03, 4.9486e-02, 2.1716e-06, 1.0862e-06, 2.9529e-03,
        9.1763e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.397

[Epoch: 151, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.5484e-06, 2.4473e-03, 2.2717e-03, 4.5806e-06, 8.1830e-03, 9.8709e-01,
        9.9748e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.930

[Epoch: 151, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0109, 0.6134, 0.0106, 0.0045, 0.0043, 0.3518, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 151, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.6842e-02, 2.9512e-03, 9.2252e-01, 3.5253e-05, 3.6956e-03, 1.1753e-05,
        3.9461e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 151, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3638e-01, 3.1935e-06, 1.4507e-01, 1.8523e-02, 4.3849e-06, 8.0328e-06,
        1.0687e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.315

[Epoch: 152, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9200e-02, 2.7853e-03, 6.4504e-02, 2.2146e-06, 8.5998e-07, 2.8793e-03,
        9.0063e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 152, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.2817e-06, 2.6639e-03, 2.7758e-03, 2.7517e-06, 8.1850e-03, 9.8637e-01,
        4.5080e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.940

[Epoch: 152, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6496, 0.0112, 0.0046, 0.0049, 0.3158, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 152, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7165e-02, 3.8520e-03, 9.3188e-01, 3.6452e-05, 3.4594e-03, 1.0952e-05,
        3.5921e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 152, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3497e-01, 2.9282e-06, 1.4610e-01, 1.8911e-02, 2.4449e-06, 3.8405e-06,
        7.8191e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.329

[Epoch: 153, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4573e-02, 3.2422e-03, 6.6664e-02, 1.5321e-06, 1.0473e-06, 3.0380e-03,
        8.9248e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.430

[Epoch: 153, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8440e-06, 3.3947e-03, 3.8838e-03, 3.6254e-06, 1.0229e-02, 9.8249e-01,
        3.8627e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 153, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0097, 0.6522, 0.0108, 0.0042, 0.0042, 0.3153, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 153, batch: 164/207] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4802e-02, 2.8050e-03, 9.3590e-01, 2.8270e-05, 3.3906e-03, 5.9554e-06,
        3.0639e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 153, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3597e-01, 2.0409e-06, 1.4252e-01, 2.1496e-02, 1.6283e-06, 3.0252e-06,
        6.3469e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.331

[Epoch: 154, batch: 41/207] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7435e-02, 2.5911e-03, 6.6005e-02, 1.6704e-06, 9.1669e-07, 2.7515e-03,
        9.0121e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.419

[Epoch: 154, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9659e-06, 5.1186e-03, 3.8227e-03, 3.5628e-06, 1.3565e-02, 9.7749e-01,
        6.0932e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.933

[Epoch: 154, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0095, 0.6414, 0.0096, 0.0038, 0.0040, 0.3285, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 154, batch: 164/207] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0574e-02, 3.1466e-03, 9.2956e-01, 2.2868e-05, 3.4959e-03, 7.7591e-06,
        3.1958e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 154, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2491e-01, 2.3308e-06, 1.5464e-01, 2.0443e-02, 1.5912e-06, 3.2274e-06,
        5.2115e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.328

[Epoch: 155, batch: 41/207] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4533e-02, 3.4151e-03, 6.9441e-02, 1.6854e-06, 1.0473e-06, 3.0906e-03,
        8.8952e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.409

[Epoch: 155, batch: 82/207] total loss per batch: 0.739
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3803e-06, 3.5174e-03, 3.3172e-03, 3.4888e-06, 1.0238e-02, 9.8292e-01,
        5.9773e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.936

[Epoch: 155, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0092, 0.6501, 0.0094, 0.0036, 0.0037, 0.3211, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 155, batch: 164/207] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1224e-02, 3.6372e-03, 9.2807e-01, 2.6140e-05, 3.6339e-03, 5.1882e-06,
        3.4042e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 155, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2431e-01, 2.1831e-06, 1.5387e-01, 2.1808e-02, 1.7316e-06, 3.4113e-06,
        5.1954e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.334

[Epoch: 156, batch: 41/207] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6529e-02, 4.0981e-03, 6.1752e-02, 2.4252e-06, 1.1150e-06, 4.1122e-03,
        9.0351e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 156, batch: 82/207] total loss per batch: 0.739
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.9887e-06, 3.1735e-03, 2.8333e-03, 3.0246e-06, 8.8424e-03, 9.8515e-01,
        4.6576e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 156, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0104, 0.6279, 0.0099, 0.0039, 0.0040, 0.3406, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 156, batch: 164/207] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3289e-02, 3.8568e-03, 9.2584e-01, 2.5665e-05, 3.4267e-03, 4.6153e-06,
        3.5618e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 156, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3730e-01, 1.9239e-06, 1.4375e-01, 1.8941e-02, 1.3351e-06, 2.9205e-06,
        3.7094e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.339

[Epoch: 157, batch: 41/207] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8417e-02, 3.0070e-03, 6.0945e-02, 1.9843e-06, 8.1454e-07, 3.6330e-03,
        9.0399e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 157, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.4740e-06, 2.4676e-03, 3.0673e-03, 2.5233e-06, 8.0431e-03, 9.8642e-01,
        2.8045e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 157, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0100, 0.6653, 0.0109, 0.0045, 0.0041, 0.3015, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.015

[Epoch: 157, batch: 164/207] total loss per batch: 0.741
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1904e-02, 3.7453e-03, 9.2774e-01, 2.1083e-05, 3.3144e-03, 4.5707e-06,
        3.2664e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 157, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2326e-01, 2.4125e-06, 1.5613e-01, 2.0604e-02, 1.7372e-06, 2.6063e-06,
        4.7287e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.314

[Epoch: 158, batch: 41/207] total loss per batch: 0.713
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7535e-02, 2.6387e-03, 5.1714e-02, 1.0186e-06, 3.7962e-07, 2.8472e-03,
        9.1526e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.411

[Epoch: 158, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.7903e-06, 2.7331e-03, 2.7685e-03, 2.0569e-06, 1.1989e-02, 9.8251e-01,
        2.9844e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.936

[Epoch: 158, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0095, 0.6357, 0.0107, 0.0039, 0.0044, 0.3319, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 158, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2510e-02, 3.0829e-03, 9.3858e-01, 2.3243e-05, 3.1362e-03, 4.9901e-06,
        2.6604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 158, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3469e-01, 1.6952e-06, 1.4763e-01, 1.7669e-02, 1.2922e-06, 2.7336e-06,
        3.6507e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.336

[Epoch: 159, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6841e-02, 2.4221e-03, 6.8798e-02, 1.0067e-06, 6.0647e-07, 2.8181e-03,
        8.9912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 159, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.1789e-06, 4.4796e-03, 5.5024e-03, 3.6998e-06, 8.5197e-03, 9.8149e-01,
        3.6943e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.933

[Epoch: 159, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0087, 0.6567, 0.0112, 0.0044, 0.0048, 0.3105, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.015

[Epoch: 159, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6638e-02, 2.3672e-03, 9.3561e-01, 1.3244e-05, 2.9306e-03, 5.3209e-06,
        2.4386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 159, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3500e-01, 2.1749e-06, 1.4284e-01, 2.2153e-02, 1.4915e-06, 2.8506e-06,
        5.5609e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.337

[Epoch: 160, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4971e-02, 3.3859e-03, 7.5337e-02, 1.2838e-06, 8.9486e-07, 2.8399e-03,
        8.8346e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.399

[Epoch: 160, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.9833e-06, 3.0014e-03, 2.3815e-03, 2.5037e-06, 1.2884e-02, 9.8173e-01,
        5.0628e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 160, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6528, 0.0102, 0.0042, 0.0041, 0.3156, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 160, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9258e-02, 3.1448e-03, 9.3079e-01, 1.8076e-05, 3.0492e-03, 3.1041e-06,
        3.7403e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.158

[Epoch: 160, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2107e-01, 2.0254e-06, 1.5852e-01, 2.0394e-02, 1.5800e-06, 2.3535e-06,
        3.3052e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.335

[Epoch: 161, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1783e-02, 3.7453e-03, 6.0834e-02, 2.2126e-06, 1.1848e-06, 4.9999e-03,
        8.9863e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.407

[Epoch: 161, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.5362e-06, 5.0425e-03, 4.9567e-03, 1.0736e-05, 1.4972e-02, 9.7501e-01,
        1.0875e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.918

[Epoch: 161, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0105, 0.6091, 0.0096, 0.0047, 0.0054, 0.3569, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 161, batch: 164/207] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.2951e-02, 3.0797e-03, 9.3829e-01, 1.9021e-05, 2.8602e-03, 5.5469e-06,
        2.7910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.094

[Epoch: 161, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3756e-01, 2.5410e-06, 1.4279e-01, 1.9635e-02, 3.0619e-06, 3.1191e-06,
        7.0795e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.356

[Epoch: 162, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7028e-02, 2.8491e-03, 6.8666e-02, 2.8112e-06, 1.1659e-06, 2.8941e-03,
        8.9856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 162, batch: 82/207] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7551e-06, 2.7083e-03, 3.0578e-03, 4.3015e-06, 9.2606e-03, 9.8496e-01,
        4.7392e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 162, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0089, 0.6744, 0.0101, 0.0034, 0.0036, 0.2948, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.015

[Epoch: 162, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.9868e-02, 4.7561e-03, 9.1661e-01, 4.2081e-05, 3.6412e-03, 7.2420e-06,
        5.0742e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.150

[Epoch: 162, batch: 205/207] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1662e-01, 2.7223e-06, 1.6036e-01, 2.2997e-02, 2.6363e-06, 3.7287e-06,
        7.0293e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.326

[Epoch: 163, batch: 41/207] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6845e-02, 3.4492e-03, 5.1258e-02, 1.7288e-06, 1.0773e-06, 3.9379e-03,
        9.1451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 163, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.3967e-06, 2.7379e-03, 3.1586e-03, 4.1597e-06, 6.9479e-03, 9.8715e-01,
        3.9984e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 163, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0084, 0.6291, 0.0085, 0.0034, 0.0040, 0.3435, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 163, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.1848e-02, 5.7286e-03, 9.1108e-01, 4.0388e-05, 5.0402e-03, 1.1023e-05,
        6.2468e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 163, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4193e-01, 3.4158e-06, 1.4305e-01, 1.5007e-02, 3.1612e-06, 4.0903e-06,
        4.7210e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.316

[Epoch: 164, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2647e-02, 2.9316e-03, 6.0497e-02, 2.1968e-06, 7.4965e-07, 3.0968e-03,
        9.0082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 164, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3364e-06, 2.5840e-03, 3.6516e-03, 3.5363e-06, 7.9101e-03, 9.8585e-01,
        3.6100e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 164, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6545, 0.0097, 0.0039, 0.0040, 0.3149, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 164, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4697e-02, 3.5368e-03, 9.3438e-01, 3.8594e-05, 4.0119e-03, 8.8928e-06,
        3.3245e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 164, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3929e-01, 2.3620e-06, 1.4144e-01, 1.9253e-02, 1.7588e-06, 2.6545e-06,
        5.5860e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.321

[Epoch: 165, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4150e-02, 3.2834e-03, 6.7519e-02, 1.0856e-06, 9.9303e-07, 3.6202e-03,
        8.9142e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.413

[Epoch: 165, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.5295e-06, 2.3466e-03, 2.9683e-03, 3.9679e-06, 8.5707e-03, 9.8611e-01,
        2.5056e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.932

[Epoch: 165, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6407, 0.0113, 0.0040, 0.0047, 0.3253, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 165, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2544e-02, 2.9626e-03, 9.2758e-01, 2.2710e-05, 3.3355e-03, 4.8207e-06,
        3.5461e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 165, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2169e-01, 2.4494e-06, 1.5898e-01, 1.9319e-02, 2.3322e-06, 3.6791e-06,
        4.6607e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.335

[Epoch: 166, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1446e-02, 2.9427e-03, 5.9725e-02, 1.2421e-06, 5.5885e-07, 2.4696e-03,
        9.0342e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 166, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.1761e-06, 4.0284e-03, 3.7187e-03, 4.3084e-06, 1.6399e-02, 9.7585e-01,
        4.2746e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.939

[Epoch: 166, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0104, 0.6418, 0.0092, 0.0044, 0.0039, 0.3272, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.013

[Epoch: 166, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3439e-02, 3.0210e-03, 9.3694e-01, 2.4170e-05, 3.4692e-03, 6.7382e-06,
        3.1007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.148

[Epoch: 166, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2396e-01, 2.3501e-06, 1.5396e-01, 2.2061e-02, 2.0721e-06, 3.7753e-06,
        7.3477e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.317

[Epoch: 167, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4136e-02, 3.9394e-03, 6.9925e-02, 1.3479e-06, 8.6219e-07, 2.7703e-03,
        8.8923e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.403

[Epoch: 167, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.8732e-06, 3.7155e-03, 4.3131e-03, 6.1092e-06, 8.6040e-03, 9.8336e-01,
        9.5502e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 167, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0100, 0.6498, 0.0096, 0.0038, 0.0042, 0.3187, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 167, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0669e-02, 2.4962e-03, 9.3125e-01, 1.5039e-05, 2.9772e-03, 5.4253e-06,
        2.5842e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 167, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3187e-01, 2.4953e-06, 1.4830e-01, 1.9812e-02, 1.7910e-06, 4.3413e-06,
        4.4532e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 168, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2770e-02, 5.3171e-03, 7.2160e-02, 2.5702e-06, 1.2810e-06, 4.0041e-03,
        8.8574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.406

[Epoch: 168, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8650e-06, 3.0998e-03, 2.8607e-03, 4.2982e-06, 1.0263e-02, 9.8377e-01,
        4.6778e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 168, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0097, 0.6349, 0.0094, 0.0036, 0.0039, 0.3352, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 168, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3927e-02, 3.0134e-03, 9.2586e-01, 1.8298e-05, 3.7095e-03, 3.8960e-06,
        3.4687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 168, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2601e-01, 2.3846e-06, 1.5351e-01, 2.0467e-02, 1.8504e-06, 3.8504e-06,
        6.0908e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.327

[Epoch: 169, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.3841e-02, 3.4728e-03, 5.0215e-02, 2.2753e-06, 6.7916e-07, 3.9156e-03,
        9.1855e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.413

[Epoch: 169, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.1555e-06, 3.5516e-03, 2.7459e-03, 3.2929e-06, 7.5141e-03, 9.8618e-01,
        3.9710e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.924

[Epoch: 169, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0094, 0.6544, 0.0099, 0.0032, 0.0035, 0.3160, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 169, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8428e-02, 3.4644e-03, 9.3187e-01, 2.6212e-05, 2.9467e-03, 9.1487e-06,
        3.2546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 169, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2767e-01, 3.0065e-06, 1.5241e-01, 1.9904e-02, 1.8594e-06, 3.6731e-06,
        5.5243e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.333

[Epoch: 170, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.4344e-02, 2.2922e-03, 4.5673e-02, 1.2745e-06, 4.7847e-07, 2.7616e-03,
        9.2493e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.409

[Epoch: 170, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3343e-06, 2.3104e-03, 2.1598e-03, 2.4449e-06, 6.3109e-03, 9.8921e-01,
        2.5678e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 170, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6320, 0.0116, 0.0043, 0.0049, 0.3336, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 170, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2835e-02, 3.3726e-03, 9.2529e-01, 2.8066e-05, 4.2942e-03, 4.7360e-06,
        4.1786e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.142

[Epoch: 170, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4423e-01, 2.0443e-06, 1.3906e-01, 1.6694e-02, 2.0647e-06, 3.2341e-06,
        3.5755e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.339

[Epoch: 171, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.6612e-02, 2.6900e-03, 6.9949e-02, 1.3133e-06, 5.4641e-07, 3.1077e-03,
        8.8764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.431

[Epoch: 171, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8307e-06, 3.8781e-03, 4.1632e-03, 2.4883e-06, 1.0586e-02, 9.8137e-01,
        2.6079e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.939

[Epoch: 171, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0090, 0.6887, 0.0095, 0.0033, 0.0045, 0.2814, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 171, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0345e-02, 2.3265e-03, 9.4168e-01, 2.4010e-05, 3.0675e-03, 7.0107e-06,
        2.5483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 171, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1498e-01, 2.6234e-06, 1.6227e-01, 2.2735e-02, 1.7047e-06, 4.2001e-06,
        4.3850e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.342

[Epoch: 172, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7852e-02, 3.3583e-03, 6.9794e-02, 1.4649e-06, 8.9331e-07, 2.6721e-03,
        8.9632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.405

[Epoch: 172, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.7218e-06, 7.9566e-03, 4.1375e-03, 6.4423e-06, 1.3580e-02, 9.7431e-01,
        8.0531e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 172, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6162, 0.0117, 0.0042, 0.0045, 0.3495, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 172, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7613e-02, 3.0505e-03, 9.3323e-01, 1.8107e-05, 3.3291e-03, 5.0468e-06,
        2.7551e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 172, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4598e-01, 2.0078e-06, 1.3564e-01, 1.8361e-02, 2.0229e-06, 3.7603e-06,
        3.8580e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.287

[Epoch: 173, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.8025e-02, 4.0403e-03, 7.7602e-02, 1.3798e-06, 8.8911e-07, 3.9491e-03,
        8.7638e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.424

[Epoch: 173, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.0809e-06, 3.6458e-03, 3.5721e-03, 5.0941e-06, 1.0372e-02, 9.8240e-01,
        1.0957e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 173, batch: 123/207] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0095, 0.6525, 0.0078, 0.0031, 0.0033, 0.3212, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 173, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3319e-02, 2.8594e-03, 9.2541e-01, 2.2703e-05, 4.7249e-03, 6.9403e-06,
        3.6553e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.126

[Epoch: 173, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1360e-01, 2.6050e-06, 1.6424e-01, 2.2149e-02, 1.2504e-06, 4.1119e-06,
        3.9651e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.363

[Epoch: 174, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9546e-02, 2.8699e-03, 6.0607e-02, 2.1958e-06, 1.0019e-06, 3.4973e-03,
        9.0348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.429

[Epoch: 174, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.0288e-06, 3.1424e-03, 2.9160e-03, 5.1155e-06, 1.0691e-02, 9.8324e-01,
        6.0457e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 174, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0091, 0.6277, 0.0088, 0.0027, 0.0032, 0.3448, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 174, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8656e-02, 4.5816e-03, 9.2897e-01, 3.0037e-05, 3.4463e-03, 8.2775e-06,
        4.3087e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 174, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3241e-01, 3.0718e-06, 1.4522e-01, 2.2352e-02, 1.5962e-06, 4.4021e-06,
        3.8262e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.302

[Epoch: 175, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1647e-02, 4.2616e-03, 6.0738e-02, 1.5441e-06, 8.4526e-07, 3.7112e-03,
        8.9964e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.421

[Epoch: 175, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.1753e-06, 1.8945e-03, 2.8038e-03, 3.3069e-06, 6.0221e-03, 9.8927e-01,
        4.2419e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.926

[Epoch: 175, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0090, 0.6676, 0.0106, 0.0039, 0.0031, 0.3033, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 175, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.4941e-02, 4.2790e-03, 9.2245e-01, 2.6379e-05, 3.6883e-03, 4.9431e-06,
        4.6119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 175, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3105e-01, 2.2655e-06, 1.5076e-01, 1.8180e-02, 1.7310e-06, 2.6584e-06,
        2.9501e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.365

[Epoch: 176, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8134e-02, 3.2489e-03, 6.0436e-02, 1.5915e-06, 7.7775e-07, 3.7189e-03,
        9.0446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.416

[Epoch: 176, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.6156e-06, 2.9249e-03, 3.1350e-03, 2.5462e-06, 7.8041e-03, 9.8613e-01,
        2.8652e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.943

[Epoch: 176, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0095, 0.6365, 0.0106, 0.0043, 0.0035, 0.3318, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 176, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0371e-02, 4.9669e-03, 9.2814e-01, 2.1507e-05, 3.3941e-03, 4.9097e-06,
        3.1032e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 176, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2005e-01, 2.5775e-06, 1.5722e-01, 2.2720e-02, 1.6317e-06, 2.8464e-06,
        3.3755e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.312

[Epoch: 177, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0570e-02, 2.9295e-03, 6.2650e-02, 1.0753e-06, 5.5632e-07, 3.2630e-03,
        9.0059e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 177, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8518e-06, 3.2483e-03, 3.2625e-03, 3.2099e-06, 1.2485e-02, 9.8100e-01,
        3.8982e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.941

[Epoch: 177, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0104, 0.6476, 0.0109, 0.0041, 0.0049, 0.3179, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.006

[Epoch: 177, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1020e-02, 2.4226e-03, 9.2972e-01, 1.4881e-05, 3.8917e-03, 4.2759e-06,
        2.9253e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.131

[Epoch: 177, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4291e-01, 2.3211e-06, 1.4025e-01, 1.6824e-02, 1.4522e-06, 2.3939e-06,
        3.3556e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.351

[Epoch: 178, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7725e-02, 3.6826e-03, 6.6263e-02, 1.2457e-06, 8.0813e-07, 3.1908e-03,
        8.9914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.419

[Epoch: 178, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.1134e-06, 3.9981e-03, 4.2182e-03, 3.7462e-06, 1.0752e-02, 9.8102e-01,
        8.7660e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 178, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.6427, 0.0124, 0.0046, 0.0038, 0.3229, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.014

[Epoch: 178, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.0474e-02, 2.4979e-03, 9.4162e-01, 1.2958e-05, 2.9053e-03, 4.6994e-06,
        2.4833e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.129

[Epoch: 178, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2838e-01, 2.3327e-06, 1.4726e-01, 2.4356e-02, 1.1723e-06, 2.6714e-06,
        4.7278e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.326

[Epoch: 179, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1184e-02, 2.6523e-03, 5.8620e-02, 9.3982e-07, 4.9006e-07, 2.9424e-03,
        9.0460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.418

[Epoch: 179, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([5.1936e-06, 4.7822e-03, 4.0019e-03, 4.9410e-06, 1.2836e-02, 9.7837e-01,
        1.2778e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 179, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0101, 0.6407, 0.0088, 0.0033, 0.0036, 0.3302, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 179, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1173e-02, 2.4067e-03, 9.3041e-01, 1.4238e-05, 3.0741e-03, 5.1395e-06,
        2.9173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 179, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2360e-01, 3.5330e-06, 1.5766e-01, 1.8729e-02, 1.6129e-06, 3.2422e-06,
        3.6484e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.334

[Epoch: 180, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6182e-02, 3.1192e-03, 5.7344e-02, 1.4945e-06, 6.2823e-07, 3.0937e-03,
        9.1026e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 180, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.1677e-06, 3.1170e-03, 2.8041e-03, 2.7463e-06, 7.6719e-03, 9.8640e-01,
        4.4475e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.925

[Epoch: 180, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0082, 0.6752, 0.0088, 0.0031, 0.0033, 0.2983, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 180, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6047e-02, 3.5950e-03, 9.3364e-01, 1.8048e-05, 3.3748e-03, 3.8871e-06,
        3.3256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 180, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4372e-01, 2.3106e-06, 1.3574e-01, 2.0532e-02, 1.2520e-06, 1.9492e-06,
        3.6936e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.341

[Epoch: 181, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9438e-02, 2.6064e-03, 7.0032e-02, 1.4276e-06, 4.8487e-07, 3.2369e-03,
        8.9468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 181, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8128e-06, 3.4640e-03, 3.2166e-03, 3.7531e-06, 9.2927e-03, 9.8402e-01,
        3.7861e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 181, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0086, 0.6359, 0.0085, 0.0033, 0.0030, 0.3374, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.009

[Epoch: 181, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.2544e-02, 3.7258e-03, 9.2650e-01, 2.3864e-05, 3.3147e-03, 5.5802e-06,
        3.8831e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.132

[Epoch: 181, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1512e-01, 3.0642e-06, 1.6345e-01, 2.1420e-02, 2.1614e-06, 2.9182e-06,
        4.5363e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.331

[Epoch: 182, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.7894e-02, 3.2523e-03, 5.2333e-02, 7.8363e-07, 5.6191e-07, 3.3883e-03,
        9.1313e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.425

[Epoch: 182, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.1951e-06, 2.9624e-03, 2.8144e-03, 2.0982e-06, 8.2320e-03, 9.8599e-01,
        3.7727e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 182, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0119, 0.6426, 0.0103, 0.0049, 0.0043, 0.3227, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 182, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.1599e-02, 4.6302e-03, 9.2370e-01, 4.1644e-05, 5.1488e-03, 4.0245e-06,
        4.8733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.138

[Epoch: 182, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3655e-01, 2.9879e-06, 1.4459e-01, 1.8843e-02, 2.9264e-06, 2.5335e-06,
        4.4865e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.332

[Epoch: 183, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1920e-02, 3.6020e-03, 6.9997e-02, 1.4473e-06, 8.4924e-07, 2.9834e-03,
        8.9149e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 183, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.8457e-06, 2.0188e-03, 3.7587e-03, 2.6727e-06, 9.6299e-03, 9.8459e-01,
        4.1710e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.943

[Epoch: 183, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0089, 0.6562, 0.0099, 0.0042, 0.0042, 0.3125, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 183, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.4070e-02, 3.7216e-03, 9.1388e-01, 2.4409e-05, 3.7966e-03, 6.0435e-06,
        4.4989e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.133

[Epoch: 183, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4738e-01, 2.5392e-06, 1.3370e-01, 1.8909e-02, 1.5380e-06, 2.6268e-06,
        3.7655e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.330

[Epoch: 184, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.7861e-02, 3.2601e-03, 6.9576e-02, 8.4030e-07, 7.5013e-07, 2.7298e-03,
        8.8657e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.419

[Epoch: 184, batch: 82/207] total loss per batch: 0.741
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8875e-06, 4.7831e-03, 4.7071e-03, 4.7432e-06, 1.6940e-02, 9.7356e-01,
        8.0300e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.938

[Epoch: 184, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0115, 0.6013, 0.0112, 0.0042, 0.0040, 0.3641, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 184, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3699e-02, 3.0363e-03, 9.2780e-01, 1.4441e-05, 2.9664e-03, 4.6493e-06,
        2.4761e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.130

[Epoch: 184, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0796e-01, 2.3445e-06, 1.7183e-01, 2.0206e-02, 1.5775e-06, 2.4397e-06,
        5.0230e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.327

[Epoch: 185, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9965e-02, 3.4146e-03, 6.3494e-02, 1.2073e-06, 1.1279e-06, 3.1655e-03,
        8.9996e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.424

[Epoch: 185, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.2697e-06, 3.3547e-03, 3.5333e-03, 4.5333e-06, 8.4743e-03, 9.8463e-01,
        9.0199e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.923

[Epoch: 185, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6717, 0.0101, 0.0038, 0.0030, 0.2986, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.010

[Epoch: 185, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.0305e-02, 3.8898e-03, 9.1779e-01, 2.7149e-05, 4.1035e-03, 3.8985e-06,
        3.8827e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.136

[Epoch: 185, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4204e-01, 2.0025e-06, 1.3680e-01, 2.1149e-02, 9.4519e-07, 3.0898e-06,
        3.2278e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 186, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.2364e-02, 3.2956e-03, 7.1532e-02, 1.7173e-06, 7.9003e-07, 3.8104e-03,
        8.8900e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.384

[Epoch: 186, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.0150e-06, 3.2884e-03, 3.9239e-03, 4.0834e-06, 9.1500e-03, 9.8363e-01,
        4.7676e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 186, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0096, 0.6313, 0.0089, 0.0032, 0.0035, 0.3406, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 186, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8321e-02, 2.6041e-03, 9.3376e-01, 1.7351e-05, 2.9123e-03, 4.6609e-06,
        2.3794e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.141

[Epoch: 186, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2067e-01, 2.0236e-06, 1.5879e-01, 2.0535e-02, 1.0568e-06, 2.4905e-06,
        2.4428e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.306

[Epoch: 187, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1183e-02, 4.0433e-03, 6.1437e-02, 1.8319e-06, 1.0387e-06, 4.2926e-03,
        8.9904e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.414

[Epoch: 187, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.3702e-06, 2.7852e-03, 2.4373e-03, 3.0235e-06, 8.7233e-03, 9.8605e-01,
        3.8123e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 187, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0099, 0.6518, 0.0113, 0.0043, 0.0036, 0.3160, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 187, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.9272e-02, 4.5513e-03, 9.2886e-01, 2.3088e-05, 3.7636e-03, 3.8502e-06,
        3.5298e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 187, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1611e-01, 3.5537e-06, 1.6206e-01, 2.1815e-02, 1.8766e-06, 4.0102e-06,
        6.6462e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.374

[Epoch: 188, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([1.9706e-02, 2.7483e-03, 5.6627e-02, 1.6302e-06, 6.3793e-07, 3.7362e-03,
        9.1718e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.391

[Epoch: 188, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.4220e-06, 2.0780e-03, 2.9986e-03, 2.2526e-06, 7.7374e-03, 9.8718e-01,
        3.8134e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 188, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0123, 0.6469, 0.0111, 0.0043, 0.0045, 0.3163, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 188, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7919e-02, 3.6770e-03, 9.3080e-01, 2.7619e-05, 3.3923e-03, 4.1322e-06,
        4.1814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.125

[Epoch: 188, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5503e-01, 2.2533e-06, 1.2525e-01, 1.9713e-02, 1.4371e-06, 2.6192e-06,
        2.9008e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.322

[Epoch: 189, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.1366e-02, 2.7955e-03, 5.5025e-02, 1.4161e-06, 3.7833e-07, 3.8735e-03,
        9.0694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.439

[Epoch: 189, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8785e-06, 3.9838e-03, 2.9108e-03, 2.4711e-06, 1.4037e-02, 9.7906e-01,
        3.0200e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.943

[Epoch: 189, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0107, 0.6503, 0.0102, 0.0044, 0.0042, 0.3166, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.015

[Epoch: 189, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.3758e-02, 2.7195e-03, 9.3785e-01, 1.8294e-05, 3.1311e-03, 7.4221e-06,
        2.5121e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 189, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0857e-01, 3.8445e-06, 1.6998e-01, 2.1437e-02, 1.8772e-06, 4.2575e-06,
        5.7538e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.328

[Epoch: 190, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4025e-02, 3.7901e-03, 7.6526e-02, 1.2105e-06, 6.9579e-07, 3.4568e-03,
        8.8220e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 190, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.6772e-06, 3.3083e-03, 5.2669e-03, 4.4248e-06, 8.4833e-03, 9.8293e-01,
        7.5482e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.928

[Epoch: 190, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6511, 0.0100, 0.0038, 0.0037, 0.3176, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 190, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.7738e-02, 3.3494e-03, 9.3263e-01, 1.9703e-05, 3.3067e-03, 2.3082e-06,
        2.9530e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 190, batch: 205/207] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4333e-01, 2.1776e-06, 1.3712e-01, 1.9543e-02, 1.0706e-06, 3.0986e-06,
        3.3058e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.335

[Epoch: 191, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0507e-02, 2.4175e-03, 5.3420e-02, 6.8305e-07, 4.1756e-07, 2.9190e-03,
        9.1074e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.412

[Epoch: 191, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.9474e-06, 4.5215e-03, 2.6881e-03, 3.3986e-06, 1.2370e-02, 9.8041e-01,
        5.3960e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 191, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6419, 0.0081, 0.0029, 0.0030, 0.3321, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.012

[Epoch: 191, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.0502e-02, 2.7396e-03, 9.3047e-01, 2.2955e-05, 3.2652e-03, 5.1503e-06,
        2.9992e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.142

[Epoch: 191, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3900e-01, 2.6320e-06, 1.4086e-01, 2.0134e-02, 1.0380e-06, 2.4399e-06,
        4.3800e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.317

[Epoch: 192, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8942e-02, 2.8871e-03, 5.6966e-02, 1.2934e-06, 6.3968e-07, 3.5464e-03,
        9.0766e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.420

[Epoch: 192, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.7067e-06, 4.5529e-03, 3.8666e-03, 4.3731e-06, 9.2523e-03, 9.8232e-01,
        1.0271e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.934

[Epoch: 192, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0072, 0.6664, 0.0084, 0.0031, 0.0031, 0.3092, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.011

[Epoch: 192, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3988e-02, 4.6222e-03, 9.2310e-01, 3.1382e-05, 4.3281e-03, 4.3138e-06,
        3.9246e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.135

[Epoch: 192, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0242e-01, 2.3614e-06, 1.7526e-01, 2.2309e-02, 1.8588e-06, 4.5871e-06,
        4.0421e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.352

[Epoch: 193, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.8794e-02, 3.7604e-03, 7.0043e-02, 1.5154e-06, 9.2786e-07, 3.3039e-03,
        8.9410e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.389

[Epoch: 193, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.0579e-06, 2.7868e-03, 2.8132e-03, 4.0899e-06, 8.0975e-03, 9.8630e-01,
        4.0325e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 193, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0105, 0.6316, 0.0091, 0.0038, 0.0039, 0.3373, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.015

[Epoch: 193, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([7.0975e-02, 3.5966e-03, 9.1538e-01, 4.5564e-05, 4.7052e-03, 4.9450e-06,
        5.2943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 193, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4024e-01, 2.9960e-06, 1.4182e-01, 1.7931e-02, 1.4171e-06, 3.9157e-06,
        3.2609e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.292

[Epoch: 194, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.4095e-02, 4.1041e-03, 6.3840e-02, 1.9388e-06, 7.8953e-07, 3.6763e-03,
        8.9428e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.415

[Epoch: 194, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.7136e-06, 4.2262e-03, 3.2352e-03, 4.2324e-06, 9.9615e-03, 9.8257e-01,
        8.1721e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.939

[Epoch: 194, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0102, 0.6461, 0.0112, 0.0047, 0.0039, 0.3202, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.017

[Epoch: 194, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5827e-02, 5.3590e-03, 9.3004e-01, 3.2653e-05, 4.1738e-03, 3.4494e-06,
        4.5619e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.137

[Epoch: 194, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3260e-01, 2.9102e-06, 1.4442e-01, 2.2964e-02, 2.1826e-06, 5.6723e-06,
        5.8721e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.383

[Epoch: 195, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.3268e-02, 4.5107e-03, 7.5871e-02, 1.5048e-06, 1.1047e-06, 3.5981e-03,
        8.8275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.404

[Epoch: 195, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.0295e-06, 2.4251e-03, 3.5379e-03, 4.3208e-06, 1.0288e-02, 9.8374e-01,
        4.3557e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.930

[Epoch: 195, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0111, 0.6365, 0.0097, 0.0047, 0.0049, 0.3292, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 195, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.4794e-02, 2.6159e-03, 9.3619e-01, 2.4335e-05, 3.2341e-03, 5.3861e-06,
        3.1341e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.134

[Epoch: 195, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2611e-01, 2.2477e-06, 1.5706e-01, 1.6821e-02, 1.0394e-06, 4.0231e-06,
        2.9744e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.279

[Epoch: 196, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.0604e-02, 3.1219e-03, 6.0086e-02, 1.4179e-06, 8.3076e-07, 3.2566e-03,
        9.0293e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.426

[Epoch: 196, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([1.8134e-06, 3.2295e-03, 3.3839e-03, 3.9486e-06, 1.0264e-02, 9.8312e-01,
        5.6623e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.935

[Epoch: 196, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0108, 0.6296, 0.0119, 0.0046, 0.0043, 0.3351, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.013

[Epoch: 196, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6330e-02, 2.1827e-03, 9.3634e-01, 1.4809e-05, 3.0774e-03, 3.6625e-06,
        2.0512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.127

[Epoch: 196, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3904e-01, 2.6448e-06, 1.3866e-01, 2.2286e-02, 1.1894e-06, 3.1181e-06,
        4.4063e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.345

[Epoch: 197, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.9985e-02, 2.1203e-03, 5.6980e-02, 9.1689e-07, 7.0514e-07, 3.0226e-03,
        9.0789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.408

[Epoch: 197, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.8337e-06, 4.0921e-03, 3.3705e-03, 3.6656e-06, 1.1823e-02, 9.8071e-01,
        8.7025e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.937

[Epoch: 197, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0093, 0.6588, 0.0092, 0.0044, 0.0045, 0.3103, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.008

[Epoch: 197, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.5503e-02, 3.0446e-03, 9.3559e-01, 1.7172e-05, 3.3730e-03, 1.4653e-06,
        2.4705e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.140

[Epoch: 197, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3225e-01, 1.6924e-06, 1.5127e-01, 1.6469e-02, 9.1657e-07, 2.6178e-06,
        3.7187e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.343

[Epoch: 198, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([3.5256e-02, 3.5944e-03, 6.7491e-02, 1.4020e-06, 7.5293e-07, 3.0073e-03,
        8.9065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.398

[Epoch: 198, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([4.3644e-06, 4.0477e-03, 4.4691e-03, 3.8704e-06, 1.0371e-02, 9.8110e-01,
        1.0144e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.927

[Epoch: 198, batch: 123/207] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0098, 0.6512, 0.0093, 0.0043, 0.0042, 0.3181, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.013

[Epoch: 198, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.8083e-02, 2.9823e-03, 9.3288e-01, 1.5162e-05, 3.3242e-03, 2.1665e-06,
        2.7101e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.144

[Epoch: 198, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.2331e-01, 1.8898e-06, 1.5239e-01, 2.4289e-02, 1.6308e-06, 2.4427e-06,
        4.5359e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.319

[Epoch: 199, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.6721e-02, 2.6412e-03, 6.2273e-02, 2.0714e-06, 6.5607e-07, 4.1116e-03,
        9.0425e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.419

[Epoch: 199, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([2.4561e-06, 2.7256e-03, 2.5746e-03, 3.3387e-06, 9.1951e-03, 9.8550e-01,
        5.5532e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.924

[Epoch: 199, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0092, 0.6338, 0.0113, 0.0034, 0.0039, 0.3349, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.016

[Epoch: 199, batch: 164/207] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([6.3365e-02, 3.3985e-03, 9.2599e-01, 1.6585e-05, 3.7032e-03, 3.1088e-06,
        3.5266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.129

[Epoch: 199, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1921e-01, 3.6221e-06, 1.6211e-01, 1.8671e-02, 1.5172e-06, 3.0545e-06,
        6.1692e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.353

[Epoch: 200, batch: 41/207] total loss per batch: 0.714
Policy (actual, predicted): 6 6
Policy data: tensor([0.0300, 0.0033, 0.0633, 0.0000, 0.0000, 0.0033, 0.9000])
Policy pred: tensor([2.3406e-02, 2.6395e-03, 4.7302e-02, 1.3684e-06, 4.4629e-07, 3.3030e-03,
        9.2335e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.413 -0.440

[Epoch: 200, batch: 82/207] total loss per batch: 0.740
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0033, 0.0033, 0.0000, 0.0100, 0.9833, 0.0000])
Policy pred: tensor([3.0268e-06, 3.2495e-03, 3.7283e-03, 3.4947e-06, 8.0244e-03, 9.8499e-01,
        4.3833e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.933 -0.929

[Epoch: 200, batch: 123/207] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.6467, 0.0100, 0.0033, 0.0033, 0.3233, 0.0033])
Policy pred: tensor([0.0106, 0.6401, 0.0096, 0.0042, 0.0041, 0.3283, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.014 0.016

[Epoch: 200, batch: 164/207] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0600, 0.0033, 0.9300, 0.0000, 0.0033, 0.0000, 0.0033])
Policy pred: tensor([5.6258e-02, 3.9862e-03, 9.3134e-01, 2.9409e-05, 4.1144e-03, 2.4420e-06,
        4.2708e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.134 0.139

[Epoch: 200, batch: 205/207] total loss per batch: 0.683
Policy (actual, predicted): 0 0
Policy data: tensor([0.8300, 0.0000, 0.1500, 0.0200, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4374e-01, 2.6699e-06, 1.3388e-01, 2.2365e-02, 1.7783e-06, 4.7456e-06,
        3.5762e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.332 -0.329

