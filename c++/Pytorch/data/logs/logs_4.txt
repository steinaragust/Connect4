Training set samples: 6209
Batch size: 32
[Epoch: 1, batch: 39/195] total loss per batch: 0.962
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0084, 0.3761, 0.4399, 0.0477, 0.0909, 0.0230, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 1, batch: 78/195] total loss per batch: 0.965
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0880, 0.0377, 0.0135, 0.2153, 0.0340, 0.5200, 0.0915],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 1, batch: 117/195] total loss per batch: 0.919
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0092, 0.0092, 0.6724, 0.0108, 0.1496, 0.1122, 0.0366],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 1, batch: 156/195] total loss per batch: 0.942
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0025, 0.0039, 0.9802, 0.0026, 0.0036, 0.0027, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 1, batch: 195/195] total loss per batch: 0.965
Policy (actual, predicted): 1 2
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0117, 0.0227, 0.4647, 0.3025, 0.0077, 0.1117, 0.0791],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.001

[Epoch: 2, batch: 39/195] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0271, 0.1032, 0.7676, 0.0278, 0.0477, 0.0085, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 2, batch: 78/195] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.1456, 0.0146, 0.0091, 0.2553, 0.0285, 0.4887, 0.0582],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 2, batch: 117/195] total loss per batch: 0.740
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0043, 0.0027, 0.8814, 0.0029, 0.0548, 0.0436, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.009

[Epoch: 2, batch: 156/195] total loss per batch: 0.761
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0075, 0.9702, 0.0056, 0.0023, 0.0024, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 2, batch: 195/195] total loss per batch: 0.788
Policy (actual, predicted): 1 6
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0153, 0.1291, 0.0011, 0.0677, 0.0029, 0.2019, 0.5821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 3, batch: 39/195] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0158, 0.0505, 0.8698, 0.0159, 0.0362, 0.0044, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.001

[Epoch: 3, batch: 78/195] total loss per batch: 0.687
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.1051, 0.0067, 0.0104, 0.1660, 0.0351, 0.6213, 0.0554],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 3, batch: 117/195] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0085, 0.0117, 0.6096, 0.0031, 0.2630, 0.0499, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 3, batch: 156/195] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.9776, 0.0046, 0.0012, 0.0025, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 3, batch: 195/195] total loss per batch: 0.761
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([3.4602e-02, 7.3869e-02, 4.2113e-03, 8.2827e-01, 1.2428e-02, 4.6115e-02,
        5.0459e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 4, batch: 39/195] total loss per batch: 0.673
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0167, 0.0546, 0.8836, 0.0134, 0.0250, 0.0025, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 0.001

[Epoch: 4, batch: 78/195] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0585, 0.0077, 0.0142, 0.2804, 0.0355, 0.5762, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.011

[Epoch: 4, batch: 117/195] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0027, 0.0067, 0.7358, 0.0097, 0.1966, 0.0433, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 4, batch: 156/195] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0084, 0.9680, 0.0058, 0.0029, 0.0027, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 4, batch: 195/195] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0882, 0.4047, 0.0395, 0.0633, 0.0350, 0.3498, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.004

[Epoch: 5, batch: 39/195] total loss per batch: 0.664
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0212, 0.0959, 0.7114, 0.0362, 0.1153, 0.0082, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 5, batch: 78/195] total loss per batch: 0.651
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0391, 0.0034, 0.0149, 0.1143, 0.0140, 0.7395, 0.0748],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 5, batch: 117/195] total loss per batch: 0.663
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0059, 0.0082, 0.8197, 0.0069, 0.0935, 0.0526, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 5, batch: 156/195] total loss per batch: 0.667
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0077, 0.9627, 0.0055, 0.0031, 0.0070, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 5, batch: 195/195] total loss per batch: 0.710
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0430, 0.0167, 0.0899, 0.5141, 0.0739, 0.0054, 0.2570],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 6, batch: 39/195] total loss per batch: 0.647
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0271, 0.0689, 0.8204, 0.0161, 0.0578, 0.0035, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 6, batch: 78/195] total loss per batch: 0.640
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0510, 0.0057, 0.0118, 0.2802, 0.0248, 0.5845, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 6, batch: 117/195] total loss per batch: 0.652
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0061, 0.5574, 0.0130, 0.2554, 0.1295, 0.0338],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 6, batch: 156/195] total loss per batch: 0.658
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0050, 0.9773, 0.0017, 0.0043, 0.0029, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 6, batch: 195/195] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1209, 0.5141, 0.0379, 0.0906, 0.0170, 0.1670, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 7, batch: 39/195] total loss per batch: 0.637
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0286, 0.8910, 0.0120, 0.0564, 0.0027, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 7, batch: 78/195] total loss per batch: 0.628
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0617, 0.0041, 0.0135, 0.2275, 0.0282, 0.6081, 0.0568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 7, batch: 117/195] total loss per batch: 0.643
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.8056, 0.0059, 0.1164, 0.0257, 0.0362],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 7, batch: 156/195] total loss per batch: 0.648
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0056, 0.9748, 0.0030, 0.0014, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 7, batch: 195/195] total loss per batch: 0.694
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0351, 0.0017, 0.0306, 0.3215, 0.0227, 0.2894, 0.2991],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.000

[Epoch: 8, batch: 39/195] total loss per batch: 0.633
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.1635, 0.7502, 0.0094, 0.0545, 0.0051, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.000

[Epoch: 8, batch: 78/195] total loss per batch: 0.622
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0549, 0.0026, 0.0086, 0.2019, 0.0157, 0.6825, 0.0338],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 8, batch: 117/195] total loss per batch: 0.637
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0037, 0.0094, 0.8147, 0.0057, 0.1072, 0.0383, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 8, batch: 156/195] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0061, 0.9670, 0.0056, 0.0043, 0.0053, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 8, batch: 195/195] total loss per batch: 0.669
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1601, 0.1604, 0.1075, 0.3085, 0.0996, 0.1321, 0.0318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 9, batch: 39/195] total loss per batch: 0.628
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.0488, 0.8633, 0.0155, 0.0525, 0.0033, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.001

[Epoch: 9, batch: 78/195] total loss per batch: 0.624
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0675, 0.0047, 0.0139, 0.1837, 0.0133, 0.6812, 0.0358],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.000

[Epoch: 9, batch: 117/195] total loss per batch: 0.632
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0034, 0.0049, 0.6685, 0.0041, 0.2522, 0.0332, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 9, batch: 156/195] total loss per batch: 0.638
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.9782, 0.0036, 0.0021, 0.0035, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 9, batch: 195/195] total loss per batch: 0.676
Policy (actual, predicted): 1 6
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0262, 0.3908, 0.0128, 0.0404, 0.0054, 0.0715, 0.4530],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.001

[Epoch: 10, batch: 39/195] total loss per batch: 0.625
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0127, 0.1006, 0.7551, 0.0152, 0.1073, 0.0036, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 10, batch: 78/195] total loss per batch: 0.621
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0815, 0.0044, 0.0060, 0.1760, 0.0240, 0.6271, 0.0808],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.002

[Epoch: 10, batch: 117/195] total loss per batch: 0.631
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0030, 0.0049, 0.7518, 0.0083, 0.1511, 0.0717, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 10, batch: 156/195] total loss per batch: 0.636
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.9672, 0.0031, 0.0046, 0.0043, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 10, batch: 195/195] total loss per batch: 0.669
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1510, 0.0916, 0.0739, 0.4829, 0.0107, 0.1665, 0.0234],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 11, batch: 39/195] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.0240, 0.9064, 0.0117, 0.0386, 0.0026, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 11, batch: 78/195] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0535, 0.0018, 0.0122, 0.2317, 0.0094, 0.6568, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 0.001

[Epoch: 11, batch: 117/195] total loss per batch: 0.625
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0037, 0.0044, 0.7991, 0.0083, 0.1310, 0.0371, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 11, batch: 156/195] total loss per batch: 0.631
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0064, 0.9711, 0.0049, 0.0032, 0.0044, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 11, batch: 195/195] total loss per batch: 0.655
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1363, 0.2963, 0.0461, 0.0621, 0.0940, 0.1680, 0.1973],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.001

[Epoch: 12, batch: 39/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.0882, 0.7929, 0.0101, 0.0893, 0.0063, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.002

[Epoch: 12, batch: 78/195] total loss per batch: 0.603
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0554, 0.0038, 0.0096, 0.1191, 0.0209, 0.7724, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.000

[Epoch: 12, batch: 117/195] total loss per batch: 0.613
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0040, 0.0079, 0.7497, 0.0069, 0.1624, 0.0560, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 12, batch: 156/195] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0067, 0.9679, 0.0046, 0.0023, 0.0050, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 12, batch: 195/195] total loss per batch: 0.646
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1148, 0.1422, 0.0775, 0.4500, 0.0174, 0.0530, 0.1452],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 13, batch: 39/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0133, 0.0631, 0.8393, 0.0168, 0.0587, 0.0046, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 13, batch: 78/195] total loss per batch: 0.598
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0639, 0.0030, 0.0125, 0.2035, 0.0242, 0.6661, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 13, batch: 117/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0053, 0.0061, 0.7869, 0.0057, 0.1356, 0.0464, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.000

[Epoch: 13, batch: 156/195] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.9739, 0.0034, 0.0037, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 13, batch: 195/195] total loss per batch: 0.642
Policy (actual, predicted): 1 5
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0739, 0.2724, 0.0535, 0.1090, 0.0304, 0.2817, 0.1791],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 14, batch: 39/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0127, 0.0977, 0.7800, 0.0110, 0.0890, 0.0046, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 14, batch: 78/195] total loss per batch: 0.600
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0648, 0.0043, 0.0103, 0.1729, 0.0196, 0.6686, 0.0595],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 14, batch: 117/195] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0065, 0.7215, 0.0071, 0.2132, 0.0387, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 14, batch: 156/195] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0069, 0.9636, 0.0056, 0.0037, 0.0065, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 14, batch: 195/195] total loss per batch: 0.644
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1825, 0.0675, 0.0984, 0.3297, 0.0706, 0.0635, 0.1879],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 15, batch: 39/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0615, 0.8779, 0.0097, 0.0360, 0.0044, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 15, batch: 78/195] total loss per batch: 0.600
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0571, 0.0028, 0.0130, 0.2281, 0.0146, 0.6588, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.000

[Epoch: 15, batch: 117/195] total loss per batch: 0.611
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0076, 0.8339, 0.0059, 0.0929, 0.0461, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 15, batch: 156/195] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.9727, 0.0035, 0.0032, 0.0052, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 15, batch: 195/195] total loss per batch: 0.644
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0356, 0.4412, 0.0213, 0.2204, 0.0349, 0.1161, 0.1305],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 16, batch: 39/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0125, 0.0928, 0.7682, 0.0088, 0.1051, 0.0075, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 16, batch: 78/195] total loss per batch: 0.601
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0851, 0.0044, 0.0080, 0.1730, 0.0306, 0.6534, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.000

[Epoch: 16, batch: 117/195] total loss per batch: 0.613
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0055, 0.0103, 0.7370, 0.0053, 0.1944, 0.0283, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.000

[Epoch: 16, batch: 156/195] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0046, 0.9712, 0.0037, 0.0032, 0.0051, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 16, batch: 195/195] total loss per batch: 0.645
Policy (actual, predicted): 1 6
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1004, 0.0674, 0.0733, 0.2551, 0.0419, 0.1568, 0.3052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.001

[Epoch: 17, batch: 39/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0642, 0.8482, 0.0141, 0.0583, 0.0050, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 17, batch: 78/195] total loss per batch: 0.602
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0439, 0.0034, 0.0142, 0.1670, 0.0112, 0.7184, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 17, batch: 117/195] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0049, 0.8423, 0.0063, 0.0953, 0.0382, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.000

[Epoch: 17, batch: 156/195] total loss per batch: 0.620
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0062, 0.9742, 0.0037, 0.0025, 0.0033, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 17, batch: 195/195] total loss per batch: 0.645
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1692, 0.2696, 0.0638, 0.2099, 0.0566, 0.1282, 0.1025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 18, batch: 39/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.0765, 0.8385, 0.0077, 0.0608, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 18, batch: 78/195] total loss per batch: 0.604
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0569, 0.0030, 0.0111, 0.2508, 0.0150, 0.6335, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 18, batch: 117/195] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0042, 0.0069, 0.6898, 0.0064, 0.2324, 0.0481, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 18, batch: 156/195] total loss per batch: 0.620
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.9718, 0.0042, 0.0034, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 18, batch: 195/195] total loss per batch: 0.644
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0905, 0.2152, 0.0593, 0.2217, 0.0467, 0.1528, 0.2139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.001

[Epoch: 19, batch: 39/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0127, 0.1325, 0.7589, 0.0096, 0.0757, 0.0045, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 19, batch: 78/195] total loss per batch: 0.606
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0546, 0.0030, 0.0098, 0.1225, 0.0154, 0.7430, 0.0517],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 19, batch: 117/195] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0043, 0.0057, 0.8267, 0.0042, 0.1085, 0.0328, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 19, batch: 156/195] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0034, 0.0045, 0.9764, 0.0032, 0.0023, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 19, batch: 195/195] total loss per batch: 0.642
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1186, 0.2421, 0.0585, 0.2358, 0.0391, 0.1354, 0.1704],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 20, batch: 39/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.0361, 0.8715, 0.0090, 0.0694, 0.0021, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 20, batch: 78/195] total loss per batch: 0.604
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0694, 0.0047, 0.0124, 0.2079, 0.0464, 0.6337, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 20, batch: 117/195] total loss per batch: 0.614
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0065, 0.0071, 0.6674, 0.0100, 0.2006, 0.0940, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 20, batch: 156/195] total loss per batch: 0.619
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0047, 0.9668, 0.0043, 0.0041, 0.0074, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 20, batch: 195/195] total loss per batch: 0.642
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1217, 0.2163, 0.0523, 0.2038, 0.0494, 0.1489, 0.2076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 21, batch: 39/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0084, 0.0578, 0.8096, 0.0128, 0.0980, 0.0069, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 21, batch: 78/195] total loss per batch: 0.602
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0534, 0.0047, 0.0075, 0.1487, 0.0123, 0.7257, 0.0478],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 21, batch: 117/195] total loss per batch: 0.614
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0028, 0.0053, 0.8483, 0.0022, 0.1048, 0.0298, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 21, batch: 156/195] total loss per batch: 0.617
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.9705, 0.0048, 0.0035, 0.0041, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 21, batch: 195/195] total loss per batch: 0.639
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0993, 0.2391, 0.0570, 0.2665, 0.0435, 0.1326, 0.1621],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 22, batch: 39/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0621, 0.8512, 0.0101, 0.0576, 0.0042, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 22, batch: 78/195] total loss per batch: 0.595
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0605, 0.0042, 0.0132, 0.2153, 0.0171, 0.6345, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 22, batch: 117/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0056, 0.7783, 0.0071, 0.1531, 0.0451, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 22, batch: 156/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0050, 0.9734, 0.0033, 0.0033, 0.0044, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 22, batch: 195/195] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1384, 0.2129, 0.0544, 0.2051, 0.0424, 0.1463, 0.2005],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 23, batch: 39/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0888, 0.8238, 0.0073, 0.0664, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 23, batch: 78/195] total loss per batch: 0.593
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0410, 0.0035, 0.0105, 0.1445, 0.0331, 0.7271, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 23, batch: 117/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0030, 0.0055, 0.7786, 0.0036, 0.1687, 0.0338, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 23, batch: 156/195] total loss per batch: 0.608
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0038, 0.9741, 0.0035, 0.0031, 0.0045, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 23, batch: 195/195] total loss per batch: 0.632
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0940, 0.2494, 0.0538, 0.2535, 0.0484, 0.1344, 0.1666],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 24, batch: 39/195] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0671, 0.8486, 0.0076, 0.0623, 0.0055, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 24, batch: 78/195] total loss per batch: 0.592
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0585, 0.0041, 0.0116, 0.1688, 0.0132, 0.7061, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 24, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0059, 0.7820, 0.0039, 0.1542, 0.0407, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 24, batch: 156/195] total loss per batch: 0.607
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0055, 0.9754, 0.0035, 0.0026, 0.0041, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 24, batch: 195/195] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1324, 0.2199, 0.0605, 0.2056, 0.0412, 0.1417, 0.1988],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 25, batch: 39/195] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0964, 0.8235, 0.0063, 0.0612, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 25, batch: 78/195] total loss per batch: 0.592
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0611, 0.0037, 0.0088, 0.1909, 0.0368, 0.6386, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 25, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0033, 0.0070, 0.7655, 0.0063, 0.1638, 0.0463, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 25, batch: 156/195] total loss per batch: 0.608
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0043, 0.9729, 0.0028, 0.0029, 0.0044, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 25, batch: 195/195] total loss per batch: 0.632
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0983, 0.2334, 0.0497, 0.2493, 0.0533, 0.1431, 0.1728],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 26, batch: 39/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0784, 0.8128, 0.0089, 0.0802, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 26, batch: 78/195] total loss per batch: 0.593
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0594, 0.0028, 0.0092, 0.1834, 0.0147, 0.7018, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 26, batch: 117/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0034, 0.0083, 0.7657, 0.0038, 0.1635, 0.0458, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 26, batch: 156/195] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0076, 0.9670, 0.0049, 0.0031, 0.0052, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 26, batch: 195/195] total loss per batch: 0.633
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1353, 0.2261, 0.0571, 0.2211, 0.0332, 0.1303, 0.1969],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 27, batch: 39/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0917, 0.8198, 0.0062, 0.0685, 0.0059, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 27, batch: 78/195] total loss per batch: 0.595
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0647, 0.0048, 0.0086, 0.2732, 0.0205, 0.5849, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.001

[Epoch: 27, batch: 117/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0034, 0.0058, 0.7795, 0.0069, 0.1642, 0.0344, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 27, batch: 156/195] total loss per batch: 0.611
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0040, 0.9713, 0.0043, 0.0032, 0.0047, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 27, batch: 195/195] total loss per batch: 0.635
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.0948, 0.2290, 0.0555, 0.2324, 0.0615, 0.1607, 0.1661],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 28, batch: 39/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0085, 0.0791, 0.8159, 0.0099, 0.0779, 0.0041, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 28, batch: 78/195] total loss per batch: 0.596
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0619, 0.0029, 0.0141, 0.1412, 0.0213, 0.7316, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 28, batch: 117/195] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0080, 0.7284, 0.0039, 0.1864, 0.0562, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 28, batch: 156/195] total loss per batch: 0.612
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0062, 0.9648, 0.0060, 0.0046, 0.0055, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 28, batch: 195/195] total loss per batch: 0.636
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1264, 0.2222, 0.0560, 0.2389, 0.0350, 0.1189, 0.2027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 29, batch: 39/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.1296, 0.7857, 0.0072, 0.0606, 0.0072, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 29, batch: 78/195] total loss per batch: 0.597
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0705, 0.0040, 0.0072, 0.2358, 0.0252, 0.6126, 0.0447],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 29, batch: 117/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0048, 0.7863, 0.0071, 0.1628, 0.0302, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 29, batch: 156/195] total loss per batch: 0.614
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0038, 0.9761, 0.0030, 0.0022, 0.0044, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 29, batch: 195/195] total loss per batch: 0.638
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1069, 0.2481, 0.0451, 0.2125, 0.0486, 0.1692, 0.1697],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 30, batch: 39/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0460, 0.8426, 0.0107, 0.0828, 0.0078, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.006

[Epoch: 30, batch: 78/195] total loss per batch: 0.598
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0550, 0.0042, 0.0120, 0.1660, 0.0134, 0.7094, 0.0400],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 30, batch: 117/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0053, 0.0067, 0.7908, 0.0052, 0.1220, 0.0510, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 30, batch: 156/195] total loss per batch: 0.615
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.9714, 0.0048, 0.0030, 0.0041, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 30, batch: 195/195] total loss per batch: 0.637
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1191, 0.2005, 0.0757, 0.2478, 0.0474, 0.1223, 0.1871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 31, batch: 39/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.1066, 0.8177, 0.0044, 0.0581, 0.0029, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 31, batch: 78/195] total loss per batch: 0.597
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0538, 0.0025, 0.0065, 0.1440, 0.0280, 0.7402, 0.0250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 31, batch: 117/195] total loss per batch: 0.608
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0045, 0.7635, 0.0039, 0.1908, 0.0263, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 31, batch: 156/195] total loss per batch: 0.612
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0058, 0.9692, 0.0036, 0.0036, 0.0054, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 31, batch: 195/195] total loss per batch: 0.635
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1180, 0.2424, 0.0406, 0.2158, 0.0417, 0.1560, 0.1855],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.003

[Epoch: 32, batch: 39/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0612, 0.8665, 0.0068, 0.0500, 0.0066, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 32, batch: 78/195] total loss per batch: 0.593
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0568, 0.0028, 0.0094, 0.1985, 0.0160, 0.6700, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 32, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0074, 0.7112, 0.0036, 0.2154, 0.0471, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 32, batch: 156/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0057, 0.9703, 0.0045, 0.0044, 0.0048, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 32, batch: 195/195] total loss per batch: 0.630
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1097, 0.2315, 0.0620, 0.2365, 0.0495, 0.1313, 0.1794],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 33, batch: 39/195] total loss per batch: 0.594
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0903, 0.7863, 0.0091, 0.0993, 0.0041, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 33, batch: 78/195] total loss per batch: 0.591
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0535, 0.0044, 0.0076, 0.1951, 0.0219, 0.6719, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 33, batch: 117/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0044, 0.0056, 0.8211, 0.0059, 0.1040, 0.0496, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 33, batch: 156/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0061, 0.9696, 0.0038, 0.0037, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 33, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1174, 0.2268, 0.0517, 0.2216, 0.0415, 0.1453, 0.1956],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 34, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0879, 0.8123, 0.0069, 0.0770, 0.0054, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 34, batch: 78/195] total loss per batch: 0.590
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0636, 0.0030, 0.0083, 0.1898, 0.0155, 0.6789, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 34, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0058, 0.7189, 0.0046, 0.2153, 0.0446, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 34, batch: 156/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.9695, 0.0052, 0.0040, 0.0046, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 34, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1157, 0.2351, 0.0573, 0.2401, 0.0468, 0.1346, 0.1705],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 35, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0553, 0.8535, 0.0063, 0.0724, 0.0046, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 35, batch: 78/195] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0583, 0.0040, 0.0072, 0.1970, 0.0182, 0.6660, 0.0494],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 35, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0037, 0.0044, 0.8240, 0.0049, 0.1174, 0.0389, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 35, batch: 156/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0058, 0.9700, 0.0036, 0.0036, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 35, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1127, 0.2202, 0.0545, 0.2171, 0.0434, 0.1447, 0.2075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 36, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.1058, 0.7989, 0.0066, 0.0743, 0.0038, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 36, batch: 78/195] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0515, 0.0029, 0.0064, 0.1730, 0.0242, 0.7005, 0.0414],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 36, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0048, 0.0069, 0.7008, 0.0059, 0.2240, 0.0502, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 36, batch: 156/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.9676, 0.0064, 0.0047, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 36, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1162, 0.2470, 0.0508, 0.2427, 0.0433, 0.1386, 0.1612],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 37, batch: 39/195] total loss per batch: 0.594
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0644, 0.8445, 0.0063, 0.0700, 0.0065, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 37, batch: 78/195] total loss per batch: 0.590
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0610, 0.0034, 0.0067, 0.1945, 0.0169, 0.6905, 0.0270],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 37, batch: 117/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0028, 0.8678, 0.0033, 0.0845, 0.0336, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 37, batch: 156/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0052, 0.9749, 0.0027, 0.0029, 0.0039, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 37, batch: 195/195] total loss per batch: 0.631
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1174, 0.2027, 0.0685, 0.2164, 0.0510, 0.1381, 0.2058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 38, batch: 39/195] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0071, 0.1070, 0.7855, 0.0067, 0.0849, 0.0047, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 38, batch: 78/195] total loss per batch: 0.592
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0626, 0.0050, 0.0078, 0.2142, 0.0187, 0.6462, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 38, batch: 117/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0061, 0.6001, 0.0084, 0.3198, 0.0535, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 38, batch: 156/195] total loss per batch: 0.608
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0065, 0.9631, 0.0070, 0.0051, 0.0062, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 38, batch: 195/195] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1075, 0.2587, 0.0317, 0.2459, 0.0337, 0.1504, 0.1721],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 39, batch: 39/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0564, 0.8643, 0.0051, 0.0615, 0.0046, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 39, batch: 78/195] total loss per batch: 0.594
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0449, 0.0025, 0.0075, 0.1725, 0.0223, 0.7113, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 39, batch: 117/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0041, 0.8789, 0.0031, 0.0684, 0.0374, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 39, batch: 156/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0047, 0.9746, 0.0026, 0.0027, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 39, batch: 195/195] total loss per batch: 0.634
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1257, 0.1987, 0.0895, 0.2156, 0.0709, 0.1023, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 40, batch: 39/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0133, 0.0984, 0.7979, 0.0097, 0.0688, 0.0060, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 40, batch: 78/195] total loss per batch: 0.595
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0672, 0.0072, 0.0129, 0.1923, 0.0211, 0.6527, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 40, batch: 117/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0060, 0.7772, 0.0070, 0.1584, 0.0402, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 40, batch: 156/195] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0049, 0.9698, 0.0062, 0.0042, 0.0048, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 40, batch: 195/195] total loss per batch: 0.634
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1188, 0.2217, 0.0368, 0.2336, 0.0218, 0.1960, 0.1713],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 41, batch: 39/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0074, 0.0681, 0.8288, 0.0082, 0.0786, 0.0055, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 41, batch: 78/195] total loss per batch: 0.594
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0526, 0.0028, 0.0063, 0.1892, 0.0223, 0.6781, 0.0487],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 41, batch: 117/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0039, 0.7699, 0.0035, 0.1605, 0.0519, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 41, batch: 156/195] total loss per batch: 0.608
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0071, 0.9625, 0.0048, 0.0052, 0.0065, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 41, batch: 195/195] total loss per batch: 0.632
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1002, 0.2273, 0.0593, 0.2199, 0.0706, 0.1230, 0.1998],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 42, batch: 39/195] total loss per batch: 0.594
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0067, 0.0798, 0.8298, 0.0075, 0.0659, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 42, batch: 78/195] total loss per batch: 0.590
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0500, 0.0027, 0.0073, 0.2010, 0.0204, 0.6850, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 42, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0066, 0.7534, 0.0058, 0.1754, 0.0466, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 42, batch: 156/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0070, 0.9637, 0.0058, 0.0047, 0.0057, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 42, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1278, 0.2410, 0.0568, 0.2292, 0.0339, 0.1364, 0.1749],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 43, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0076, 0.0907, 0.8146, 0.0077, 0.0704, 0.0062, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 43, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0545, 0.0034, 0.0062, 0.1998, 0.0189, 0.6831, 0.0340],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 43, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0035, 0.0049, 0.7588, 0.0054, 0.1686, 0.0522, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 43, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0061, 0.9661, 0.0049, 0.0040, 0.0056, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 43, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1061, 0.2209, 0.0529, 0.2319, 0.0469, 0.1431, 0.1982],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 44, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0721, 0.8314, 0.0072, 0.0740, 0.0050, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 44, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0497, 0.0033, 0.0062, 0.1770, 0.0203, 0.7099, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 44, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0034, 0.0054, 0.7746, 0.0062, 0.1640, 0.0403, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 44, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0067, 0.9650, 0.0054, 0.0048, 0.0057, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 44, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1201, 0.2389, 0.0569, 0.2274, 0.0450, 0.1380, 0.1738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 45, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0893, 0.8265, 0.0063, 0.0642, 0.0055, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 45, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0582, 0.0039, 0.0055, 0.2011, 0.0194, 0.6794, 0.0325],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 45, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0054, 0.7669, 0.0060, 0.1628, 0.0472, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 45, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0065, 0.9640, 0.0052, 0.0042, 0.0061, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 45, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1126, 0.2198, 0.0552, 0.2309, 0.0441, 0.1402, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 46, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0862, 0.8175, 0.0057, 0.0764, 0.0044, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 46, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0674, 0.0037, 0.0063, 0.1862, 0.0229, 0.6588, 0.0546],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 46, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0037, 0.0052, 0.7783, 0.0051, 0.1613, 0.0408, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 46, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0059, 0.9681, 0.0048, 0.0047, 0.0048, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 46, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1148, 0.2435, 0.0532, 0.2322, 0.0449, 0.1417, 0.1697],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 47, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0693, 0.8557, 0.0058, 0.0551, 0.0061, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 47, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0504, 0.0040, 0.0050, 0.1930, 0.0178, 0.7019, 0.0277],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 47, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0034, 0.0034, 0.8062, 0.0040, 0.1299, 0.0489, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 47, batch: 156/195] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0059, 0.9659, 0.0043, 0.0044, 0.0056, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 47, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1150, 0.2171, 0.0566, 0.2231, 0.0449, 0.1370, 0.2063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 48, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.1195, 0.7641, 0.0066, 0.0933, 0.0052, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 48, batch: 78/195] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0593, 0.0039, 0.0049, 0.2054, 0.0239, 0.6493, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.002

[Epoch: 48, batch: 117/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0031, 0.0063, 0.7110, 0.0060, 0.2152, 0.0514, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 48, batch: 156/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0064, 0.9658, 0.0058, 0.0047, 0.0056, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 48, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1165, 0.2372, 0.0526, 0.2268, 0.0470, 0.1483, 0.1717],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 49, batch: 39/195] total loss per batch: 0.594
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0036, 0.0428, 0.8980, 0.0033, 0.0471, 0.0026, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 49, batch: 78/195] total loss per batch: 0.591
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0594, 0.0040, 0.0061, 0.1851, 0.0271, 0.6903, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 49, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.8006, 0.0050, 0.1371, 0.0429, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 49, batch: 156/195] total loss per batch: 0.607
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0039, 0.9756, 0.0026, 0.0033, 0.0041, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 49, batch: 195/195] total loss per batch: 0.631
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1130, 0.2203, 0.0584, 0.2372, 0.0410, 0.1330, 0.1972],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 50, batch: 39/195] total loss per batch: 0.595
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0069, 0.1504, 0.7222, 0.0100, 0.0901, 0.0144, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 50, batch: 78/195] total loss per batch: 0.592
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0487, 0.0067, 0.0074, 0.1968, 0.0146, 0.6763, 0.0496],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 50, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0030, 0.0045, 0.7804, 0.0052, 0.1602, 0.0406, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 50, batch: 156/195] total loss per batch: 0.607
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0067, 0.9617, 0.0066, 0.0053, 0.0061, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 50, batch: 195/195] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1180, 0.2354, 0.0514, 0.2277, 0.0498, 0.1428, 0.1748],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 51, batch: 39/195] total loss per batch: 0.595
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0584, 0.8406, 0.0060, 0.0813, 0.0035, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.005

[Epoch: 51, batch: 78/195] total loss per batch: 0.591
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0629, 0.0044, 0.0053, 0.2077, 0.0249, 0.6627, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 51, batch: 117/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0040, 0.7570, 0.0048, 0.1766, 0.0463, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 51, batch: 156/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0063, 0.9672, 0.0048, 0.0046, 0.0053, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 51, batch: 195/195] total loss per batch: 0.630
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1106, 0.2263, 0.0573, 0.2290, 0.0432, 0.1423, 0.1914],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 52, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0732, 0.8488, 0.0055, 0.0579, 0.0060, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 52, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0566, 0.0038, 0.0063, 0.1995, 0.0214, 0.6753, 0.0371],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 52, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0044, 0.7756, 0.0044, 0.1635, 0.0423, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 52, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.9682, 0.0049, 0.0043, 0.0050, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 52, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1180, 0.2323, 0.0555, 0.2274, 0.0463, 0.1388, 0.1816],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 53, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0859, 0.8216, 0.0063, 0.0717, 0.0051, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 53, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0576, 0.0043, 0.0062, 0.1870, 0.0183, 0.6825, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 53, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.7597, 0.0048, 0.1708, 0.0490, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 53, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0062, 0.9666, 0.0050, 0.0046, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 53, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1121, 0.2259, 0.0559, 0.2361, 0.0430, 0.1402, 0.1869],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 54, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0853, 0.8188, 0.0063, 0.0743, 0.0055, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 54, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0562, 0.0041, 0.0054, 0.1981, 0.0178, 0.6797, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 54, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0047, 0.7723, 0.0044, 0.1606, 0.0476, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 54, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0055, 0.9694, 0.0047, 0.0042, 0.0050, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 54, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1164, 0.2358, 0.0542, 0.2235, 0.0468, 0.1405, 0.1828],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 55, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0783, 0.8384, 0.0056, 0.0645, 0.0044, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 55, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0527, 0.0040, 0.0053, 0.1783, 0.0174, 0.7069, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 55, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0050, 0.7691, 0.0056, 0.1649, 0.0448, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 55, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0057, 0.9690, 0.0045, 0.0044, 0.0049, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 55, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1153, 0.2237, 0.0551, 0.2365, 0.0426, 0.1390, 0.1878],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 56, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0856, 0.8180, 0.0054, 0.0772, 0.0048, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 56, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0507, 0.0048, 0.0060, 0.2093, 0.0188, 0.6697, 0.0408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 56, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0037, 0.0050, 0.7801, 0.0050, 0.1556, 0.0454, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 56, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0053, 0.9703, 0.0048, 0.0039, 0.0050, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 56, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1139, 0.2372, 0.0546, 0.2223, 0.0479, 0.1426, 0.1814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 57, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0915, 0.8239, 0.0058, 0.0651, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 57, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0614, 0.0049, 0.0056, 0.1851, 0.0224, 0.6809, 0.0396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 57, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0043, 0.0057, 0.7409, 0.0058, 0.1865, 0.0491, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 57, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.9707, 0.0043, 0.0042, 0.0045, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 57, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1149, 0.2212, 0.0557, 0.2400, 0.0414, 0.1365, 0.1903],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 58, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0778, 0.8246, 0.0054, 0.0778, 0.0052, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 58, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0563, 0.0039, 0.0065, 0.1939, 0.0154, 0.6886, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 58, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0038, 0.0052, 0.7955, 0.0058, 0.1433, 0.0416, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 58, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.9721, 0.0045, 0.0038, 0.0044, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 58, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1195, 0.2379, 0.0536, 0.2149, 0.0492, 0.1435, 0.1814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 59, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0923, 0.8342, 0.0061, 0.0540, 0.0040, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 59, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0561, 0.0056, 0.0049, 0.1845, 0.0235, 0.6772, 0.0483],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 59, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0066, 0.7682, 0.0048, 0.1691, 0.0404, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 59, batch: 156/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.9720, 0.0043, 0.0046, 0.0040, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 59, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1053, 0.2195, 0.0574, 0.2464, 0.0403, 0.1395, 0.1916],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 60, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0887, 0.7892, 0.0087, 0.0936, 0.0071, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 60, batch: 78/195] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0636, 0.0044, 0.0069, 0.2014, 0.0160, 0.6754, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 60, batch: 117/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.7351, 0.0057, 0.1871, 0.0568, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 60, batch: 156/195] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.9713, 0.0043, 0.0033, 0.0048, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 60, batch: 195/195] total loss per batch: 0.629
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1260, 0.2421, 0.0523, 0.2165, 0.0480, 0.1374, 0.1776],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 61, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0908, 0.8240, 0.0046, 0.0658, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 61, batch: 78/195] total loss per batch: 0.589
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0505, 0.0046, 0.0046, 0.2203, 0.0174, 0.6611, 0.0416],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 61, batch: 117/195] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0039, 0.0041, 0.8317, 0.0036, 0.1203, 0.0314, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 61, batch: 156/195] total loss per batch: 0.605
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0047, 0.9747, 0.0038, 0.0041, 0.0040, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 61, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1062, 0.2232, 0.0562, 0.2364, 0.0447, 0.1440, 0.1894],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 62, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0773, 0.8334, 0.0063, 0.0667, 0.0056, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 62, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0546, 0.0044, 0.0065, 0.1765, 0.0198, 0.7000, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 62, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0057, 0.7468, 0.0058, 0.1850, 0.0462, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 62, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9709, 0.0045, 0.0039, 0.0046, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 62, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1216, 0.2288, 0.0563, 0.2272, 0.0455, 0.1388, 0.1818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 63, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0862, 0.8199, 0.0058, 0.0720, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 63, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0576, 0.0050, 0.0059, 0.1919, 0.0193, 0.6709, 0.0493],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 63, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0049, 0.7782, 0.0049, 0.1614, 0.0409, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 63, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0057, 0.9679, 0.0050, 0.0045, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 63, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1088, 0.2334, 0.0523, 0.2318, 0.0441, 0.1416, 0.1881],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 64, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0808, 0.8331, 0.0058, 0.0658, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 64, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0555, 0.0048, 0.0061, 0.1934, 0.0213, 0.6716, 0.0473],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 64, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0042, 0.0053, 0.7630, 0.0051, 0.1715, 0.0452, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 64, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0057, 0.9673, 0.0049, 0.0050, 0.0055, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 64, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1220, 0.2250, 0.0591, 0.2293, 0.0461, 0.1379, 0.1806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 65, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0811, 0.8317, 0.0051, 0.0687, 0.0042, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 65, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0597, 0.0049, 0.0062, 0.1992, 0.0212, 0.6651, 0.0437],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 65, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0058, 0.7681, 0.0057, 0.1629, 0.0468, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 65, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0057, 0.9678, 0.0050, 0.0046, 0.0053, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 65, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1058, 0.2366, 0.0508, 0.2304, 0.0428, 0.1429, 0.1908],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 66, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0896, 0.8233, 0.0053, 0.0674, 0.0049, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 66, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0534, 0.0047, 0.0064, 0.1901, 0.0209, 0.6787, 0.0457],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 66, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0043, 0.0048, 0.7788, 0.0047, 0.1620, 0.0404, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 66, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0056, 0.9681, 0.0047, 0.0048, 0.0053, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 66, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1272, 0.2231, 0.0588, 0.2298, 0.0460, 0.1352, 0.1798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 67, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0781, 0.8322, 0.0050, 0.0710, 0.0044, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 67, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0549, 0.0045, 0.0064, 0.1991, 0.0200, 0.6786, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 67, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0059, 0.7650, 0.0057, 0.1685, 0.0442, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 67, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.9669, 0.0051, 0.0047, 0.0056, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 67, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1040, 0.2355, 0.0510, 0.2310, 0.0436, 0.1465, 0.1884],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 68, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0941, 0.8208, 0.0050, 0.0659, 0.0049, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 68, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0509, 0.0042, 0.0053, 0.1849, 0.0216, 0.6893, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 68, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.7868, 0.0041, 0.1494, 0.0466, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 68, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9696, 0.0047, 0.0045, 0.0048, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 68, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1257, 0.2238, 0.0590, 0.2273, 0.0466, 0.1341, 0.1834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 69, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0806, 0.8154, 0.0056, 0.0810, 0.0060, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 69, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0610, 0.0055, 0.0076, 0.2021, 0.0214, 0.6677, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 69, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.7682, 0.0048, 0.1761, 0.0371, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 69, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.9695, 0.0048, 0.0045, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 69, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1038, 0.2358, 0.0518, 0.2352, 0.0421, 0.1453, 0.1862],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 70, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0948, 0.8191, 0.0058, 0.0642, 0.0050, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 70, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0438, 0.0038, 0.0044, 0.1649, 0.0214, 0.7190, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 70, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0043, 0.0038, 0.7806, 0.0041, 0.1498, 0.0530, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 70, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0061, 0.9657, 0.0056, 0.0046, 0.0058, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 70, batch: 195/195] total loss per batch: 0.628
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1254, 0.2202, 0.0587, 0.2233, 0.0525, 0.1379, 0.1820],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 71, batch: 39/195] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0811, 0.8173, 0.0070, 0.0756, 0.0070, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 71, batch: 78/195] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0650, 0.0052, 0.0061, 0.2116, 0.0212, 0.6560, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 71, batch: 117/195] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0044, 0.0058, 0.7411, 0.0045, 0.1907, 0.0472, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 71, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0058, 0.9663, 0.0056, 0.0050, 0.0059, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 71, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1060, 0.2382, 0.0535, 0.2321, 0.0399, 0.1404, 0.1899],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 72, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0782, 0.8331, 0.0053, 0.0694, 0.0042, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 72, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0546, 0.0041, 0.0050, 0.1885, 0.0160, 0.6971, 0.0347],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 72, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0041, 0.0045, 0.7682, 0.0054, 0.1662, 0.0466, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 72, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0060, 0.9659, 0.0053, 0.0049, 0.0057, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 72, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1224, 0.2259, 0.0562, 0.2300, 0.0461, 0.1398, 0.1796],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 73, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0796, 0.8380, 0.0055, 0.0633, 0.0044, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 73, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0575, 0.0045, 0.0047, 0.2009, 0.0178, 0.6754, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 73, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0050, 0.0060, 0.7550, 0.0059, 0.1733, 0.0486, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 73, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0060, 0.9673, 0.0050, 0.0046, 0.0053, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 73, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1074, 0.2334, 0.0534, 0.2300, 0.0444, 0.1405, 0.1910],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 74, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0806, 0.8332, 0.0049, 0.0676, 0.0043, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 74, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0538, 0.0045, 0.0058, 0.1902, 0.0193, 0.6904, 0.0360],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 74, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0047, 0.0051, 0.7758, 0.0058, 0.1606, 0.0426, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 74, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0055, 0.9692, 0.0048, 0.0043, 0.0050, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 74, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1227, 0.2276, 0.0555, 0.2290, 0.0442, 0.1386, 0.1824],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 75, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0875, 0.8208, 0.0054, 0.0711, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 75, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0533, 0.0048, 0.0052, 0.2038, 0.0195, 0.6756, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 75, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.7745, 0.0052, 0.1617, 0.0435, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 75, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0054, 0.9691, 0.0047, 0.0045, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 75, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1111, 0.2295, 0.0546, 0.2314, 0.0464, 0.1416, 0.1854],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 76, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0854, 0.8213, 0.0056, 0.0720, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 76, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0512, 0.0046, 0.0054, 0.1955, 0.0195, 0.6880, 0.0358],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 76, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.7793, 0.0048, 0.1614, 0.0411, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 76, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.9693, 0.0050, 0.0044, 0.0050, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 76, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1178, 0.2297, 0.0558, 0.2287, 0.0437, 0.1393, 0.1851],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 77, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0855, 0.8214, 0.0057, 0.0713, 0.0056, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 77, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0521, 0.0045, 0.0049, 0.2049, 0.0181, 0.6778, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 77, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.7708, 0.0048, 0.1660, 0.0443, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 77, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.9682, 0.0051, 0.0045, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 77, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1135, 0.2305, 0.0538, 0.2316, 0.0462, 0.1405, 0.1838],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 78, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0830, 0.8225, 0.0055, 0.0737, 0.0054, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 78, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0540, 0.0045, 0.0052, 0.1948, 0.0184, 0.6875, 0.0356],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 78, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0042, 0.7765, 0.0040, 0.1607, 0.0457, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 78, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9694, 0.0050, 0.0044, 0.0050, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 78, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1160, 0.2290, 0.0564, 0.2289, 0.0439, 0.1394, 0.1864],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 79, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0836, 0.8320, 0.0058, 0.0637, 0.0047, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 79, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0529, 0.0046, 0.0050, 0.1859, 0.0185, 0.6937, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 79, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.7571, 0.0059, 0.1713, 0.0493, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 79, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.9664, 0.0053, 0.0044, 0.0059, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 79, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1150, 0.2329, 0.0528, 0.2304, 0.0457, 0.1401, 0.1831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 80, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0838, 0.8186, 0.0055, 0.0776, 0.0049, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 80, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0572, 0.0049, 0.0063, 0.2049, 0.0183, 0.6627, 0.0457],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 80, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0049, 0.7702, 0.0043, 0.1649, 0.0463, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 80, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.9672, 0.0053, 0.0048, 0.0055, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 80, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1161, 0.2235, 0.0591, 0.2302, 0.0438, 0.1395, 0.1878],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 81, batch: 39/195] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0879, 0.8292, 0.0055, 0.0628, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 81, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0623, 0.0047, 0.0045, 0.1888, 0.0193, 0.6794, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 81, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0052, 0.0066, 0.7686, 0.0057, 0.1624, 0.0447, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 81, batch: 156/195] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0060, 0.9686, 0.0048, 0.0043, 0.0053, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 81, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1130, 0.2365, 0.0504, 0.2302, 0.0463, 0.1403, 0.1835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 82, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0787, 0.8302, 0.0051, 0.0710, 0.0053, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 82, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0508, 0.0045, 0.0056, 0.1983, 0.0181, 0.6810, 0.0417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 82, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.7782, 0.0046, 0.1626, 0.0401, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 82, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.9718, 0.0044, 0.0039, 0.0044, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 82, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1161, 0.2264, 0.0574, 0.2287, 0.0432, 0.1399, 0.1882],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 83, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0914, 0.8147, 0.0058, 0.0714, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 83, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0533, 0.0045, 0.0046, 0.2009, 0.0187, 0.6810, 0.0370],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 83, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0048, 0.7768, 0.0041, 0.1641, 0.0411, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 83, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.9735, 0.0041, 0.0037, 0.0045, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 83, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1147, 0.2310, 0.0539, 0.2301, 0.0470, 0.1414, 0.1820],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 84, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0851, 0.8196, 0.0056, 0.0730, 0.0062, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 84, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0535, 0.0042, 0.0048, 0.1989, 0.0191, 0.6817, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 84, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0044, 0.0047, 0.7769, 0.0039, 0.1635, 0.0420, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 84, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.9721, 0.0045, 0.0039, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 84, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1154, 0.2286, 0.0558, 0.2312, 0.0440, 0.1392, 0.1859],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 85, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0845, 0.8236, 0.0055, 0.0711, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 85, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0583, 0.0045, 0.0047, 0.1889, 0.0187, 0.6846, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 85, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0047, 0.0051, 0.7639, 0.0043, 0.1698, 0.0475, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 85, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.9719, 0.0044, 0.0039, 0.0047, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 85, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1152, 0.2310, 0.0548, 0.2286, 0.0459, 0.1405, 0.1840],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 86, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0825, 0.8297, 0.0053, 0.0677, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 86, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0563, 0.0044, 0.0052, 0.1951, 0.0192, 0.6784, 0.0414],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 86, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0051, 0.7738, 0.0042, 0.1619, 0.0454, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 86, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.9718, 0.0046, 0.0040, 0.0047, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 86, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1146, 0.2295, 0.0548, 0.2317, 0.0435, 0.1396, 0.1862],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 87, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0834, 0.8294, 0.0052, 0.0681, 0.0044, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 87, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0604, 0.0047, 0.0051, 0.1940, 0.0200, 0.6716, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 87, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0049, 0.0057, 0.7624, 0.0047, 0.1696, 0.0478, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 87, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.9730, 0.0041, 0.0037, 0.0045, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 87, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1152, 0.2309, 0.0554, 0.2277, 0.0466, 0.1398, 0.1844],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 88, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0860, 0.8256, 0.0046, 0.0697, 0.0049, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 88, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0531, 0.0046, 0.0057, 0.1941, 0.0194, 0.6803, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 88, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.7746, 0.0046, 0.1634, 0.0424, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 88, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.9733, 0.0043, 0.0037, 0.0044, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 88, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1146, 0.2291, 0.0542, 0.2330, 0.0428, 0.1405, 0.1858],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 89, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0847, 0.8224, 0.0057, 0.0717, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 89, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0584, 0.0047, 0.0053, 0.1957, 0.0218, 0.6741, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 89, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.7766, 0.0046, 0.1588, 0.0452, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 89, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.9748, 0.0037, 0.0036, 0.0041, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 89, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1158, 0.2297, 0.0563, 0.2251, 0.0477, 0.1393, 0.1861],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 90, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0891, 0.8203, 0.0046, 0.0705, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.004

[Epoch: 90, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0524, 0.0047, 0.0052, 0.1980, 0.0199, 0.6791, 0.0408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 90, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.7623, 0.0046, 0.1721, 0.0453, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 90, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0043, 0.9739, 0.0042, 0.0037, 0.0042, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 90, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1138, 0.2308, 0.0539, 0.2357, 0.0427, 0.1418, 0.1813],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 91, batch: 39/195] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0791, 0.8215, 0.0065, 0.0760, 0.0055, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 91, batch: 78/195] total loss per batch: 0.587
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0521, 0.0042, 0.0051, 0.1953, 0.0188, 0.6906, 0.0338],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 91, batch: 117/195] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.7793, 0.0049, 0.1552, 0.0461, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 91, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.9721, 0.0042, 0.0040, 0.0047, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 91, batch: 195/195] total loss per batch: 0.627
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1163, 0.2276, 0.0552, 0.2291, 0.0456, 0.1374, 0.1888],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 92, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0841, 0.8294, 0.0052, 0.0663, 0.0047, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 92, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0565, 0.0045, 0.0048, 0.1999, 0.0193, 0.6738, 0.0412],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 92, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.7484, 0.0062, 0.1774, 0.0509, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 92, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.9701, 0.0047, 0.0045, 0.0050, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 92, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1138, 0.2325, 0.0555, 0.2298, 0.0452, 0.1420, 0.1812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 93, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0875, 0.8235, 0.0055, 0.0687, 0.0049, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 93, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0563, 0.0045, 0.0052, 0.1914, 0.0187, 0.6845, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 93, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0051, 0.0059, 0.7662, 0.0061, 0.1654, 0.0451, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 93, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.9688, 0.0048, 0.0048, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 93, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1153, 0.2290, 0.0547, 0.2311, 0.0443, 0.1379, 0.1877],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 94, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0825, 0.8293, 0.0052, 0.0683, 0.0045, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 94, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0563, 0.0047, 0.0053, 0.2010, 0.0200, 0.6702, 0.0424],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 94, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0051, 0.0059, 0.7655, 0.0063, 0.1659, 0.0452, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 94, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.9690, 0.0048, 0.0048, 0.0051, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 94, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1146, 0.2310, 0.0549, 0.2286, 0.0455, 0.1416, 0.1838],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 95, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0881, 0.8209, 0.0055, 0.0700, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 95, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0545, 0.0046, 0.0055, 0.2000, 0.0211, 0.6743, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 95, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0050, 0.0056, 0.7728, 0.0056, 0.1628, 0.0427, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 95, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.9679, 0.0051, 0.0051, 0.0053, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 95, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1152, 0.2292, 0.0549, 0.2315, 0.0442, 0.1389, 0.1861],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 96, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0827, 0.8253, 0.0055, 0.0710, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 96, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0545, 0.0046, 0.0052, 0.1981, 0.0195, 0.6789, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 96, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0049, 0.0053, 0.7677, 0.0056, 0.1680, 0.0430, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 96, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0058, 0.9663, 0.0054, 0.0053, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 96, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1154, 0.2300, 0.0555, 0.2281, 0.0460, 0.1414, 0.1836],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 97, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0869, 0.8215, 0.0056, 0.0704, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 97, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0538, 0.0046, 0.0052, 0.1964, 0.0206, 0.6806, 0.0388],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 97, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0048, 0.0051, 0.7749, 0.0053, 0.1596, 0.0451, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 97, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0060, 0.9646, 0.0057, 0.0057, 0.0059, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 97, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1146, 0.2297, 0.0545, 0.2319, 0.0439, 0.1390, 0.1864],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 98, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0796, 0.8310, 0.0053, 0.0694, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 98, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0547, 0.0046, 0.0051, 0.1879, 0.0187, 0.6911, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 98, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.7624, 0.0055, 0.1712, 0.0450, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 98, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0062, 0.9642, 0.0057, 0.0055, 0.0061, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 98, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1159, 0.2305, 0.0555, 0.2279, 0.0461, 0.1409, 0.1832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 99, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0886, 0.8221, 0.0056, 0.0686, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 99, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0532, 0.0044, 0.0054, 0.1942, 0.0211, 0.6825, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 99, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0046, 0.0051, 0.7767, 0.0051, 0.1586, 0.0448, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 99, batch: 156/195] total loss per batch: 0.601
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0062, 0.9638, 0.0059, 0.0058, 0.0061, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 99, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 3
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1134, 0.2295, 0.0545, 0.2325, 0.0439, 0.1384, 0.1877],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

[Epoch: 100, batch: 39/195] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0850, 0.8250, 0.0050, 0.0700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0792, 0.8333, 0.0051, 0.0683, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.002 -0.003

[Epoch: 100, batch: 78/195] total loss per batch: 0.586
Policy (actual, predicted): 5 5
Policy data: tensor([0.0550, 0.0050, 0.0050, 0.1950, 0.0200, 0.6800, 0.0400])
Policy pred: tensor([0.0590, 0.0053, 0.0051, 0.1946, 0.0183, 0.6768, 0.0409],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.011 -0.003

[Epoch: 100, batch: 117/195] total loss per batch: 0.597
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.7700, 0.0050, 0.1650, 0.0450, 0.0050])
Policy pred: tensor([0.0052, 0.0054, 0.7588, 0.0057, 0.1761, 0.0432, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 100, batch: 156/195] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9700, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0064, 0.9645, 0.0055, 0.0052, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 100, batch: 195/195] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.1150, 0.2300, 0.0550, 0.2300, 0.0450, 0.1400, 0.1850])
Policy pred: tensor([0.1176, 0.2300, 0.0554, 0.2266, 0.0463, 0.1423, 0.1818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.193 -0.002

