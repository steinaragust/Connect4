Training set samples: 6918
Batch size: 32
[Epoch: 1, batch: 43/217] total loss per batch: 1.041
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.5134e-10, 2.6899e-10, 1.0000e+00, 1.8298e-12, 5.2024e-12, 6.9210e-11,
        8.6909e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 1, batch: 86/217] total loss per batch: 0.991
Policy (actual, predicted): 0 2
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([2.6687e-01, 9.9921e-08, 3.8159e-01, 1.2047e-06, 1.9323e-09, 3.5154e-01,
        1.0247e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.392

[Epoch: 1, batch: 129/217] total loss per batch: 0.908
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0308, 0.0182, 0.0045, 0.9044, 0.0136, 0.0071, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.021

[Epoch: 1, batch: 172/217] total loss per batch: 0.926
Policy (actual, predicted): 5 6
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.3953e-01, 2.8811e-08, 3.2569e-01, 5.4230e-08, 3.9564e-07, 1.8228e-01,
        3.5251e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 -0.000

[Epoch: 1, batch: 215/217] total loss per batch: 0.937
Policy (actual, predicted): 0 1
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.2560, 0.6075, 0.0208, 0.0370, 0.0073, 0.0351, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 2, batch: 43/217] total loss per batch: 0.731
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.9979e-10, 8.0995e-11, 1.0000e+00, 1.1792e-12, 2.6674e-12, 1.4124e-10,
        5.8125e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 2, batch: 86/217] total loss per batch: 0.727
Policy (actual, predicted): 0 2
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([2.7187e-01, 1.0932e-08, 3.8391e-01, 3.0435e-07, 4.7470e-10, 3.4422e-01,
        1.1867e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.645

[Epoch: 2, batch: 129/217] total loss per batch: 0.659
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0293, 0.0298, 0.0059, 0.9120, 0.0046, 0.0074, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.028

[Epoch: 2, batch: 172/217] total loss per batch: 0.654
Policy (actual, predicted): 5 2
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.0316e-01, 4.1606e-09, 4.2698e-01, 6.7372e-09, 2.5361e-08, 1.7275e-01,
        2.9711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.041

[Epoch: 2, batch: 215/217] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9761, 0.0024, 0.0040, 0.0059, 0.0014, 0.0064, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 3, batch: 43/217] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.2848e-11, 1.1878e-11, 1.0000e+00, 6.6073e-14, 1.1763e-12, 7.6349e-12,
        2.4186e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 3, batch: 86/217] total loss per batch: 0.590
Policy (actual, predicted): 0 2
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([3.0296e-01, 2.0533e-09, 4.4631e-01, 1.6542e-08, 3.2415e-11, 2.5073e-01,
        1.0730e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.723

[Epoch: 3, batch: 129/217] total loss per batch: 0.544
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0081, 0.0437, 0.0049, 0.9308, 0.0022, 0.0049, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.012

[Epoch: 3, batch: 172/217] total loss per batch: 0.545
Policy (actual, predicted): 5 2
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.1267e-01, 3.5497e-10, 3.9986e-01, 1.3642e-09, 6.9136e-09, 2.3763e-01,
        2.4984e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.068

[Epoch: 3, batch: 215/217] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9795, 0.0027, 0.0052, 0.0049, 0.0014, 0.0037, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 4, batch: 43/217] total loss per batch: 0.520
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1074e-11, 8.6431e-13, 1.0000e+00, 1.0147e-14, 2.1946e-13, 3.9696e-12,
        1.2685e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 4, batch: 86/217] total loss per batch: 0.531
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.2993e-01, 2.9760e-09, 3.4376e-01, 2.8323e-08, 4.3767e-11, 2.2631e-01,
        1.2471e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.901

[Epoch: 4, batch: 129/217] total loss per batch: 0.495
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0088, 0.0212, 0.0065, 0.9485, 0.0026, 0.0056, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.040

[Epoch: 4, batch: 172/217] total loss per batch: 0.504
Policy (actual, predicted): 5 2
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.3653e-01, 6.5288e-10, 3.9639e-01, 1.1510e-09, 3.8149e-09, 2.3249e-01,
        2.3459e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.053

[Epoch: 4, batch: 215/217] total loss per batch: 0.512
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9729, 0.0041, 0.0030, 0.0077, 0.0024, 0.0056, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.026

[Epoch: 5, batch: 43/217] total loss per batch: 0.497
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2721e-11, 8.3302e-13, 1.0000e+00, 3.0587e-14, 2.1377e-13, 2.8723e-12,
        1.7125e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 5, batch: 86/217] total loss per batch: 0.508
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1093e-01, 2.5083e-09, 2.8833e-01, 2.2824e-08, 2.8450e-11, 2.0074e-01,
        1.7025e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.905

[Epoch: 5, batch: 129/217] total loss per batch: 0.475
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0096, 0.0095, 0.0087, 0.9554, 0.0022, 0.0048, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.018

[Epoch: 5, batch: 172/217] total loss per batch: 0.481
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.4063e-01, 6.1506e-10, 2.9712e-01, 2.2283e-09, 5.8483e-09, 3.5646e-01,
        2.0578e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.075

[Epoch: 5, batch: 215/217] total loss per batch: 0.491
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9764, 0.0045, 0.0058, 0.0038, 0.0024, 0.0032, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.025

[Epoch: 6, batch: 43/217] total loss per batch: 0.482
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2326e-11, 2.4829e-12, 1.0000e+00, 5.0491e-14, 4.2706e-13, 2.2311e-12,
        1.1745e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 6, batch: 86/217] total loss per batch: 0.495
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.8347e-01, 1.0887e-08, 2.8822e-01, 7.2718e-08, 1.1384e-10, 2.2831e-01,
        3.4752e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 6, batch: 129/217] total loss per batch: 0.459
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0063, 0.0062, 0.0045, 0.9676, 0.0021, 0.0055, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.036

[Epoch: 6, batch: 172/217] total loss per batch: 0.469
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.3158e-01, 2.1556e-09, 2.4778e-01, 5.0128e-09, 1.2007e-08, 3.3811e-01,
        2.8254e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.085

[Epoch: 6, batch: 215/217] total loss per batch: 0.482
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9634, 0.0069, 0.0088, 0.0056, 0.0033, 0.0059, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.023

[Epoch: 7, batch: 43/217] total loss per batch: 0.475
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.6673e-11, 2.0553e-12, 1.0000e+00, 3.2045e-14, 3.2216e-13, 2.6075e-12,
        5.8020e-14], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 7, batch: 86/217] total loss per batch: 0.484
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.9512e-01, 5.5864e-09, 1.9355e-01, 6.7803e-08, 8.5618e-11, 3.1133e-01,
        2.0438e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.899

[Epoch: 7, batch: 129/217] total loss per batch: 0.454
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0075, 0.0129, 0.0092, 0.9484, 0.0031, 0.0075, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.020

[Epoch: 7, batch: 172/217] total loss per batch: 0.459
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.7371e-01, 5.5530e-10, 1.5559e-01, 2.5511e-09, 6.4316e-09, 4.0684e-01,
        2.6386e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.097

[Epoch: 7, batch: 215/217] total loss per batch: 0.476
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9681, 0.0061, 0.0060, 0.0059, 0.0034, 0.0047, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 8, batch: 43/217] total loss per batch: 0.466
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.6161e-11, 3.5641e-12, 1.0000e+00, 8.0414e-14, 9.2700e-13, 4.4948e-12,
        1.1103e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 8, batch: 86/217] total loss per batch: 0.475
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5501e-01, 1.0165e-08, 3.2174e-01, 4.8569e-08, 1.4997e-10, 1.2325e-01,
        5.1306e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.912

[Epoch: 8, batch: 129/217] total loss per batch: 0.449
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0082, 0.0110, 0.0047, 0.9585, 0.0025, 0.0053, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.031

[Epoch: 8, batch: 172/217] total loss per batch: 0.453
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9574e-01, 1.5965e-09, 1.3318e-01, 2.8364e-09, 8.5654e-09, 4.1665e-01,
        2.5443e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.119

[Epoch: 8, batch: 215/217] total loss per batch: 0.469
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9435, 0.0114, 0.0089, 0.0127, 0.0057, 0.0087, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 9, batch: 43/217] total loss per batch: 0.461
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0854e-10, 5.7662e-12, 1.0000e+00, 9.4255e-14, 4.9129e-13, 4.9399e-12,
        5.4281e-14], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 9, batch: 86/217] total loss per batch: 0.469
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5054e-01, 1.9119e-08, 1.9286e-01, 1.1097e-07, 2.6747e-10, 2.5660e-01,
        2.0694e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.928

[Epoch: 9, batch: 129/217] total loss per batch: 0.443
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0092, 0.0185, 0.0084, 0.9372, 0.0033, 0.0105, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.021

[Epoch: 9, batch: 172/217] total loss per batch: 0.449
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1774e-01, 1.4745e-09, 8.0659e-02, 5.0180e-09, 1.1343e-08, 4.1154e-01,
        2.9006e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.146

[Epoch: 9, batch: 215/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9427, 0.0165, 0.0081, 0.0097, 0.0053, 0.0096, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 10, batch: 43/217] total loss per batch: 0.459
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.4390e-11, 1.0461e-11, 1.0000e+00, 2.9046e-13, 7.1288e-13, 3.9329e-12,
        2.1647e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 10, batch: 86/217] total loss per batch: 0.465
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7490e-01, 2.7340e-08, 2.6439e-01, 1.6145e-07, 6.5258e-10, 1.6071e-01,
        2.5637e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.925

[Epoch: 10, batch: 129/217] total loss per batch: 0.439
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0082, 0.0158, 0.0078, 0.9449, 0.0047, 0.0075, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.025

[Epoch: 10, batch: 172/217] total loss per batch: 0.446
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3222e-01, 1.6642e-09, 7.9443e-02, 4.5554e-09, 1.7541e-08, 4.3078e-01,
        2.5756e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.162

[Epoch: 10, batch: 215/217] total loss per batch: 0.460
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9430, 0.0173, 0.0078, 0.0072, 0.0065, 0.0086, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 11, batch: 43/217] total loss per batch: 0.459
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.4451e-11, 4.8573e-12, 1.0000e+00, 9.4786e-14, 9.0587e-13, 2.5689e-12,
        2.7046e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 11, batch: 86/217] total loss per batch: 0.464
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7914e-01, 2.1923e-08, 2.0072e-01, 1.4698e-07, 3.4174e-10, 2.2014e-01,
        7.8243e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.949

[Epoch: 11, batch: 129/217] total loss per batch: 0.438
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0066, 0.0216, 0.0070, 0.9408, 0.0036, 0.0074, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.012

[Epoch: 11, batch: 172/217] total loss per batch: 0.445
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9751e-01, 2.3238e-09, 6.2014e-02, 7.5622e-09, 1.3280e-08, 4.5218e-01,
        2.8829e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.181

[Epoch: 11, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9406, 0.0181, 0.0074, 0.0089, 0.0064, 0.0086, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 12, batch: 43/217] total loss per batch: 0.457
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5014e-10, 1.3379e-11, 1.0000e+00, 4.5965e-13, 1.7915e-12, 2.8007e-12,
        7.9166e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 12, batch: 86/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4425e-01, 4.8172e-08, 2.0642e-01, 1.9330e-07, 8.5245e-10, 2.4933e-01,
        4.7731e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.924

[Epoch: 12, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0067, 0.0182, 0.0076, 0.9449, 0.0038, 0.0063, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 12, batch: 172/217] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9696e-01, 4.0881e-09, 4.9829e-02, 8.2097e-09, 3.3121e-08, 4.6583e-01,
        2.8738e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.197

[Epoch: 12, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9156, 0.0278, 0.0091, 0.0122, 0.0069, 0.0111, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 13, batch: 43/217] total loss per batch: 0.456
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0956e-10, 3.1735e-11, 1.0000e+00, 2.7777e-13, 3.9267e-12, 9.1237e-12,
        2.1035e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 13, batch: 86/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2905e-01, 4.4793e-08, 2.6664e-01, 1.4203e-07, 3.7355e-10, 2.0431e-01,
        2.2381e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 13, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0046, 0.0139, 0.0034, 0.9633, 0.0022, 0.0051, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.020

[Epoch: 13, batch: 172/217] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3924e-01, 4.2859e-09, 5.8107e-02, 2.1115e-08, 1.5263e-08, 4.5357e-01,
        2.4908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.184

[Epoch: 13, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9431, 0.0198, 0.0059, 0.0061, 0.0062, 0.0084, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 14, batch: 43/217] total loss per batch: 0.457
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2875e-10, 3.3236e-11, 1.0000e+00, 3.6867e-12, 4.6945e-12, 1.2093e-11,
        1.0951e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 14, batch: 86/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4066e-01, 9.3034e-08, 2.0385e-01, 3.4755e-07, 1.8121e-09, 2.5549e-01,
        3.5427e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.914

[Epoch: 14, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0097, 0.0225, 0.0082, 0.9305, 0.0055, 0.0113, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 14, batch: 172/217] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2474e-01, 6.0060e-09, 4.5049e-02, 2.5838e-08, 8.0069e-08, 4.4823e-01,
        2.8198e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.220

[Epoch: 14, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9446, 0.0201, 0.0055, 0.0086, 0.0060, 0.0075, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 15, batch: 43/217] total loss per batch: 0.454
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9381e-10, 3.9017e-11, 1.0000e+00, 1.1324e-12, 4.3825e-12, 2.3894e-11,
        5.0687e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 15, batch: 86/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3031e-01, 1.0324e-07, 2.6015e-01, 5.0693e-07, 2.7993e-09, 2.0954e-01,
        2.9428e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.955

[Epoch: 15, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0099, 0.0180, 0.0070, 0.9370, 0.0060, 0.0076, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 15, batch: 172/217] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1456e-01, 5.1784e-09, 5.9198e-02, 1.4397e-08, 5.7822e-08, 4.4846e-01,
        2.7779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.220

[Epoch: 15, batch: 215/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9151, 0.0269, 0.0114, 0.0097, 0.0079, 0.0126, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 16, batch: 43/217] total loss per batch: 0.454
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8113e-10, 8.3296e-11, 1.0000e+00, 3.5783e-12, 1.8686e-11, 2.2861e-11,
        2.5092e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 16, batch: 86/217] total loss per batch: 0.459
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7866e-01, 1.7128e-07, 1.8432e-01, 3.5391e-07, 3.2975e-09, 2.3702e-01,
        3.1463e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.914

[Epoch: 16, batch: 129/217] total loss per batch: 0.435
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0071, 0.0238, 0.0049, 0.9476, 0.0036, 0.0061, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 16, batch: 172/217] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.8706e-01, 1.3129e-08, 3.8120e-02, 4.4786e-08, 6.6380e-08, 4.9710e-01,
        2.7772e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.221

[Epoch: 16, batch: 215/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9386, 0.0198, 0.0058, 0.0082, 0.0076, 0.0083, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 17, batch: 43/217] total loss per batch: 0.454
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1003e-09, 6.1098e-11, 1.0000e+00, 2.5950e-12, 6.3673e-12, 2.1221e-11,
        4.5303e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 17, batch: 86/217] total loss per batch: 0.460
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2371e-01, 1.5287e-07, 2.8073e-01, 5.3412e-07, 2.6360e-09, 1.9556e-01,
        2.9128e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.931

[Epoch: 17, batch: 129/217] total loss per batch: 0.435
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0076, 0.0187, 0.0058, 0.9444, 0.0052, 0.0053, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 17, batch: 172/217] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.8478e-01, 9.6296e-09, 6.4525e-02, 2.9391e-08, 6.0748e-08, 3.4283e-01,
        3.0787e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.244

[Epoch: 17, batch: 215/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9155, 0.0343, 0.0082, 0.0094, 0.0076, 0.0116, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 18, batch: 43/217] total loss per batch: 0.454
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.1980e-10, 2.8249e-10, 1.0000e+00, 2.1724e-12, 2.1026e-11, 1.2717e-11,
        8.8353e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 18, batch: 86/217] total loss per batch: 0.460
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2219e-01, 2.0923e-07, 2.0062e-01, 9.1253e-07, 5.9721e-09, 2.7719e-01,
        8.0900e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 18, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0037, 0.0184, 0.0048, 0.9559, 0.0038, 0.0049, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 18, batch: 172/217] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9086e-01, 1.3271e-08, 4.9850e-02, 4.2922e-08, 7.8804e-08, 5.0304e-01,
        2.5624e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.235

[Epoch: 18, batch: 215/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9582, 0.0119, 0.0052, 0.0059, 0.0047, 0.0069, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 19, batch: 43/217] total loss per batch: 0.454
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6954e-09, 9.7081e-10, 1.0000e+00, 1.2888e-11, 7.6978e-11, 4.2481e-10,
        1.1644e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 19, batch: 86/217] total loss per batch: 0.460
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5163e-01, 3.2350e-07, 2.7315e-01, 1.2855e-06, 1.2611e-08, 1.7522e-01,
        5.7019e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 19, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0086, 0.0277, 0.0057, 0.9297, 0.0071, 0.0071, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 19, batch: 172/217] total loss per batch: 0.447
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.6312e-01, 3.2179e-08, 4.2524e-02, 9.9368e-08, 2.2524e-07, 4.5601e-01,
        2.3835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.242

[Epoch: 19, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9183, 0.0301, 0.0100, 0.0100, 0.0057, 0.0116, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 20, batch: 43/217] total loss per batch: 0.459
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.3859e-10, 5.4479e-10, 1.0000e+00, 3.4819e-12, 7.6267e-12, 8.2949e-11,
        1.0091e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 20, batch: 86/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0498e-01, 2.6290e-07, 2.3942e-01, 9.5756e-07, 1.5799e-08, 2.5560e-01,
        3.3347e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.894

[Epoch: 20, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0200, 0.0056, 0.9534, 0.0045, 0.0059, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 20, batch: 172/217] total loss per batch: 0.447
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5116e-01, 2.0827e-08, 5.0546e-02, 5.8314e-08, 7.6734e-08, 4.0383e-01,
        2.9446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.240

[Epoch: 20, batch: 215/217] total loss per batch: 0.460
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9397, 0.0213, 0.0067, 0.0038, 0.0102, 0.0093, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 21, batch: 43/217] total loss per batch: 0.466
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.7183e-10, 6.3112e-10, 1.0000e+00, 6.2750e-12, 2.4886e-11, 6.0209e-11,
        6.7985e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 21, batch: 86/217] total loss per batch: 0.481
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.6603e-01, 3.7846e-07, 2.6638e-01, 4.7681e-07, 1.0378e-09, 2.6760e-01,
        8.3953e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.882

[Epoch: 21, batch: 129/217] total loss per batch: 0.503
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0142, 0.1401, 0.0057, 0.8202, 0.0069, 0.0050, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 21, batch: 172/217] total loss per batch: 0.545
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.1822e-01, 3.8565e-08, 1.6156e-01, 7.4755e-08, 2.4239e-08, 4.5273e-01,
        2.6749e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.181

[Epoch: 21, batch: 215/217] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9029, 0.0167, 0.0069, 0.0151, 0.0021, 0.0109, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.027

[Epoch: 22, batch: 43/217] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6877e-11, 5.1351e-12, 1.0000e+00, 3.9990e-13, 1.9974e-13, 6.7197e-14,
        7.0819e-15], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 22, batch: 86/217] total loss per batch: 0.583
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4486e-01, 3.7269e-08, 2.8411e-01, 2.8132e-07, 3.5436e-10, 1.7103e-01,
        1.2132e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 22, batch: 129/217] total loss per batch: 0.540
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0073, 0.0162, 0.0192, 0.8919, 0.0031, 0.0114, 0.0509],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.027

[Epoch: 22, batch: 172/217] total loss per batch: 0.533
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.7920e-01, 2.6117e-08, 1.1836e-01, 2.3282e-08, 4.1386e-09, 4.4547e-01,
        2.5697e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.161

[Epoch: 22, batch: 215/217] total loss per batch: 0.530
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.7867, 0.1913, 0.0011, 0.0039, 0.0023, 0.0105, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 23, batch: 43/217] total loss per batch: 0.513
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9882e-10, 4.5485e-11, 1.0000e+00, 1.5278e-12, 1.1277e-12, 1.8308e-13,
        2.5458e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 23, batch: 86/217] total loss per batch: 0.519
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1614e-01, 6.4567e-07, 2.9325e-01, 1.0791e-06, 3.8522e-09, 1.9061e-01,
        7.5627e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.941

[Epoch: 23, batch: 129/217] total loss per batch: 0.476
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0111, 0.0184, 0.9511, 0.0025, 0.0072, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.024

[Epoch: 23, batch: 172/217] total loss per batch: 0.466
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.0317e-01, 2.3270e-08, 9.2644e-02, 1.7577e-07, 5.7558e-08, 4.1505e-01,
        2.8914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.180

[Epoch: 23, batch: 215/217] total loss per batch: 0.478
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9839, 0.0027, 0.0021, 0.0024, 0.0019, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 24, batch: 43/217] total loss per batch: 0.472
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.2116e-10, 6.9038e-10, 1.0000e+00, 1.5833e-11, 4.6306e-11, 1.8875e-12,
        2.7631e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 24, batch: 86/217] total loss per batch: 0.474
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1807e-01, 1.0109e-06, 2.4153e-01, 4.9268e-07, 6.3212e-09, 2.4040e-01,
        2.8256e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 24, batch: 129/217] total loss per batch: 0.447
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0074, 0.0136, 0.0242, 0.9334, 0.0033, 0.0087, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.025

[Epoch: 24, batch: 172/217] total loss per batch: 0.447
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1901e-01, 2.7680e-08, 5.4748e-02, 1.0602e-07, 7.0919e-08, 4.3955e-01,
        2.8670e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.206

[Epoch: 24, batch: 215/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9729, 0.0057, 0.0026, 0.0067, 0.0024, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 25, batch: 43/217] total loss per batch: 0.456
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0886e-09, 2.2390e-09, 1.0000e+00, 4.8132e-11, 6.5263e-11, 5.2417e-12,
        2.3039e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 25, batch: 86/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4704e-01, 1.5161e-06, 2.3987e-01, 7.0424e-07, 6.1226e-09, 2.1309e-01,
        4.4791e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.921

[Epoch: 25, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0071, 0.0155, 0.0205, 0.9382, 0.0035, 0.0073, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.018

[Epoch: 25, batch: 172/217] total loss per batch: 0.441
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4163e-01, 2.9783e-08, 4.3854e-02, 1.5122e-07, 5.9014e-08, 4.2642e-01,
        2.8809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.222

[Epoch: 25, batch: 215/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9689, 0.0066, 0.0028, 0.0067, 0.0029, 0.0065, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 26, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.3314e-10, 8.2909e-10, 1.0000e+00, 1.9456e-11, 4.5399e-11, 2.4433e-12,
        1.2065e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 26, batch: 86/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6103e-01, 1.4151e-06, 2.2017e-01, 9.5615e-07, 5.4462e-09, 2.1880e-01,
        2.3388e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.916

[Epoch: 26, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0068, 0.0169, 0.0173, 0.9370, 0.0039, 0.0079, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 26, batch: 172/217] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3074e-01, 2.2543e-08, 4.2830e-02, 1.2446e-07, 6.0267e-08, 4.5756e-01,
        2.6887e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.259

[Epoch: 26, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9436, 0.0126, 0.0051, 0.0120, 0.0051, 0.0110, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 27, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0166e-09, 1.2118e-09, 1.0000e+00, 2.8679e-11, 5.0041e-11, 3.9769e-12,
        1.5750e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 27, batch: 86/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4235e-01, 1.6508e-06, 2.4928e-01, 8.4168e-07, 5.4424e-09, 2.0837e-01,
        2.5727e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.923

[Epoch: 27, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0066, 0.0160, 0.0141, 0.9429, 0.0038, 0.0067, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 27, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2966e-01, 2.4196e-08, 4.2734e-02, 1.2225e-07, 6.9522e-08, 4.5538e-01,
        2.7222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.269

[Epoch: 27, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9531, 0.0118, 0.0041, 0.0093, 0.0042, 0.0087, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 28, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.9486e-10, 9.6956e-10, 1.0000e+00, 1.6168e-11, 5.2710e-11, 2.5186e-12,
        1.1840e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 28, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5743e-01, 1.2852e-06, 2.1617e-01, 1.0914e-06, 5.7625e-09, 2.2639e-01,
        2.7351e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.926

[Epoch: 28, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0059, 0.0182, 0.0126, 0.9433, 0.0042, 0.0070, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.022

[Epoch: 28, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4211e-01, 1.8561e-08, 4.1593e-02, 1.0837e-07, 5.2798e-08, 4.4386e-01,
        2.7244e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.293

[Epoch: 28, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9393, 0.0152, 0.0055, 0.0106, 0.0056, 0.0117, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 29, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.1898e-10, 7.8208e-10, 1.0000e+00, 1.7058e-11, 2.9099e-11, 2.2533e-12,
        1.3640e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 29, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3721e-01, 1.1490e-06, 2.4658e-01, 8.4116e-07, 3.9982e-09, 2.1621e-01,
        2.7729e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.927

[Epoch: 29, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0065, 0.0175, 0.0103, 0.9452, 0.0040, 0.0069, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 29, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3628e-01, 1.9468e-08, 4.3314e-02, 9.8248e-08, 4.3051e-08, 4.4857e-01,
        2.7184e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.303

[Epoch: 29, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9387, 0.0202, 0.0048, 0.0101, 0.0053, 0.0102, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 30, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.3371e-10, 5.8658e-10, 1.0000e+00, 1.4032e-11, 3.1549e-11, 2.0729e-12,
        1.1234e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 30, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5286e-01, 8.2652e-07, 2.0302e-01, 9.3793e-07, 3.5966e-09, 2.4412e-01,
        3.3817e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 30, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0193, 0.0097, 0.9446, 0.0044, 0.0071, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.019

[Epoch: 30, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3446e-01, 1.7048e-08, 3.8417e-02, 1.1157e-07, 4.7106e-08, 4.6765e-01,
        2.5947e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.299

[Epoch: 30, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9385, 0.0179, 0.0053, 0.0100, 0.0051, 0.0113, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 31, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.7744e-10, 5.2985e-10, 1.0000e+00, 1.0452e-11, 1.8171e-11, 1.2039e-12,
        1.5240e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 31, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1951e-01, 8.9416e-07, 2.5213e-01, 8.2788e-07, 3.9523e-09, 2.2836e-01,
        2.5520e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 31, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0068, 0.0203, 0.0101, 0.9404, 0.0044, 0.0071, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 31, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2742e-01, 1.6222e-08, 4.3524e-02, 8.4058e-08, 4.8399e-08, 4.5096e-01,
        2.7809e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.296

[Epoch: 31, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9268, 0.0248, 0.0056, 0.0126, 0.0067, 0.0119, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 32, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.5309e-10, 5.8381e-10, 1.0000e+00, 2.0023e-11, 3.8209e-11, 2.2146e-12,
        1.3573e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 32, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7752e-01, 6.4987e-07, 2.1025e-01, 7.7144e-07, 3.5772e-09, 2.1222e-01,
        4.1545e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.930

[Epoch: 32, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0202, 0.0083, 0.9453, 0.0047, 0.0068, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 32, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.7418e-01, 1.6191e-08, 3.7140e-02, 1.1099e-07, 3.8165e-08, 4.2292e-01,
        2.6576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 32, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9467, 0.0154, 0.0039, 0.0098, 0.0048, 0.0094, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 33, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.4780e-10, 6.5145e-10, 1.0000e+00, 1.1964e-11, 1.7598e-11, 1.4855e-12,
        1.3066e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 33, batch: 86/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.8170e-01, 8.7966e-07, 2.6318e-01, 8.2241e-07, 3.6049e-09, 2.5511e-01,
        2.5007e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.925

[Epoch: 33, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0171, 0.0078, 0.9500, 0.0042, 0.0059, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 33, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.7710e-01, 1.7121e-08, 4.7211e-02, 8.1910e-08, 5.3040e-08, 5.3237e-01,
        2.4333e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.292

[Epoch: 33, batch: 215/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9191, 0.0282, 0.0069, 0.0117, 0.0087, 0.0134, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 34, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4136e-09, 1.1775e-09, 1.0000e+00, 3.3922e-11, 1.6222e-10, 6.7405e-12,
        6.8289e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 34, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6685e-01, 8.5383e-07, 2.3244e-01, 1.2135e-06, 7.5509e-09, 2.0071e-01,
        1.8895e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.926

[Epoch: 34, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0268, 0.0083, 0.9383, 0.0043, 0.0076, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 34, batch: 172/217] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.6981e-01, 2.4559e-08, 3.4412e-02, 1.3555e-07, 3.9637e-08, 3.6356e-01,
        3.3222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.325

[Epoch: 34, batch: 215/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9419, 0.0187, 0.0046, 0.0086, 0.0062, 0.0083, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 35, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.8835e-10, 7.9176e-10, 1.0000e+00, 1.8518e-11, 3.5261e-11, 3.4508e-12,
        1.2219e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 35, batch: 86/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4899e-01, 2.8373e-06, 2.3611e-01, 1.4248e-06, 8.2424e-09, 2.1490e-01,
        1.2138e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.904

[Epoch: 35, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0062, 0.0198, 0.0071, 0.9445, 0.0052, 0.0061, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 35, batch: 172/217] total loss per batch: 0.440
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.0352e-01, 4.6338e-08, 4.8742e-02, 1.8313e-07, 7.6488e-08, 4.9745e-01,
        2.5029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.274

[Epoch: 35, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9183, 0.0286, 0.0068, 0.0152, 0.0076, 0.0124, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 36, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0665e-09, 1.8094e-09, 1.0000e+00, 5.5729e-11, 1.0382e-10, 1.1116e-11,
        2.9227e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 36, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0267e-01, 8.3875e-07, 2.3688e-01, 9.2514e-07, 7.7319e-09, 2.6045e-01,
        9.1273e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.931

[Epoch: 36, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0239, 0.0092, 0.9410, 0.0041, 0.0066, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 36, batch: 172/217] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2559e-01, 2.4685e-08, 3.4742e-02, 1.9473e-07, 5.0878e-08, 4.1021e-01,
        3.2945e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.330

[Epoch: 36, batch: 215/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9336, 0.0260, 0.0037, 0.0089, 0.0065, 0.0096, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 37, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3099e-09, 8.2997e-10, 1.0000e+00, 2.4693e-11, 1.0042e-10, 7.2296e-12,
        1.8533e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 37, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.9725e-01, 2.7540e-06, 1.8634e-01, 2.4369e-06, 1.2675e-08, 2.1640e-01,
        2.0798e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.924

[Epoch: 37, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0064, 0.0244, 0.0083, 0.9363, 0.0054, 0.0075, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.022

[Epoch: 37, batch: 172/217] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4923e-01, 9.0165e-08, 4.4366e-02, 1.5800e-07, 8.4176e-08, 4.4222e-01,
        2.6418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.289

[Epoch: 37, batch: 215/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9393, 0.0171, 0.0059, 0.0098, 0.0077, 0.0118, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 38, batch: 43/217] total loss per batch: 0.452
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9888e-09, 1.5976e-09, 1.0000e+00, 5.9439e-11, 6.0086e-11, 1.1915e-11,
        7.0135e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 38, batch: 86/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3398e-01, 2.5227e-06, 2.6937e-01, 1.8661e-06, 6.7348e-09, 1.9664e-01,
        1.0471e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.915

[Epoch: 38, batch: 129/217] total loss per batch: 0.433
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0208, 0.0069, 0.9451, 0.0050, 0.0063, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 38, batch: 172/217] total loss per batch: 0.440
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1334e-01, 2.0418e-08, 4.9957e-02, 1.6822e-07, 4.4844e-08, 4.3837e-01,
        2.9833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.308

[Epoch: 38, batch: 215/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9114, 0.0306, 0.0044, 0.0181, 0.0081, 0.0166, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 39, batch: 43/217] total loss per batch: 0.459
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.8707e-09, 2.5221e-09, 1.0000e+00, 7.1282e-12, 3.2193e-11, 4.6816e-12,
        3.6716e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 39, batch: 86/217] total loss per batch: 0.472
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.7691e-01, 5.9173e-07, 2.4586e-01, 1.1333e-06, 1.7681e-08, 2.7723e-01,
        9.3980e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.883

[Epoch: 39, batch: 129/217] total loss per batch: 0.462
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0130, 0.0208, 0.0232, 0.9142, 0.0061, 0.0095, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.027

[Epoch: 39, batch: 172/217] total loss per batch: 0.459
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.7484e-01, 1.3632e-08, 6.0820e-02, 1.2043e-07, 3.7296e-08, 3.8310e-01,
        2.8123e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.263

[Epoch: 39, batch: 215/217] total loss per batch: 0.481
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9672, 0.0125, 0.0034, 0.0034, 0.0038, 0.0030, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 40, batch: 43/217] total loss per batch: 0.484
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.2877e-09, 1.0347e-09, 1.0000e+00, 4.8341e-11, 9.1989e-12, 1.1474e-12,
        1.3086e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 40, batch: 86/217] total loss per batch: 0.483
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.9998e-01, 1.3307e-06, 1.8900e-01, 1.2106e-06, 7.6842e-09, 3.1102e-01,
        1.2396e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.923

[Epoch: 40, batch: 129/217] total loss per batch: 0.457
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0040, 0.0229, 0.0141, 0.8499, 0.0024, 0.0119, 0.0947],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 40, batch: 172/217] total loss per batch: 0.476
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.2721e-01, 1.5960e-08, 7.4384e-02, 2.3858e-07, 1.7083e-08, 5.2128e-01,
        2.7713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.315

[Epoch: 40, batch: 215/217] total loss per batch: 0.485
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.7063, 0.2475, 0.0037, 0.0186, 0.0084, 0.0054, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.010

[Epoch: 41, batch: 43/217] total loss per batch: 0.478
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3575e-08, 7.1191e-10, 1.0000e+00, 4.7825e-12, 3.2095e-12, 1.8795e-11,
        4.6188e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 41, batch: 86/217] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([7.2297e-01, 5.8717e-06, 1.1646e-01, 1.8981e-06, 1.7312e-08, 1.6056e-01,
        9.2713e-10], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 41, batch: 129/217] total loss per batch: 0.472
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0026, 0.0118, 0.0041, 0.9716, 0.0019, 0.0046, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 41, batch: 172/217] total loss per batch: 0.469
Policy (actual, predicted): 5 6
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([3.2820e-01, 1.4230e-08, 7.5454e-02, 2.1732e-06, 1.5776e-07, 2.3984e-01,
        3.5651e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.253

[Epoch: 41, batch: 215/217] total loss per batch: 0.478
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9508, 0.0033, 0.0046, 0.0264, 0.0045, 0.0029, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 42, batch: 43/217] total loss per batch: 0.475
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.7974e-09, 2.3255e-10, 1.0000e+00, 1.3438e-11, 8.8455e-12, 4.9928e-12,
        6.9256e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 42, batch: 86/217] total loss per batch: 0.490
Policy (actual, predicted): 0 5
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([3.5650e-01, 1.3702e-05, 1.6577e-01, 1.3708e-05, 1.9400e-07, 4.7771e-01,
        3.7809e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.853

[Epoch: 42, batch: 129/217] total loss per batch: 0.451
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0332, 0.0068, 0.9372, 0.0031, 0.0063, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 42, batch: 172/217] total loss per batch: 0.450
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1330e-01, 2.1850e-08, 5.2471e-02, 7.0574e-07, 5.7110e-08, 4.6393e-01,
        2.7030e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.266

[Epoch: 42, batch: 215/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9546, 0.0082, 0.0045, 0.0194, 0.0032, 0.0029, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.024

[Epoch: 43, batch: 43/217] total loss per batch: 0.456
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2270e-08, 2.9702e-10, 1.0000e+00, 7.3475e-11, 3.4644e-11, 4.6962e-12,
        1.3661e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 43, batch: 86/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.8944e-01, 3.1462e-06, 2.6033e-01, 2.4561e-05, 8.7883e-08, 1.5020e-01,
        1.6211e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.972

[Epoch: 43, batch: 129/217] total loss per batch: 0.435
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0059, 0.0253, 0.0089, 0.9412, 0.0032, 0.0048, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 43, batch: 172/217] total loss per batch: 0.441
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3064e-01, 4.2902e-08, 4.6181e-02, 1.1271e-06, 1.6250e-07, 4.5190e-01,
        2.7128e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.261

[Epoch: 43, batch: 215/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9527, 0.0104, 0.0054, 0.0124, 0.0052, 0.0030, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 44, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.8705e-09, 3.0280e-10, 1.0000e+00, 5.7395e-11, 1.4417e-11, 1.3174e-12,
        1.6325e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 44, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4081e-01, 3.1861e-06, 2.1307e-01, 1.4548e-05, 8.1819e-08, 2.4611e-01,
        2.2000e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.968

[Epoch: 44, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0208, 0.0066, 0.9482, 0.0034, 0.0058, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 44, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5555e-01, 2.6148e-08, 3.5271e-02, 7.5196e-07, 9.1120e-08, 4.4081e-01,
        2.6837e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.236

[Epoch: 44, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9509, 0.0106, 0.0051, 0.0147, 0.0047, 0.0037, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 45, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.3565e-09, 3.9149e-10, 1.0000e+00, 9.1310e-11, 1.1303e-11, 2.1768e-12,
        1.6332e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 45, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5968e-01, 2.0900e-06, 2.3304e-01, 1.2724e-05, 5.5120e-08, 2.0726e-01,
        2.5633e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.971

[Epoch: 45, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0073, 0.0243, 0.0076, 0.9397, 0.0041, 0.0057, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 45, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4699e-01, 2.0930e-08, 3.5777e-02, 5.6073e-07, 1.1351e-07, 4.3840e-01,
        2.7883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.295

[Epoch: 45, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9482, 0.0152, 0.0056, 0.0111, 0.0060, 0.0042, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 46, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.1580e-09, 1.8811e-10, 1.0000e+00, 4.5415e-11, 9.0414e-12, 1.1997e-12,
        3.0860e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 46, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5413e-01, 1.2628e-06, 2.2373e-01, 8.8093e-06, 5.4940e-08, 2.2213e-01,
        1.9145e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.972

[Epoch: 46, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0058, 0.0241, 0.0061, 0.9447, 0.0038, 0.0058, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 46, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3773e-01, 1.4908e-08, 3.5017e-02, 5.1057e-07, 9.3830e-08, 4.4892e-01,
        2.7833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.289

[Epoch: 46, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9478, 0.0153, 0.0055, 0.0124, 0.0060, 0.0037, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 47, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9037e-09, 1.3736e-10, 1.0000e+00, 3.5269e-11, 6.3635e-12, 1.3218e-12,
        7.2430e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 47, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4952e-01, 1.3647e-06, 2.2668e-01, 9.4362e-06, 3.8635e-08, 2.2380e-01,
        1.0768e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.967

[Epoch: 47, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0065, 0.0246, 0.0063, 0.9439, 0.0037, 0.0057, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 47, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3056e-01, 1.2288e-08, 3.7651e-02, 3.3776e-07, 8.6411e-08, 4.6126e-01,
        2.7053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.295

[Epoch: 47, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9387, 0.0197, 0.0057, 0.0113, 0.0071, 0.0053, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 48, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1853e-09, 1.4129e-10, 1.0000e+00, 3.1811e-11, 5.3211e-12, 8.6214e-13,
        2.6229e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 48, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3926e-01, 1.1495e-06, 2.4569e-01, 7.3385e-06, 3.0245e-08, 2.1504e-01,
        1.4974e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.969

[Epoch: 48, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0255, 0.0057, 0.9418, 0.0042, 0.0064, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 48, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3913e-01, 1.2808e-08, 3.4074e-02, 5.0099e-07, 8.7601e-08, 4.5203e-01,
        2.7477e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.326

[Epoch: 48, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9435, 0.0197, 0.0050, 0.0106, 0.0070, 0.0045, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 49, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.7360e-09, 1.0310e-10, 1.0000e+00, 2.6351e-11, 7.6971e-12, 1.1456e-12,
        6.1048e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 49, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4039e-01, 1.3204e-06, 2.3431e-01, 7.8049e-06, 3.7973e-08, 2.2529e-01,
        1.1801e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.963

[Epoch: 49, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0204, 0.0046, 0.9541, 0.0030, 0.0043, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 49, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3092e-01, 8.6005e-09, 3.8820e-02, 2.4960e-07, 6.1856e-08, 4.5891e-01,
        2.7136e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.316

[Epoch: 49, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9354, 0.0226, 0.0061, 0.0105, 0.0077, 0.0058, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 50, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9453e-09, 1.2187e-10, 1.0000e+00, 2.8744e-11, 3.8108e-12, 1.1308e-12,
        3.1107e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 50, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6981e-01, 1.0209e-06, 2.2311e-01, 8.4719e-06, 2.7359e-08, 2.0706e-01,
        1.0075e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.963

[Epoch: 50, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0069, 0.0287, 0.0057, 0.9370, 0.0046, 0.0065, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 50, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3710e-01, 1.5083e-08, 3.1889e-02, 3.9010e-07, 8.2467e-08, 4.7513e-01,
        2.5588e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.308

[Epoch: 50, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9343, 0.0223, 0.0057, 0.0118, 0.0077, 0.0059, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 51, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.2704e-09, 9.6623e-11, 1.0000e+00, 2.7269e-11, 8.2921e-12, 1.2100e-12,
        5.1282e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 51, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0999e-01, 1.2167e-06, 2.3646e-01, 9.2609e-06, 2.3934e-08, 2.5353e-01,
        1.1918e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.958

[Epoch: 51, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0046, 0.0227, 0.0048, 0.9497, 0.0038, 0.0053, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 51, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3049e-01, 1.1475e-08, 4.0158e-02, 3.1039e-07, 7.2099e-08, 4.3954e-01,
        2.8981e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 51, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9344, 0.0246, 0.0063, 0.0090, 0.0076, 0.0065, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 52, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2776e-09, 8.9746e-11, 1.0000e+00, 2.3328e-11, 4.9042e-12, 8.6772e-13,
        1.5256e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 52, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.8021e-01, 9.4190e-07, 2.1204e-01, 6.8718e-06, 2.4246e-08, 2.0774e-01,
        5.2776e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.957

[Epoch: 52, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0063, 0.0276, 0.0051, 0.9415, 0.0042, 0.0054, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 52, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3487e-01, 9.5834e-09, 3.6926e-02, 3.0466e-07, 5.7962e-08, 4.6699e-01,
        2.6122e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.325

[Epoch: 52, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9370, 0.0223, 0.0056, 0.0099, 0.0081, 0.0065, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 53, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5431e-09, 9.7155e-11, 1.0000e+00, 2.5429e-11, 7.0340e-12, 1.1660e-12,
        1.0581e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 53, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1806e-01, 9.4924e-07, 2.3919e-01, 7.3401e-06, 1.4927e-08, 2.4274e-01,
        1.1403e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.952

[Epoch: 53, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0237, 0.0052, 0.9453, 0.0041, 0.0064, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 53, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4431e-01, 1.0562e-08, 3.4395e-02, 3.1973e-07, 6.4676e-08, 4.3923e-01,
        2.8206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.304

[Epoch: 53, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9290, 0.0254, 0.0061, 0.0100, 0.0093, 0.0070, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 54, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3689e-09, 5.2491e-11, 1.0000e+00, 1.2953e-11, 3.2848e-12, 4.3001e-13,
        1.5175e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 54, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5067e-01, 8.6103e-07, 2.3740e-01, 6.4273e-06, 2.2997e-08, 2.1192e-01,
        5.0583e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.953

[Epoch: 54, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0059, 0.0290, 0.0055, 0.9407, 0.0047, 0.0054, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 54, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2941e-01, 1.0998e-08, 4.1628e-02, 2.7642e-07, 6.1201e-08, 4.7056e-01,
        2.5840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.322

[Epoch: 54, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9370, 0.0224, 0.0056, 0.0094, 0.0083, 0.0076, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 55, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5472e-09, 9.2590e-11, 1.0000e+00, 2.7853e-11, 5.1721e-12, 8.0121e-13,
        6.9300e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 55, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3049e-01, 9.8262e-07, 2.2548e-01, 5.5808e-06, 1.3330e-08, 2.4402e-01,
        9.4324e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.960

[Epoch: 55, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0220, 0.0044, 0.9470, 0.0039, 0.0061, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 55, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2102e-01, 9.3900e-09, 3.0215e-02, 2.4425e-07, 5.3086e-08, 4.4499e-01,
        3.0378e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.281

[Epoch: 55, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9314, 0.0243, 0.0062, 0.0103, 0.0093, 0.0072, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 56, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2168e-09, 4.4357e-11, 1.0000e+00, 1.2516e-11, 3.2548e-12, 5.8437e-13,
        2.0501e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 56, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4412e-01, 1.0123e-06, 2.3613e-01, 7.1479e-06, 1.9025e-08, 2.1974e-01,
        6.9829e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.948

[Epoch: 56, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0058, 0.0254, 0.0050, 0.9460, 0.0045, 0.0053, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 56, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.7018e-01, 1.2630e-08, 5.1221e-02, 3.3011e-07, 9.8980e-08, 4.3532e-01,
        2.4328e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.367

[Epoch: 56, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9351, 0.0232, 0.0058, 0.0103, 0.0079, 0.0072, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 57, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.8212e-09, 1.8692e-10, 1.0000e+00, 2.2074e-11, 4.2416e-12, 7.2978e-13,
        1.2449e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 57, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2775e-01, 1.8380e-06, 2.6361e-01, 7.2261e-06, 2.2830e-08, 2.0864e-01,
        7.8293e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.957

[Epoch: 57, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0316, 0.0051, 0.9346, 0.0046, 0.0066, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 57, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2359e-01, 1.9567e-08, 2.8996e-02, 3.2985e-07, 1.5047e-07, 4.7011e-01,
        2.7731e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.294

[Epoch: 57, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9259, 0.0236, 0.0060, 0.0117, 0.0111, 0.0101, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 58, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.4239e-09, 2.2428e-10, 1.0000e+00, 6.6693e-11, 1.8837e-11, 2.6768e-12,
        4.0628e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 58, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4439e-01, 1.4118e-06, 2.2163e-01, 9.6501e-06, 1.7860e-08, 2.3397e-01,
        1.1548e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.955

[Epoch: 58, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0167, 0.0038, 0.9538, 0.0037, 0.0066, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 58, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3308e-01, 3.7788e-08, 4.9492e-02, 5.9004e-07, 1.1338e-07, 4.5547e-01,
        2.6196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 58, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9426, 0.0206, 0.0058, 0.0083, 0.0074, 0.0075, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 59, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.0980e-09, 3.4715e-10, 1.0000e+00, 9.5500e-11, 1.5469e-11, 3.0436e-12,
        9.0231e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 59, batch: 86/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3354e-01, 1.8140e-06, 2.0951e-01, 5.1055e-06, 1.9604e-08, 2.5694e-01,
        6.6058e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 59, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0083, 0.0317, 0.0058, 0.9286, 0.0051, 0.0073, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 59, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.0862e-01, 3.5733e-08, 3.4132e-02, 5.5443e-07, 9.0813e-08, 4.6073e-01,
        2.9652e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.264

[Epoch: 59, batch: 215/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9215, 0.0277, 0.0069, 0.0102, 0.0102, 0.0092, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 60, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6351e-09, 1.3740e-10, 1.0000e+00, 4.0287e-11, 4.0202e-12, 1.2948e-12,
        1.2889e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 60, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6108e-01, 1.9777e-06, 2.5572e-01, 9.3535e-06, 2.8259e-08, 1.8319e-01,
        1.8163e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 60, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0043, 0.0184, 0.0047, 0.9579, 0.0034, 0.0046, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 60, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.9580e-01, 2.1004e-08, 3.3922e-02, 3.8361e-07, 1.2702e-07, 4.1382e-01,
        2.5646e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 60, batch: 215/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9371, 0.0217, 0.0067, 0.0083, 0.0082, 0.0063, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.023

[Epoch: 61, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3176e-09, 1.0679e-10, 1.0000e+00, 1.2812e-11, 4.9125e-12, 5.5380e-12,
        1.3661e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 61, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5829e-01, 2.3848e-06, 2.3297e-01, 2.0273e-06, 2.7166e-08, 2.0874e-01,
        1.0683e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.892

[Epoch: 61, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0064, 0.0274, 0.0043, 0.9293, 0.0061, 0.0096, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 61, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.7710e-01, 4.1883e-08, 2.4061e-02, 4.7078e-07, 1.6466e-07, 5.1722e-01,
        2.8162e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.269

[Epoch: 61, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9520, 0.0150, 0.0046, 0.0076, 0.0055, 0.0088, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 62, batch: 43/217] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0283e-08, 1.0583e-09, 1.0000e+00, 2.7938e-10, 1.2542e-09, 7.0969e-11,
        3.4362e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 62, batch: 86/217] total loss per batch: 0.459
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6651e-01, 4.5279e-06, 2.1461e-01, 2.2548e-06, 1.9912e-08, 2.1887e-01,
        4.2238e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.901

[Epoch: 62, batch: 129/217] total loss per batch: 0.432
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0046, 0.0174, 0.0038, 0.9576, 0.0027, 0.0065, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 62, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5974e-01, 4.1041e-08, 6.4942e-02, 6.7920e-07, 1.2863e-07, 3.9213e-01,
        2.8319e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.277

[Epoch: 62, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9321, 0.0312, 0.0056, 0.0070, 0.0076, 0.0046, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.028

[Epoch: 63, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.7671e-09, 3.8452e-10, 1.0000e+00, 1.8138e-10, 1.9971e-11, 2.8863e-11,
        2.3318e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 63, batch: 86/217] total loss per batch: 0.457
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.3190e-01, 2.0838e-06, 2.6453e-01, 4.0883e-06, 3.8084e-08, 3.0357e-01,
        3.7272e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.922

[Epoch: 63, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0072, 0.0187, 0.0050, 0.9530, 0.0050, 0.0029, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 63, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3877e-01, 3.0637e-08, 6.1117e-02, 4.1939e-07, 1.3806e-07, 4.6369e-01,
        2.3642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 63, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9324, 0.0214, 0.0068, 0.0136, 0.0070, 0.0078, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 64, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8014e-09, 2.8293e-10, 1.0000e+00, 5.5814e-11, 6.4646e-11, 2.6470e-11,
        1.6515e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 64, batch: 86/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([6.2104e-01, 3.7570e-06, 2.0202e-01, 3.3438e-06, 5.6452e-08, 1.7693e-01,
        7.6827e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.916

[Epoch: 64, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0153, 0.0052, 0.9520, 0.0044, 0.0062, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 64, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.8400e-01, 4.4772e-08, 2.4080e-02, 6.9880e-07, 3.8583e-07, 4.5507e-01,
        2.3684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.292

[Epoch: 64, batch: 215/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9363, 0.0226, 0.0059, 0.0105, 0.0084, 0.0071, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.021

[Epoch: 65, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3253e-09, 1.7284e-10, 1.0000e+00, 3.5615e-11, 2.5075e-11, 1.2023e-11,
        6.8286e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 65, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0637e-01, 2.3933e-06, 2.1655e-01, 3.3813e-06, 2.8467e-08, 2.7708e-01,
        9.2023e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.949

[Epoch: 65, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0275, 0.0055, 0.9447, 0.0049, 0.0045, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 65, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1001e-01, 1.3796e-08, 5.3772e-02, 2.5852e-07, 1.0340e-07, 4.1529e-01,
        3.2093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.346

[Epoch: 65, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9355, 0.0214, 0.0058, 0.0112, 0.0068, 0.0081, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 66, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1297e-09, 4.3265e-10, 1.0000e+00, 7.1151e-11, 2.7892e-11, 1.8345e-11,
        2.0844e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 66, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5456e-01, 2.2006e-06, 2.5099e-01, 3.8729e-06, 5.0543e-08, 1.9445e-01,
        1.0190e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 66, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0072, 0.0263, 0.0074, 0.9342, 0.0067, 0.0063, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 66, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4254e-01, 1.6805e-08, 3.3488e-02, 3.5769e-07, 1.0940e-07, 4.6932e-01,
        2.5465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 66, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9340, 0.0242, 0.0057, 0.0114, 0.0086, 0.0073, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 67, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5590e-09, 1.7691e-10, 1.0000e+00, 3.3576e-11, 1.2670e-11, 1.0556e-11,
        8.3649e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 67, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5437e-01, 2.8324e-06, 2.2549e-01, 4.7933e-06, 1.7493e-08, 2.2013e-01,
        4.9207e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.932

[Epoch: 67, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0218, 0.0045, 0.9492, 0.0053, 0.0048, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 67, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2653e-01, 2.9620e-08, 3.5352e-02, 3.1916e-07, 1.2717e-07, 4.5874e-01,
        2.7938e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.335

[Epoch: 67, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9187, 0.0292, 0.0068, 0.0097, 0.0094, 0.0099, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 68, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1922e-09, 2.6413e-10, 1.0000e+00, 3.5051e-11, 1.0363e-11, 1.7352e-11,
        1.1193e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 68, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3797e-01, 2.1309e-06, 2.2239e-01, 3.9023e-06, 5.6348e-08, 2.3963e-01,
        1.2523e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.951

[Epoch: 68, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0254, 0.0066, 0.9401, 0.0050, 0.0055, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 68, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4328e-01, 2.8982e-08, 3.6695e-02, 2.9003e-07, 2.0353e-07, 4.5767e-01,
        2.6235e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.291

[Epoch: 68, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9387, 0.0211, 0.0054, 0.0110, 0.0077, 0.0081, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 69, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1627e-09, 3.0127e-10, 1.0000e+00, 5.3639e-11, 1.9264e-11, 1.2527e-11,
        2.9308e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 69, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4528e-01, 3.3123e-06, 2.2415e-01, 4.5362e-06, 1.6359e-08, 2.3056e-01,
        8.0459e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 69, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0066, 0.0252, 0.0052, 0.9419, 0.0051, 0.0055, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 69, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2715e-01, 3.2123e-08, 3.4277e-02, 2.1071e-07, 1.1010e-07, 4.5365e-01,
        2.8492e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.336

[Epoch: 69, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9214, 0.0285, 0.0061, 0.0123, 0.0096, 0.0087, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 70, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5662e-09, 2.7392e-10, 1.0000e+00, 2.2427e-11, 7.7075e-12, 7.1240e-12,
        1.1112e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 70, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3118e-01, 2.0081e-06, 2.3857e-01, 5.2682e-06, 3.5303e-08, 2.3024e-01,
        1.1078e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 70, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0304, 0.0060, 0.9361, 0.0050, 0.0058, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 70, batch: 172/217] total loss per batch: 0.445
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1629e-01, 2.6037e-08, 3.6138e-02, 1.6157e-07, 1.1864e-07, 4.6471e-01,
        2.8286e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.310

[Epoch: 70, batch: 215/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9360, 0.0247, 0.0042, 0.0117, 0.0094, 0.0074, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 71, batch: 43/217] total loss per batch: 0.456
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8207e-10, 5.5003e-10, 1.0000e+00, 3.8703e-11, 4.0605e-12, 2.1225e-12,
        1.6803e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 71, batch: 86/217] total loss per batch: 0.463
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4083e-01, 1.9664e-06, 2.0201e-01, 8.1217e-06, 1.7886e-08, 2.5708e-01,
        6.7323e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 71, batch: 129/217] total loss per batch: 0.447
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0143, 0.0068, 0.9500, 0.0062, 0.0045, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 71, batch: 172/217] total loss per batch: 0.473
Policy (actual, predicted): 5 6
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3650e-01, 2.4008e-09, 5.4114e-02, 4.8742e-07, 8.3002e-08, 3.1589e-01,
        3.9350e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.367

[Epoch: 71, batch: 215/217] total loss per batch: 0.492
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9470, 0.0105, 0.0031, 0.0127, 0.0061, 0.0091, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 72, batch: 43/217] total loss per batch: 0.498
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7371e-08, 9.6786e-10, 1.0000e+00, 5.0090e-11, 3.1507e-08, 2.6224e-08,
        2.0966e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 72, batch: 86/217] total loss per batch: 0.483
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3737e-01, 1.3975e-06, 3.6953e-01, 1.1756e-06, 6.9259e-09, 9.3098e-02,
        1.8828e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.746

[Epoch: 72, batch: 129/217] total loss per batch: 0.457
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0142, 0.0076, 0.9477, 0.0060, 0.0062, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.017

[Epoch: 72, batch: 172/217] total loss per batch: 0.462
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2394e-01, 4.9328e-08, 3.4541e-02, 3.1816e-07, 1.4289e-07, 4.8706e-01,
        2.5446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.302

[Epoch: 72, batch: 215/217] total loss per batch: 0.478
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9222, 0.0279, 0.0055, 0.0154, 0.0064, 0.0166, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.032

[Epoch: 73, batch: 43/217] total loss per batch: 0.463
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.0046e-09, 9.5526e-10, 1.0000e+00, 1.1428e-11, 2.5529e-10, 4.0016e-10,
        1.5604e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 73, batch: 86/217] total loss per batch: 0.462
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5979e-01, 2.5899e-06, 2.8146e-01, 1.4603e-06, 8.7129e-09, 1.5874e-01,
        1.1949e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.899

[Epoch: 73, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0102, 0.0089, 0.9561, 0.0068, 0.0041, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 73, batch: 172/217] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1412e-01, 6.2800e-09, 2.7783e-02, 1.5817e-07, 1.3062e-07, 5.0177e-01,
        2.5633e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.314

[Epoch: 73, batch: 215/217] total loss per batch: 0.455
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9313, 0.0206, 0.0052, 0.0195, 0.0069, 0.0103, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.022

[Epoch: 74, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8861e-09, 4.8955e-10, 1.0000e+00, 1.7996e-11, 5.3497e-10, 2.2782e-10,
        1.0743e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 74, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6282e-01, 1.9313e-06, 2.4745e-01, 1.3608e-06, 9.3665e-09, 1.8972e-01,
        9.7384e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.927

[Epoch: 74, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0133, 0.0079, 0.9544, 0.0070, 0.0054, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 74, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3497e-01, 7.8660e-09, 2.7271e-02, 1.1831e-07, 7.7204e-08, 4.5885e-01,
        2.7891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.322

[Epoch: 74, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9234, 0.0307, 0.0055, 0.0158, 0.0084, 0.0094, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 75, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2936e-09, 4.5784e-10, 1.0000e+00, 1.1296e-11, 2.5766e-10, 1.3981e-10,
        1.4549e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 75, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4080e-01, 1.7490e-06, 2.4586e-01, 1.1775e-06, 5.4057e-09, 2.1334e-01,
        9.2746e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.926

[Epoch: 75, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0159, 0.0077, 0.9506, 0.0069, 0.0056, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 75, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4101e-01, 6.5547e-09, 2.6725e-02, 9.0063e-08, 6.4396e-08, 4.5113e-01,
        2.8113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.315

[Epoch: 75, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9259, 0.0235, 0.0053, 0.0182, 0.0088, 0.0110, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 76, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1768e-09, 3.8362e-10, 1.0000e+00, 9.7257e-12, 2.2963e-10, 1.2257e-10,
        1.5105e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 76, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4239e-01, 1.5795e-06, 2.3870e-01, 1.3016e-06, 5.1782e-09, 2.1891e-01,
        9.1758e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 76, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0185, 0.0077, 0.9480, 0.0069, 0.0055, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 76, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3904e-01, 5.4228e-09, 2.8679e-02, 8.6929e-08, 5.5166e-08, 4.6036e-01,
        2.7193e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 76, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9267, 0.0264, 0.0055, 0.0152, 0.0088, 0.0097, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 77, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5653e-09, 2.5610e-10, 1.0000e+00, 6.1435e-12, 1.5934e-10, 8.1993e-11,
        1.1884e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 77, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5004e-01, 1.3555e-06, 2.3016e-01, 1.0497e-06, 3.7668e-09, 2.1980e-01,
        6.7158e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 77, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0209, 0.0077, 0.9449, 0.0067, 0.0056, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 77, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2912e-01, 4.7279e-09, 3.0509e-02, 7.4956e-08, 4.9074e-08, 4.6546e-01,
        2.7490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.326

[Epoch: 77, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9298, 0.0242, 0.0053, 0.0141, 0.0090, 0.0096, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 78, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6705e-09, 2.1854e-10, 1.0000e+00, 5.4301e-12, 1.3823e-10, 7.0572e-11,
        1.4195e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 78, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3803e-01, 1.2134e-06, 2.3771e-01, 1.0097e-06, 3.0769e-09, 2.2426e-01,
        7.8610e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.932

[Epoch: 78, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0211, 0.0075, 0.9454, 0.0064, 0.0056, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 78, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4173e-01, 3.8053e-09, 3.2487e-02, 7.0683e-08, 4.3912e-08, 4.5339e-01,
        2.7240e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.328

[Epoch: 78, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9306, 0.0244, 0.0053, 0.0126, 0.0090, 0.0097, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 79, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2019e-09, 1.6893e-10, 1.0000e+00, 3.6601e-12, 1.0154e-10, 5.3854e-11,
        1.2028e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 79, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4764e-01, 1.0606e-06, 2.2812e-01, 8.0990e-07, 2.5434e-09, 2.2424e-01,
        5.1550e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.930

[Epoch: 79, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0241, 0.0074, 0.9422, 0.0066, 0.0058, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 79, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2817e-01, 3.5043e-09, 3.3237e-02, 6.0271e-08, 3.8274e-08, 4.6736e-01,
        2.7123e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 79, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9298, 0.0244, 0.0054, 0.0123, 0.0095, 0.0097, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.019

[Epoch: 80, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2964e-09, 1.5617e-10, 1.0000e+00, 3.4312e-12, 8.9196e-11, 4.2712e-11,
        1.0085e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 80, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4272e-01, 9.0948e-07, 2.3655e-01, 7.1744e-07, 2.0667e-09, 2.2073e-01,
        5.9769e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 80, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0219, 0.0070, 0.9453, 0.0061, 0.0056, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 80, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4073e-01, 3.0801e-09, 3.4064e-02, 5.5405e-08, 4.0805e-08, 4.4944e-01,
        2.7577e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.327

[Epoch: 80, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9309, 0.0242, 0.0052, 0.0118, 0.0094, 0.0095, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 81, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.3767e-10, 1.3917e-10, 1.0000e+00, 2.4338e-12, 7.5404e-11, 3.5463e-11,
        1.5435e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 81, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3496e-01, 9.2640e-07, 2.3387e-01, 6.8719e-07, 1.9321e-09, 2.3117e-01,
        3.6910e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 81, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0059, 0.0259, 0.0071, 0.9402, 0.0064, 0.0059, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 81, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3237e-01, 2.6908e-09, 3.3950e-02, 4.3360e-08, 2.7672e-08, 4.7200e-01,
        2.6168e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 81, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9292, 0.0258, 0.0052, 0.0106, 0.0096, 0.0097, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.018

[Epoch: 82, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.1716e-10, 1.3356e-10, 1.0000e+00, 2.1666e-12, 4.4273e-11, 2.1771e-11,
        6.3575e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 82, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5739e-01, 9.0166e-07, 2.2245e-01, 6.1454e-07, 1.8436e-09, 2.2015e-01,
        3.5422e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.937

[Epoch: 82, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0203, 0.0067, 0.9480, 0.0058, 0.0053, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 82, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3281e-01, 2.0113e-09, 3.4928e-02, 4.4105e-08, 3.2026e-08, 4.3766e-01,
        2.9460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 82, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9332, 0.0220, 0.0049, 0.0116, 0.0094, 0.0094, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 83, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3131e-09, 1.2784e-10, 1.0000e+00, 2.8611e-12, 1.0002e-10, 3.6257e-11,
        4.3933e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 83, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2134e-01, 7.2643e-07, 2.4652e-01, 8.1326e-07, 1.9468e-09, 2.3214e-01,
        5.2282e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.933

[Epoch: 83, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0059, 0.0293, 0.0066, 0.9371, 0.0063, 0.0061, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 83, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3850e-01, 3.1400e-09, 3.5374e-02, 7.4207e-08, 3.4906e-08, 4.9888e-01,
        2.2725e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.311

[Epoch: 83, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9247, 0.0273, 0.0059, 0.0114, 0.0105, 0.0100, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.017

[Epoch: 84, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2665e-09, 1.6652e-10, 1.0000e+00, 4.2023e-12, 5.0971e-11, 2.3182e-11,
        6.1514e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 84, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6814e-01, 1.4983e-06, 2.3396e-01, 9.3361e-07, 2.2254e-09, 1.9790e-01,
        2.5841e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 84, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0201, 0.0056, 0.9480, 0.0057, 0.0053, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 84, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3967e-01, 2.8402e-09, 3.7905e-02, 4.9367e-08, 2.4146e-08, 4.1345e-01,
        3.0897e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.325

[Epoch: 84, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9342, 0.0217, 0.0050, 0.0107, 0.0085, 0.0096, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 85, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0694e-09, 2.5960e-10, 1.0000e+00, 8.5287e-12, 1.3791e-10, 4.2022e-11,
        4.8388e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 85, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2845e-01, 1.1995e-06, 2.2817e-01, 9.8005e-07, 1.7326e-09, 2.4337e-01,
        8.6971e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.932

[Epoch: 85, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0058, 0.0230, 0.0061, 0.9457, 0.0054, 0.0053, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 85, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.0300e-01, 1.0126e-08, 4.2478e-02, 1.3288e-07, 9.5824e-08, 4.8479e-01,
        2.6973e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.298

[Epoch: 85, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9282, 0.0256, 0.0058, 0.0125, 0.0104, 0.0087, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.016

[Epoch: 86, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0105e-09, 2.7813e-10, 1.0000e+00, 6.5268e-12, 8.5404e-11, 3.3604e-11,
        1.9021e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 86, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4940e-01, 2.3729e-06, 2.3875e-01, 1.3990e-06, 2.7117e-09, 2.1184e-01,
        6.3772e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 86, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0061, 0.0289, 0.0061, 0.9358, 0.0063, 0.0055, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 86, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.8171e-01, 5.2056e-09, 4.0953e-02, 7.5189e-08, 2.0131e-08, 3.9652e-01,
        2.8082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.310

[Epoch: 86, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9260, 0.0239, 0.0060, 0.0103, 0.0102, 0.0120, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 87, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3320e-09, 2.0358e-10, 1.0000e+00, 6.6534e-12, 1.0416e-10, 6.3548e-11,
        1.2388e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 87, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2982e-01, 1.4978e-06, 2.3359e-01, 1.1407e-06, 3.5042e-09, 2.3659e-01,
        6.5398e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 87, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0181, 0.0058, 0.9521, 0.0052, 0.0061, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 87, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1378e-01, 1.0529e-08, 3.6645e-02, 1.3873e-07, 9.1941e-08, 4.7840e-01,
        2.7118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.312

[Epoch: 87, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9394, 0.0244, 0.0053, 0.0084, 0.0090, 0.0070, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 88, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.6605e-09, 2.7685e-10, 1.0000e+00, 9.5096e-12, 1.3658e-10, 2.6319e-11,
        9.0941e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 88, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3845e-01, 1.9356e-06, 2.2744e-01, 1.7413e-06, 2.8717e-09, 2.3410e-01,
        2.3761e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 88, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0065, 0.0318, 0.0061, 0.9343, 0.0058, 0.0053, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 88, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4902e-01, 4.5016e-09, 2.7714e-02, 1.1383e-07, 1.6200e-08, 4.4613e-01,
        2.7713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.308

[Epoch: 88, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9158, 0.0293, 0.0067, 0.0115, 0.0108, 0.0128, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 89, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9985e-09, 1.7602e-10, 1.0000e+00, 7.0906e-12, 8.5981e-11, 5.8752e-11,
        1.1433e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 89, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5738e-01, 2.4882e-06, 2.4366e-01, 1.5905e-06, 4.6326e-09, 1.9896e-01,
        5.2367e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.936

[Epoch: 89, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0172, 0.0058, 0.9520, 0.0063, 0.0062, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 89, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3795e-01, 1.3496e-08, 4.3564e-02, 1.0271e-07, 6.3700e-08, 4.6347e-01,
        2.5502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 89, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9377, 0.0214, 0.0054, 0.0091, 0.0097, 0.0083, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 90, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0519e-09, 2.9749e-10, 1.0000e+00, 1.5114e-11, 1.0837e-10, 3.9493e-11,
        4.4657e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 90, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4966e-01, 2.1349e-06, 2.2385e-01, 1.5277e-06, 3.9946e-09, 2.2649e-01,
        2.6703e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.928

[Epoch: 90, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0263, 0.0050, 0.9391, 0.0054, 0.0051, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 90, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2240e-01, 9.1531e-09, 3.9228e-02, 1.2690e-07, 5.2039e-08, 4.6672e-01,
        2.7165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.290

[Epoch: 90, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9353, 0.0229, 0.0063, 0.0095, 0.0080, 0.0091, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 91, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1048e-09, 1.9747e-10, 1.0000e+00, 5.6835e-12, 5.5886e-11, 3.0953e-11,
        6.3499e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 91, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4909e-01, 1.1258e-06, 2.2583e-01, 1.0386e-06, 2.8349e-09, 2.2507e-01,
        3.0480e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.946

[Epoch: 91, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0210, 0.0053, 0.9490, 0.0059, 0.0068, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 91, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5696e-01, 4.9213e-09, 3.9797e-02, 7.1676e-08, 3.3026e-08, 4.0918e-01,
        2.9406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.339

[Epoch: 91, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9237, 0.0245, 0.0068, 0.0135, 0.0104, 0.0095, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 92, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1695e-09, 2.2296e-10, 1.0000e+00, 8.9877e-12, 5.4923e-11, 5.3430e-11,
        4.4148e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 92, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5034e-01, 2.1004e-06, 2.3548e-01, 1.1302e-06, 4.4005e-09, 2.1417e-01,
        1.4051e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 92, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0203, 0.0057, 0.9493, 0.0061, 0.0048, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 92, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1332e-01, 7.8929e-09, 3.7768e-02, 1.0649e-07, 4.1364e-08, 5.0536e-01,
        2.4355e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.299

[Epoch: 92, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9239, 0.0291, 0.0058, 0.0111, 0.0109, 0.0105, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 93, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4515e-09, 2.8503e-10, 1.0000e+00, 8.6134e-12, 5.4728e-11, 1.8542e-11,
        6.5710e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 93, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0046e-01, 1.9832e-06, 2.3808e-01, 2.3362e-06, 3.6665e-09, 2.6146e-01,
        3.1387e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.933

[Epoch: 93, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0068, 0.0311, 0.0066, 0.9284, 0.0064, 0.0070, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 93, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4842e-01, 8.0234e-09, 4.3024e-02, 1.0775e-07, 3.5863e-08, 4.2791e-01,
        2.8065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 93, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9396, 0.0223, 0.0066, 0.0073, 0.0065, 0.0084, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 94, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9223e-09, 2.3825e-10, 1.0000e+00, 7.1097e-12, 2.6897e-11, 3.7040e-11,
        4.4122e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 94, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.8113e-01, 2.4957e-06, 2.2396e-01, 9.4373e-07, 2.3231e-09, 1.9491e-01,
        2.5185e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.937

[Epoch: 94, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0181, 0.0051, 0.9543, 0.0059, 0.0045, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 94, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2593e-01, 8.1099e-09, 3.0577e-02, 1.2340e-07, 2.6891e-08, 4.5237e-01,
        2.9113e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.301

[Epoch: 94, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9299, 0.0216, 0.0050, 0.0110, 0.0099, 0.0097, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 95, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7044e-09, 1.0397e-09, 1.0000e+00, 2.5777e-11, 2.2240e-10, 3.9778e-11,
        1.3438e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 95, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4668e-01, 1.9104e-06, 2.2485e-01, 2.1278e-06, 4.6722e-09, 2.2846e-01,
        1.0464e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.932

[Epoch: 95, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0040, 0.0246, 0.0054, 0.9483, 0.0054, 0.0054, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 95, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5012e-01, 1.6841e-08, 4.7942e-02, 1.6064e-07, 5.7019e-08, 4.4874e-01,
        2.5320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 95, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9168, 0.0281, 0.0072, 0.0119, 0.0122, 0.0128, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 96, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1105e-09, 1.4595e-10, 1.0000e+00, 8.3186e-12, 2.3325e-11, 6.8591e-11,
        8.2291e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 96, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1955e-01, 1.3466e-06, 2.4921e-01, 3.1710e-06, 3.1091e-09, 2.3124e-01,
        6.0364e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.940

[Epoch: 96, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0214, 0.0057, 0.9384, 0.0066, 0.0083, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 96, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1502e-01, 6.7161e-09, 3.4264e-02, 2.0567e-07, 4.2910e-08, 5.0319e-01,
        2.4753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 96, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9452, 0.0166, 0.0052, 0.0095, 0.0072, 0.0079, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 97, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.7665e-09, 5.5521e-10, 1.0000e+00, 2.7498e-11, 1.6326e-10, 9.4703e-11,
        2.1470e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 97, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7364e-01, 4.2997e-06, 2.0980e-01, 2.6112e-06, 8.5862e-09, 2.1656e-01,
        6.5158e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.931

[Epoch: 97, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0289, 0.0069, 0.9405, 0.0045, 0.0039, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 97, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.6024e-01, 9.2113e-09, 3.4796e-02, 1.9948e-07, 1.3196e-08, 4.0159e-01,
        3.0337e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.332

[Epoch: 97, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9309, 0.0277, 0.0053, 0.0090, 0.0085, 0.0093, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 98, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7411e-09, 2.4950e-10, 1.0000e+00, 2.0924e-11, 3.1292e-11, 2.1313e-10,
        2.4781e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 98, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1605e-01, 1.6044e-06, 2.5117e-01, 1.7265e-06, 1.9603e-09, 2.3277e-01,
        1.7870e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 98, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0030, 0.0208, 0.0044, 0.9516, 0.0064, 0.0079, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 98, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9569e-01, 8.0980e-09, 3.6919e-02, 3.2406e-07, 4.7310e-08, 4.9486e-01,
        2.7253e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.260

[Epoch: 98, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9379, 0.0165, 0.0047, 0.0147, 0.0077, 0.0108, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 99, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.0896e-09, 2.3155e-10, 1.0000e+00, 3.5718e-11, 4.9551e-11, 1.1555e-10,
        6.7592e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 99, batch: 86/217] total loss per batch: 0.491
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7036e-01, 1.7771e-06, 2.4169e-01, 2.0424e-06, 6.4579e-09, 1.8795e-01,
        1.0561e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.929

[Epoch: 99, batch: 129/217] total loss per batch: 0.481
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0038, 0.0216, 0.0065, 0.9301, 0.0024, 0.0091, 0.0265],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 99, batch: 172/217] total loss per batch: 0.473
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1558e-01, 4.8557e-08, 6.0384e-02, 2.1232e-07, 5.2852e-08, 5.2831e-01,
        1.9572e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.256

[Epoch: 99, batch: 215/217] total loss per batch: 0.473
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9017, 0.0450, 0.0055, 0.0118, 0.0174, 0.0075, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.021

[Epoch: 100, batch: 43/217] total loss per batch: 0.474
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.2857e-10, 2.9045e-11, 1.0000e+00, 1.4634e-12, 5.5674e-12, 8.1929e-11,
        8.4391e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 100, batch: 86/217] total loss per batch: 0.481
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4943e-01, 7.1455e-07, 2.4046e-01, 3.8125e-06, 2.5818e-09, 2.1010e-01,
        9.0880e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.956

[Epoch: 100, batch: 129/217] total loss per batch: 0.446
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0015, 0.0056, 0.0045, 0.9773, 0.0027, 0.0057, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.021

[Epoch: 100, batch: 172/217] total loss per batch: 0.446
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4480e-01, 4.9844e-08, 4.1656e-02, 4.7340e-07, 3.7243e-08, 3.8108e-01,
        3.3247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.297

[Epoch: 100, batch: 215/217] total loss per batch: 0.458
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9212, 0.0314, 0.0090, 0.0060, 0.0142, 0.0082, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.020

[Epoch: 101, batch: 43/217] total loss per batch: 0.453
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5674e-09, 1.6668e-10, 1.0000e+00, 9.2973e-12, 1.2326e-11, 5.3095e-10,
        2.2381e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 101, batch: 86/217] total loss per batch: 0.459
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2223e-01, 5.8447e-07, 2.4228e-01, 1.4021e-06, 1.9896e-09, 2.3549e-01,
        6.4984e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.959

[Epoch: 101, batch: 129/217] total loss per batch: 0.433
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0025, 0.0110, 0.0054, 0.9677, 0.0036, 0.0064, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.017

[Epoch: 101, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5179e-01, 4.8086e-08, 4.3790e-02, 4.1631e-07, 3.2272e-08, 3.8533e-01,
        3.1910e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 101, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9323, 0.0200, 0.0076, 0.0090, 0.0128, 0.0075, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.015

[Epoch: 102, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2175e-09, 1.5546e-10, 1.0000e+00, 9.1739e-12, 1.3334e-11, 4.2835e-10,
        1.6382e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 102, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1982e-01, 5.1531e-07, 2.3848e-01, 1.3677e-06, 1.7989e-09, 2.4170e-01,
        7.4026e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.957

[Epoch: 102, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0029, 0.0127, 0.0054, 0.9655, 0.0035, 0.0063, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.017

[Epoch: 102, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4243e-01, 3.8256e-08, 3.9380e-02, 3.2922e-07, 3.5430e-08, 4.0424e-01,
        3.1395e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.304

[Epoch: 102, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9303, 0.0215, 0.0077, 0.0088, 0.0132, 0.0080, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 103, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1727e-09, 1.4769e-10, 1.0000e+00, 7.3519e-12, 1.1317e-11, 3.2153e-10,
        3.6920e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 103, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3237e-01, 4.4822e-07, 2.3235e-01, 1.1274e-06, 1.5417e-09, 2.3528e-01,
        1.9508e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.957

[Epoch: 103, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0033, 0.0161, 0.0056, 0.9606, 0.0039, 0.0063, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.015

[Epoch: 103, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3681e-01, 3.1230e-08, 3.7848e-02, 2.4589e-07, 2.9955e-08, 4.2101e-01,
        3.0433e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.304

[Epoch: 103, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9293, 0.0224, 0.0073, 0.0094, 0.0127, 0.0086, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 104, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0218e-09, 1.2903e-10, 1.0000e+00, 6.0775e-12, 1.0050e-11, 2.5534e-10,
        2.9675e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 104, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3608e-01, 4.2351e-07, 2.3152e-01, 1.0959e-06, 1.3613e-09, 2.3240e-01,
        1.7287e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.955

[Epoch: 104, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0036, 0.0189, 0.0058, 0.9567, 0.0041, 0.0061, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.013

[Epoch: 104, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3076e-01, 2.6304e-08, 3.6932e-02, 1.9570e-07, 2.8061e-08, 4.3518e-01,
        2.9712e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.303

[Epoch: 104, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9283, 0.0231, 0.0072, 0.0095, 0.0124, 0.0089, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 105, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.1857e-10, 1.1264e-10, 1.0000e+00, 4.8277e-12, 8.3923e-12, 2.0371e-10,
        2.3194e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 105, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4029e-01, 3.5976e-07, 2.3040e-01, 9.3894e-07, 1.1255e-09, 2.2930e-01,
        1.2936e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.955

[Epoch: 105, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0040, 0.0212, 0.0059, 0.9534, 0.0043, 0.0059, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.012

[Epoch: 105, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3533e-01, 2.3357e-08, 3.6606e-02, 1.6281e-07, 2.4631e-08, 4.4079e-01,
        2.8728e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.305

[Epoch: 105, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9282, 0.0236, 0.0070, 0.0096, 0.0122, 0.0092, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 106, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.1438e-10, 9.8018e-11, 1.0000e+00, 4.2548e-12, 7.5523e-12, 1.7512e-10,
        2.0654e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 106, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4145e-01, 3.3752e-07, 2.2979e-01, 9.1052e-07, 1.0640e-09, 2.2875e-01,
        1.1665e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.954

[Epoch: 106, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0043, 0.0235, 0.0060, 0.9500, 0.0044, 0.0060, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 106, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3113e-01, 2.0105e-08, 3.6304e-02, 1.3854e-07, 2.2301e-08, 4.4949e-01,
        2.8308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.306

[Epoch: 106, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9277, 0.0238, 0.0069, 0.0097, 0.0120, 0.0095, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 107, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.2498e-10, 8.1967e-11, 1.0000e+00, 3.5211e-12, 6.2292e-12, 1.5320e-10,
        1.6146e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 107, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4364e-01, 3.0033e-07, 2.2946e-01, 7.8938e-07, 8.8446e-10, 2.2690e-01,
        9.5900e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.954

[Epoch: 107, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0250, 0.0061, 0.9476, 0.0045, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 107, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3734e-01, 1.7869e-08, 3.6018e-02, 1.2112e-07, 2.0263e-08, 4.4894e-01,
        2.7771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.310

[Epoch: 107, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9273, 0.0245, 0.0067, 0.0098, 0.0118, 0.0097, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 108, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.3585e-10, 7.2178e-11, 1.0000e+00, 3.0679e-12, 5.2215e-12, 1.2547e-10,
        1.3953e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 108, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4232e-01, 2.7927e-07, 2.3017e-01, 7.4539e-07, 8.1535e-10, 2.2751e-01,
        8.5030e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.953

[Epoch: 108, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0253, 0.0060, 0.9465, 0.0046, 0.0058, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.011

[Epoch: 108, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2905e-01, 1.5596e-08, 3.5677e-02, 1.0458e-07, 1.8024e-08, 4.5833e-01,
        2.7694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 108, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9275, 0.0244, 0.0066, 0.0099, 0.0115, 0.0097, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 109, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.7739e-10, 6.1071e-11, 1.0000e+00, 2.5938e-12, 4.5714e-12, 1.1385e-10,
        1.1649e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 109, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4421e-01, 2.3798e-07, 2.3074e-01, 6.3594e-07, 6.5834e-10, 2.2505e-01,
        7.2686e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.954

[Epoch: 109, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0258, 0.0061, 0.9457, 0.0047, 0.0056, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 109, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3817e-01, 1.3470e-08, 3.4936e-02, 9.0687e-08, 1.6041e-08, 4.5494e-01,
        2.7195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.314

[Epoch: 109, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9279, 0.0246, 0.0064, 0.0098, 0.0115, 0.0099, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 110, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.1817e-10, 4.9809e-11, 1.0000e+00, 2.2387e-12, 3.7369e-12, 9.5460e-11,
        1.0193e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 110, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4499e-01, 2.3299e-07, 2.2935e-01, 6.3077e-07, 6.3506e-10, 2.2565e-01,
        6.0392e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.952

[Epoch: 110, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0258, 0.0060, 0.9442, 0.0049, 0.0058, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 110, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3612e-01, 1.2892e-08, 3.5510e-02, 8.7451e-08, 1.4048e-08, 4.5520e-01,
        2.7317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 110, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9282, 0.0243, 0.0064, 0.0096, 0.0110, 0.0098, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 111, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7001e-10, 4.8138e-11, 1.0000e+00, 2.1699e-12, 3.7058e-12, 9.2861e-11,
        8.7315e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 111, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4682e-01, 2.0302e-07, 2.3041e-01, 5.1257e-07, 4.8406e-10, 2.2277e-01,
        5.7095e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.953

[Epoch: 111, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0248, 0.0061, 0.9460, 0.0050, 0.0052, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 111, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2283e-01, 1.0137e-08, 3.3599e-02, 6.6549e-08, 1.2857e-08, 4.7064e-01,
        2.7294e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.311

[Epoch: 111, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9286, 0.0246, 0.0064, 0.0099, 0.0111, 0.0100, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 112, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.3796e-10, 3.7426e-11, 1.0000e+00, 1.7014e-12, 3.2837e-12, 7.2730e-11,
        6.7355e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 112, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4560e-01, 2.3056e-07, 2.2445e-01, 5.6013e-07, 6.0057e-10, 2.2995e-01,
        3.6762e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.951

[Epoch: 112, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0252, 0.0058, 0.9439, 0.0050, 0.0060, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 112, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5543e-01, 1.3975e-08, 3.5955e-02, 8.4821e-08, 1.2173e-08, 4.3536e-01,
        2.7325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 112, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9278, 0.0247, 0.0063, 0.0101, 0.0103, 0.0096, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 113, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.2225e-10, 4.2714e-11, 1.0000e+00, 1.7367e-12, 2.8901e-12, 8.3412e-11,
        1.1237e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 113, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4113e-01, 2.1192e-07, 2.3546e-01, 4.3384e-07, 4.2264e-10, 2.2341e-01,
        7.3010e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.950

[Epoch: 113, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0251, 0.0059, 0.9449, 0.0055, 0.0050, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 113, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1558e-01, 1.1309e-08, 3.6911e-02, 6.4974e-08, 1.2111e-08, 4.7923e-01,
        2.6828e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.306

[Epoch: 113, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9314, 0.0238, 0.0054, 0.0090, 0.0116, 0.0097, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 114, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.7409e-10, 2.7072e-11, 1.0000e+00, 1.1230e-12, 2.4321e-12, 3.2761e-11,
        3.9999e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 114, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4477e-01, 2.3668e-07, 2.2341e-01, 5.7664e-07, 9.4978e-10, 2.3182e-01,
        2.0640e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.948

[Epoch: 114, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0251, 0.0060, 0.9453, 0.0047, 0.0054, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 114, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4905e-01, 1.6618e-08, 3.0239e-02, 7.7451e-08, 1.3042e-08, 4.3569e-01,
        2.8502e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.329

[Epoch: 114, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9192, 0.0267, 0.0078, 0.0115, 0.0112, 0.0109, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 115, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.1576e-10, 6.8681e-11, 1.0000e+00, 2.8415e-12, 5.2095e-12, 1.0569e-10,
        1.5753e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 115, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3084e-01, 3.7341e-07, 2.3748e-01, 6.6516e-07, 6.2444e-10, 2.3168e-01,
        4.9777e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.954

[Epoch: 115, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0222, 0.0057, 0.9471, 0.0056, 0.0062, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 115, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2216e-01, 1.7037e-08, 4.1119e-02, 1.0169e-07, 1.3090e-08, 4.7049e-01,
        2.6623e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.301

[Epoch: 115, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9313, 0.0224, 0.0057, 0.0106, 0.0110, 0.0098, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 116, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.7207e-10, 5.9065e-11, 1.0000e+00, 2.5171e-12, 8.7889e-12, 7.7855e-11,
        5.9061e-13], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 116, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6351e-01, 3.0977e-07, 2.2162e-01, 6.3581e-07, 1.1599e-09, 2.1487e-01,
        2.2710e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 116, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0263, 0.0063, 0.9415, 0.0055, 0.0055, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.012

[Epoch: 116, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4649e-01, 2.0306e-08, 3.5695e-02, 8.9235e-08, 1.3031e-08, 4.3017e-01,
        2.8765e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.334

[Epoch: 116, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9274, 0.0239, 0.0066, 0.0098, 0.0100, 0.0096, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 117, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.7453e-10, 6.4776e-11, 1.0000e+00, 3.0661e-12, 4.0563e-12, 7.6407e-11,
        1.0032e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 117, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1764e-01, 7.0433e-07, 2.5247e-01, 1.5532e-06, 1.5371e-09, 2.2989e-01,
        5.6172e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 117, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0213, 0.0055, 0.9495, 0.0053, 0.0049, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.012

[Epoch: 117, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1724e-01, 2.5805e-08, 3.8619e-02, 1.5165e-07, 1.7691e-08, 4.7501e-01,
        2.6912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.287

[Epoch: 117, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9255, 0.0258, 0.0064, 0.0107, 0.0114, 0.0105, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.012

[Epoch: 118, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.5199e-10, 7.8172e-11, 1.0000e+00, 3.5316e-12, 1.3593e-11, 1.2466e-10,
        1.2945e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 118, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6056e-01, 2.1066e-07, 2.1734e-01, 5.8179e-07, 7.7898e-10, 2.2209e-01,
        1.0379e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 118, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0301, 0.0064, 0.9388, 0.0048, 0.0051, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 118, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4019e-01, 2.2442e-08, 2.8412e-02, 9.7852e-08, 1.5369e-08, 4.5092e-01,
        2.8048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 118, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9342, 0.0198, 0.0064, 0.0094, 0.0106, 0.0090, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 119, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.0527e-10, 1.0844e-10, 1.0000e+00, 4.2443e-12, 4.4689e-12, 2.8121e-11,
        2.4064e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 119, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0881e-01, 5.5594e-07, 2.7165e-01, 1.1234e-06, 1.8510e-09, 2.1953e-01,
        9.0844e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.954

[Epoch: 119, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0040, 0.0186, 0.0053, 0.9544, 0.0045, 0.0048, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 119, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2400e-01, 2.4980e-08, 4.2183e-02, 1.4432e-07, 1.4928e-08, 4.6482e-01,
        2.6899e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.307

[Epoch: 119, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9379, 0.0215, 0.0054, 0.0096, 0.0082, 0.0091, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 120, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3306e-09, 5.1941e-11, 1.0000e+00, 3.7893e-12, 5.3656e-12, 1.3501e-10,
        1.0382e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 120, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.8509e-01, 5.0535e-07, 1.8650e-01, 1.2324e-06, 1.2121e-09, 2.2841e-01,
        4.4612e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.937

[Epoch: 120, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0062, 0.0277, 0.0060, 0.9397, 0.0050, 0.0051, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 120, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5446e-01, 2.3603e-08, 3.3307e-02, 1.1484e-07, 1.4464e-08, 4.3963e-01,
        2.7260e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.330

[Epoch: 120, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9175, 0.0295, 0.0068, 0.0119, 0.0108, 0.0109, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 121, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7155e-09, 1.4347e-10, 1.0000e+00, 7.1867e-12, 7.3357e-12, 1.2849e-10,
        2.8871e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 121, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3639e-01, 4.9308e-07, 2.4066e-01, 1.2289e-06, 2.3105e-09, 2.2295e-01,
        1.0356e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 121, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0233, 0.0055, 0.9459, 0.0055, 0.0055, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 121, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9921e-01, 1.9010e-08, 2.7807e-02, 9.8511e-08, 1.7950e-08, 5.0767e-01,
        2.6531e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.259

[Epoch: 121, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9267, 0.0254, 0.0057, 0.0103, 0.0113, 0.0104, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.014

[Epoch: 122, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5693e-09, 1.1224e-10, 1.0000e+00, 4.8671e-12, 1.1860e-11, 1.0404e-10,
        1.8134e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 122, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4883e-01, 5.9102e-07, 2.2935e-01, 1.6034e-06, 2.0974e-09, 2.2181e-01,
        5.4037e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 122, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0253, 0.0058, 0.9442, 0.0050, 0.0046, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 122, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.8618e-01, 3.3776e-08, 5.1851e-02, 1.7680e-07, 8.7847e-09, 3.8210e-01,
        2.7987e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.362

[Epoch: 122, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9363, 0.0236, 0.0053, 0.0091, 0.0089, 0.0094, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 123, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3694e-09, 1.3659e-10, 1.0000e+00, 6.4363e-12, 4.0139e-12, 4.6126e-11,
        7.7535e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 123, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3183e-01, 2.9672e-07, 2.1954e-01, 7.8880e-07, 1.4963e-09, 2.4863e-01,
        4.1128e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 123, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0248, 0.0064, 0.9435, 0.0051, 0.0057, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 123, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.8914e-01, 1.6281e-08, 2.6676e-02, 1.4333e-07, 2.3192e-08, 5.3768e-01,
        2.4650e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.298

[Epoch: 123, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9259, 0.0225, 0.0061, 0.0110, 0.0105, 0.0117, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 124, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.6935e-10, 1.1725e-10, 1.0000e+00, 7.7230e-12, 1.3770e-11, 1.1069e-10,
        3.2011e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 124, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6850e-01, 4.4470e-07, 2.3576e-01, 2.6290e-06, 5.2432e-09, 1.9573e-01,
        1.3680e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 124, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0209, 0.0050, 0.9515, 0.0042, 0.0043, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 124, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.8711e-01, 2.7569e-08, 4.6235e-02, 1.5945e-07, 3.0496e-08, 3.9246e-01,
        2.7419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.328

[Epoch: 124, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9271, 0.0284, 0.0057, 0.0100, 0.0101, 0.0100, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 125, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.6701e-09, 2.2622e-10, 1.0000e+00, 7.6841e-12, 1.0747e-11, 8.0473e-11,
        5.8491e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 125, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2013e-01, 4.0438e-07, 2.3455e-01, 7.8477e-07, 1.9014e-09, 2.4532e-01,
        2.0368e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 125, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0362, 0.0058, 0.9322, 0.0056, 0.0061, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 125, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2678e-01, 2.5866e-08, 5.4963e-02, 3.3903e-07, 1.6457e-08, 4.4205e-01,
        2.7621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.345

[Epoch: 125, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9414, 0.0184, 0.0060, 0.0095, 0.0085, 0.0087, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 126, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0619e-09, 2.1681e-10, 1.0000e+00, 1.2024e-11, 1.1584e-11, 4.6359e-11,
        6.1667e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 126, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3067e-01, 5.6183e-07, 2.5246e-01, 2.0860e-06, 6.7744e-09, 2.1687e-01,
        6.5910e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.941

[Epoch: 126, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0135, 0.0049, 0.9560, 0.0058, 0.0050, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 126, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3748e-01, 1.0003e-08, 2.6015e-02, 4.1315e-07, 1.3504e-08, 4.5636e-01,
        2.8014e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 126, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9205, 0.0239, 0.0056, 0.0104, 0.0115, 0.0121, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 127, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1758e-09, 2.7243e-10, 1.0000e+00, 1.0633e-11, 2.5136e-11, 3.5242e-11,
        3.1990e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 127, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6193e-01, 4.2817e-07, 2.1956e-01, 1.1167e-06, 1.4406e-09, 2.1851e-01,
        8.2688e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 127, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0218, 0.0049, 0.9481, 0.0058, 0.0050, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 127, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3111e-01, 1.7112e-08, 3.1316e-02, 1.6771e-07, 1.0512e-08, 4.5962e-01,
        2.7796e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.311

[Epoch: 127, batch: 215/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9330, 0.0236, 0.0064, 0.0103, 0.0086, 0.0101, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 128, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3182e-09, 2.4450e-10, 1.0000e+00, 9.2879e-12, 1.0908e-11, 4.5252e-11,
        3.7213e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 128, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2865e-01, 5.2377e-07, 2.2350e-01, 1.6098e-06, 6.8734e-09, 2.4785e-01,
        3.7072e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 128, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0251, 0.0063, 0.9413, 0.0065, 0.0062, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 128, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1591e-01, 1.5519e-08, 4.0187e-02, 3.0526e-07, 1.3102e-08, 4.6682e-01,
        2.7709e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.300

[Epoch: 128, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9367, 0.0221, 0.0056, 0.0086, 0.0092, 0.0098, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 129, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1952e-09, 4.6176e-10, 1.0000e+00, 1.1215e-11, 1.2180e-11, 4.5258e-11,
        3.0233e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 129, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7315e-01, 6.2447e-07, 2.3227e-01, 1.2190e-06, 2.3071e-09, 1.9458e-01,
        1.2252e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.940

[Epoch: 129, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0239, 0.0050, 0.9477, 0.0048, 0.0045, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 129, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5494e-01, 1.8762e-08, 3.0508e-02, 1.6414e-07, 1.1426e-08, 4.4618e-01,
        2.6837e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.309

[Epoch: 129, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9217, 0.0289, 0.0060, 0.0100, 0.0121, 0.0110, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 130, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5971e-09, 1.5615e-10, 1.0000e+00, 8.9792e-12, 2.0761e-11, 9.7341e-11,
        3.2147e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 130, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2689e-01, 2.8580e-07, 2.4257e-01, 1.1799e-06, 2.3245e-09, 2.3053e-01,
        2.0115e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 130, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0232, 0.0059, 0.9503, 0.0040, 0.0043, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 130, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2828e-01, 1.0600e-08, 3.1118e-02, 1.6365e-07, 1.2400e-08, 4.6336e-01,
        2.7724e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.302

[Epoch: 130, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9358, 0.0219, 0.0053, 0.0101, 0.0091, 0.0095, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 131, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4760e-09, 1.0887e-10, 1.0000e+00, 4.4689e-12, 2.1431e-12, 3.0251e-11,
        7.2343e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 131, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5477e-01, 2.8530e-07, 2.0516e-01, 1.0955e-06, 2.0967e-09, 2.4007e-01,
        8.1767e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.941

[Epoch: 131, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0248, 0.0056, 0.9446, 0.0050, 0.0050, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 131, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4442e-01, 1.0230e-08, 3.8326e-02, 1.6389e-07, 8.5232e-09, 4.5320e-01,
        2.6406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.342

[Epoch: 131, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9231, 0.0270, 0.0055, 0.0099, 0.0106, 0.0110, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 132, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5577e-09, 1.5628e-10, 1.0000e+00, 6.4787e-12, 4.9740e-12, 2.3864e-11,
        3.2031e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 132, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4148e-01, 2.4100e-07, 2.5688e-01, 1.0075e-06, 1.6061e-09, 2.0164e-01,
        2.3547e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.946

[Epoch: 132, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0237, 0.0056, 0.9464, 0.0050, 0.0049, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 132, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2143e-01, 7.7321e-09, 3.2008e-02, 1.3206e-07, 8.4910e-09, 4.6149e-01,
        2.8507e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.297

[Epoch: 132, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9368, 0.0217, 0.0051, 0.0093, 0.0103, 0.0096, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 133, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6110e-09, 1.2833e-10, 1.0000e+00, 3.6966e-12, 2.5521e-12, 2.7874e-11,
        6.5042e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 133, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4163e-01, 2.9319e-07, 2.0934e-01, 1.2369e-06, 2.5116e-09, 2.4903e-01,
        5.2220e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.937

[Epoch: 133, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0253, 0.0053, 0.9423, 0.0055, 0.0057, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 133, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4363e-01, 1.1106e-08, 3.3567e-02, 1.8573e-07, 9.4525e-09, 4.6246e-01,
        2.6034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 133, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9267, 0.0260, 0.0057, 0.0105, 0.0106, 0.0101, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 134, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3700e-09, 1.0687e-10, 1.0000e+00, 3.6388e-12, 4.3168e-12, 1.5537e-11,
        3.8632e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 134, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3593e-01, 2.7776e-07, 2.5121e-01, 8.6538e-07, 2.1047e-09, 2.1286e-01,
        4.3382e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.950

[Epoch: 134, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0256, 0.0053, 0.9445, 0.0053, 0.0048, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 134, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3422e-01, 9.3963e-09, 4.2447e-02, 1.3663e-07, 1.0506e-08, 4.4716e-01,
        2.7617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.340

[Epoch: 134, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9240, 0.0268, 0.0060, 0.0106, 0.0110, 0.0117, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 135, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3664e-09, 9.2494e-11, 1.0000e+00, 4.5959e-12, 3.8140e-12, 2.6918e-11,
        5.3096e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 135, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6101e-01, 3.8618e-07, 2.1834e-01, 1.1947e-06, 1.9384e-09, 2.2065e-01,
        3.8224e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 135, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0224, 0.0057, 0.9486, 0.0043, 0.0047, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 135, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3505e-01, 1.0841e-08, 2.8931e-02, 1.9171e-07, 9.2312e-09, 4.6037e-01,
        2.7565e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.289

[Epoch: 135, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9323, 0.0248, 0.0060, 0.0102, 0.0087, 0.0088, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 136, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9281e-09, 3.0941e-10, 1.0000e+00, 6.5731e-12, 2.0062e-11, 4.4685e-11,
        6.9464e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 136, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4020e-01, 2.2782e-07, 2.3504e-01, 7.8330e-07, 2.4435e-09, 2.2476e-01,
        2.2185e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 136, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0285, 0.0064, 0.9345, 0.0057, 0.0066, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 136, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2915e-01, 1.0278e-08, 4.0919e-02, 9.8786e-08, 1.1090e-08, 4.6379e-01,
        2.6614e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 136, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9315, 0.0225, 0.0057, 0.0104, 0.0102, 0.0105, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 137, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2669e-09, 1.6964e-10, 1.0000e+00, 4.3828e-12, 2.9999e-12, 1.7632e-11,
        1.1202e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 137, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3056e-01, 2.7850e-07, 2.2614e-01, 1.0438e-06, 2.5603e-09, 2.4331e-01,
        1.7786e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 137, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0238, 0.0054, 0.9512, 0.0043, 0.0038, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 137, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.6469e-01, 1.4402e-08, 3.4913e-02, 2.1847e-07, 1.3153e-08, 4.2128e-01,
        2.7911e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.342

[Epoch: 137, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9384, 0.0225, 0.0062, 0.0082, 0.0072, 0.0088, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 138, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6095e-09, 1.7502e-10, 1.0000e+00, 4.8888e-12, 1.0158e-11, 3.8421e-11,
        1.1547e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 138, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7632e-01, 6.4529e-07, 2.3518e-01, 1.1621e-06, 2.0990e-09, 1.8849e-01,
        8.9512e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.933

[Epoch: 138, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0221, 0.0045, 0.9526, 0.0040, 0.0040, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 138, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([1.9908e-01, 1.3353e-08, 3.4253e-02, 1.0521e-07, 1.6345e-08, 4.9836e-01,
        2.6830e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.292

[Epoch: 138, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9086, 0.0389, 0.0058, 0.0106, 0.0105, 0.0128, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 139, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.4045e-10, 3.8443e-10, 1.0000e+00, 1.1257e-11, 1.9448e-11, 3.2641e-11,
        4.7835e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 139, batch: 86/217] total loss per batch: 0.454
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.0345e-01, 1.3637e-07, 2.2108e-01, 4.9548e-07, 1.8337e-09, 2.7547e-01,
        2.8653e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.940

[Epoch: 139, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0063, 0.0219, 0.0055, 0.9459, 0.0060, 0.0050, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 139, batch: 172/217] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4026e-01, 3.6222e-08, 5.6583e-02, 4.4186e-07, 1.3103e-08, 4.2646e-01,
        2.7669e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.358

[Epoch: 139, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9356, 0.0176, 0.0064, 0.0127, 0.0091, 0.0091, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 140, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9591e-09, 4.8914e-10, 1.0000e+00, 9.9551e-12, 7.3604e-12, 8.5716e-11,
        3.9700e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 140, batch: 86/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6569e-01, 4.3506e-07, 2.3749e-01, 5.0645e-07, 9.4007e-10, 1.9682e-01,
        1.6568e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 140, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0044, 0.0188, 0.0048, 0.9532, 0.0045, 0.0050, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 140, batch: 172/217] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5011e-01, 2.3020e-08, 3.3797e-02, 9.1983e-08, 1.2882e-08, 4.5356e-01,
        2.6253e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.305

[Epoch: 140, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9360, 0.0169, 0.0050, 0.0137, 0.0079, 0.0098, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 141, batch: 43/217] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.4124e-10, 1.2666e-10, 1.0000e+00, 3.3202e-12, 5.7057e-12, 1.0198e-11,
        1.0197e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 141, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6354e-01, 8.1362e-07, 2.3715e-01, 1.1075e-06, 2.0698e-09, 1.9930e-01,
        3.9881e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.927

[Epoch: 141, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0058, 0.0347, 0.0060, 0.9332, 0.0049, 0.0048, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 141, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4777e-01, 1.4526e-08, 3.5002e-02, 1.5235e-07, 4.0295e-09, 4.5728e-01,
        2.5995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.272

[Epoch: 141, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9308, 0.0267, 0.0049, 0.0109, 0.0094, 0.0094, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 142, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5115e-09, 4.5587e-10, 1.0000e+00, 9.5623e-12, 5.6750e-12, 2.2426e-11,
        3.0112e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 142, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6106e-01, 4.3836e-07, 2.2708e-01, 9.7450e-07, 1.7575e-09, 2.1186e-01,
        4.7446e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 142, batch: 129/217] total loss per batch: 0.433
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0581, 0.0054, 0.9072, 0.0075, 0.0055, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 142, batch: 172/217] total loss per batch: 0.443
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3032e-01, 1.5720e-08, 4.1041e-02, 1.5365e-07, 6.3092e-09, 4.6573e-01,
        2.6291e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.286

[Epoch: 142, batch: 215/217] total loss per batch: 0.456
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9480, 0.0104, 0.0056, 0.0151, 0.0041, 0.0097, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 143, batch: 43/217] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4190e-09, 1.7938e-10, 1.0000e+00, 2.2883e-11, 1.9583e-11, 5.5624e-11,
        2.0144e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 143, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.1959e-01, 3.2033e-07, 2.4089e-01, 1.2812e-06, 1.3260e-09, 2.3951e-01,
        6.4069e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 143, batch: 129/217] total loss per batch: 0.436
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0116, 0.0113, 0.0051, 0.9571, 0.0052, 0.0046, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 143, batch: 172/217] total loss per batch: 0.438
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3046e-01, 1.3457e-08, 3.3217e-02, 1.4862e-07, 3.7061e-09, 4.4590e-01,
        2.9042e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.327

[Epoch: 143, batch: 215/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9270, 0.0205, 0.0059, 0.0127, 0.0098, 0.0161, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 144, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.0356e-09, 7.3094e-11, 1.0000e+00, 1.0398e-11, 9.3785e-12, 3.9604e-11,
        5.9634e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 144, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6379e-01, 2.0850e-07, 2.1608e-01, 5.9479e-07, 5.7180e-10, 2.2013e-01,
        6.6582e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 144, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0068, 0.0188, 0.0055, 0.9489, 0.0047, 0.0041, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 144, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3110e-01, 9.3750e-09, 3.3396e-02, 1.0395e-07, 3.8969e-09, 4.4879e-01,
        2.8672e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 144, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9289, 0.0213, 0.0056, 0.0117, 0.0111, 0.0115, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 145, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9285e-09, 1.3758e-10, 1.0000e+00, 1.1251e-11, 1.0240e-11, 3.3631e-11,
        9.8274e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 145, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5206e-01, 1.9326e-07, 2.2074e-01, 5.5467e-07, 6.8397e-10, 2.2720e-01,
        4.8550e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 145, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0057, 0.0222, 0.0055, 0.9456, 0.0054, 0.0041, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 145, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3399e-01, 1.0001e-08, 3.3286e-02, 1.1105e-07, 4.1671e-09, 4.5076e-01,
        2.8197e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 145, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9303, 0.0229, 0.0055, 0.0108, 0.0102, 0.0109, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 146, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9359e-09, 1.0472e-10, 1.0000e+00, 9.7002e-12, 7.6568e-12, 2.6220e-11,
        8.2284e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 146, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4688e-01, 1.6730e-07, 2.2696e-01, 4.7975e-07, 6.3484e-10, 2.2615e-01,
        3.2451e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 146, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0232, 0.0053, 0.9448, 0.0057, 0.0044, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 146, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3488e-01, 9.5878e-09, 3.4339e-02, 1.0111e-07, 4.0595e-09, 4.5436e-01,
        2.7642e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 146, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9286, 0.0249, 0.0055, 0.0103, 0.0104, 0.0106, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 147, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4464e-09, 8.6274e-11, 1.0000e+00, 7.1261e-12, 6.1192e-12, 2.1504e-11,
        5.6425e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 147, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4793e-01, 1.4934e-07, 2.2937e-01, 4.2715e-07, 5.6255e-10, 2.2270e-01,
        3.3317e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 147, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0235, 0.0053, 0.9452, 0.0057, 0.0044, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 147, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3416e-01, 9.3887e-09, 3.4337e-02, 9.3895e-08, 3.9181e-09, 4.5637e-01,
        2.7514e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 147, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9289, 0.0250, 0.0054, 0.0104, 0.0102, 0.0107, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 148, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0261e-09, 6.6555e-11, 1.0000e+00, 5.6527e-12, 4.5659e-12, 1.6783e-11,
        5.8068e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 148, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4287e-01, 1.3795e-07, 2.3176e-01, 3.8473e-07, 4.8864e-10, 2.2537e-01,
        2.9809e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 148, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0235, 0.0052, 0.9455, 0.0056, 0.0045, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 148, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3713e-01, 8.8990e-09, 3.5058e-02, 8.4455e-08, 3.4381e-09, 4.5591e-01,
        2.7190e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 148, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9293, 0.0250, 0.0054, 0.0100, 0.0103, 0.0104, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 149, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6396e-09, 5.5184e-11, 1.0000e+00, 4.4137e-12, 3.7654e-12, 1.4278e-11,
        4.5683e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 149, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5016e-01, 1.1496e-07, 2.2796e-01, 3.3979e-07, 4.0699e-10, 2.2188e-01,
        2.7280e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 149, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0236, 0.0051, 0.9460, 0.0056, 0.0045, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 149, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3212e-01, 7.6624e-09, 3.4369e-02, 7.3978e-08, 3.0714e-09, 4.6053e-01,
        2.7298e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 149, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9294, 0.0253, 0.0053, 0.0101, 0.0101, 0.0102, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 150, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5624e-09, 4.7125e-11, 1.0000e+00, 3.9073e-12, 3.0582e-12, 1.1654e-11,
        5.1925e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 150, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4024e-01, 1.0722e-07, 2.3392e-01, 3.0541e-07, 3.6347e-10, 2.2584e-01,
        2.4310e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 150, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0238, 0.0050, 0.9459, 0.0055, 0.0048, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 150, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3835e-01, 7.2763e-09, 3.4901e-02, 6.7073e-08, 2.7698e-09, 4.5677e-01,
        2.6998e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 150, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9303, 0.0248, 0.0053, 0.0097, 0.0101, 0.0103, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 151, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1421e-09, 3.8580e-11, 1.0000e+00, 2.9479e-12, 2.5181e-12, 9.8711e-12,
        4.4620e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 151, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5012e-01, 9.3151e-08, 2.2567e-01, 2.7833e-07, 3.0647e-10, 2.2421e-01,
        2.5447e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 151, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0242, 0.0049, 0.9459, 0.0055, 0.0047, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 151, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3123e-01, 6.2100e-09, 3.4620e-02, 6.2163e-08, 2.5048e-09, 4.6321e-01,
        2.7094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 151, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9294, 0.0254, 0.0052, 0.0101, 0.0101, 0.0099, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 152, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2550e-09, 3.7189e-11, 1.0000e+00, 3.0162e-12, 2.2801e-12, 9.2207e-12,
        4.0061e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 152, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4263e-01, 8.5924e-08, 2.3347e-01, 2.4990e-07, 2.7394e-10, 2.2390e-01,
        2.1903e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 152, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0241, 0.0048, 0.9461, 0.0054, 0.0048, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 152, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3777e-01, 6.4005e-09, 3.4970e-02, 5.5139e-08, 2.3574e-09, 4.5797e-01,
        2.6928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 152, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9304, 0.0250, 0.0053, 0.0095, 0.0101, 0.0101, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 153, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.2294e-10, 3.0950e-11, 1.0000e+00, 2.2251e-12, 1.8481e-12, 7.1487e-12,
        3.7595e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 153, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4443e-01, 7.9503e-08, 2.3001e-01, 2.4542e-07, 2.4785e-10, 2.2556e-01,
        2.1496e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 153, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0046, 0.0246, 0.0050, 0.9455, 0.0054, 0.0048, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 153, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3175e-01, 5.3777e-09, 3.5019e-02, 5.4462e-08, 2.1489e-09, 4.6160e-01,
        2.7163e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 153, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9298, 0.0250, 0.0051, 0.0102, 0.0101, 0.0099, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 154, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0688e-09, 3.4507e-11, 1.0000e+00, 2.6958e-12, 2.2242e-12, 8.4909e-12,
        4.7850e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 154, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4834e-01, 7.2109e-08, 2.2892e-01, 2.0454e-07, 2.0838e-10, 2.2274e-01,
        1.8120e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 154, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0239, 0.0047, 0.9463, 0.0054, 0.0050, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 154, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3821e-01, 5.8460e-09, 3.4916e-02, 4.5310e-08, 2.0651e-09, 4.5870e-01,
        2.6817e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 154, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9287, 0.0258, 0.0054, 0.0095, 0.0102, 0.0103, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 155, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.8766e-10, 2.5493e-11, 1.0000e+00, 1.7365e-12, 1.3854e-12, 5.6964e-12,
        3.0054e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 155, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3932e-01, 7.5547e-08, 2.2876e-01, 2.3530e-07, 2.3407e-10, 2.3192e-01,
        2.4513e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 155, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0252, 0.0051, 0.9452, 0.0052, 0.0050, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 155, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3442e-01, 5.0514e-09, 3.6527e-02, 5.2474e-08, 2.0341e-09, 4.5667e-01,
        2.7238e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.316

[Epoch: 155, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9335, 0.0240, 0.0049, 0.0103, 0.0092, 0.0092, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 156, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0056e-09, 4.0231e-11, 1.0000e+00, 2.8949e-12, 2.7361e-12, 1.0308e-11,
        4.6404e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 156, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5229e-01, 6.8532e-08, 2.3398e-01, 1.6795e-07, 1.8215e-10, 2.1374e-01,
        1.1134e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 156, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0228, 0.0046, 0.9469, 0.0056, 0.0052, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 156, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2936e-01, 6.7639e-09, 3.3706e-02, 4.9614e-08, 2.3568e-09, 4.6640e-01,
        2.7054e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.321

[Epoch: 156, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9278, 0.0262, 0.0050, 0.0094, 0.0105, 0.0100, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 157, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.1258e-10, 2.7792e-11, 1.0000e+00, 1.7578e-12, 1.4487e-12, 4.0904e-12,
        4.0140e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 157, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3954e-01, 9.4355e-08, 2.2062e-01, 2.8561e-07, 3.0368e-10, 2.3985e-01,
        4.4165e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.949

[Epoch: 157, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0269, 0.0053, 0.9427, 0.0052, 0.0053, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 157, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4758e-01, 6.0215e-09, 3.9451e-02, 6.4827e-08, 2.1697e-09, 4.4446e-01,
        2.6851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.310

[Epoch: 157, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9310, 0.0251, 0.0058, 0.0099, 0.0098, 0.0092, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 158, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7305e-09, 7.1426e-11, 1.0000e+00, 5.4895e-12, 3.9395e-12, 1.7039e-11,
        4.4501e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 158, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5207e-01, 9.5900e-08, 2.4317e-01, 2.9038e-07, 3.3014e-10, 2.0476e-01,
        2.0749e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 158, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0217, 0.0046, 0.9496, 0.0057, 0.0049, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 158, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3619e-01, 1.2600e-08, 3.3197e-02, 8.4416e-08, 3.7290e-09, 4.5954e-01,
        2.7107e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 158, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9297, 0.0245, 0.0048, 0.0099, 0.0102, 0.0098, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 159, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5651e-09, 9.5747e-11, 1.0000e+00, 5.4584e-12, 8.0175e-12, 1.6015e-11,
        1.8219e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 159, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3091e-01, 1.1876e-07, 2.2700e-01, 4.7096e-07, 4.6969e-10, 2.4209e-01,
        3.0708e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.950

[Epoch: 159, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0312, 0.0061, 0.9349, 0.0052, 0.0054, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 159, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2428e-01, 6.6553e-09, 3.6525e-02, 1.0109e-07, 2.8121e-09, 4.5917e-01,
        2.8003e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 159, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9238, 0.0293, 0.0064, 0.0106, 0.0096, 0.0098, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 160, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5354e-09, 7.0489e-11, 1.0000e+00, 7.5337e-12, 6.1890e-12, 1.8780e-11,
        8.9322e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 160, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5188e-01, 1.4727e-07, 2.3748e-01, 5.7265e-07, 7.2865e-10, 2.1064e-01,
        5.3166e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 160, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0045, 0.0225, 0.0050, 0.9479, 0.0057, 0.0050, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 160, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4875e-01, 1.2041e-08, 3.3924e-02, 1.0353e-07, 3.7975e-09, 4.5425e-01,
        2.6308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.304

[Epoch: 160, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9315, 0.0222, 0.0051, 0.0098, 0.0102, 0.0102, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 161, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4178e-09, 6.8465e-11, 1.0000e+00, 3.1002e-12, 3.8416e-12, 7.4421e-12,
        1.1945e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 161, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4618e-01, 1.3145e-07, 2.2948e-01, 7.2494e-07, 7.0003e-10, 2.2434e-01,
        4.3006e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.952

[Epoch: 161, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0245, 0.0051, 0.9443, 0.0053, 0.0053, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 161, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2501e-01, 1.2521e-08, 3.9119e-02, 1.1569e-07, 4.8246e-09, 4.5484e-01,
        2.8104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 161, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9294, 0.0254, 0.0055, 0.0101, 0.0096, 0.0096, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 162, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4731e-09, 7.5664e-11, 1.0000e+00, 4.5778e-12, 5.3281e-12, 1.4244e-11,
        1.3920e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 162, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5239e-01, 1.5444e-07, 2.2171e-01, 5.1645e-07, 6.4735e-10, 2.2590e-01,
        7.7271e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 162, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0253, 0.0057, 0.9431, 0.0054, 0.0051, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 162, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3864e-01, 1.0412e-08, 3.4442e-02, 9.4543e-08, 3.9058e-09, 4.6202e-01,
        2.6490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 162, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9258, 0.0263, 0.0055, 0.0114, 0.0105, 0.0104, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 163, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4745e-09, 7.0250e-11, 1.0000e+00, 3.0219e-12, 4.2477e-12, 5.1463e-12,
        9.2896e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 163, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3310e-01, 1.4583e-07, 2.3329e-01, 6.5202e-07, 7.3184e-10, 2.3361e-01,
        6.8607e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 163, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0253, 0.0048, 0.9451, 0.0052, 0.0051, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 163, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3356e-01, 1.4703e-08, 3.7284e-02, 1.0782e-07, 4.4685e-09, 4.5564e-01,
        2.7351e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.301

[Epoch: 163, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9401, 0.0208, 0.0048, 0.0080, 0.0087, 0.0086, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 164, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1276e-09, 6.2613e-11, 1.0000e+00, 3.0994e-12, 2.7636e-12, 7.6055e-12,
        1.3463e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 164, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5967e-01, 1.5102e-07, 2.2404e-01, 4.2453e-07, 5.8729e-10, 2.1629e-01,
        5.6273e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 164, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0046, 0.0210, 0.0049, 0.9512, 0.0044, 0.0045, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 164, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3925e-01, 9.2517e-09, 3.6149e-02, 8.8115e-08, 4.0246e-09, 4.5521e-01,
        2.6938e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.334

[Epoch: 164, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9274, 0.0281, 0.0049, 0.0106, 0.0094, 0.0100, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 165, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0911e-09, 7.6059e-11, 1.0000e+00, 3.6852e-12, 3.1281e-12, 5.2863e-12,
        9.5271e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 165, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4037e-01, 1.0545e-07, 2.3899e-01, 4.0283e-07, 3.8179e-10, 2.2064e-01,
        4.5372e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.940

[Epoch: 165, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0279, 0.0047, 0.9410, 0.0056, 0.0051, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 165, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2016e-01, 1.3427e-08, 3.5123e-02, 8.3446e-08, 4.1659e-09, 4.7079e-01,
        2.7393e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.305

[Epoch: 165, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9251, 0.0258, 0.0058, 0.0111, 0.0105, 0.0104, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 166, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.4520e-10, 5.6450e-11, 1.0000e+00, 2.7545e-12, 2.1354e-12, 8.1290e-12,
        5.2545e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 166, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3866e-01, 1.6798e-07, 2.2312e-01, 4.8925e-07, 7.6623e-10, 2.3822e-01,
        2.8040e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 166, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0275, 0.0062, 0.9372, 0.0052, 0.0062, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 166, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5873e-01, 1.3176e-08, 3.5248e-02, 1.2857e-07, 6.9437e-09, 4.3999e-01,
        2.6603e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.314

[Epoch: 166, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9378, 0.0209, 0.0045, 0.0096, 0.0096, 0.0091, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 167, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.6815e-10, 5.2902e-11, 1.0000e+00, 3.5197e-12, 2.3581e-12, 3.8948e-12,
        1.6102e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 167, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5543e-01, 1.2695e-07, 2.2761e-01, 3.9653e-07, 5.5466e-10, 2.1696e-01,
        2.4807e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 167, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0042, 0.0200, 0.0055, 0.9538, 0.0046, 0.0035, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 167, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1596e-01, 1.2729e-08, 3.1724e-02, 9.2339e-08, 3.0480e-09, 4.8172e-01,
        2.7060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.308

[Epoch: 167, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9235, 0.0288, 0.0055, 0.0097, 0.0107, 0.0106, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 168, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.3616e-10, 7.2570e-11, 1.0000e+00, 3.9792e-12, 2.0140e-12, 1.3727e-11,
        1.3150e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 168, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4378e-01, 1.9358e-07, 2.3933e-01, 8.6080e-07, 1.1544e-09, 2.1688e-01,
        3.3464e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.950

[Epoch: 168, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0249, 0.0048, 0.9429, 0.0052, 0.0057, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 168, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.6011e-01, 9.5325e-09, 2.9621e-02, 1.1351e-07, 2.3559e-09, 4.4246e-01,
        2.6781e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 168, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9329, 0.0238, 0.0050, 0.0117, 0.0087, 0.0092, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 169, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2799e-09, 1.3856e-10, 1.0000e+00, 7.1288e-12, 4.3932e-12, 1.5683e-11,
        6.5317e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 169, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3424e-01, 9.8411e-08, 2.2993e-01, 4.5526e-07, 5.1478e-10, 2.3583e-01,
        5.8373e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 169, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0058, 0.0266, 0.0061, 0.9411, 0.0050, 0.0044, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 169, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1785e-01, 1.9470e-08, 3.8381e-02, 1.5902e-07, 4.0693e-09, 4.6934e-01,
        2.7443e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 169, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9253, 0.0237, 0.0059, 0.0104, 0.0122, 0.0103, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 170, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2651e-09, 9.2251e-11, 1.0000e+00, 5.1056e-12, 2.5323e-12, 9.5237e-12,
        1.7111e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 170, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5550e-01, 1.7809e-07, 2.2147e-01, 8.3219e-07, 6.1270e-10, 2.2302e-01,
        7.5437e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.944

[Epoch: 170, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0226, 0.0052, 0.9490, 0.0045, 0.0048, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 170, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5165e-01, 1.2193e-08, 3.3711e-02, 1.9028e-07, 3.3179e-09, 4.4401e-01,
        2.7063e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.322

[Epoch: 170, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9354, 0.0233, 0.0046, 0.0106, 0.0088, 0.0101, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 171, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7790e-09, 1.4140e-10, 1.0000e+00, 6.4263e-12, 2.7039e-12, 9.1290e-12,
        2.7022e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 171, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2521e-01, 2.2212e-07, 2.4204e-01, 9.6132e-07, 1.1810e-09, 2.3275e-01,
        9.3019e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 171, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0044, 0.0228, 0.0045, 0.9476, 0.0046, 0.0052, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 171, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3155e-01, 1.1969e-08, 3.5921e-02, 1.7071e-07, 4.9923e-09, 4.7801e-01,
        2.5452e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 171, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9319, 0.0253, 0.0052, 0.0097, 0.0093, 0.0098, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 172, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.8327e-09, 1.3865e-10, 1.0000e+00, 6.6033e-12, 5.8562e-12, 2.6618e-11,
        2.5382e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 172, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4568e-01, 2.4945e-07, 2.3216e-01, 7.6580e-07, 9.3978e-10, 2.2215e-01,
        3.7023e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 172, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0039, 0.0256, 0.0047, 0.9481, 0.0045, 0.0041, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 172, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3135e-01, 9.3957e-09, 3.3830e-02, 2.1606e-07, 4.6187e-09, 4.5505e-01,
        2.7977e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.324

[Epoch: 172, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9183, 0.0274, 0.0059, 0.0122, 0.0112, 0.0131, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 173, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4909e-09, 7.8621e-11, 1.0000e+00, 2.8407e-12, 1.3840e-12, 6.5466e-12,
        1.6257e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 173, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.6359e-01, 2.6384e-07, 2.0577e-01, 7.6316e-07, 8.3776e-10, 2.3064e-01,
        8.2893e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.949

[Epoch: 173, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0261, 0.0049, 0.9444, 0.0050, 0.0049, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 173, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.5364e-01, 1.7905e-08, 3.6675e-02, 1.8740e-07, 6.5929e-09, 4.2608e-01,
        2.8360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.306

[Epoch: 173, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9290, 0.0253, 0.0055, 0.0093, 0.0106, 0.0102, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 174, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3062e-09, 9.8266e-11, 1.0000e+00, 5.0306e-12, 4.0724e-12, 1.5151e-11,
        1.3704e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 174, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4721e-01, 1.5232e-07, 2.4062e-01, 5.7685e-07, 8.4802e-10, 2.1216e-01,
        6.7382e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.948

[Epoch: 174, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0054, 0.0243, 0.0051, 0.9437, 0.0055, 0.0053, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 174, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.1098e-01, 9.2983e-09, 3.7202e-02, 1.3694e-07, 5.6694e-09, 4.9333e-01,
        2.5849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 174, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9341, 0.0240, 0.0049, 0.0102, 0.0089, 0.0098, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 175, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.6597e-09, 5.0021e-11, 1.0000e+00, 2.0578e-12, 1.1902e-12, 3.2712e-12,
        2.2772e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 175, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2464e-01, 3.2844e-07, 2.4891e-01, 4.3991e-07, 9.8914e-10, 2.2645e-01,
        1.4513e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.946

[Epoch: 175, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0037, 0.0244, 0.0048, 0.9520, 0.0047, 0.0036, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 175, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3966e-01, 4.0016e-08, 3.7267e-02, 2.0608e-07, 1.0157e-08, 4.5928e-01,
        2.6380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.315

[Epoch: 175, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9262, 0.0243, 0.0057, 0.0112, 0.0093, 0.0110, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 176, batch: 43/217] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1307e-09, 8.4693e-11, 1.0000e+00, 5.1874e-12, 3.3482e-12, 2.3795e-11,
        1.2906e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 176, batch: 86/217] total loss per batch: 0.453
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.8385e-01, 2.7387e-07, 2.0053e-01, 4.4417e-07, 4.4927e-10, 2.1563e-01,
        3.8953e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.948

[Epoch: 176, batch: 129/217] total loss per batch: 0.429
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0047, 0.0189, 0.0043, 0.9545, 0.0048, 0.0053, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 176, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4549e-01, 3.4358e-08, 3.5807e-02, 3.9797e-07, 1.3534e-08, 4.1811e-01,
        3.0060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 176, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9344, 0.0246, 0.0052, 0.0088, 0.0091, 0.0090, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 177, batch: 43/217] total loss per batch: 0.455
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.3400e-10, 3.2280e-10, 1.0000e+00, 8.5776e-12, 3.6434e-12, 5.4532e-12,
        2.6651e-12], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 177, batch: 86/217] total loss per batch: 0.461
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([4.9680e-01, 1.1345e-06, 2.5234e-01, 1.4867e-06, 4.4131e-10, 2.5084e-01,
        1.4030e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.926

[Epoch: 177, batch: 129/217] total loss per batch: 0.430
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0060, 0.0252, 0.0064, 0.9456, 0.0056, 0.0047, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 177, batch: 172/217] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4534e-01, 2.7762e-08, 4.7075e-02, 4.7270e-07, 1.9798e-08, 4.4692e-01,
        2.6066e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.279

[Epoch: 177, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9304, 0.0270, 0.0061, 0.0105, 0.0090, 0.0078, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 178, batch: 43/217] total loss per batch: 0.450
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1393e-09, 1.4824e-09, 1.0000e+00, 1.8677e-11, 7.5531e-12, 2.1304e-11,
        5.1819e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 178, batch: 86/217] total loss per batch: 0.452
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3195e-01, 9.8349e-07, 2.2891e-01, 1.5096e-06, 8.5926e-10, 2.3913e-01,
        3.4585e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.932

[Epoch: 178, batch: 129/217] total loss per batch: 0.431
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0074, 0.0327, 0.0079, 0.9313, 0.0058, 0.0053, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 178, batch: 172/217] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2877e-01, 2.8031e-08, 3.7416e-02, 2.5216e-07, 1.5415e-08, 4.6491e-01,
        2.6891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.297

[Epoch: 178, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9346, 0.0260, 0.0052, 0.0088, 0.0088, 0.0075, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 179, batch: 43/217] total loss per batch: 0.447
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.2780e-10, 3.2064e-10, 1.0000e+00, 6.9959e-12, 2.8484e-12, 3.2042e-11,
        6.7940e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 179, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2976e-01, 9.9625e-07, 2.2555e-01, 1.9107e-06, 6.0863e-10, 2.4468e-01,
        1.2788e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 179, batch: 129/217] total loss per batch: 0.428
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0056, 0.0228, 0.0063, 0.9478, 0.0051, 0.0043, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 179, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3570e-01, 2.7514e-08, 3.8981e-02, 3.1884e-07, 1.2896e-08, 4.5566e-01,
        2.6966e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.244

[Epoch: 179, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9320, 0.0258, 0.0053, 0.0095, 0.0094, 0.0084, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.011

[Epoch: 180, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.2850e-10, 2.2038e-10, 1.0000e+00, 5.9657e-12, 2.7794e-12, 3.8252e-11,
        4.1538e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 180, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4690e-01, 7.8275e-07, 2.3067e-01, 9.9186e-07, 4.4289e-10, 2.2242e-01,
        3.7902e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.931

[Epoch: 180, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0260, 0.0062, 0.9441, 0.0049, 0.0042, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 180, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3365e-01, 1.9263e-08, 3.7333e-02, 2.7597e-07, 9.7323e-09, 4.6006e-01,
        2.6896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 180, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9308, 0.0257, 0.0051, 0.0097, 0.0098, 0.0087, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.010

[Epoch: 181, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.2673e-10, 2.9970e-10, 1.0000e+00, 6.9073e-12, 3.3716e-12, 3.5824e-11,
        5.8767e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 181, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4646e-01, 7.6058e-07, 2.3172e-01, 9.5678e-07, 4.2676e-10, 2.2181e-01,
        4.2720e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.934

[Epoch: 181, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0250, 0.0059, 0.9459, 0.0049, 0.0042, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 181, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3677e-01, 1.8702e-08, 3.5472e-02, 2.3454e-07, 8.4488e-09, 4.5930e-01,
        2.6846e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.322

[Epoch: 181, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9295, 0.0263, 0.0052, 0.0098, 0.0098, 0.0090, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 182, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.3548e-10, 2.3792e-10, 1.0000e+00, 5.2529e-12, 2.5548e-12, 2.8131e-11,
        5.4130e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 182, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4716e-01, 6.8529e-07, 2.3049e-01, 8.3898e-07, 3.5606e-10, 2.2234e-01,
        3.8594e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.935

[Epoch: 182, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0253, 0.0058, 0.9455, 0.0049, 0.0042, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 182, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3510e-01, 1.6643e-08, 3.5469e-02, 2.0625e-07, 7.2442e-09, 4.6011e-01,
        2.6932e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 182, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9299, 0.0259, 0.0052, 0.0098, 0.0097, 0.0091, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 183, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6838e-10, 1.9785e-10, 1.0000e+00, 4.2829e-12, 2.0039e-12, 2.2794e-11,
        4.7871e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 183, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4631e-01, 6.0465e-07, 2.2950e-01, 7.3702e-07, 3.0072e-10, 2.2419e-01,
        3.4705e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.937

[Epoch: 183, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0251, 0.0056, 0.9458, 0.0050, 0.0043, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 183, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3565e-01, 1.4582e-08, 3.4730e-02, 1.8169e-07, 6.3641e-09, 4.6028e-01,
        2.6934e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 183, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9297, 0.0257, 0.0052, 0.0099, 0.0097, 0.0093, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 184, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1782e-10, 1.6445e-10, 1.0000e+00, 3.5460e-12, 1.6393e-12, 1.9577e-11,
        4.3287e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 184, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4607e-01, 5.4863e-07, 2.2952e-01, 6.6043e-07, 2.6367e-10, 2.2440e-01,
        3.2227e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.938

[Epoch: 184, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0253, 0.0055, 0.9455, 0.0050, 0.0043, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 184, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3591e-01, 1.2632e-08, 3.5010e-02, 1.6093e-07, 5.5038e-09, 4.5991e-01,
        2.6916e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 184, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9300, 0.0254, 0.0052, 0.0099, 0.0096, 0.0093, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 185, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.7727e-10, 1.3737e-10, 1.0000e+00, 2.9172e-12, 1.3393e-12, 1.6007e-11,
        3.9504e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 185, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4510e-01, 4.7960e-07, 2.3004e-01, 5.8451e-07, 2.2123e-10, 2.2485e-01,
        2.8833e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 185, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0250, 0.0054, 0.9458, 0.0050, 0.0043, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 185, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3476e-01, 1.1142e-08, 3.4674e-02, 1.4338e-07, 4.8802e-09, 4.6092e-01,
        2.6965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.316

[Epoch: 185, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9295, 0.0255, 0.0053, 0.0099, 0.0098, 0.0095, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 186, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3829e-10, 1.1250e-10, 1.0000e+00, 2.4186e-12, 1.0716e-12, 1.3550e-11,
        3.4037e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 186, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4538e-01, 4.3753e-07, 2.2968e-01, 5.2905e-07, 1.9360e-10, 2.2494e-01,
        2.6229e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.940

[Epoch: 186, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0251, 0.0054, 0.9453, 0.0051, 0.0044, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 186, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3661e-01, 9.8808e-09, 3.5277e-02, 1.2838e-07, 4.2493e-09, 4.5925e-01,
        2.6886e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 186, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9300, 0.0251, 0.0053, 0.0100, 0.0096, 0.0095, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 187, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0796e-10, 9.3984e-11, 1.0000e+00, 1.9696e-12, 8.7075e-13, 1.0917e-11,
        3.1718e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 187, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4460e-01, 3.8296e-07, 2.3019e-01, 4.6959e-07, 1.5914e-10, 2.2521e-01,
        2.3661e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.941

[Epoch: 187, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0252, 0.0052, 0.9455, 0.0051, 0.0044, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 187, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3403e-01, 8.1831e-09, 3.4205e-02, 1.1112e-07, 3.6135e-09, 4.6151e-01,
        2.7026e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.315

[Epoch: 187, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9295, 0.0254, 0.0053, 0.0099, 0.0098, 0.0097, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 188, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.8326e-10, 7.9667e-11, 1.0000e+00, 1.6854e-12, 7.3418e-13, 9.8846e-12,
        2.6913e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 188, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4423e-01, 3.3842e-07, 2.3187e-01, 4.1946e-07, 1.4169e-10, 2.2390e-01,
        2.0190e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.941

[Epoch: 188, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0247, 0.0053, 0.9455, 0.0051, 0.0045, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 188, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3556e-01, 7.7419e-09, 3.6084e-02, 1.0113e-07, 3.1933e-09, 4.5947e-01,
        2.6889e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.319

[Epoch: 188, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9306, 0.0246, 0.0052, 0.0100, 0.0096, 0.0097, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 189, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5046e-10, 6.1953e-11, 1.0000e+00, 1.2871e-12, 5.7076e-13, 7.1358e-12,
        2.5116e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 189, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4614e-01, 3.0666e-07, 2.2716e-01, 3.7865e-07, 1.1568e-10, 2.2669e-01,
        1.9811e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 189, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0050, 0.0258, 0.0051, 0.9444, 0.0052, 0.0047, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 189, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3486e-01, 6.1648e-09, 3.2839e-02, 8.7956e-08, 2.8598e-09, 4.6148e-01,
        2.7082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.311

[Epoch: 189, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9294, 0.0256, 0.0053, 0.0099, 0.0099, 0.0098, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 190, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4030e-10, 6.2152e-11, 1.0000e+00, 1.2281e-12, 5.2103e-13, 7.9377e-12,
        2.4907e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 190, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4598e-01, 2.7245e-07, 2.3290e-01, 3.4472e-07, 1.1095e-10, 2.2112e-01,
        1.6707e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 190, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0226, 0.0052, 0.9484, 0.0048, 0.0045, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 190, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3374e-01, 6.5066e-09, 3.9258e-02, 8.0356e-08, 2.5000e-09, 4.5959e-01,
        2.6741e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 190, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9306, 0.0247, 0.0050, 0.0100, 0.0096, 0.0097, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 191, batch: 43/217] total loss per batch: 0.444
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5950e-10, 5.2788e-11, 1.0000e+00, 1.2857e-12, 5.3866e-13, 7.6549e-12,
        1.8973e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 191, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4225e-01, 2.7954e-07, 2.2574e-01, 3.6118e-07, 9.0195e-11, 2.3201e-01,
        1.9805e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 191, batch: 129/217] total loss per batch: 0.426
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0052, 0.0305, 0.0050, 0.9381, 0.0055, 0.0048, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 191, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3259e-01, 5.9831e-09, 3.0356e-02, 8.9237e-08, 3.0780e-09, 4.6420e-01,
        2.7285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.314

[Epoch: 191, batch: 215/217] total loss per batch: 0.447
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9269, 0.0270, 0.0057, 0.0100, 0.0099, 0.0098, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 192, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2240e-10, 5.3745e-11, 1.0000e+00, 1.2128e-12, 4.0899e-13, 6.6718e-12,
        2.8021e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 192, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5494e-01, 2.7747e-07, 2.3096e-01, 4.9089e-07, 1.3270e-10, 2.1410e-01,
        1.5104e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.948

[Epoch: 192, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0153, 0.0053, 0.9569, 0.0042, 0.0047, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 192, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3923e-01, 6.7616e-09, 4.4909e-02, 1.0484e-07, 2.3950e-09, 4.5061e-01,
        2.6524e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.323

[Epoch: 192, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9353, 0.0218, 0.0049, 0.0098, 0.0091, 0.0094, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 193, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9354e-10, 7.8408e-11, 1.0000e+00, 2.4534e-12, 8.4587e-13, 1.1333e-11,
        2.2147e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 193, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4377e-01, 3.2493e-07, 2.1659e-01, 4.0309e-07, 1.2571e-10, 2.3964e-01,
        2.1443e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.939

[Epoch: 193, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0061, 0.0292, 0.0055, 0.9361, 0.0060, 0.0054, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 193, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2580e-01, 1.2552e-08, 3.1935e-02, 1.3531e-07, 3.9890e-09, 4.6424e-01,
        2.7802e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.314

[Epoch: 193, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9197, 0.0313, 0.0064, 0.0105, 0.0102, 0.0103, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 194, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4626e-10, 6.0119e-11, 1.0000e+00, 1.8860e-12, 1.3032e-12, 1.3154e-11,
        4.3688e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 194, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3297e-01, 6.0739e-07, 2.4666e-01, 5.4747e-07, 1.9532e-10, 2.2037e-01,
        2.8605e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.946

[Epoch: 194, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0048, 0.0212, 0.0049, 0.9496, 0.0051, 0.0047, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 194, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3969e-01, 9.2616e-09, 3.2767e-02, 1.2554e-07, 4.4016e-09, 4.6168e-01,
        2.6586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.317

[Epoch: 194, batch: 215/217] total loss per batch: 0.449
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9370, 0.0206, 0.0052, 0.0102, 0.0088, 0.0093, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 195, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3542e-10, 7.4144e-11, 1.0000e+00, 2.1223e-12, 7.0581e-13, 7.2870e-12,
        3.3741e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 195, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.7802e-01, 2.2189e-07, 2.1449e-01, 3.6868e-07, 4.6871e-10, 2.0749e-01,
        3.1434e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.946

[Epoch: 195, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0053, 0.0270, 0.0055, 0.9408, 0.0054, 0.0051, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 195, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4308e-01, 1.2772e-08, 3.9089e-02, 1.6180e-07, 9.1819e-09, 4.5911e-01,
        2.5872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.313

[Epoch: 195, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9280, 0.0285, 0.0055, 0.0088, 0.0093, 0.0101, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 196, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1248e-10, 1.3296e-10, 1.0000e+00, 2.6635e-12, 1.4073e-12, 1.8720e-11,
        4.9952e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 196, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.2587e-01, 5.8736e-07, 2.2956e-01, 8.2871e-07, 3.7661e-10, 2.4456e-01,
        2.5435e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.947

[Epoch: 196, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0236, 0.0049, 0.9467, 0.0052, 0.0053, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 196, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2251e-01, 1.0440e-08, 3.0015e-02, 1.5085e-07, 5.5800e-09, 4.6447e-01,
        2.8300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 196, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9284, 0.0236, 0.0057, 0.0110, 0.0100, 0.0105, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 197, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.7576e-10, 1.3667e-10, 1.0000e+00, 3.7365e-12, 1.8793e-12, 1.2599e-11,
        9.8885e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 197, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3827e-01, 5.0370e-07, 2.4408e-01, 7.4392e-07, 3.5640e-10, 2.1765e-01,
        3.3649e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 197, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0226, 0.0053, 0.9459, 0.0052, 0.0054, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 197, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3646e-01, 1.4497e-08, 4.0430e-02, 1.2085e-07, 5.5899e-09, 4.5595e-01,
        2.6715e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.318

[Epoch: 197, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9347, 0.0240, 0.0052, 0.0090, 0.0088, 0.0094, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 198, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5604e-10, 8.6962e-11, 1.0000e+00, 1.8014e-12, 8.2014e-13, 8.9567e-12,
        5.1997e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 198, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.4342e-01, 5.7354e-07, 2.2196e-01, 8.3786e-07, 4.3991e-10, 2.3461e-01,
        3.6702e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.945

[Epoch: 198, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0049, 0.0296, 0.0050, 0.9404, 0.0051, 0.0052, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 198, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.4192e-01, 1.5491e-08, 3.4992e-02, 1.9419e-07, 7.1598e-09, 4.4844e-01,
        2.7465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.312

[Epoch: 198, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9266, 0.0278, 0.0054, 0.0105, 0.0099, 0.0096, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 199, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.0054e-10, 9.7687e-11, 1.0000e+00, 2.2990e-12, 1.2823e-12, 8.4365e-12,
        4.5797e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 199, batch: 86/217] total loss per batch: 0.450
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.5368e-01, 4.9228e-07, 2.3477e-01, 6.2257e-07, 3.8322e-10, 2.1154e-01,
        3.3010e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.943

[Epoch: 199, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0055, 0.0206, 0.0050, 0.9474, 0.0054, 0.0053, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 199, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.2449e-01, 1.2804e-08, 3.8388e-02, 1.2906e-07, 6.9240e-09, 4.6270e-01,
        2.7442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.320

[Epoch: 199, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9314, 0.0233, 0.0054, 0.0099, 0.0095, 0.0101, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 200, batch: 43/217] total loss per batch: 0.445
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8082e-10, 1.0276e-10, 1.0000e+00, 2.1831e-12, 9.1100e-13, 1.0383e-11,
        3.2843e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -1.000 -1.000

[Epoch: 200, batch: 86/217] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.5450, 0.0000, 0.2300, 0.0000, 0.0000, 0.2250, 0.0000])
Policy pred: tensor([5.3952e-01, 4.2359e-07, 2.2602e-01, 5.4058e-07, 2.5166e-10, 2.3445e-01,
        1.9280e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.947 0.942

[Epoch: 200, batch: 129/217] total loss per batch: 0.427
Policy (actual, predicted): 3 3
Policy data: tensor([0.0050, 0.0250, 0.0050, 0.9450, 0.0050, 0.0050, 0.0100])
Policy pred: tensor([0.0051, 0.0254, 0.0053, 0.9442, 0.0050, 0.0051, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 200, batch: 172/217] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.2350, 0.0000, 0.0350, 0.0000, 0.0000, 0.4600, 0.2700])
Policy pred: tensor([2.3536e-01, 1.1742e-08, 3.3037e-02, 1.1890e-07, 6.6882e-09, 4.6213e-01,
        2.6947e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.316 0.322

[Epoch: 200, batch: 215/217] total loss per batch: 0.448
Policy (actual, predicted): 0 0
Policy data: tensor([0.9300, 0.0250, 0.0050, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.9340, 0.0250, 0.0053, 0.0092, 0.0091, 0.0092, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

