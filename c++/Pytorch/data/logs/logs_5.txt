Training set samples: 6481
Batch size: 32
[Epoch: 1, batch: 40/203] total loss per batch: 1.731
Policy (actual, predicted): 1 4
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0685, 0.0377, 0.0931, 0.0421, 0.5272, 0.0659, 0.1654],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.036

[Epoch: 1, batch: 80/203] total loss per batch: 1.591
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.1505, 0.0597, 0.3372, 0.0932, 0.0233, 0.1939, 0.1420],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 1, batch: 120/203] total loss per batch: 1.605
Policy (actual, predicted): 5 3
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([7.0587e-02, 3.2090e-01, 5.5928e-06, 3.8040e-01, 1.8365e-06, 1.9249e-01,
        3.5618e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.707

[Epoch: 1, batch: 160/203] total loss per batch: 1.496
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0192, 0.0146, 0.1175, 0.0385, 0.7802, 0.0104, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 1, batch: 200/203] total loss per batch: 1.556
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0200, 0.1836, 0.0717, 0.0199, 0.4381, 0.0601, 0.2066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 2, batch: 40/203] total loss per batch: 1.235
Policy (actual, predicted): 1 4
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0351, 0.0376, 0.0652, 0.0395, 0.5816, 0.0845, 0.1566],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.010

[Epoch: 2, batch: 80/203] total loss per batch: 1.212
Policy (actual, predicted): 1 3
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.1094, 0.2156, 0.2143, 0.2302, 0.0119, 0.0871, 0.1316],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.050

[Epoch: 2, batch: 120/203] total loss per batch: 1.236
Policy (actual, predicted): 5 3
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4307e-02, 2.3707e-01, 9.4168e-07, 5.6800e-01, 9.5625e-08, 1.6578e-01,
        4.8354e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 2, batch: 160/203] total loss per batch: 1.165
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0230, 0.0062, 0.1177, 0.0088, 0.8314, 0.0035, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 2, batch: 200/203] total loss per batch: 1.231
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0060, 0.0591, 0.0188, 0.0062, 0.7713, 0.0302, 0.1084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 3, batch: 40/203] total loss per batch: 0.988
Policy (actual, predicted): 1 4
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0230, 0.0477, 0.1002, 0.0766, 0.4032, 0.1955, 0.1537],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.030

[Epoch: 3, batch: 80/203] total loss per batch: 0.967
Policy (actual, predicted): 1 3
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0288, 0.2697, 0.0435, 0.5394, 0.0225, 0.0402, 0.0558],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.068

[Epoch: 3, batch: 120/203] total loss per batch: 0.972
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.1983e-02, 2.7608e-01, 3.5050e-07, 1.3152e-01, 2.1891e-08, 5.1866e-01,
        6.1749e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.447

[Epoch: 3, batch: 160/203] total loss per batch: 0.902
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0256, 0.0043, 0.1153, 0.0075, 0.8361, 0.0033, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 3, batch: 200/203] total loss per batch: 0.962
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0026, 0.0136, 0.0109, 0.0038, 0.8498, 0.0221, 0.0973],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 4, batch: 40/203] total loss per batch: 0.859
Policy (actual, predicted): 1 4
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0235, 0.0513, 0.1588, 0.0556, 0.3269, 0.1532, 0.2308],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.039

[Epoch: 4, batch: 80/203] total loss per batch: 0.867
Policy (actual, predicted): 1 3
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0248, 0.2448, 0.0272, 0.6193, 0.0174, 0.0384, 0.0282],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 4, batch: 120/203] total loss per batch: 0.885
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.8638e-03, 1.1359e-01, 6.2622e-08, 2.0149e-01, 5.6330e-09, 6.8225e-01,
        8.0241e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.989

[Epoch: 4, batch: 160/203] total loss per batch: 0.826
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0261, 0.0037, 0.1178, 0.0075, 0.8336, 0.0036, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.009

[Epoch: 4, batch: 200/203] total loss per batch: 0.878
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0022, 0.0125, 0.0094, 0.0032, 0.8854, 0.0261, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 5, batch: 40/203] total loss per batch: 0.819
Policy (actual, predicted): 1 4
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0215, 0.1303, 0.1900, 0.0795, 0.2577, 0.2552, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.046

[Epoch: 5, batch: 80/203] total loss per batch: 0.837
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0207, 0.6894, 0.0165, 0.1664, 0.0278, 0.0173, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.072

[Epoch: 5, batch: 120/203] total loss per batch: 0.860
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.6091e-04, 1.5755e-02, 4.4396e-09, 7.7121e-05, 4.4198e-10, 9.8282e-01,
        1.1901e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 5, batch: 160/203] total loss per batch: 0.808
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0235, 0.0032, 0.1148, 0.0076, 0.8424, 0.0030, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 5, batch: 200/203] total loss per batch: 0.853
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0015, 0.0064, 0.0070, 0.0022, 0.7258, 0.0190, 0.2382],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 6, batch: 40/203] total loss per batch: 0.804
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0142, 0.1288, 0.2404, 0.0187, 0.1563, 0.3142, 0.1274],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.050

[Epoch: 6, batch: 80/203] total loss per batch: 0.817
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0206, 0.6164, 0.0244, 0.2096, 0.0187, 0.0176, 0.0926],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.048

[Epoch: 6, batch: 120/203] total loss per batch: 0.843
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.1360e-04, 6.5897e-03, 5.1897e-09, 8.4305e-05, 4.0964e-10, 9.9218e-01,
        1.0368e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.999

[Epoch: 6, batch: 160/203] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0210, 0.0031, 0.1024, 0.0062, 0.8565, 0.0032, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 6, batch: 200/203] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0014, 0.0096, 0.0067, 0.0044, 0.7358, 0.0327, 0.2094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 7, batch: 40/203] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0081, 0.3873, 0.2212, 0.0284, 0.1413, 0.1430, 0.0708],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.051

[Epoch: 7, batch: 80/203] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0255, 0.7911, 0.0203, 0.0807, 0.0185, 0.0097, 0.0541],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.069

[Epoch: 7, batch: 120/203] total loss per batch: 0.828
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.9921e-04, 8.9834e-03, 1.0292e-08, 8.8193e-05, 7.5493e-10, 9.8882e-01,
        1.9075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.999

[Epoch: 7, batch: 160/203] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0029, 0.1209, 0.0063, 0.8400, 0.0024, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 7, batch: 200/203] total loss per batch: 0.816
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0021, 0.0076, 0.0088, 0.0034, 0.7897, 0.0127, 0.1757],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 8, batch: 40/203] total loss per batch: 0.776
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0117, 0.1836, 0.1287, 0.0312, 0.0926, 0.4787, 0.0736],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.042

[Epoch: 8, batch: 80/203] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0172, 0.4433, 0.0323, 0.3235, 0.0364, 0.0402, 0.1071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.058

[Epoch: 8, batch: 120/203] total loss per batch: 0.817
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5562e-04, 1.2638e-02, 2.0887e-08, 3.9541e-04, 7.3555e-10, 9.8368e-01,
        3.0319e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 8, batch: 160/203] total loss per batch: 0.759
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0031, 0.1244, 0.0069, 0.8332, 0.0031, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 8, batch: 200/203] total loss per batch: 0.807
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0015, 0.0083, 0.0045, 0.0031, 0.7085, 0.0217, 0.2524],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 9, batch: 40/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0111, 0.2764, 0.2681, 0.0233, 0.1139, 0.1932, 0.1140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.049

[Epoch: 9, batch: 80/203] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0090, 0.4699, 0.0271, 0.3696, 0.0191, 0.0363, 0.0691],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.067

[Epoch: 9, batch: 120/203] total loss per batch: 0.811
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3074e-04, 8.3258e-03, 1.1603e-08, 7.6128e-04, 1.7695e-09, 9.8617e-01,
        4.5086e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.996

[Epoch: 9, batch: 160/203] total loss per batch: 0.752
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0026, 0.1217, 0.0065, 0.8366, 0.0031, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.013

[Epoch: 9, batch: 200/203] total loss per batch: 0.800
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0021, 0.0040, 0.0063, 0.0028, 0.7549, 0.0220, 0.2081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 10, batch: 40/203] total loss per batch: 0.766
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0090, 0.2687, 0.1817, 0.0283, 0.1211, 0.2854, 0.1058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.050

[Epoch: 10, batch: 80/203] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0106, 0.4379, 0.0380, 0.3417, 0.0272, 0.0404, 0.1041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.056

[Epoch: 10, batch: 120/203] total loss per batch: 0.809
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0344e-04, 1.6123e-02, 3.1506e-08, 1.2930e-03, 1.5492e-09, 9.7084e-01,
        1.1443e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.999

[Epoch: 10, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0212, 0.0028, 0.1160, 0.0059, 0.8441, 0.0030, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 10, batch: 200/203] total loss per batch: 0.797
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0020, 0.0057, 0.0036, 0.0047, 0.7597, 0.0199, 0.2044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 11, batch: 40/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0067, 0.3234, 0.2365, 0.0195, 0.0876, 0.2398, 0.0866],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.044

[Epoch: 11, batch: 80/203] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0092, 0.4719, 0.0227, 0.3219, 0.0400, 0.0409, 0.0934],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.060

[Epoch: 11, batch: 120/203] total loss per batch: 0.807
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.2498e-04, 1.3415e-02, 3.5762e-08, 1.6401e-03, 2.1420e-09, 9.8018e-01,
        4.3428e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.999

[Epoch: 11, batch: 160/203] total loss per batch: 0.749
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0236, 0.0029, 0.1296, 0.0071, 0.8262, 0.0036, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 11, batch: 200/203] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0019, 0.0051, 0.0061, 0.0033, 0.7605, 0.0159, 0.2072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.032

[Epoch: 12, batch: 40/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0068, 0.3489, 0.2046, 0.0254, 0.1035, 0.2541, 0.0568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.031

[Epoch: 12, batch: 80/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0107, 0.4264, 0.0245, 0.3737, 0.0303, 0.0487, 0.0858],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.057

[Epoch: 12, batch: 120/203] total loss per batch: 0.804
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.9663e-04, 1.6220e-02, 5.9918e-08, 2.2954e-03, 1.7666e-09, 9.7404e-01,
        6.9436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 12, batch: 160/203] total loss per batch: 0.746
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0191, 0.0028, 0.1152, 0.0062, 0.8477, 0.0030, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 12, batch: 200/203] total loss per batch: 0.792
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0024, 0.0040, 0.0040, 0.0052, 0.7186, 0.0150, 0.2506],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 13, batch: 40/203] total loss per batch: 0.758
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0082, 0.2267, 0.2281, 0.0166, 0.0682, 0.2969, 0.1552],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.033

[Epoch: 13, batch: 80/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0103, 0.5019, 0.0403, 0.2881, 0.0377, 0.0507, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.047

[Epoch: 13, batch: 120/203] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.5726e-04, 1.5368e-02, 1.1082e-07, 4.5826e-03, 2.5199e-09, 9.7443e-01,
        5.1597e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.995

[Epoch: 13, batch: 160/203] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0242, 0.0030, 0.1202, 0.0068, 0.8355, 0.0031, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 13, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0022, 0.0042, 0.0060, 0.0037, 0.7440, 0.0180, 0.2219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 14, batch: 40/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0047, 0.3707, 0.1813, 0.0222, 0.1197, 0.2593, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.043

[Epoch: 14, batch: 80/203] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0044, 0.6193, 0.0223, 0.2448, 0.0142, 0.0200, 0.0749],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.048

[Epoch: 14, batch: 120/203] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.0055e-03, 1.3562e-02, 8.4138e-08, 7.1949e-03, 3.6197e-09, 9.7041e-01,
        7.8272e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 14, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0033, 0.1273, 0.0068, 0.8313, 0.0025, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.011

[Epoch: 14, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0020, 0.0037, 0.0041, 0.0039, 0.7863, 0.0214, 0.1785],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 15, batch: 40/203] total loss per batch: 0.758
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0068, 0.2788, 0.3090, 0.0175, 0.0602, 0.2524, 0.0752],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.037

[Epoch: 15, batch: 80/203] total loss per batch: 0.773
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0066, 0.6214, 0.0242, 0.2148, 0.0307, 0.0398, 0.0624],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 15, batch: 120/203] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([5.6687e-04, 1.5206e-02, 5.3717e-08, 7.9743e-03, 4.0432e-09, 9.7137e-01,
        4.8800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 15, batch: 160/203] total loss per batch: 0.743
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0202, 0.0028, 0.1148, 0.0045, 0.8487, 0.0033, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 15, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0023, 0.0029, 0.0046, 0.0031, 0.7132, 0.0147, 0.2591],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 16, batch: 40/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0050, 0.3918, 0.1192, 0.0257, 0.0736, 0.3010, 0.0837],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.032

[Epoch: 16, batch: 80/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0066, 0.5738, 0.0265, 0.2840, 0.0299, 0.0282, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 16, batch: 120/203] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.7172e-04, 7.4032e-03, 3.5331e-08, 5.0396e-03, 3.3911e-09, 9.8278e-01,
        4.3013e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.998

[Epoch: 16, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0256, 0.0030, 0.1283, 0.0073, 0.8257, 0.0031, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 16, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0020, 0.0032, 0.0041, 0.0041, 0.7130, 0.0151, 0.2585],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 17, batch: 40/203] total loss per batch: 0.756
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0060, 0.2099, 0.2822, 0.0223, 0.0927, 0.2476, 0.1394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 17, batch: 80/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0074, 0.4086, 0.0268, 0.3845, 0.0363, 0.0420, 0.0944],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.060

[Epoch: 17, batch: 120/203] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([6.6017e-04, 1.6223e-02, 3.4781e-07, 1.2424e-02, 1.1551e-08, 9.5964e-01,
        1.1052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 17, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0027, 0.1215, 0.0065, 0.8367, 0.0028, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.008

[Epoch: 17, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0023, 0.0033, 0.0026, 0.0046, 0.7581, 0.0249, 0.2041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 18, batch: 40/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3540, 0.2126, 0.0275, 0.0939, 0.2675, 0.0404],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.043

[Epoch: 18, batch: 80/203] total loss per batch: 0.771
Policy (actual, predicted): 1 3
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0087, 0.3528, 0.0376, 0.4220, 0.0299, 0.0425, 0.1064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.070

[Epoch: 18, batch: 120/203] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([6.3094e-04, 8.4404e-03, 1.3932e-07, 1.0832e-02, 9.0499e-09, 9.7619e-01,
        3.9092e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.998

[Epoch: 18, batch: 160/203] total loss per batch: 0.740
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0214, 0.0029, 0.1125, 0.0060, 0.8480, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 18, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0030, 0.0031, 0.0036, 0.7508, 0.0153, 0.2215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.034

[Epoch: 19, batch: 40/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0051, 0.3129, 0.1748, 0.0148, 0.0843, 0.2987, 0.1093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.019

[Epoch: 19, batch: 80/203] total loss per batch: 0.771
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0101, 0.4701, 0.0287, 0.3251, 0.0441, 0.0298, 0.0921],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.063

[Epoch: 19, batch: 120/203] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([6.9107e-04, 1.1681e-02, 1.4370e-07, 1.5747e-02, 1.9095e-08, 9.5977e-01,
        1.2106e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 19, batch: 160/203] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0029, 0.1323, 0.0051, 0.8265, 0.0029, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.009

[Epoch: 19, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0041, 0.0041, 0.0056, 0.7166, 0.0138, 0.2531],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 20, batch: 40/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0056, 0.3309, 0.2887, 0.0229, 0.0782, 0.1475, 0.1263],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.035

[Epoch: 20, batch: 80/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0072, 0.5436, 0.0302, 0.2929, 0.0286, 0.0432, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.067

[Epoch: 20, batch: 120/203] total loss per batch: 0.800
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([5.4523e-04, 7.7134e-03, 1.1038e-07, 1.0872e-02, 3.2006e-09, 9.7904e-01,
        1.8280e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.998

[Epoch: 20, batch: 160/203] total loss per batch: 0.738
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0028, 0.1071, 0.0062, 0.8514, 0.0033, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.012

[Epoch: 20, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0021, 0.0029, 0.0031, 0.0032, 0.7857, 0.0168, 0.1862],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.037

[Epoch: 21, batch: 40/203] total loss per batch: 0.757
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.3046, 0.1142, 0.0239, 0.0977, 0.4209, 0.0343],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.023

[Epoch: 21, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0086, 0.5228, 0.0247, 0.3143, 0.0305, 0.0305, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.065

[Epoch: 21, batch: 120/203] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.8752e-04, 7.4492e-03, 1.1783e-07, 8.0439e-03, 8.4964e-09, 9.7961e-01,
        4.4133e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 21, batch: 160/203] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0032, 0.1210, 0.0056, 0.8393, 0.0027, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 21, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0032, 0.0028, 0.0032, 0.7815, 0.0130, 0.1934],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 22, batch: 40/203] total loss per batch: 0.758
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0049, 0.2852, 0.3379, 0.0253, 0.0647, 0.2163, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.023

[Epoch: 22, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0052, 0.5382, 0.0281, 0.2578, 0.0486, 0.0423, 0.0797],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 22, batch: 120/203] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([5.7875e-04, 7.5361e-03, 2.4065e-07, 1.6499e-02, 8.7889e-09, 9.6471e-01,
        1.0673e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.995

[Epoch: 22, batch: 160/203] total loss per batch: 0.740
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0247, 0.0028, 0.1269, 0.0059, 0.8316, 0.0029, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 22, batch: 200/203] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0045, 0.0034, 0.0043, 0.7388, 0.0106, 0.2345],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 23, batch: 40/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0074, 0.3085, 0.2019, 0.0262, 0.1230, 0.2448, 0.0881],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 23, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0048, 0.6066, 0.0189, 0.2505, 0.0376, 0.0295, 0.0521],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.054

[Epoch: 23, batch: 120/203] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.0224e-03, 6.8013e-03, 2.5529e-07, 3.1544e-02, 5.1468e-08, 9.5719e-01,
        3.4396e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.997

[Epoch: 23, batch: 160/203] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0031, 0.1214, 0.0061, 0.8364, 0.0033, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 23, batch: 200/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0022, 0.0041, 0.0039, 0.0027, 0.7280, 0.0116, 0.2475],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 24, batch: 40/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.4399, 0.1786, 0.0201, 0.0543, 0.2350, 0.0682],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 24, batch: 80/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0049, 0.5573, 0.0233, 0.3071, 0.0122, 0.0310, 0.0642],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.054

[Epoch: 24, batch: 120/203] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.7760e-03, 1.2995e-02, 8.1214e-08, 2.8216e-03, 7.0982e-09, 9.7441e-01,
        7.9975e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.994

[Epoch: 24, batch: 160/203] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0030, 0.1238, 0.0060, 0.8364, 0.0027, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 24, batch: 200/203] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0021, 0.0034, 0.0049, 0.0038, 0.7196, 0.0176, 0.2486],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.029

[Epoch: 25, batch: 40/203] total loss per batch: 0.755
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0069, 0.2106, 0.2546, 0.0264, 0.0517, 0.3661, 0.0837],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.027

[Epoch: 25, batch: 80/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0041, 0.5179, 0.0230, 0.3429, 0.0198, 0.0318, 0.0606],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.064

[Epoch: 25, batch: 120/203] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([9.8694e-04, 3.8969e-03, 7.2247e-08, 5.4753e-03, 7.8538e-09, 9.8632e-01,
        3.3191e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.984

[Epoch: 25, batch: 160/203] total loss per batch: 0.741
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0031, 0.1160, 0.0058, 0.8431, 0.0029, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.010

[Epoch: 25, batch: 200/203] total loss per batch: 0.786
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0023, 0.0025, 0.0026, 0.7773, 0.0116, 0.2008],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 26, batch: 40/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0048, 0.3503, 0.1732, 0.0415, 0.1466, 0.2083, 0.0753],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 26, batch: 80/203] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0050, 0.4545, 0.0293, 0.3947, 0.0210, 0.0466, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.075

[Epoch: 26, batch: 120/203] total loss per batch: 0.802
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5367e-03, 1.5974e-02, 2.4832e-07, 1.1144e-02, 2.1365e-08, 9.5938e-01,
        1.0966e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.986

[Epoch: 26, batch: 160/203] total loss per batch: 0.740
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0245, 0.0038, 0.1185, 0.0048, 0.8395, 0.0030, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 26, batch: 200/203] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0030, 0.0063, 0.0043, 0.7636, 0.0163, 0.2036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.026

[Epoch: 27, batch: 40/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.3583, 0.2408, 0.0267, 0.0681, 0.2289, 0.0729],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.012

[Epoch: 27, batch: 80/203] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0053, 0.4557, 0.0295, 0.3671, 0.0323, 0.0218, 0.0883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.044

[Epoch: 27, batch: 120/203] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([6.2790e-04, 3.1953e-03, 1.5373e-07, 7.8644e-03, 5.3789e-09, 9.8316e-01,
        5.1550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.995

[Epoch: 27, batch: 160/203] total loss per batch: 0.737
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0032, 0.1163, 0.0059, 0.8447, 0.0026, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 27, batch: 200/203] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0034, 0.0040, 0.0054, 0.7298, 0.0136, 0.2404],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 28, batch: 40/203] total loss per batch: 0.753
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0068, 0.2484, 0.1613, 0.0237, 0.0660, 0.3635, 0.1304],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.021

[Epoch: 28, batch: 80/203] total loss per batch: 0.770
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0068, 0.4108, 0.0324, 0.3248, 0.0347, 0.0984, 0.0921],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.044

[Epoch: 28, batch: 120/203] total loss per batch: 0.798
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3919e-03, 1.1714e-02, 2.7504e-07, 7.9258e-03, 2.8699e-08, 9.7026e-01,
        7.7066e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.990

[Epoch: 28, batch: 160/203] total loss per batch: 0.737
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0246, 0.0030, 0.1158, 0.0063, 0.8400, 0.0031, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 28, batch: 200/203] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0037, 0.0031, 0.0030, 0.7559, 0.0173, 0.2137],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 29, batch: 40/203] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3844, 0.2395, 0.0248, 0.1076, 0.1930, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.019

[Epoch: 29, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0111, 0.4740, 0.0240, 0.3350, 0.0314, 0.0235, 0.1010],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.031

[Epoch: 29, batch: 120/203] total loss per batch: 0.797
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.6370e-03, 4.4271e-03, 1.9440e-07, 1.0104e-02, 1.2299e-08, 9.7732e-01,
        6.5159e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.981

[Epoch: 29, batch: 160/203] total loss per batch: 0.736
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0205, 0.0029, 0.1099, 0.0060, 0.8523, 0.0030, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.012

[Epoch: 29, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0019, 0.0020, 0.0044, 0.0051, 0.7552, 0.0134, 0.2179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 30, batch: 40/203] total loss per batch: 0.751
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0061, 0.2549, 0.2341, 0.0223, 0.0893, 0.2929, 0.1004],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 30, batch: 80/203] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0064, 0.5266, 0.0362, 0.3070, 0.0414, 0.0156, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 30, batch: 120/203] total loss per batch: 0.794
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.2164e-03, 6.7672e-03, 4.5035e-07, 3.3609e-02, 2.7807e-08, 9.5085e-01,
        6.5532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.980

[Epoch: 30, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0030, 0.1235, 0.0055, 0.8364, 0.0031, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 30, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0037, 0.0036, 0.0049, 0.7080, 0.0180, 0.2577],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 31, batch: 40/203] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0043, 0.3258, 0.2214, 0.0221, 0.0669, 0.2957, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 31, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0060, 0.5279, 0.0257, 0.2870, 0.0395, 0.0321, 0.0818],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 31, batch: 120/203] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.7762e-03, 4.4633e-03, 8.0263e-08, 2.4865e-03, 7.1211e-09, 9.8759e-01,
        3.6862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.988

[Epoch: 31, batch: 160/203] total loss per batch: 0.733
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0030, 0.1203, 0.0066, 0.8375, 0.0030, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 31, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0040, 0.0032, 0.0035, 0.7401, 0.0106, 0.2356],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 32, batch: 40/203] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0049, 0.3781, 0.1338, 0.0222, 0.0806, 0.2740, 0.1063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 32, batch: 80/203] total loss per batch: 0.764
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0050, 0.5066, 0.0245, 0.3520, 0.0276, 0.0290, 0.0553],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 32, batch: 120/203] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.5061e-03, 5.0537e-03, 8.2809e-08, 5.9420e-03, 8.1904e-09, 9.7154e-01,
        1.2957e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.979

[Epoch: 32, batch: 160/203] total loss per batch: 0.733
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0028, 0.1113, 0.0053, 0.8478, 0.0033, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 32, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0033, 0.0029, 0.0036, 0.7693, 0.0140, 0.2032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.020

[Epoch: 33, batch: 40/203] total loss per batch: 0.747
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0052, 0.2731, 0.3515, 0.0207, 0.0580, 0.2263, 0.0651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 33, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5666, 0.0307, 0.2807, 0.0375, 0.0278, 0.0519],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.054

[Epoch: 33, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7836e-03, 8.5583e-03, 2.7888e-07, 2.1168e-02, 2.8508e-08, 9.6268e-01,
        3.8119e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.985

[Epoch: 33, batch: 160/203] total loss per batch: 0.732
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0232, 0.0031, 0.1206, 0.0063, 0.8379, 0.0030, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.009

[Epoch: 33, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0046, 0.0031, 0.0039, 0.0051, 0.7515, 0.0191, 0.2127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 34, batch: 40/203] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0063, 0.3147, 0.1504, 0.0279, 0.1278, 0.2730, 0.0998],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 34, batch: 80/203] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.5086, 0.0231, 0.3291, 0.0260, 0.0344, 0.0748],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 34, batch: 120/203] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5471e-03, 2.8434e-03, 3.3459e-07, 4.3888e-03, 1.2086e-08, 9.7738e-01,
        1.2845e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.981

[Epoch: 34, batch: 160/203] total loss per batch: 0.732
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0241, 0.0029, 0.1194, 0.0066, 0.8374, 0.0029, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 34, batch: 200/203] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0032, 0.0041, 0.0041, 0.7454, 0.0129, 0.2275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 35, batch: 40/203] total loss per batch: 0.748
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3132, 0.2035, 0.0153, 0.0590, 0.3543, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.015

[Epoch: 35, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.4836, 0.0255, 0.3263, 0.0470, 0.0408, 0.0728],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.048

[Epoch: 35, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7058e-03, 4.1927e-03, 2.4839e-07, 2.0506e-02, 1.1725e-08, 9.6842e-01,
        4.1727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.970

[Epoch: 35, batch: 160/203] total loss per batch: 0.733
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0236, 0.0030, 0.1166, 0.0057, 0.8419, 0.0027, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 35, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0035, 0.0035, 0.0036, 0.7505, 0.0142, 0.2212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 36, batch: 40/203] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0073, 0.3420, 0.2463, 0.0247, 0.0663, 0.1816, 0.1318],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 36, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0054, 0.4902, 0.0407, 0.3345, 0.0273, 0.0297, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.059

[Epoch: 36, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7385e-03, 3.5992e-03, 4.2109e-07, 6.3669e-03, 3.3389e-08, 9.8171e-01,
        5.5846e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.962

[Epoch: 36, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0208, 0.0027, 0.1194, 0.0065, 0.8426, 0.0029, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 36, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0024, 0.0043, 0.0048, 0.7466, 0.0118, 0.2274],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 37, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0101, 0.2764, 0.1717, 0.0355, 0.0972, 0.2509, 0.1582],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 37, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0087, 0.4999, 0.0207, 0.2862, 0.0481, 0.0304, 0.1060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.053

[Epoch: 37, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9197e-03, 6.8627e-03, 9.3921e-07, 2.7492e-02, 3.5856e-08, 9.5297e-01,
        9.7512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.969

[Epoch: 37, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0249, 0.0032, 0.1170, 0.0074, 0.8382, 0.0030, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 37, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0044, 0.0045, 0.0039, 0.0036, 0.7441, 0.0153, 0.2240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 38, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0052, 0.4194, 0.1941, 0.0156, 0.1222, 0.2208, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.025

[Epoch: 38, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0046, 0.5451, 0.0420, 0.2552, 0.0363, 0.0343, 0.0824],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.080

[Epoch: 38, batch: 120/203] total loss per batch: 0.790
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6099e-03, 5.6228e-03, 6.2636e-07, 4.9909e-03, 1.9556e-08, 9.7835e-01,
        7.4277e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.947

[Epoch: 38, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0033, 0.1222, 0.0056, 0.8366, 0.0028, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 38, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0025, 0.0029, 0.0050, 0.0061, 0.7633, 0.0235, 0.1968],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 39, batch: 40/203] total loss per batch: 0.749
Policy (actual, predicted): 1 2
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0063, 0.2254, 0.3109, 0.0191, 0.0815, 0.2963, 0.0605],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 39, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.6127, 0.0227, 0.2574, 0.0254, 0.0262, 0.0513],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.050

[Epoch: 39, batch: 120/203] total loss per batch: 0.790
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.6866e-03, 4.1599e-03, 5.2438e-07, 8.0743e-03, 5.1224e-08, 9.7711e-01,
        8.9731e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.962

[Epoch: 39, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0233, 0.0028, 0.1328, 0.0065, 0.8251, 0.0030, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 39, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0021, 0.0030, 0.0026, 0.0024, 0.7271, 0.0119, 0.2510],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 40, batch: 40/203] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0071, 0.3421, 0.1312, 0.0239, 0.0649, 0.3248, 0.1059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 40, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0050, 0.5093, 0.0182, 0.3053, 0.0530, 0.0409, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 40, batch: 120/203] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7184e-03, 3.5051e-03, 5.2871e-07, 1.8004e-02, 2.0121e-08, 9.7161e-01,
        4.1603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.923

[Epoch: 40, batch: 160/203] total loss per batch: 0.733
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0030, 0.1120, 0.0063, 0.8476, 0.0027, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 40, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0020, 0.0030, 0.0036, 0.0027, 0.7482, 0.0090, 0.2315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 41, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0066, 0.3292, 0.2447, 0.0174, 0.0960, 0.2338, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 41, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5038, 0.0322, 0.3593, 0.0326, 0.0259, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.055

[Epoch: 41, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([5.0691e-03, 1.1166e-02, 1.2334e-06, 2.2120e-02, 9.7585e-08, 9.5417e-01,
        7.4741e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.954

[Epoch: 41, batch: 160/203] total loss per batch: 0.732
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0033, 0.1098, 0.0058, 0.8505, 0.0031, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.006

[Epoch: 41, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0044, 0.0041, 0.0038, 0.0052, 0.7439, 0.0130, 0.2257],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 42, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0056, 0.2835, 0.2634, 0.0211, 0.0765, 0.2796, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 42, batch: 80/203] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0056, 0.5124, 0.0308, 0.3109, 0.0251, 0.0400, 0.0752],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 42, batch: 120/203] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.4402e-03, 2.4843e-03, 3.3507e-07, 2.2738e-03, 1.6904e-08, 9.8539e-01,
        5.4118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.955

[Epoch: 42, batch: 160/203] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0242, 0.0030, 0.1228, 0.0071, 0.8324, 0.0031, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 42, batch: 200/203] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0039, 0.0044, 0.0051, 0.7453, 0.0173, 0.2208],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 43, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0042, 0.3484, 0.2157, 0.0168, 0.0649, 0.2509, 0.0990],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 43, batch: 80/203] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.5434, 0.0347, 0.3089, 0.0251, 0.0323, 0.0516],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 43, batch: 120/203] total loss per batch: 0.793
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.1136e-03, 2.2665e-03, 4.6509e-07, 1.7457e-02, 1.1263e-07, 9.7143e-01,
        6.7367e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.934

[Epoch: 43, batch: 160/203] total loss per batch: 0.735
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0221, 0.0031, 0.1132, 0.0061, 0.8465, 0.0030, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.011

[Epoch: 43, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0046, 0.0031, 0.0037, 0.7251, 0.0170, 0.2428],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 44, batch: 40/203] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0055, 0.3048, 0.2065, 0.0263, 0.1252, 0.2678, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 44, batch: 80/203] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0053, 0.5494, 0.0208, 0.2974, 0.0260, 0.0308, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 44, batch: 120/203] total loss per batch: 0.810
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.4024e-03, 1.1377e-02, 3.1125e-06, 4.8085e-02, 2.1539e-06, 9.2141e-01,
        1.4721e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.867

[Epoch: 44, batch: 160/203] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0189, 0.0026, 0.1081, 0.0056, 0.8557, 0.0034, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.011

[Epoch: 44, batch: 200/203] total loss per batch: 0.883
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0045, 0.0015, 0.0022, 0.7340, 0.0202, 0.2346],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 45, batch: 40/203] total loss per batch: 0.866
Policy (actual, predicted): 1 6
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0169, 0.1759, 0.2080, 0.0249, 0.0289, 0.1903, 0.3551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.035

[Epoch: 45, batch: 80/203] total loss per batch: 0.878
Policy (actual, predicted): 1 3
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0092, 0.1007, 0.0220, 0.6684, 0.0467, 0.0215, 0.1315],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.094

[Epoch: 45, batch: 120/203] total loss per batch: 0.888
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3331e-04, 4.7493e-03, 1.3190e-07, 1.9153e-03, 8.9289e-09, 9.8052e-01,
        1.2481e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.984

[Epoch: 45, batch: 160/203] total loss per batch: 0.833
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0202, 0.0040, 0.1290, 0.0090, 0.8245, 0.0031, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 45, batch: 200/203] total loss per batch: 0.884
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0024, 0.0046, 0.0014, 0.0048, 0.7949, 0.0122, 0.1798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.034

[Epoch: 46, batch: 40/203] total loss per batch: 0.821
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0071, 0.3512, 0.2964, 0.0343, 0.0781, 0.1962, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.012

[Epoch: 46, batch: 80/203] total loss per batch: 0.821
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0148, 0.6013, 0.0504, 0.2323, 0.0306, 0.0213, 0.0494],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.077

[Epoch: 46, batch: 120/203] total loss per batch: 0.838
Policy (actual, predicted): 5 6
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([9.2782e-04, 2.5752e-02, 2.1750e-07, 2.8300e-03, 1.2542e-07, 2.5034e-01,
        7.2015e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.892

[Epoch: 46, batch: 160/203] total loss per batch: 0.768
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0177, 0.0031, 0.1220, 0.0067, 0.8415, 0.0029, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 46, batch: 200/203] total loss per batch: 0.808
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0049, 0.0028, 0.0046, 0.7590, 0.0139, 0.2115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 47, batch: 40/203] total loss per batch: 0.770
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0084, 0.2699, 0.2216, 0.0345, 0.1033, 0.3315, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 47, batch: 80/203] total loss per batch: 0.775
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0113, 0.5243, 0.0387, 0.2802, 0.0568, 0.0277, 0.0611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.082

[Epoch: 47, batch: 120/203] total loss per batch: 0.801
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([1.6624e-02, 8.4942e-02, 2.6555e-06, 2.9477e-02, 1.2980e-06, 8.5596e-01,
        1.2991e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.948

[Epoch: 47, batch: 160/203] total loss per batch: 0.738
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0217, 0.0035, 0.1242, 0.0077, 0.8331, 0.0031, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 47, batch: 200/203] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0046, 0.0031, 0.0039, 0.7845, 0.0101, 0.1907],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.021

[Epoch: 48, batch: 40/203] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0083, 0.3523, 0.1767, 0.0265, 0.0791, 0.3039, 0.0532],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 48, batch: 80/203] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0093, 0.5370, 0.0301, 0.2754, 0.0480, 0.0291, 0.0710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.079

[Epoch: 48, batch: 120/203] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7520e-03, 1.2312e-02, 3.1863e-07, 8.2416e-03, 1.9752e-07, 9.7290e-01,
        3.7895e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.960

[Epoch: 48, batch: 160/203] total loss per batch: 0.730
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0214, 0.0031, 0.1219, 0.0071, 0.8372, 0.0032, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 48, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0039, 0.0030, 0.0036, 0.7387, 0.0112, 0.2367],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 49, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0084, 0.3520, 0.2124, 0.0232, 0.0786, 0.2615, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 49, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0065, 0.5277, 0.0313, 0.2976, 0.0438, 0.0295, 0.0636],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.079

[Epoch: 49, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3317e-03, 9.2025e-03, 2.9247e-07, 9.4436e-03, 2.1475e-07, 9.7514e-01,
        3.8849e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.950

[Epoch: 49, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0212, 0.0030, 0.1244, 0.0069, 0.8353, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 49, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0040, 0.0031, 0.0042, 0.7595, 0.0106, 0.2159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 50, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0077, 0.3260, 0.2148, 0.0218, 0.0742, 0.2885, 0.0669],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 50, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0066, 0.5273, 0.0283, 0.3004, 0.0445, 0.0300, 0.0628],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.076

[Epoch: 50, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5193e-03, 8.7785e-03, 3.0475e-07, 9.5720e-03, 2.1067e-07, 9.7475e-01,
        4.3798e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 50, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0031, 0.1204, 0.0070, 0.8389, 0.0032, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 50, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0039, 0.0030, 0.0040, 0.7562, 0.0104, 0.2198],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 51, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0069, 0.3339, 0.2230, 0.0199, 0.0783, 0.2616, 0.0765],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 51, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0054, 0.4987, 0.0274, 0.3287, 0.0423, 0.0315, 0.0660],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.074

[Epoch: 51, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.1964e-03, 6.7845e-03, 2.8438e-07, 1.2063e-02, 2.2646e-07, 9.7462e-01,
        4.3358e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 51, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0218, 0.0030, 0.1205, 0.0069, 0.8388, 0.0031, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 51, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0037, 0.0030, 0.0043, 0.7602, 0.0104, 0.2154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 52, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0073, 0.3264, 0.2173, 0.0201, 0.0712, 0.2781, 0.0796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 52, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0055, 0.5029, 0.0295, 0.3202, 0.0427, 0.0337, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.074

[Epoch: 52, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.1658e-03, 6.2748e-03, 2.3058e-07, 1.2071e-02, 1.7870e-07, 9.7496e-01,
        4.5292e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.943

[Epoch: 52, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0031, 0.1217, 0.0070, 0.8374, 0.0032, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 52, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0035, 0.0030, 0.0044, 0.7498, 0.0111, 0.2251],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 53, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0064, 0.3286, 0.2147, 0.0195, 0.0724, 0.2755, 0.0829],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 53, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0048, 0.5125, 0.0270, 0.3202, 0.0403, 0.0311, 0.0641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.071

[Epoch: 53, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3672e-03, 6.4176e-03, 2.8526e-07, 1.2725e-02, 2.0589e-07, 9.7319e-01,
        5.2956e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 53, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0218, 0.0030, 0.1215, 0.0069, 0.8380, 0.0031, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 53, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0034, 0.0031, 0.0044, 0.7562, 0.0113, 0.2185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 54, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0063, 0.3306, 0.2217, 0.0198, 0.0746, 0.2671, 0.0798],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 54, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5068, 0.0287, 0.3224, 0.0403, 0.0323, 0.0649],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.069

[Epoch: 54, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.2291e-03, 5.6620e-03, 2.0662e-07, 1.3565e-02, 1.7364e-07, 9.7370e-01,
        4.8409e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.942

[Epoch: 54, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0031, 0.1203, 0.0070, 0.8388, 0.0032, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 54, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0033, 0.0031, 0.0044, 0.7564, 0.0117, 0.2181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 55, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0058, 0.3248, 0.2123, 0.0190, 0.0703, 0.2838, 0.0841],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 55, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5132, 0.0285, 0.3192, 0.0403, 0.0308, 0.0637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.069

[Epoch: 55, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3560e-03, 5.4801e-03, 2.4929e-07, 1.1961e-02, 1.8149e-07, 9.7452e-01,
        5.6853e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.934

[Epoch: 55, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0222, 0.0030, 0.1214, 0.0068, 0.8378, 0.0031, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 55, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0032, 0.0031, 0.0044, 0.7549, 0.0118, 0.2194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 56, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0057, 0.3356, 0.2302, 0.0219, 0.0765, 0.2506, 0.0795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 56, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5192, 0.0282, 0.3147, 0.0382, 0.0315, 0.0639],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.065

[Epoch: 56, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3788e-03, 5.4663e-03, 2.0546e-07, 1.5014e-02, 1.7763e-07, 9.7165e-01,
        5.4934e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.937

[Epoch: 56, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0221, 0.0031, 0.1189, 0.0070, 0.8398, 0.0032, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 56, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0033, 0.0031, 0.0042, 0.7613, 0.0117, 0.2135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 57, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0055, 0.3166, 0.2005, 0.0174, 0.0682, 0.3073, 0.0845],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 57, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0041, 0.5216, 0.0288, 0.3106, 0.0420, 0.0304, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.065

[Epoch: 57, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3662e-03, 5.1356e-03, 2.6693e-07, 1.1939e-02, 1.9417e-07, 9.7399e-01,
        6.5669e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 57, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0220, 0.0029, 0.1187, 0.0068, 0.8407, 0.0031, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 57, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0031, 0.0031, 0.0038, 0.7418, 0.0116, 0.2333],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 58, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0054, 0.3564, 0.2214, 0.0252, 0.0810, 0.2359, 0.0747],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 58, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0043, 0.5078, 0.0298, 0.3258, 0.0322, 0.0305, 0.0696],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.066

[Epoch: 58, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6244e-03, 5.4756e-03, 2.1115e-07, 1.7094e-02, 1.8207e-07, 9.6845e-01,
        6.3549e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 58, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0034, 0.1223, 0.0073, 0.8353, 0.0031, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 58, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0034, 0.0033, 0.0041, 0.7665, 0.0116, 0.2079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 59, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0057, 0.2910, 0.1955, 0.0173, 0.0604, 0.3274, 0.1026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 59, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5221, 0.0294, 0.3100, 0.0426, 0.0298, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 59, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4206e-03, 4.2528e-03, 2.6557e-07, 9.7331e-03, 1.7914e-07, 9.7833e-01,
        5.2646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.930

[Epoch: 59, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0209, 0.0028, 0.1170, 0.0064, 0.8447, 0.0029, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 59, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0034, 0.0034, 0.0035, 0.7428, 0.0128, 0.2307],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 60, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0043, 0.3812, 0.2420, 0.0315, 0.1030, 0.1623, 0.0756],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 60, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.4908, 0.0370, 0.3323, 0.0343, 0.0371, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.064

[Epoch: 60, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1809e-03, 6.0395e-03, 4.1402e-07, 1.7359e-02, 2.9837e-07, 9.6512e-01,
        8.2954e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.943

[Epoch: 60, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0238, 0.0034, 0.1286, 0.0074, 0.8268, 0.0033, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 60, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0029, 0.0033, 0.0029, 0.7826, 0.0102, 0.1948],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 61, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.2346, 0.2134, 0.0140, 0.0477, 0.4249, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 61, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0048, 0.5186, 0.0266, 0.2898, 0.0554, 0.0324, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.059

[Epoch: 61, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8501e-03, 4.4241e-03, 2.0014e-07, 1.1720e-02, 1.4931e-07, 9.7455e-01,
        6.4556e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 61, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0211, 0.0031, 0.1101, 0.0066, 0.8507, 0.0029, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.004

[Epoch: 61, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0038, 0.0030, 0.0038, 0.7382, 0.0131, 0.2348],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 62, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0053, 0.3198, 0.2070, 0.0203, 0.0646, 0.2765, 0.1063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 62, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.5005, 0.0349, 0.3251, 0.0423, 0.0354, 0.0583],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.067

[Epoch: 62, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5769e-03, 4.5384e-03, 1.9886e-07, 1.5113e-02, 1.7667e-07, 9.7043e-01,
        7.3454e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.928

[Epoch: 62, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0033, 0.1275, 0.0069, 0.8305, 0.0031, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 62, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0028, 0.0027, 0.0029, 0.7809, 0.0118, 0.1957],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 63, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3134, 0.2238, 0.0252, 0.0820, 0.2794, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 63, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5109, 0.0206, 0.3316, 0.0370, 0.0258, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.055

[Epoch: 63, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5979e-03, 3.7340e-03, 2.0679e-07, 1.5210e-02, 1.3188e-07, 9.7321e-01,
        5.2493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.938

[Epoch: 63, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0197, 0.0029, 0.1131, 0.0064, 0.8490, 0.0031, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 63, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0050, 0.0030, 0.0036, 0.0049, 0.7421, 0.0117, 0.2296],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 64, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0057, 0.3457, 0.2103, 0.0150, 0.0766, 0.2694, 0.0773],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 64, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0044, 0.5229, 0.0347, 0.3086, 0.0341, 0.0302, 0.0650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.071

[Epoch: 64, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8840e-03, 4.7318e-03, 2.7398e-07, 1.2663e-02, 2.2042e-07, 9.7145e-01,
        8.2667e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.910

[Epoch: 64, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0032, 0.1178, 0.0068, 0.8407, 0.0030, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 64, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0036, 0.0037, 0.0034, 0.7430, 0.0120, 0.2307],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 65, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0053, 0.3230, 0.2064, 0.0203, 0.0756, 0.2700, 0.0994],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 65, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5224, 0.0380, 0.3157, 0.0365, 0.0304, 0.0529],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.050

[Epoch: 65, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6393e-03, 3.4795e-03, 2.9941e-07, 1.7990e-02, 1.7216e-07, 9.6927e-01,
        6.6178e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.940

[Epoch: 65, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0238, 0.0035, 0.1221, 0.0074, 0.8330, 0.0035, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 65, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0048, 0.0033, 0.0032, 0.0040, 0.7526, 0.0154, 0.2167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 66, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0042, 0.3143, 0.2224, 0.0223, 0.0794, 0.2934, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 66, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0058, 0.5485, 0.0247, 0.2803, 0.0335, 0.0372, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 66, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2982e-03, 3.1602e-03, 2.9391e-07, 1.1386e-02, 1.4560e-07, 9.7518e-01,
        6.9755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.903

[Epoch: 66, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0195, 0.0027, 0.1158, 0.0063, 0.8472, 0.0028, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 66, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0034, 0.0033, 0.0037, 0.7473, 0.0113, 0.2278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 67, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.3201, 0.2080, 0.0208, 0.0870, 0.2800, 0.0796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.008

[Epoch: 67, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5159, 0.0258, 0.3295, 0.0387, 0.0236, 0.0635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 67, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1454e-03, 3.8816e-03, 3.6416e-07, 1.2159e-02, 2.8126e-07, 9.7304e-01,
        7.7773e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 67, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0031, 0.1105, 0.0060, 0.8507, 0.0027, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 67, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0028, 0.0029, 0.0026, 0.7452, 0.0103, 0.2322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 68, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0047, 0.3274, 0.2056, 0.0170, 0.0662, 0.2895, 0.0896],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 68, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0049, 0.5452, 0.0341, 0.2925, 0.0337, 0.0328, 0.0568],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 68, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.0942e-03, 2.2071e-03, 8.2866e-08, 1.1163e-02, 6.1403e-08, 9.8055e-01,
        3.9820e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.957

[Epoch: 68, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0243, 0.0034, 0.1244, 0.0072, 0.8300, 0.0036, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 68, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0041, 0.0047, 0.0041, 0.7628, 0.0135, 0.2079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 69, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3070, 0.2354, 0.0230, 0.0829, 0.2518, 0.0962],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.016

[Epoch: 69, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.4947, 0.0289, 0.3333, 0.0361, 0.0397, 0.0633],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.057

[Epoch: 69, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6030e-03, 4.2246e-03, 8.2045e-07, 1.4143e-02, 5.7536e-07, 9.6828e-01,
        9.7513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.909

[Epoch: 69, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0195, 0.0027, 0.1089, 0.0058, 0.8548, 0.0030, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 69, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0029, 0.0033, 0.0034, 0.7569, 0.0148, 0.2145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 70, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0050, 0.3661, 0.1934, 0.0234, 0.0636, 0.2809, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 70, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5006, 0.0273, 0.3305, 0.0374, 0.0296, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 70, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0819e-03, 3.9712e-03, 2.3534e-07, 1.4807e-02, 1.3947e-07, 9.7075e-01,
        7.3910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 70, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0260, 0.0038, 0.1219, 0.0076, 0.8304, 0.0035, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 70, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0026, 0.0032, 0.0036, 0.0036, 0.7519, 0.0146, 0.2204],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 71, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.2936, 0.2669, 0.0179, 0.0843, 0.2671, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.022

[Epoch: 71, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5256, 0.0333, 0.3318, 0.0290, 0.0279, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.047

[Epoch: 71, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4759e-03, 1.9873e-03, 2.0315e-07, 1.1819e-02, 1.2611e-07, 9.7866e-01,
        5.0562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 71, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0031, 0.1210, 0.0071, 0.8376, 0.0030, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 71, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0030, 0.0034, 0.0034, 0.7283, 0.0125, 0.2464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 72, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0052, 0.3283, 0.1624, 0.0257, 0.0653, 0.2745, 0.1386],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 72, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0052, 0.4914, 0.0302, 0.3206, 0.0435, 0.0372, 0.0719],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.050

[Epoch: 72, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.1678e-03, 4.7787e-03, 2.9195e-07, 1.8947e-02, 1.9246e-07, 9.6101e-01,
        1.1098e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.918

[Epoch: 72, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0198, 0.0029, 0.1162, 0.0060, 0.8467, 0.0028, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 72, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0034, 0.0046, 0.0039, 0.7503, 0.0136, 0.2211],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 73, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0037, 0.3324, 0.2035, 0.0168, 0.1031, 0.2884, 0.0521],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.021

[Epoch: 73, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5504, 0.0199, 0.3081, 0.0263, 0.0321, 0.0595],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.053

[Epoch: 73, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7304e-03, 2.7101e-03, 1.7527e-07, 7.6724e-03, 1.4596e-07, 9.8368e-01,
        3.2106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.918

[Epoch: 73, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0033, 0.1181, 0.0065, 0.8410, 0.0031, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 73, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0051, 0.0034, 0.0039, 0.0032, 0.7462, 0.0150, 0.2233],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 74, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0050, 0.3557, 0.2004, 0.0165, 0.0650, 0.2775, 0.0799],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 74, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5358, 0.0355, 0.3045, 0.0301, 0.0387, 0.0507],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.060

[Epoch: 74, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8227e-03, 3.8159e-03, 2.8052e-07, 1.9004e-02, 9.0698e-08, 9.6665e-01,
        7.7049e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 74, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0242, 0.0036, 0.1263, 0.0079, 0.8282, 0.0036, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 74, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0025, 0.0028, 0.0036, 0.0042, 0.7784, 0.0129, 0.1955],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.020

[Epoch: 75, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0051, 0.3152, 0.2373, 0.0162, 0.0691, 0.2632, 0.0938],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 75, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5012, 0.0283, 0.3120, 0.0674, 0.0268, 0.0614],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 75, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3338e-03, 3.8097e-03, 4.9150e-07, 1.5626e-02, 2.2962e-07, 9.6678e-01,
        1.0446e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.896

[Epoch: 75, batch: 160/203] total loss per batch: 0.729
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0217, 0.0029, 0.1174, 0.0058, 0.8426, 0.0031, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 75, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0032, 0.0040, 0.0035, 0.7298, 0.0172, 0.2393],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 76, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0052, 0.3321, 0.2100, 0.0214, 0.0718, 0.2862, 0.0732],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 76, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5166, 0.0328, 0.3095, 0.0265, 0.0350, 0.0753],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.049

[Epoch: 76, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8532e-03, 3.5158e-03, 2.1351e-07, 1.5466e-02, 1.7404e-07, 9.7405e-01,
        4.1138e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.967

[Epoch: 76, batch: 160/203] total loss per batch: 0.729
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0221, 0.0030, 0.1163, 0.0060, 0.8443, 0.0030, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 76, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0034, 0.0030, 0.0026, 0.7317, 0.0134, 0.2431],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 77, batch: 40/203] total loss per batch: 0.747
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0046, 0.3013, 0.2114, 0.0228, 0.0775, 0.2870, 0.0955],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 77, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.4955, 0.0313, 0.3152, 0.0527, 0.0269, 0.0749],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.064

[Epoch: 77, batch: 120/203] total loss per batch: 0.792
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6862e-03, 3.6424e-03, 3.1511e-07, 1.2109e-02, 4.9052e-07, 9.7562e-01,
        5.9389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.975

[Epoch: 77, batch: 160/203] total loss per batch: 0.736
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0200, 0.0030, 0.1297, 0.0067, 0.8309, 0.0028, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 77, batch: 200/203] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0039, 0.0032, 0.0046, 0.7873, 0.0105, 0.1871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 78, batch: 40/203] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3029, 0.2419, 0.0213, 0.0670, 0.2739, 0.0890],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 78, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0063, 0.5037, 0.0426, 0.3119, 0.0367, 0.0351, 0.0636],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.056

[Epoch: 78, batch: 120/203] total loss per batch: 0.791
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6003e-03, 3.3496e-03, 4.2493e-07, 1.6894e-02, 1.7069e-07, 9.7190e-01,
        4.2597e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.962

[Epoch: 78, batch: 160/203] total loss per batch: 0.732
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0030, 0.1209, 0.0066, 0.8370, 0.0029, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 78, batch: 200/203] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0041, 0.0044, 0.0046, 0.7631, 0.0121, 0.2078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 79, batch: 40/203] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0071, 0.3223, 0.1766, 0.0240, 0.0673, 0.2677, 0.1350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 79, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.5767, 0.0163, 0.2896, 0.0323, 0.0277, 0.0538],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.050

[Epoch: 79, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0463e-03, 4.5383e-03, 3.6464e-07, 1.4400e-02, 1.8190e-07, 9.7159e-01,
        6.4208e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.896

[Epoch: 79, batch: 160/203] total loss per batch: 0.729
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0224, 0.0031, 0.1130, 0.0066, 0.8454, 0.0029, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.005

[Epoch: 79, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0037, 0.0038, 0.0032, 0.7367, 0.0135, 0.2350],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 80, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.3135, 0.2214, 0.0278, 0.1024, 0.2932, 0.0372],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.014

[Epoch: 80, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5166, 0.0379, 0.2924, 0.0457, 0.0303, 0.0729],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.061

[Epoch: 80, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7401e-03, 2.9241e-03, 4.1958e-07, 1.2507e-02, 2.1997e-07, 9.7280e-01,
        8.0241e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.938

[Epoch: 80, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0032, 0.1149, 0.0063, 0.8445, 0.0029, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 80, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0034, 0.0038, 0.0037, 0.7766, 0.0163, 0.1931],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 81, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0052, 0.3490, 0.2264, 0.0157, 0.0762, 0.2591, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.011

[Epoch: 81, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.4998, 0.0263, 0.3448, 0.0373, 0.0285, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.045

[Epoch: 81, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.2535e-03, 7.4536e-03, 6.3691e-07, 1.7558e-02, 4.6792e-07, 9.5981e-01,
        1.0928e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.946

[Epoch: 81, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0029, 0.1206, 0.0061, 0.8402, 0.0030, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 81, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0044, 0.0037, 0.0036, 0.0041, 0.7329, 0.0137, 0.2376],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 82, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.3189, 0.2330, 0.0184, 0.0656, 0.2841, 0.0756],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 82, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5173, 0.0240, 0.3124, 0.0390, 0.0327, 0.0708],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.053

[Epoch: 82, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8573e-03, 3.4433e-03, 2.5045e-07, 9.2163e-03, 1.8276e-07, 9.7870e-01,
        5.7860e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 82, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0222, 0.0033, 0.1270, 0.0066, 0.8315, 0.0032, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 82, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0032, 0.0034, 0.0038, 0.7516, 0.0150, 0.2200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 83, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0050, 0.3276, 0.2039, 0.0184, 0.0773, 0.2604, 0.1074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 83, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.4972, 0.0283, 0.3271, 0.0381, 0.0349, 0.0712],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 83, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.1278e-03, 4.1129e-03, 3.2698e-07, 1.2441e-02, 2.4775e-07, 9.7308e-01,
        6.2338e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.918

[Epoch: 83, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0034, 0.1212, 0.0063, 0.8372, 0.0030, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 83, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0036, 0.0036, 0.0040, 0.7484, 0.0158, 0.2215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 84, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3475, 0.2157, 0.0210, 0.0808, 0.2645, 0.0672],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 84, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5299, 0.0261, 0.3154, 0.0321, 0.0270, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.047

[Epoch: 84, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4917e-03, 3.4045e-03, 2.4791e-07, 1.4791e-02, 2.6505e-07, 9.7283e-01,
        6.4827e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.935

[Epoch: 84, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0028, 0.1178, 0.0058, 0.8439, 0.0028, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 84, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0036, 0.0049, 0.0041, 0.7550, 0.0111, 0.2185],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 85, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3253, 0.2120, 0.0203, 0.0828, 0.2761, 0.0795],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 85, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5223, 0.0326, 0.3090, 0.0382, 0.0298, 0.0650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 85, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4006e-03, 3.5824e-03, 3.2867e-07, 1.3769e-02, 2.2406e-07, 9.7136e-01,
        7.8911e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 85, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0221, 0.0031, 0.1126, 0.0066, 0.8462, 0.0033, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 85, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0032, 0.0034, 0.0042, 0.7622, 0.0146, 0.2093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 86, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0047, 0.3082, 0.2273, 0.0181, 0.0594, 0.2860, 0.0963],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 86, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5119, 0.0284, 0.3068, 0.0449, 0.0294, 0.0740],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 86, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.9083e-03, 4.6664e-03, 2.9931e-07, 1.5886e-02, 2.2622e-07, 9.6958e-01,
        5.9563e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.946

[Epoch: 86, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0038, 0.1182, 0.0065, 0.8400, 0.0031, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.009

[Epoch: 86, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0038, 0.0039, 0.0037, 0.7476, 0.0139, 0.2234],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 87, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.4107, 0.1877, 0.0170, 0.0853, 0.2284, 0.0672],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.019

[Epoch: 87, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0045, 0.5518, 0.0307, 0.2725, 0.0376, 0.0329, 0.0699],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 87, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2628e-03, 3.8947e-03, 2.1330e-07, 1.1766e-02, 3.3401e-07, 9.7401e-01,
        7.0678e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.903

[Epoch: 87, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0031, 0.1247, 0.0070, 0.8338, 0.0030, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 87, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0034, 0.0060, 0.0044, 0.7384, 0.0146, 0.2293],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 88, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0055, 0.2422, 0.2360, 0.0335, 0.0810, 0.3204, 0.0814],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 88, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0046, 0.4899, 0.0393, 0.3323, 0.0416, 0.0347, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.045

[Epoch: 88, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4450e-03, 3.4007e-03, 3.9260e-07, 9.8086e-03, 2.2389e-07, 9.7679e-01,
        7.5531e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 88, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0223, 0.0032, 0.1084, 0.0066, 0.8491, 0.0033, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 88, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0035, 0.0034, 0.0040, 0.7461, 0.0130, 0.2271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 89, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0062, 0.2847, 0.2117, 0.0189, 0.0794, 0.2874, 0.1117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 89, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5071, 0.0300, 0.3047, 0.0545, 0.0294, 0.0708],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 89, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7340e-03, 4.4642e-03, 3.9359e-07, 2.0444e-02, 3.3643e-07, 9.6445e-01,
        6.9082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.950

[Epoch: 89, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0260, 0.0041, 0.1234, 0.0080, 0.8282, 0.0036, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 89, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0028, 0.0031, 0.0035, 0.7717, 0.0100, 0.2063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 90, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0064, 0.3105, 0.2471, 0.0203, 0.0963, 0.2536, 0.0657],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 90, batch: 80/203] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.5611, 0.0233, 0.3015, 0.0223, 0.0243, 0.0633],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.047

[Epoch: 90, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.7248e-03, 5.8178e-03, 3.2990e-07, 1.8162e-02, 4.4610e-07, 9.6209e-01,
        9.2033e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 90, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0206, 0.0031, 0.1293, 0.0065, 0.8320, 0.0029, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 90, batch: 200/203] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0031, 0.0037, 0.0039, 0.7407, 0.0116, 0.2341],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.021

[Epoch: 91, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0062, 0.3069, 0.2216, 0.0139, 0.0729, 0.2758, 0.1027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 91, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.4943, 0.0250, 0.3295, 0.0456, 0.0357, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.054

[Epoch: 91, batch: 120/203] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1799e-03, 4.4216e-03, 4.1199e-07, 1.0621e-02, 4.1255e-07, 9.7521e-01,
        6.5714e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.880

[Epoch: 91, batch: 160/203] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0034, 0.1251, 0.0066, 0.8320, 0.0032, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 91, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0037, 0.0051, 0.0041, 0.7641, 0.0157, 0.2041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 92, batch: 40/203] total loss per batch: 0.744
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0053, 0.3364, 0.2117, 0.0230, 0.0702, 0.2830, 0.0704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 92, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5247, 0.0311, 0.3250, 0.0212, 0.0250, 0.0701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 92, batch: 120/203] total loss per batch: 0.786
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3500e-03, 3.9674e-03, 4.2082e-07, 1.8121e-02, 4.4280e-07, 9.6951e-01,
        6.0493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.968

[Epoch: 92, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0218, 0.0033, 0.1200, 0.0068, 0.8401, 0.0028, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 92, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0042, 0.0045, 0.0042, 0.7095, 0.0158, 0.2582],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 93, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0048, 0.3379, 0.2372, 0.0158, 0.0616, 0.2419, 0.1008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 93, batch: 80/203] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0045, 0.5022, 0.0309, 0.3195, 0.0337, 0.0425, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.051

[Epoch: 93, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3295e-03, 3.7123e-03, 2.7400e-07, 1.7104e-02, 2.4609e-07, 9.7147e-01,
        4.3868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.949

[Epoch: 93, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0198, 0.0027, 0.1150, 0.0057, 0.8486, 0.0027, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.006

[Epoch: 93, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0043, 0.0034, 0.0035, 0.7569, 0.0127, 0.2161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.024

[Epoch: 94, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0050, 0.3296, 0.2036, 0.0233, 0.0828, 0.2813, 0.0743],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 94, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5353, 0.0250, 0.3127, 0.0304, 0.0210, 0.0725],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 94, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0519e-03, 3.6992e-03, 7.6612e-07, 1.0996e-02, 5.1754e-07, 9.7412e-01,
        8.1313e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.912

[Epoch: 94, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0233, 0.0033, 0.1246, 0.0069, 0.8324, 0.0032, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 94, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0038, 0.0040, 0.0047, 0.0043, 0.7750, 0.0157, 0.1924],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 95, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.3253, 0.1972, 0.0216, 0.0885, 0.2784, 0.0846],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 95, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0045, 0.5239, 0.0351, 0.3188, 0.0380, 0.0272, 0.0526],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.048

[Epoch: 95, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6232e-03, 3.5341e-03, 4.2590e-07, 1.7038e-02, 3.0052e-07, 9.7019e-01,
        5.6159e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.964

[Epoch: 95, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0233, 0.0034, 0.1286, 0.0076, 0.8273, 0.0032, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 95, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0034, 0.0038, 0.0042, 0.7439, 0.0127, 0.2292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 96, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.3453, 0.2440, 0.0138, 0.0618, 0.2533, 0.0773],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 96, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0044, 0.5092, 0.0361, 0.2883, 0.0502, 0.0382, 0.0735],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.049

[Epoch: 96, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2675e-03, 5.6249e-03, 5.2252e-07, 9.9422e-03, 5.1778e-07, 9.7487e-01,
        6.2972e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 96, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0209, 0.0030, 0.1187, 0.0062, 0.8424, 0.0029, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 96, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0035, 0.0031, 0.0042, 0.7286, 0.0128, 0.2442],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 97, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0042, 0.3142, 0.1817, 0.0214, 0.0870, 0.3147, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 97, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5213, 0.0205, 0.3245, 0.0295, 0.0314, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 97, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.5918e-03, 3.5102e-03, 3.4493e-07, 1.6779e-02, 3.7846e-07, 9.6936e-01,
        6.7571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.909

[Epoch: 97, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0223, 0.0027, 0.1243, 0.0062, 0.8354, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 97, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0029, 0.0038, 0.0038, 0.7581, 0.0129, 0.2151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 98, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0047, 0.3301, 0.2239, 0.0260, 0.0724, 0.2335, 0.1093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 98, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5024, 0.0337, 0.3277, 0.0381, 0.0247, 0.0698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 98, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6850e-03, 2.3677e-03, 2.8982e-07, 7.2344e-03, 5.2476e-07, 9.8348e-01,
        4.2285e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.920

[Epoch: 98, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0223, 0.0033, 0.1152, 0.0069, 0.8433, 0.0031, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 98, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0034, 0.0029, 0.0034, 0.7601, 0.0110, 0.2161],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 99, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3140, 0.2278, 0.0142, 0.0853, 0.2923, 0.0632],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 99, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.4891, 0.0323, 0.3296, 0.0465, 0.0317, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 99, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.0830e-03, 3.5966e-03, 3.4261e-07, 1.9891e-02, 2.9772e-07, 9.6721e-01,
        7.2221e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.953

[Epoch: 99, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0222, 0.0032, 0.1140, 0.0062, 0.8453, 0.0031, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 99, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0038, 0.0043, 0.0053, 0.7361, 0.0149, 0.2314],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 100, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3606, 0.1889, 0.0186, 0.0676, 0.2850, 0.0761],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 100, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5251, 0.0306, 0.2908, 0.0329, 0.0425, 0.0741],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.053

[Epoch: 100, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.2375e-03, 4.5317e-03, 6.2679e-07, 1.4382e-02, 7.2039e-07, 9.6970e-01,
        7.1434e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 100, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0224, 0.0032, 0.1186, 0.0066, 0.8391, 0.0034, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 100, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0026, 0.0034, 0.0041, 0.7547, 0.0191, 0.2125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 101, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0058, 0.3052, 0.2391, 0.0336, 0.0730, 0.2433, 0.0999],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 101, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0022, 0.5147, 0.0245, 0.3205, 0.0392, 0.0207, 0.0783],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 101, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7754e-03, 2.5878e-03, 3.5274e-07, 1.2737e-02, 6.7602e-07, 9.7537e-01,
        6.5328e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.946

[Epoch: 101, batch: 160/203] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0034, 0.1261, 0.0068, 0.8320, 0.0028, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 101, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0025, 0.0030, 0.0024, 0.0031, 0.7368, 0.0114, 0.2408],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 102, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.3228, 0.2153, 0.0213, 0.0810, 0.2689, 0.0861],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 102, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5197, 0.0360, 0.3142, 0.0370, 0.0294, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 102, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.2763e-03, 2.3179e-03, 3.7158e-07, 1.1361e-02, 2.8345e-07, 9.7850e-01,
        5.5400e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.924

[Epoch: 102, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0213, 0.0029, 0.1211, 0.0063, 0.8394, 0.0030, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 102, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0028, 0.0028, 0.0034, 0.7681, 0.0120, 0.2080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 103, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3280, 0.2116, 0.0175, 0.0728, 0.2770, 0.0891],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 103, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0027, 0.5160, 0.0262, 0.3296, 0.0311, 0.0318, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 103, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6683e-03, 3.8045e-03, 4.4636e-07, 1.7150e-02, 3.2667e-07, 9.6706e-01,
        8.3168e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 103, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0028, 0.1212, 0.0061, 0.8387, 0.0029, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 103, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0028, 0.0032, 0.0039, 0.7508, 0.0120, 0.2239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 104, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3340, 0.2217, 0.0212, 0.0696, 0.2756, 0.0739],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 104, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0027, 0.5325, 0.0253, 0.3112, 0.0347, 0.0262, 0.0674],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.030

[Epoch: 104, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3940e-03, 3.1259e-03, 3.1311e-07, 1.1501e-02, 4.2388e-07, 9.7660e-01,
        5.3807e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.940

[Epoch: 104, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0220, 0.0030, 0.1184, 0.0064, 0.8412, 0.0029, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 104, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0034, 0.0037, 0.0041, 0.7581, 0.0145, 0.2127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 105, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3145, 0.2146, 0.0195, 0.0856, 0.2641, 0.0978],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 105, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5212, 0.0305, 0.3111, 0.0423, 0.0313, 0.0602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 105, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4101e-03, 3.3434e-03, 3.7768e-07, 1.2815e-02, 3.7361e-07, 9.7409e-01,
        6.3374e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.921

[Epoch: 105, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0031, 0.1215, 0.0066, 0.8366, 0.0032, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 105, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0031, 0.0036, 0.0039, 0.7529, 0.0152, 0.2179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 106, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0036, 0.3459, 0.2188, 0.0176, 0.0695, 0.2789, 0.0657],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 106, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5003, 0.0293, 0.3301, 0.0375, 0.0337, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 106, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.9759e-03, 4.1176e-03, 4.0500e-07, 1.2597e-02, 3.9356e-07, 9.6943e-01,
        9.8813e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.902

[Epoch: 106, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0029, 0.1207, 0.0063, 0.8396, 0.0029, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 106, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0029, 0.0032, 0.0031, 0.7457, 0.0117, 0.2302],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 107, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3270, 0.2079, 0.0209, 0.0693, 0.2750, 0.0961],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 107, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5351, 0.0287, 0.3063, 0.0308, 0.0276, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 107, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7674e-03, 3.4916e-03, 3.0938e-07, 1.2698e-02, 2.7808e-07, 9.7484e-01,
        5.2016e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.955

[Epoch: 107, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0224, 0.0033, 0.1219, 0.0066, 0.8367, 0.0032, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 107, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0030, 0.0035, 0.0038, 0.7665, 0.0121, 0.2084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 108, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3205, 0.2260, 0.0202, 0.0760, 0.2670, 0.0867],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 108, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5083, 0.0295, 0.3243, 0.0408, 0.0299, 0.0641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 108, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4502e-03, 2.5556e-03, 2.6957e-07, 1.1293e-02, 2.0800e-07, 9.7847e-01,
        5.2308e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.934

[Epoch: 108, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0203, 0.0030, 0.1216, 0.0064, 0.8397, 0.0029, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 108, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0030, 0.0043, 0.0041, 0.7617, 0.0137, 0.2103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 109, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3460, 0.2050, 0.0197, 0.0763, 0.2706, 0.0792],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 109, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5010, 0.0322, 0.3243, 0.0380, 0.0333, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 109, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7502e-03, 3.9560e-03, 4.0056e-07, 2.4255e-02, 3.7733e-07, 9.5889e-01,
        9.1523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.924

[Epoch: 109, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0209, 0.0029, 0.1135, 0.0058, 0.8476, 0.0029, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 109, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0048, 0.0042, 0.0041, 0.0030, 0.7034, 0.0147, 0.2659],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 110, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.2942, 0.2211, 0.0198, 0.0782, 0.2999, 0.0824],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 110, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5329, 0.0267, 0.3110, 0.0292, 0.0281, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.028

[Epoch: 110, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.8988e-03, 4.1205e-03, 4.6026e-07, 6.7114e-03, 4.5907e-07, 9.7936e-01,
        5.9127e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.918

[Epoch: 110, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0255, 0.0040, 0.1168, 0.0079, 0.8347, 0.0037, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 110, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0024, 0.0027, 0.0026, 0.0030, 0.7789, 0.0118, 0.1985],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 111, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3853, 0.2125, 0.0188, 0.0567, 0.2207, 0.1019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 111, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0053, 0.4937, 0.0428, 0.3132, 0.0476, 0.0312, 0.0662],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.046

[Epoch: 111, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1691e-03, 2.9608e-03, 6.5254e-07, 1.7704e-02, 5.8508e-07, 9.7093e-01,
        5.2317e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.913

[Epoch: 111, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0033, 0.1318, 0.0074, 0.8249, 0.0032, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 111, batch: 200/203] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0028, 0.0030, 0.0035, 0.7451, 0.0139, 0.2288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 112, batch: 40/203] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0029, 0.3174, 0.2039, 0.0230, 0.0883, 0.3130, 0.0515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.009

[Epoch: 112, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.4789, 0.0326, 0.3488, 0.0499, 0.0261, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 112, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9917e-03, 3.4835e-03, 3.6286e-07, 7.6546e-03, 5.2458e-07, 9.7977e-01,
        6.1013e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.946

[Epoch: 112, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0210, 0.0028, 0.1203, 0.0055, 0.8420, 0.0025, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 112, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0043, 0.0033, 0.0036, 0.0031, 0.7436, 0.0108, 0.2313],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 113, batch: 40/203] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0028, 0.3060, 0.2458, 0.0243, 0.0933, 0.2484, 0.0794],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 113, batch: 80/203] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5377, 0.0250, 0.3080, 0.0313, 0.0265, 0.0681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.028

[Epoch: 113, batch: 120/203] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2321e-03, 2.0742e-03, 3.7237e-07, 1.9289e-02, 4.4783e-07, 9.6891e-01,
        6.4922e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.950

[Epoch: 113, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0031, 0.1213, 0.0061, 0.8375, 0.0030, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 113, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0046, 0.0036, 0.0041, 0.0066, 0.7549, 0.0135, 0.2127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 114, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0047, 0.3271, 0.1953, 0.0239, 0.0823, 0.2745, 0.0923],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 114, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5064, 0.0295, 0.3375, 0.0265, 0.0269, 0.0704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 114, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.4589e-03, 3.8974e-03, 6.3468e-07, 1.3424e-02, 1.0958e-06, 9.6967e-01,
        8.5471e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.876

[Epoch: 114, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0031, 0.1149, 0.0063, 0.8440, 0.0031, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 114, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0037, 0.0038, 0.0036, 0.7551, 0.0115, 0.2182],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 115, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3221, 0.2159, 0.0158, 0.0711, 0.2769, 0.0949],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 115, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0041, 0.5042, 0.0286, 0.3241, 0.0380, 0.0344, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 115, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2225e-03, 3.2291e-03, 5.5448e-07, 1.5002e-02, 1.4311e-06, 9.7028e-01,
        8.2658e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.958

[Epoch: 115, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0033, 0.1192, 0.0067, 0.8379, 0.0033, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 115, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0038, 0.0039, 0.0042, 0.7534, 0.0142, 0.2169],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 116, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3462, 0.2119, 0.0213, 0.0771, 0.2685, 0.0717],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 116, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5330, 0.0287, 0.3089, 0.0342, 0.0298, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 116, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1338e-03, 2.9879e-03, 3.7862e-07, 1.4051e-02, 7.2503e-07, 9.7275e-01,
        7.0785e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.959

[Epoch: 116, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0033, 0.1242, 0.0069, 0.8330, 0.0032, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 116, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0034, 0.0034, 0.0043, 0.7489, 0.0132, 0.2233],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 117, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.3192, 0.2304, 0.0212, 0.0632, 0.2717, 0.0898],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 117, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5176, 0.0289, 0.3220, 0.0353, 0.0276, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 117, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0290e-03, 3.1955e-03, 3.4799e-07, 1.2928e-02, 7.7287e-07, 9.7364e-01,
        7.2078e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.947

[Epoch: 117, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0031, 0.1172, 0.0060, 0.8434, 0.0029, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 117, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0033, 0.0031, 0.0042, 0.7477, 0.0147, 0.2237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 118, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3413, 0.2061, 0.0182, 0.0779, 0.2739, 0.0793],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 118, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5094, 0.0299, 0.3174, 0.0382, 0.0342, 0.0673],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 118, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8299e-03, 2.5966e-03, 2.9331e-07, 1.2214e-02, 5.1505e-07, 9.7668e-01,
        5.6765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.905

[Epoch: 118, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0030, 0.1153, 0.0061, 0.8450, 0.0029, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 118, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0035, 0.0032, 0.0037, 0.7492, 0.0139, 0.2230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 119, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3175, 0.2224, 0.0207, 0.0765, 0.2750, 0.0840],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 119, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5024, 0.0317, 0.3293, 0.0380, 0.0301, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 119, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9066e-03, 2.8785e-03, 2.8715e-07, 1.4710e-02, 5.4004e-07, 9.7239e-01,
        7.1136e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.955

[Epoch: 119, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0233, 0.0033, 0.1181, 0.0068, 0.8385, 0.0033, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 119, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0036, 0.0036, 0.0039, 0.7601, 0.0148, 0.2106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 120, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3428, 0.2115, 0.0191, 0.0743, 0.2656, 0.0835],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 120, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5014, 0.0329, 0.3224, 0.0378, 0.0332, 0.0689],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 120, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2566e-03, 3.0262e-03, 2.5989e-07, 1.1403e-02, 4.6048e-07, 9.7631e-01,
        6.0054e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 120, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0240, 0.0037, 0.1217, 0.0073, 0.8327, 0.0035, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 120, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0034, 0.0037, 0.0033, 0.7676, 0.0121, 0.2068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 121, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0036, 0.3139, 0.2185, 0.0185, 0.0719, 0.2892, 0.0843],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 121, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5340, 0.0285, 0.2998, 0.0374, 0.0318, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 121, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6922e-03, 3.2301e-03, 1.8595e-07, 1.3840e-02, 3.3523e-07, 9.7381e-01,
        6.4229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.940

[Epoch: 121, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0032, 0.1183, 0.0068, 0.8399, 0.0032, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 121, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0042, 0.0036, 0.0044, 0.0046, 0.7356, 0.0146, 0.2330],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 122, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0044, 0.3395, 0.2139, 0.0237, 0.0803, 0.2588, 0.0793],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 122, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.5384, 0.0249, 0.3100, 0.0366, 0.0244, 0.0622],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.044

[Epoch: 122, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3855e-03, 4.0598e-03, 4.0794e-07, 1.3186e-02, 6.4765e-07, 9.7298e-01,
        6.3931e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.897

[Epoch: 122, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0221, 0.0031, 0.1199, 0.0068, 0.8384, 0.0031, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 122, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0032, 0.0030, 0.0037, 0.7347, 0.0169, 0.2352],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 123, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0046, 0.3180, 0.2044, 0.0164, 0.0681, 0.2813, 0.1072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.010

[Epoch: 123, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0027, 0.5161, 0.0254, 0.3170, 0.0369, 0.0312, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 123, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.5690e-03, 3.7813e-03, 4.0104e-07, 1.3923e-02, 4.7094e-07, 9.7147e-01,
        7.2592e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.965

[Epoch: 123, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0237, 0.0036, 0.1188, 0.0067, 0.8375, 0.0034, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 123, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0023, 0.0036, 0.0021, 0.0027, 0.7660, 0.0126, 0.2107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 124, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3188, 0.2307, 0.0229, 0.0907, 0.2629, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 124, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5106, 0.0297, 0.3371, 0.0292, 0.0260, 0.0644],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 124, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3354e-03, 3.3064e-03, 3.2163e-07, 1.1512e-02, 3.2582e-07, 9.7437e-01,
        7.4732e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.922

[Epoch: 124, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0261, 0.0038, 0.1245, 0.0076, 0.8268, 0.0034, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 124, batch: 200/203] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0037, 0.0041, 0.0035, 0.7707, 0.0101, 0.2042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.021

[Epoch: 125, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3558, 0.2295, 0.0141, 0.0622, 0.2461, 0.0883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 125, batch: 80/203] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5085, 0.0229, 0.3292, 0.0321, 0.0366, 0.0675],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.043

[Epoch: 125, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9025e-03, 3.7485e-03, 2.6429e-07, 1.8816e-02, 6.7927e-07, 9.6756e-01,
        6.9755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.945

[Epoch: 125, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0214, 0.0031, 0.1221, 0.0059, 0.8387, 0.0027, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 125, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0036, 0.0037, 0.0033, 0.7523, 0.0172, 0.2167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 126, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0027, 0.2970, 0.1916, 0.0232, 0.0735, 0.3520, 0.0600],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 126, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5198, 0.0186, 0.3232, 0.0303, 0.0368, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.033

[Epoch: 126, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7486e-03, 3.8328e-03, 4.1761e-07, 1.1811e-02, 5.7800e-07, 9.7545e-01,
        6.1536e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.897

[Epoch: 126, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0029, 0.1158, 0.0059, 0.8450, 0.0029, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 126, batch: 200/203] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0049, 0.0040, 0.0039, 0.0042, 0.7381, 0.0127, 0.2322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 127, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3290, 0.2038, 0.0188, 0.0721, 0.2768, 0.0954],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 127, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.5151, 0.0395, 0.3080, 0.0442, 0.0254, 0.0639],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.043

[Epoch: 127, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.0644e-03, 3.6089e-03, 9.0779e-07, 1.2525e-02, 1.0573e-06, 9.7222e-01,
        7.5787e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.957

[Epoch: 127, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0235, 0.0030, 0.1274, 0.0074, 0.8286, 0.0032, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 127, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0032, 0.0040, 0.0040, 0.7564, 0.0141, 0.2151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 128, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3418, 0.2268, 0.0178, 0.0729, 0.2666, 0.0707],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 128, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.4860, 0.0273, 0.3431, 0.0348, 0.0363, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 128, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.8623e-03, 3.5621e-03, 3.7744e-07, 1.2998e-02, 3.8221e-07, 9.7281e-01,
        6.7670e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.948

[Epoch: 128, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0205, 0.0028, 0.1136, 0.0055, 0.8495, 0.0027, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 128, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0032, 0.0038, 0.0039, 0.7458, 0.0163, 0.2236],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 129, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3094, 0.2237, 0.0216, 0.0836, 0.2640, 0.0939],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 129, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5312, 0.0296, 0.3013, 0.0398, 0.0288, 0.0664],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 129, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4817e-03, 2.8151e-03, 3.3495e-07, 1.2777e-02, 5.7922e-07, 9.7439e-01,
        6.5367e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.920

[Epoch: 129, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0029, 0.1114, 0.0058, 0.8497, 0.0027, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 129, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0030, 0.0033, 0.0034, 0.7600, 0.0153, 0.2120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 130, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3644, 0.2051, 0.0169, 0.0626, 0.2767, 0.0712],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.005

[Epoch: 130, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.4976, 0.0333, 0.3282, 0.0396, 0.0298, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 130, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.2737e-03, 2.1300e-03, 3.2481e-07, 1.1613e-02, 3.1045e-07, 9.8026e-01,
        3.7215e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.938

[Epoch: 130, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0030, 0.1214, 0.0069, 0.8363, 0.0032, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 130, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0036, 0.0030, 0.0037, 0.7356, 0.0139, 0.2370],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 131, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0043, 0.3007, 0.2256, 0.0227, 0.0811, 0.2724, 0.0932],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 131, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5036, 0.0291, 0.3280, 0.0366, 0.0312, 0.0681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 131, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6491e-03, 4.0993e-03, 3.3125e-07, 1.5435e-02, 5.5497e-07, 9.6805e-01,
        8.7669e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.917

[Epoch: 131, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0236, 0.0034, 0.1142, 0.0070, 0.8416, 0.0034, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 131, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0043, 0.0040, 0.0038, 0.0035, 0.7399, 0.0130, 0.2315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 132, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3214, 0.2225, 0.0175, 0.0675, 0.2860, 0.0821],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 132, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5363, 0.0288, 0.3001, 0.0367, 0.0269, 0.0674],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 132, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.5748e-03, 5.4781e-03, 6.0466e-07, 1.4575e-02, 6.8347e-07, 9.6674e-01,
        8.6351e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.926

[Epoch: 132, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0265, 0.0041, 0.1185, 0.0075, 0.8323, 0.0037, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 132, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0025, 0.0041, 0.0044, 0.7517, 0.0135, 0.2199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 133, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3356, 0.2202, 0.0231, 0.0887, 0.2556, 0.0734],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 133, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5007, 0.0311, 0.3294, 0.0453, 0.0260, 0.0645],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 133, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8413e-03, 2.9745e-03, 3.5838e-07, 1.0286e-02, 4.3698e-07, 9.7664e-01,
        7.2541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.935

[Epoch: 133, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0238, 0.0034, 0.1324, 0.0075, 0.8227, 0.0033, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 133, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0032, 0.0029, 0.0044, 0.7478, 0.0156, 0.2232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 134, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3242, 0.2003, 0.0177, 0.0645, 0.2886, 0.1004],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.007

[Epoch: 134, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.4893, 0.0291, 0.3418, 0.0359, 0.0299, 0.0710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 134, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8734e-03, 3.0601e-03, 5.1603e-07, 1.5309e-02, 3.8270e-07, 9.7291e-01,
        5.8429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.921

[Epoch: 134, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0208, 0.0028, 0.1216, 0.0056, 0.8413, 0.0024, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 134, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0025, 0.0031, 0.0023, 0.0025, 0.7576, 0.0119, 0.2201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 135, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3429, 0.2182, 0.0162, 0.0731, 0.2715, 0.0747],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 135, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0028, 0.5553, 0.0250, 0.2993, 0.0277, 0.0273, 0.0625],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.029

[Epoch: 135, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6090e-03, 3.2174e-03, 2.8358e-07, 1.5314e-02, 3.7102e-07, 9.7167e-01,
        6.1908e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.934

[Epoch: 135, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0212, 0.0028, 0.1126, 0.0056, 0.8491, 0.0028, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 135, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0030, 0.0030, 0.0023, 0.7573, 0.0108, 0.2203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 136, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3039, 0.2215, 0.0278, 0.0722, 0.2859, 0.0854],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.004

[Epoch: 136, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0026, 0.5082, 0.0320, 0.3158, 0.0368, 0.0336, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 136, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2609e-03, 2.9280e-03, 5.6423e-07, 1.1821e-02, 4.4766e-07, 9.7531e-01,
        6.6750e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.894

[Epoch: 136, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0032, 0.1194, 0.0064, 0.8388, 0.0031, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 136, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0040, 0.0045, 0.0058, 0.7675, 0.0146, 0.1998],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 137, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0051, 0.3252, 0.2218, 0.0207, 0.0869, 0.2484, 0.0919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 137, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0047, 0.5134, 0.0322, 0.3011, 0.0430, 0.0333, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.034

[Epoch: 137, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8986e-03, 3.4298e-03, 4.9307e-07, 1.2594e-02, 5.7154e-07, 9.7659e-01,
        4.4897e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.954

[Epoch: 137, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0031, 0.1152, 0.0062, 0.8429, 0.0030, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 137, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0045, 0.0040, 0.0049, 0.0043, 0.7414, 0.0155, 0.2255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 138, batch: 40/203] total loss per batch: 0.743
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3867, 0.1991, 0.0149, 0.0661, 0.2655, 0.0643],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 138, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.4915, 0.0349, 0.3422, 0.0365, 0.0301, 0.0616],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 138, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6860e-03, 4.4804e-03, 7.7557e-07, 1.3948e-02, 4.1413e-07, 9.7040e-01,
        7.4792e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.933

[Epoch: 138, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0224, 0.0030, 0.1190, 0.0063, 0.8404, 0.0029, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 138, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0031, 0.0028, 0.0032, 0.7425, 0.0117, 0.2334],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 139, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 5
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0045, 0.2690, 0.2192, 0.0212, 0.0648, 0.2927, 0.1286],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 139, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5140, 0.0291, 0.3166, 0.0389, 0.0315, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.033

[Epoch: 139, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.8814e-03, 3.9368e-03, 6.5683e-07, 1.8509e-02, 4.3798e-07, 9.6671e-01,
        6.9641e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.955

[Epoch: 139, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0034, 0.1184, 0.0061, 0.8405, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 139, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0031, 0.0029, 0.0039, 0.7529, 0.0143, 0.2199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 140, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0027, 0.3433, 0.2217, 0.0231, 0.0997, 0.2569, 0.0526],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.006

[Epoch: 140, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5312, 0.0294, 0.3026, 0.0408, 0.0286, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 140, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.2260e-03, 2.9108e-03, 4.0629e-07, 8.0831e-03, 2.1588e-07, 9.8149e-01,
        5.2917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.849

[Epoch: 140, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0220, 0.0031, 0.1172, 0.0063, 0.8425, 0.0029, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 140, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0038, 0.0044, 0.0043, 0.7522, 0.0124, 0.2188],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 141, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0024, 0.3134, 0.2193, 0.0228, 0.0884, 0.2787, 0.0748],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 141, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.5109, 0.0358, 0.3095, 0.0379, 0.0334, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 141, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7805e-03, 2.7600e-03, 8.1866e-07, 2.3424e-02, 4.1420e-07, 9.6456e-01,
        6.4765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.991

[Epoch: 141, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0030, 0.1215, 0.0068, 0.8363, 0.0032, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 141, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0032, 0.0036, 0.0038, 0.7716, 0.0127, 0.2010],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 142, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3488, 0.1934, 0.0181, 0.0634, 0.2752, 0.0979],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 142, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.5077, 0.0331, 0.3146, 0.0385, 0.0324, 0.0698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 142, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2265e-03, 4.1883e-03, 5.8686e-07, 8.7076e-03, 5.3034e-07, 9.7819e-01,
        5.6891e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.986

[Epoch: 142, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0035, 0.1177, 0.0073, 0.8376, 0.0035, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 142, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0033, 0.0035, 0.0036, 0.7455, 0.0126, 0.2285],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 143, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3237, 0.2397, 0.0166, 0.0799, 0.2630, 0.0737],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.013

[Epoch: 143, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5001, 0.0296, 0.3387, 0.0360, 0.0278, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 143, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.9460e-03, 4.7722e-03, 5.3998e-07, 1.3135e-02, 3.9168e-07, 9.7073e-01,
        7.4161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.977

[Epoch: 143, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0236, 0.0036, 0.1188, 0.0066, 0.8375, 0.0033, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 143, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0030, 0.0035, 0.0042, 0.7392, 0.0126, 0.2343],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 144, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0037, 0.3295, 0.2206, 0.0172, 0.0634, 0.2849, 0.0807],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 144, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5138, 0.0261, 0.3228, 0.0382, 0.0247, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.034

[Epoch: 144, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2012e-03, 2.7785e-03, 3.6142e-07, 1.2247e-02, 3.1932e-07, 9.7651e-01,
        5.2577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.968

[Epoch: 144, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0029, 0.1280, 0.0063, 0.8308, 0.0030, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 144, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0028, 0.0026, 0.0030, 0.7683, 0.0147, 0.2057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 145, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3285, 0.2128, 0.0257, 0.0738, 0.2600, 0.0954],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 145, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0028, 0.5062, 0.0272, 0.3298, 0.0363, 0.0280, 0.0698],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 145, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.5804e-03, 2.9946e-03, 2.2019e-07, 9.8870e-03, 2.5957e-07, 9.7994e-01,
        4.5951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.967

[Epoch: 145, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0219, 0.0028, 0.1159, 0.0058, 0.8451, 0.0027, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 145, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0035, 0.0033, 0.0028, 0.7435, 0.0107, 0.2329],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 146, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3247, 0.2181, 0.0206, 0.0796, 0.2820, 0.0715],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 146, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5445, 0.0274, 0.2942, 0.0302, 0.0327, 0.0676],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.030

[Epoch: 146, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.6380e-03, 4.4660e-03, 1.0160e-06, 2.2833e-02, 8.3205e-07, 9.5819e-01,
        9.8673e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.953

[Epoch: 146, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0032, 0.1123, 0.0065, 0.8463, 0.0032, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 146, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0048, 0.0035, 0.0035, 0.0045, 0.7480, 0.0147, 0.2210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 147, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3222, 0.2203, 0.0139, 0.0715, 0.2821, 0.0867],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 147, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0046, 0.5154, 0.0270, 0.3160, 0.0361, 0.0347, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 147, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.2284e-03, 5.2609e-03, 1.0872e-06, 9.1045e-03, 9.1046e-07, 9.7237e-01,
        9.0333e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.896

[Epoch: 147, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0247, 0.0038, 0.1206, 0.0073, 0.8325, 0.0034, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 147, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0038, 0.0040, 0.0045, 0.0052, 0.7487, 0.0145, 0.2193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 148, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0036, 0.3294, 0.2190, 0.0212, 0.0749, 0.2647, 0.0873],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 148, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.4746, 0.0327, 0.3425, 0.0406, 0.0307, 0.0755],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 148, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.5084e-03, 2.9887e-03, 1.2153e-06, 1.0823e-02, 1.0500e-06, 9.7758e-01,
        5.0943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.950

[Epoch: 148, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0242, 0.0030, 0.1308, 0.0065, 0.8257, 0.0031, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 148, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0026, 0.0027, 0.0039, 0.0043, 0.7634, 0.0129, 0.2103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 149, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0036, 0.3521, 0.2093, 0.0218, 0.0684, 0.2616, 0.0833],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 149, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5376, 0.0298, 0.2939, 0.0369, 0.0323, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.043

[Epoch: 149, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.5845e-03, 3.5021e-03, 5.0620e-07, 1.6169e-02, 3.7372e-07, 9.7057e-01,
        6.1784e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.949

[Epoch: 149, batch: 160/203] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0200, 0.0026, 0.1189, 0.0057, 0.8455, 0.0024, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 149, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0035, 0.0024, 0.0025, 0.7606, 0.0134, 0.2148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 150, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0037, 0.3148, 0.2170, 0.0190, 0.0751, 0.2863, 0.0841],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.003

[Epoch: 150, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5448, 0.0322, 0.3002, 0.0386, 0.0221, 0.0591],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.034

[Epoch: 150, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8475e-03, 2.2242e-03, 6.3777e-07, 1.5298e-02, 3.6409e-07, 9.7327e-01,
        6.3599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.942

[Epoch: 150, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0197, 0.0029, 0.1151, 0.0055, 0.8482, 0.0026, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.007

[Epoch: 150, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0035, 0.0037, 0.0035, 0.7403, 0.0135, 0.2326],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 151, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3251, 0.2136, 0.0208, 0.0733, 0.2843, 0.0791],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 151, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.5191, 0.0274, 0.3085, 0.0308, 0.0297, 0.0809],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 151, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4607e-03, 3.3518e-03, 3.5428e-07, 9.3413e-03, 4.0257e-07, 9.7644e-01,
        7.4006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.905

[Epoch: 151, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0249, 0.0036, 0.1122, 0.0075, 0.8412, 0.0036, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 151, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0034, 0.0037, 0.0045, 0.7651, 0.0121, 0.2070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 152, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3362, 0.2286, 0.0234, 0.0813, 0.2424, 0.0844],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 152, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5068, 0.0340, 0.3126, 0.0400, 0.0362, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 152, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3861e-03, 4.1075e-03, 5.3425e-07, 1.2706e-02, 4.7364e-07, 9.7216e-01,
        7.6437e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 152, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0236, 0.0035, 0.1222, 0.0072, 0.8329, 0.0034, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 152, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0033, 0.0039, 0.0039, 0.7558, 0.0120, 0.2183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 153, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3275, 0.2008, 0.0187, 0.0688, 0.2992, 0.0815],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 153, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0031, 0.5222, 0.0296, 0.3121, 0.0361, 0.0297, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 153, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9332e-03, 3.6003e-03, 3.2305e-07, 1.4095e-02, 2.5744e-07, 9.7339e-01,
        5.9809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.936

[Epoch: 153, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0034, 0.1244, 0.0067, 0.8328, 0.0031, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 153, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0028, 0.0030, 0.0033, 0.7439, 0.0124, 0.2315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 154, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0037, 0.3310, 0.2192, 0.0202, 0.0752, 0.2660, 0.0848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 154, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5227, 0.0272, 0.3174, 0.0334, 0.0278, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 154, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9926e-03, 3.2959e-03, 2.9492e-07, 1.2241e-02, 2.4765e-07, 9.7567e-01,
        5.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.923

[Epoch: 154, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0032, 0.1235, 0.0068, 0.8340, 0.0030, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 154, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0031, 0.0030, 0.0034, 0.7492, 0.0135, 0.2246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 155, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3277, 0.2168, 0.0216, 0.0764, 0.2704, 0.0836],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 155, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5159, 0.0303, 0.3200, 0.0350, 0.0304, 0.0651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 155, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8625e-03, 3.0522e-03, 3.0825e-07, 1.3215e-02, 2.5250e-07, 9.7482e-01,
        6.0517e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.935

[Epoch: 155, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0032, 0.1195, 0.0068, 0.8383, 0.0030, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 155, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0035, 0.0036, 0.0040, 0.7515, 0.0135, 0.2203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 156, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3267, 0.2242, 0.0195, 0.0745, 0.2695, 0.0820],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 156, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5093, 0.0303, 0.3213, 0.0361, 0.0317, 0.0678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 156, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.8738e-03, 3.8728e-03, 4.0258e-07, 1.4627e-02, 3.1669e-07, 9.6849e-01,
        9.1396e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.901

[Epoch: 156, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0033, 0.1201, 0.0065, 0.8373, 0.0031, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 156, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0036, 0.0037, 0.0039, 0.7480, 0.0133, 0.2238],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 157, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3299, 0.2095, 0.0192, 0.0718, 0.2820, 0.0841],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 157, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5074, 0.0295, 0.3230, 0.0379, 0.0309, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 157, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3156e-03, 3.9305e-03, 2.6995e-07, 1.3241e-02, 2.4933e-07, 9.7334e-01,
        6.1734e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.953

[Epoch: 157, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0032, 0.1218, 0.0065, 0.8360, 0.0030, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 157, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0030, 0.0032, 0.0035, 0.7581, 0.0124, 0.2165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 158, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3398, 0.2143, 0.0205, 0.0677, 0.2696, 0.0847],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 158, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5147, 0.0288, 0.3223, 0.0338, 0.0293, 0.0680],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 158, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1151e-03, 2.8855e-03, 2.2283e-07, 1.1724e-02, 2.2933e-07, 9.7698e-01,
        5.2918e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.921

[Epoch: 158, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0214, 0.0028, 0.1214, 0.0062, 0.8395, 0.0028, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 158, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0027, 0.0030, 0.0033, 0.0031, 0.7564, 0.0134, 0.2181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 159, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3295, 0.2222, 0.0226, 0.0774, 0.2646, 0.0802],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 159, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0027, 0.5115, 0.0308, 0.3222, 0.0371, 0.0299, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 159, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4200e-03, 2.5625e-03, 2.0574e-07, 1.4111e-02, 1.4007e-07, 9.7495e-01,
        5.9539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 159, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0212, 0.0028, 0.1150, 0.0063, 0.8462, 0.0027, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 159, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0036, 0.0036, 0.0036, 0.7383, 0.0146, 0.2323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 160, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3223, 0.2117, 0.0167, 0.0714, 0.2964, 0.0781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 160, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0037, 0.5023, 0.0323, 0.3207, 0.0359, 0.0347, 0.0704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 160, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6110e-03, 3.1704e-03, 2.7356e-07, 1.2987e-02, 2.2328e-07, 9.7241e-01,
        7.8236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.920

[Epoch: 160, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0238, 0.0035, 0.1171, 0.0069, 0.8377, 0.0035, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.003

[Epoch: 160, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0036, 0.0036, 0.0042, 0.7351, 0.0149, 0.2351],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 161, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3174, 0.2223, 0.0170, 0.0770, 0.2689, 0.0933],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.008

[Epoch: 161, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0045, 0.5277, 0.0363, 0.2928, 0.0349, 0.0338, 0.0700],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.043

[Epoch: 161, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.3737e-03, 5.5312e-03, 5.5970e-07, 1.7972e-02, 6.7187e-07, 9.6376e-01,
        8.3634e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 161, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0246, 0.0041, 0.1205, 0.0075, 0.8319, 0.0037, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 161, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0035, 0.0036, 0.0043, 0.7626, 0.0120, 0.2109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 162, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3344, 0.2048, 0.0273, 0.0744, 0.2790, 0.0768],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 162, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5033, 0.0277, 0.3343, 0.0414, 0.0222, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 162, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3303e-03, 2.9394e-03, 2.2624e-07, 1.0167e-02, 3.3400e-07, 9.7788e-01,
        5.6838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.884

[Epoch: 162, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0031, 0.1219, 0.0065, 0.8365, 0.0031, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 162, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0023, 0.0027, 0.0036, 0.0031, 0.7620, 0.0133, 0.2131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 163, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3324, 0.2122, 0.0168, 0.0805, 0.2678, 0.0867],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 163, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5266, 0.0325, 0.3108, 0.0388, 0.0265, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 163, batch: 120/203] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.1144e-03, 1.7019e-03, 4.3404e-07, 9.2050e-03, 1.6641e-07, 9.8305e-01,
        3.9315e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.974

[Epoch: 163, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0030, 0.1186, 0.0066, 0.8396, 0.0029, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 163, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0026, 0.0023, 0.0021, 0.7370, 0.0120, 0.2403],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 164, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0041, 0.3288, 0.2207, 0.0179, 0.0743, 0.2723, 0.0819],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 164, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5054, 0.0257, 0.3309, 0.0306, 0.0290, 0.0753],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 164, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6235e-03, 4.3144e-03, 4.0363e-07, 1.5887e-02, 2.1728e-07, 9.6503e-01,
        1.1146e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.979

[Epoch: 164, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0030, 0.1179, 0.0059, 0.8428, 0.0028, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 164, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0032, 0.0032, 0.0041, 0.7638, 0.0123, 0.2099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 165, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0037, 0.3245, 0.2224, 0.0196, 0.0734, 0.2716, 0.0847],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 165, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5262, 0.0267, 0.3220, 0.0364, 0.0257, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 165, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4861e-03, 2.9024e-03, 4.8712e-07, 1.4918e-02, 2.5997e-07, 9.7413e-01,
        5.5643e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.951

[Epoch: 165, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0228, 0.0032, 0.1177, 0.0065, 0.8401, 0.0031, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 165, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0037, 0.0042, 0.0048, 0.7579, 0.0121, 0.2139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 166, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3304, 0.2181, 0.0212, 0.0714, 0.2673, 0.0877],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 166, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5053, 0.0297, 0.3218, 0.0407, 0.0321, 0.0666],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 166, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.9377e-03, 4.7260e-03, 5.9387e-07, 1.4949e-02, 4.5710e-07, 9.6793e-01,
        8.4549e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.941

[Epoch: 166, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0235, 0.0032, 0.1192, 0.0065, 0.8378, 0.0031, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 166, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0038, 0.0037, 0.0036, 0.0036, 0.7355, 0.0140, 0.2358],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 167, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3305, 0.2136, 0.0195, 0.0748, 0.2814, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 167, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5137, 0.0271, 0.3220, 0.0333, 0.0312, 0.0697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 167, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0057e-03, 3.1354e-03, 2.9790e-07, 1.1373e-02, 1.9454e-07, 9.7610e-01,
        6.3864e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.938

[Epoch: 167, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0223, 0.0030, 0.1220, 0.0063, 0.8372, 0.0030, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 167, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0031, 0.0031, 0.0033, 0.7516, 0.0148, 0.2210],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 168, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0029, 0.3365, 0.2114, 0.0199, 0.0679, 0.2787, 0.0826],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 168, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5202, 0.0298, 0.3191, 0.0349, 0.0303, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 168, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2461e-03, 3.1707e-03, 3.9968e-07, 1.2471e-02, 2.1403e-07, 9.7432e-01,
        6.7940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.921

[Epoch: 168, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0215, 0.0029, 0.1204, 0.0062, 0.8401, 0.0028, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 168, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0031, 0.0032, 0.0032, 0.0034, 0.7575, 0.0141, 0.2155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 169, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3209, 0.2232, 0.0201, 0.0780, 0.2641, 0.0905],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 169, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5096, 0.0308, 0.3200, 0.0371, 0.0305, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 169, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2028e-03, 2.9776e-03, 3.4247e-07, 1.3774e-02, 2.1018e-07, 9.7394e-01,
        6.1072e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.927

[Epoch: 169, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0224, 0.0032, 0.1149, 0.0062, 0.8440, 0.0030, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 169, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0041, 0.0040, 0.0034, 0.0041, 0.7379, 0.0144, 0.2322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 170, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3350, 0.2132, 0.0185, 0.0705, 0.2864, 0.0734],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 170, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5117, 0.0307, 0.3155, 0.0360, 0.0330, 0.0697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 170, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.2546e-03, 3.5848e-03, 3.4465e-07, 1.6560e-02, 1.6535e-07, 9.6877e-01,
        7.8272e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.928

[Epoch: 170, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0034, 0.1147, 0.0065, 0.8418, 0.0033, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 170, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0032, 0.0036, 0.0033, 0.7586, 0.0128, 0.2153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 171, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3141, 0.2221, 0.0216, 0.0780, 0.2695, 0.0911],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 171, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.4918, 0.0382, 0.3256, 0.0412, 0.0346, 0.0647],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 171, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6696e-03, 3.1537e-03, 3.3811e-07, 1.0910e-02, 3.3453e-07, 9.7553e-01,
        6.7374e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 171, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0234, 0.0033, 0.1233, 0.0072, 0.8326, 0.0032, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 171, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0028, 0.0033, 0.0029, 0.7582, 0.0119, 0.2181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 172, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3384, 0.2093, 0.0219, 0.0715, 0.2751, 0.0803],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 172, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0037, 0.4961, 0.0315, 0.3345, 0.0374, 0.0296, 0.0672],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 172, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7265e-03, 3.4003e-03, 3.0250e-07, 1.1635e-02, 2.1845e-07, 9.7605e-01,
        6.1918e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.912

[Epoch: 172, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0033, 0.1205, 0.0070, 0.8368, 0.0032, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 172, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0032, 0.0033, 0.0028, 0.0030, 0.7548, 0.0129, 0.2200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 173, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3340, 0.2203, 0.0181, 0.0765, 0.2654, 0.0818],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 173, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5248, 0.0260, 0.3138, 0.0328, 0.0297, 0.0697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 173, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.4769e-03, 2.8288e-03, 1.6713e-07, 1.2293e-02, 8.5700e-08, 9.7610e-01,
        6.2976e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.934

[Epoch: 173, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0030, 0.1178, 0.0064, 0.8407, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 173, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0029, 0.0033, 0.0042, 0.7398, 0.0156, 0.2305],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 174, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3240, 0.2167, 0.0204, 0.0734, 0.2793, 0.0831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 174, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0023, 0.5397, 0.0270, 0.3144, 0.0299, 0.0248, 0.0620],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 174, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3375e-03, 2.8733e-03, 3.1098e-07, 1.6698e-02, 2.5504e-07, 9.7216e-01,
        4.9353e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.942

[Epoch: 174, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0232, 0.0032, 0.1199, 0.0063, 0.8377, 0.0030, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 174, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0030, 0.0032, 0.0031, 0.0035, 0.7445, 0.0133, 0.2294],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 175, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3304, 0.2177, 0.0239, 0.0757, 0.2561, 0.0924],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 175, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5495, 0.0218, 0.3029, 0.0304, 0.0250, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 175, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6133e-03, 4.0899e-03, 5.9329e-07, 1.3911e-02, 4.0108e-07, 9.7012e-01,
        8.2685e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.922

[Epoch: 175, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0240, 0.0035, 0.1235, 0.0071, 0.8317, 0.0033, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 175, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0042, 0.0043, 0.0047, 0.0035, 0.7541, 0.0136, 0.2156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 176, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0027, 0.3279, 0.2140, 0.0179, 0.0800, 0.2873, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 176, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0038, 0.5068, 0.0323, 0.3193, 0.0408, 0.0324, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 176, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.0437e-03, 4.6647e-03, 4.9693e-07, 1.2334e-02, 2.7900e-07, 9.7114e-01,
        7.8150e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.911

[Epoch: 176, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0030, 0.1215, 0.0068, 0.8360, 0.0030, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 176, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0031, 0.0033, 0.0028, 0.7501, 0.0147, 0.2225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 177, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3321, 0.2139, 0.0191, 0.0712, 0.2629, 0.0976],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 177, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0039, 0.4885, 0.0358, 0.3304, 0.0388, 0.0311, 0.0714],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 177, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.2397e-03, 3.6933e-03, 5.5992e-07, 1.4437e-02, 7.0272e-07, 9.7091e-01,
        6.7162e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 177, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0210, 0.0028, 0.1186, 0.0058, 0.8434, 0.0027, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 177, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0030, 0.0035, 0.0039, 0.7589, 0.0114, 0.2164],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 178, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3283, 0.2155, 0.0201, 0.0758, 0.2837, 0.0733],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 178, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5290, 0.0286, 0.3139, 0.0357, 0.0283, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.042

[Epoch: 178, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.3272e-03, 2.3487e-03, 3.0094e-07, 1.1539e-02, 2.0906e-07, 9.7822e-01,
        5.5641e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.931

[Epoch: 178, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0222, 0.0032, 0.1176, 0.0062, 0.8415, 0.0029, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 178, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0038, 0.0038, 0.0039, 0.7541, 0.0161, 0.2148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 179, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0036, 0.3369, 0.2264, 0.0226, 0.0714, 0.2499, 0.0892],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 179, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.4985, 0.0332, 0.3225, 0.0404, 0.0344, 0.0674],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 179, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3560e-03, 2.7621e-03, 3.3291e-07, 1.3736e-02, 2.0681e-07, 9.7361e-01,
        6.5304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.925

[Epoch: 179, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0031, 0.1160, 0.0063, 0.8420, 0.0030, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 179, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0036, 0.0047, 0.0037, 0.7497, 0.0147, 0.2196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 180, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3230, 0.2078, 0.0194, 0.0700, 0.2959, 0.0807],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.006

[Epoch: 180, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0042, 0.4966, 0.0344, 0.3173, 0.0404, 0.0327, 0.0744],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 180, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3510e-03, 3.5653e-03, 4.0934e-07, 1.3676e-02, 2.4731e-07, 9.7278e-01,
        6.6222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.906

[Epoch: 180, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0242, 0.0034, 0.1173, 0.0071, 0.8376, 0.0034, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 180, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0035, 0.0040, 0.0032, 0.7487, 0.0133, 0.2237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 181, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3269, 0.2182, 0.0183, 0.0723, 0.2803, 0.0809],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 181, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5088, 0.0281, 0.3248, 0.0361, 0.0306, 0.0682],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 181, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9395e-03, 3.1941e-03, 3.3247e-07, 1.1322e-02, 1.8178e-07, 9.7669e-01,
        5.8537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.949

[Epoch: 181, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0246, 0.0036, 0.1220, 0.0075, 0.8320, 0.0035, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 181, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0030, 0.0036, 0.0030, 0.7447, 0.0138, 0.2284],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 182, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3296, 0.2162, 0.0209, 0.0719, 0.2761, 0.0822],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 182, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.5172, 0.0294, 0.3244, 0.0350, 0.0268, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 182, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.0267e-03, 2.8956e-03, 2.9171e-07, 1.1593e-02, 1.2743e-07, 9.7543e-01,
        7.0543e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.924

[Epoch: 182, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0233, 0.0032, 0.1215, 0.0067, 0.8356, 0.0031, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 182, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0038, 0.0030, 0.0034, 0.0033, 0.7489, 0.0136, 0.2240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 183, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0031, 0.3245, 0.2202, 0.0209, 0.0808, 0.2645, 0.0860],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 183, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5285, 0.0257, 0.3180, 0.0335, 0.0262, 0.0651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 183, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6819e-03, 3.3762e-03, 3.2468e-07, 1.5093e-02, 1.7964e-07, 9.7046e-01,
        7.3912e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.910

[Epoch: 183, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0226, 0.0031, 0.1213, 0.0064, 0.8371, 0.0030, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 183, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0031, 0.0032, 0.0032, 0.7657, 0.0124, 0.2090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 184, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3393, 0.2138, 0.0187, 0.0718, 0.2705, 0.0825],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.000

[Epoch: 184, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5218, 0.0249, 0.3236, 0.0336, 0.0290, 0.0641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 184, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.0959e-03, 4.0118e-03, 2.7118e-07, 1.4561e-02, 1.6504e-07, 9.6987e-01,
        7.4618e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.940

[Epoch: 184, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0232, 0.0032, 0.1230, 0.0066, 0.8340, 0.0032, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 184, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0036, 0.0036, 0.0031, 0.7597, 0.0115, 0.2156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 185, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3058, 0.2309, 0.0224, 0.0760, 0.2792, 0.0824],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 185, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0033, 0.4933, 0.0322, 0.3309, 0.0413, 0.0331, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.033

[Epoch: 185, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.4030e-03, 3.8622e-03, 4.2226e-07, 1.5185e-02, 3.0504e-07, 9.6976e-01,
        6.7880e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.929

[Epoch: 185, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0225, 0.0030, 0.1236, 0.0066, 0.8345, 0.0030, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 185, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0034, 0.0045, 0.0032, 0.7500, 0.0136, 0.2217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 186, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3498, 0.1889, 0.0164, 0.0746, 0.2804, 0.0860],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.007

[Epoch: 186, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0035, 0.4879, 0.0308, 0.3400, 0.0387, 0.0287, 0.0704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 186, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7142e-03, 3.2450e-03, 2.6685e-07, 1.1618e-02, 2.4770e-07, 9.7705e-01,
        5.3772e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.928

[Epoch: 186, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0213, 0.0029, 0.1184, 0.0061, 0.8428, 0.0027, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 186, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0035, 0.0037, 0.0036, 0.7397, 0.0155, 0.2300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 187, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0034, 0.3330, 0.2404, 0.0230, 0.0727, 0.2468, 0.0807],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.001

[Epoch: 187, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0029, 0.5204, 0.0293, 0.3158, 0.0333, 0.0305, 0.0679],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 187, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.8512e-03, 2.7289e-03, 3.7392e-07, 1.1873e-02, 1.6464e-07, 9.7667e-01,
        5.8779e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 187, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0222, 0.0027, 0.1147, 0.0056, 0.8464, 0.0028, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 187, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0028, 0.0031, 0.0034, 0.7565, 0.0127, 0.2183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 188, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.3194, 0.2092, 0.0171, 0.0561, 0.3024, 0.0921],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 188, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0041, 0.5024, 0.0311, 0.3209, 0.0414, 0.0340, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.035

[Epoch: 188, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4025e-03, 3.2729e-03, 4.3382e-07, 1.6063e-02, 2.1618e-07, 9.7047e-01,
        6.7890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.940

[Epoch: 188, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0232, 0.0034, 0.1136, 0.0069, 0.8427, 0.0034, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.002

[Epoch: 188, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0039, 0.0041, 0.0032, 0.0039, 0.7428, 0.0124, 0.2297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 189, batch: 40/203] total loss per batch: 0.742
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0038, 0.2912, 0.2135, 0.0222, 0.1103, 0.2790, 0.0799],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.009

[Epoch: 189, batch: 80/203] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0046, 0.5023, 0.0372, 0.2941, 0.0461, 0.0381, 0.0776],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.036

[Epoch: 189, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.3223e-03, 3.9389e-03, 5.0118e-07, 1.3800e-02, 3.0757e-07, 9.7113e-01,
        6.8057e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.894

[Epoch: 189, batch: 160/203] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0035, 0.1138, 0.0071, 0.8417, 0.0037, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 189, batch: 200/203] total loss per batch: 0.773
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0031, 0.0050, 0.0036, 0.7537, 0.0194, 0.2112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 190, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3594, 0.2042, 0.0179, 0.0658, 0.2655, 0.0840],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 -0.002

[Epoch: 190, batch: 80/203] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0037, 0.5226, 0.0327, 0.3089, 0.0355, 0.0295, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.032

[Epoch: 190, batch: 120/203] total loss per batch: 0.783
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.8658e-03, 4.0356e-03, 4.4505e-07, 1.1276e-02, 2.1323e-07, 9.7408e-01,
        6.7425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.939

[Epoch: 190, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0246, 0.0036, 0.1259, 0.0074, 0.8285, 0.0034, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 190, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0029, 0.0028, 0.0041, 0.0033, 0.7625, 0.0137, 0.2107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 191, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0039, 0.3226, 0.2252, 0.0223, 0.0751, 0.2768, 0.0741],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.005

[Epoch: 191, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5285, 0.0299, 0.3145, 0.0378, 0.0242, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 191, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.7925e-03, 2.4403e-03, 2.1937e-07, 1.0690e-02, 1.1619e-07, 9.7827e-01,
        5.8110e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.948

[Epoch: 191, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0237, 0.0034, 0.1258, 0.0067, 0.8308, 0.0032, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 191, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0028, 0.0025, 0.0028, 0.0031, 0.7457, 0.0111, 0.2319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 192, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0040, 0.3248, 0.2158, 0.0210, 0.0734, 0.2649, 0.0961],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 192, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0036, 0.5071, 0.0295, 0.3290, 0.0341, 0.0277, 0.0691],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.041

[Epoch: 192, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9946e-03, 2.7929e-03, 2.2244e-07, 1.4062e-02, 1.2236e-07, 9.7469e-01,
        5.4634e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.922

[Epoch: 192, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0230, 0.0033, 0.1199, 0.0063, 0.8382, 0.0031, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 192, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0033, 0.0027, 0.0032, 0.0034, 0.7575, 0.0120, 0.2179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 193, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3376, 0.2231, 0.0194, 0.0740, 0.2682, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.000

[Epoch: 193, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5170, 0.0280, 0.3176, 0.0337, 0.0307, 0.0696],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.039

[Epoch: 193, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([4.0914e-03, 3.6027e-03, 2.5870e-07, 1.6326e-02, 1.3405e-07, 9.6883e-01,
        7.1474e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.927

[Epoch: 193, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0235, 0.0033, 0.1192, 0.0064, 0.8380, 0.0032, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 193, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0031, 0.0037, 0.0037, 0.7644, 0.0128, 0.2086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 194, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3248, 0.2200, 0.0216, 0.0751, 0.2688, 0.0863],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.003

[Epoch: 194, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0032, 0.5197, 0.0284, 0.3169, 0.0362, 0.0299, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 194, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.6207e-03, 3.4522e-03, 2.7611e-07, 1.4271e-02, 1.6917e-07, 9.7130e-01,
        7.3541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.920

[Epoch: 194, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0231, 0.0031, 0.1220, 0.0065, 0.8357, 0.0031, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 194, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0036, 0.0034, 0.0037, 0.0036, 0.7491, 0.0134, 0.2231],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 195, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0035, 0.3258, 0.2155, 0.0195, 0.0758, 0.2769, 0.0831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.002

[Epoch: 195, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5073, 0.0294, 0.3286, 0.0363, 0.0298, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 195, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.9897e-03, 3.3474e-03, 2.3590e-07, 1.0957e-02, 1.4731e-07, 9.7623e-01,
        6.4710e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.921

[Epoch: 195, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0229, 0.0030, 0.1223, 0.0067, 0.8360, 0.0029, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 195, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0037, 0.0033, 0.0035, 0.0037, 0.7485, 0.0127, 0.2246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 196, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3355, 0.2163, 0.0204, 0.0734, 0.2697, 0.0816],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 196, batch: 80/203] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5076, 0.0313, 0.3259, 0.0379, 0.0284, 0.0656],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.034

[Epoch: 196, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.4822e-03, 3.7656e-03, 2.6073e-07, 1.4451e-02, 1.2996e-07, 9.7156e-01,
        6.7426e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.916

[Epoch: 196, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0227, 0.0031, 0.1225, 0.0063, 0.8366, 0.0027, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 196, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0029, 0.0038, 0.0036, 0.7465, 0.0132, 0.2265],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 197, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3370, 0.2154, 0.0191, 0.0707, 0.2712, 0.0833],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 197, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0030, 0.5142, 0.0309, 0.3165, 0.0375, 0.0299, 0.0681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.037

[Epoch: 197, batch: 120/203] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.7114e-03, 3.5321e-03, 1.9686e-07, 1.5567e-02, 1.3690e-07, 9.7088e-01,
        6.3108e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.938

[Epoch: 197, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0218, 0.0028, 0.1231, 0.0058, 0.8377, 0.0028, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 197, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0035, 0.0030, 0.0037, 0.0041, 0.7469, 0.0141, 0.2246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 198, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0032, 0.3257, 0.2206, 0.0200, 0.0700, 0.2769, 0.0837],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.001

[Epoch: 198, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0027, 0.5122, 0.0268, 0.3186, 0.0340, 0.0330, 0.0727],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.038

[Epoch: 198, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.1609e-03, 2.9678e-03, 2.2624e-07, 1.3177e-02, 1.5234e-07, 9.7483e-01,
        5.8687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.930

[Epoch: 198, batch: 160/203] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0204, 0.0027, 0.1175, 0.0057, 0.8445, 0.0029, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 198, batch: 200/203] total loss per batch: 0.771
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0034, 0.0035, 0.0031, 0.0031, 0.7584, 0.0118, 0.2167],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 199, batch: 40/203] total loss per batch: 0.740
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0028, 0.3336, 0.2117, 0.0183, 0.0732, 0.2734, 0.0871],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 199, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0034, 0.5012, 0.0285, 0.3314, 0.0353, 0.0346, 0.0656],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.034

[Epoch: 199, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([3.3981e-03, 2.6542e-03, 2.7793e-07, 1.0022e-02, 1.7268e-07, 9.7619e-01,
        7.7394e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.912

[Epoch: 199, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0216, 0.0031, 0.1119, 0.0067, 0.8466, 0.0032, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.001

[Epoch: 199, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0035, 0.0030, 0.0025, 0.7645, 0.0109, 0.2115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 200, batch: 40/203] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.3300, 0.2167, 0.0200, 0.0733, 0.2733, 0.0833])
Policy pred: tensor([0.0033, 0.3147, 0.2217, 0.0205, 0.0802, 0.2847, 0.0750],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.003 0.004

[Epoch: 200, batch: 80/203] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.5133, 0.0300, 0.3200, 0.0367, 0.0300, 0.0667])
Policy pred: tensor([0.0040, 0.5294, 0.0339, 0.3116, 0.0397, 0.0268, 0.0548],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.035 0.040

[Epoch: 200, batch: 120/203] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0033, 0.0033, 0.0000, 0.0133, 0.0000, 0.9733, 0.0067])
Policy pred: tensor([2.6657e-03, 4.7268e-03, 4.8018e-07, 1.2934e-02, 2.1901e-07, 9.7333e-01,
        6.3420e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.924 -0.930

[Epoch: 200, batch: 160/203] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0233, 0.0033, 0.1200, 0.0067, 0.8367, 0.0033, 0.0067])
Policy pred: tensor([0.0249, 0.0037, 0.1112, 0.0073, 0.8428, 0.0035, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 200, batch: 200/203] total loss per batch: 0.772
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.7533, 0.0133, 0.2200])
Policy pred: tensor([0.0040, 0.0034, 0.0044, 0.0030, 0.7454, 0.0159, 0.2240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

