Training set samples: 7225
Batch size: 32
[Epoch: 1, batch: 45/226] total loss per batch: 0.904
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0946e-06, 2.0573e-08, 1.0000e+00, 6.3526e-08, 8.6222e-08, 4.8698e-09,
        7.0113e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 -0.080

[Epoch: 1, batch: 90/226] total loss per batch: 0.839
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1435e-01, 4.5751e-08, 1.2556e-01, 8.7519e-06, 4.0977e-02, 5.9951e-01,
        1.1959e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.167

[Epoch: 1, batch: 135/226] total loss per batch: 0.859
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([8.9877e-01, 1.2543e-02, 4.7911e-02, 4.5332e-06, 4.2980e-06, 1.4455e-02,
        2.6309e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 1, batch: 180/226] total loss per batch: 0.903
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0298, 0.1847, 0.0615, 0.0892, 0.1153, 0.0244, 0.4951],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 1, batch: 225/226] total loss per batch: 0.799
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.6089e-05, 1.7952e-01, 1.9820e-01, 8.5372e-07, 7.0652e-02, 5.5162e-01,
        2.0256e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.120

[Epoch: 2, batch: 45/226] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.5432e-06, 9.0630e-08, 9.9999e-01, 6.4972e-07, 3.0476e-07, 7.0864e-09,
        2.3808e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.991

[Epoch: 2, batch: 90/226] total loss per batch: 0.581
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.2107e-02, 4.1385e-09, 2.8531e-02, 1.2864e-06, 1.4910e-02, 8.9388e-01,
        2.0572e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.209

[Epoch: 2, batch: 135/226] total loss per batch: 0.604
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3720e-01, 1.1819e-02, 2.3441e-02, 7.6553e-07, 9.5780e-07, 9.8042e-03,
        1.7735e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.065

[Epoch: 2, batch: 180/226] total loss per batch: 0.620
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0087, 0.1026, 0.0318, 0.0197, 0.0496, 0.0118, 0.7758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 2, batch: 225/226] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.3988e-07, 1.3065e-02, 5.4702e-02, 6.5075e-08, 2.1282e-02, 9.1095e-01,
        2.7576e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.230

[Epoch: 3, batch: 45/226] total loss per batch: 0.483
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1410e-06, 2.3258e-08, 1.0000e+00, 2.0249e-07, 1.0164e-07, 1.1929e-09,
        2.0722e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 3, batch: 90/226] total loss per batch: 0.477
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([6.5055e-02, 1.9510e-09, 3.7329e-02, 1.1212e-06, 2.6836e-02, 8.5001e-01,
        2.0767e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.156

[Epoch: 3, batch: 135/226] total loss per batch: 0.494
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3325e-01, 2.2059e-02, 1.4392e-02, 1.9167e-07, 9.2172e-07, 1.2553e-02,
        1.7741e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.071

[Epoch: 3, batch: 180/226] total loss per batch: 0.493
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0040, 0.0377, 0.0144, 0.0053, 0.0172, 0.0060, 0.9153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 3, batch: 225/226] total loss per batch: 0.451
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.0323e-07, 8.9854e-03, 2.2005e-02, 2.8560e-08, 1.1871e-02, 9.5714e-01,
        3.0058e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.186

[Epoch: 4, batch: 45/226] total loss per batch: 0.434
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.5697e-07, 4.0721e-09, 1.0000e+00, 6.2054e-08, 1.9453e-08, 2.4707e-10,
        5.0760e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 4, batch: 90/226] total loss per batch: 0.420
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1361e-01, 1.6536e-09, 5.9452e-02, 1.3886e-06, 1.0108e-01, 6.8792e-01,
        3.7945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.117

[Epoch: 4, batch: 135/226] total loss per batch: 0.451
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4330e-01, 1.2332e-02, 9.1882e-03, 1.2252e-07, 6.3279e-07, 1.3103e-02,
        2.2072e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.069

[Epoch: 4, batch: 180/226] total loss per batch: 0.439
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0021, 0.0159, 0.0090, 0.0051, 0.0063, 0.0042, 0.9573],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 4, batch: 225/226] total loss per batch: 0.413
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.9464e-07, 7.1133e-03, 1.3439e-02, 3.4914e-08, 1.0433e-02, 9.6901e-01,
        6.9574e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.211

[Epoch: 5, batch: 45/226] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.3959e-07, 1.8960e-09, 1.0000e+00, 3.5454e-08, 1.0380e-08, 1.3350e-10,
        1.3304e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 5, batch: 90/226] total loss per batch: 0.402
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.0507e-02, 7.1691e-11, 6.9601e-03, 1.3664e-07, 2.7767e-02, 9.5012e-01,
        4.6456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.102

[Epoch: 5, batch: 135/226] total loss per batch: 0.432
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4591e-01, 8.0012e-03, 1.9751e-02, 6.3101e-08, 6.1466e-07, 9.9219e-03,
        1.6417e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.067

[Epoch: 5, batch: 180/226] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0025, 0.0186, 0.0055, 0.0029, 0.0089, 0.0029, 0.9586],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 5, batch: 225/226] total loss per batch: 0.397
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0529e-07, 5.4056e-03, 1.3356e-02, 1.0657e-08, 1.0740e-02, 9.7050e-01,
        2.6037e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.163

[Epoch: 6, batch: 45/226] total loss per batch: 0.406
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5767e-07, 1.1733e-09, 1.0000e+00, 4.4374e-08, 2.0039e-08, 3.9690e-11,
        5.3213e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 6, batch: 90/226] total loss per batch: 0.388
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([9.1189e-02, 5.4148e-10, 1.1561e-02, 4.3773e-07, 2.2052e-01, 6.4920e-01,
        2.7533e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.058

[Epoch: 6, batch: 135/226] total loss per batch: 0.419
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4525e-01, 8.2259e-03, 1.0242e-02, 6.2992e-08, 3.2900e-07, 1.1048e-02,
        2.5229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.067

[Epoch: 6, batch: 180/226] total loss per batch: 0.406
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0030, 0.0148, 0.0059, 0.0040, 0.0167, 0.0027, 0.9529],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.005

[Epoch: 6, batch: 225/226] total loss per batch: 0.386
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1622e-07, 3.9784e-03, 1.4997e-02, 9.0928e-09, 6.2158e-03, 9.7481e-01,
        3.2294e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.211

[Epoch: 7, batch: 45/226] total loss per batch: 0.395
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1987e-07, 4.3015e-10, 1.0000e+00, 3.9892e-08, 5.1002e-09, 2.1652e-11,
        6.4558e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 1.000

[Epoch: 7, batch: 90/226] total loss per batch: 0.374
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.5091e-03, 4.3508e-11, 3.3600e-03, 6.5766e-08, 2.9397e-02, 9.5939e-01,
        3.3482e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.050

[Epoch: 7, batch: 135/226] total loss per batch: 0.406
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5732e-01, 5.0268e-03, 8.1589e-03, 4.0212e-08, 3.3938e-07, 1.1193e-02,
        1.8302e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.060

[Epoch: 7, batch: 180/226] total loss per batch: 0.398
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0012, 0.0075, 0.0033, 0.0013, 0.0047, 0.0017, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 7, batch: 225/226] total loss per batch: 0.379
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.7407e-08, 3.1197e-03, 8.3906e-03, 7.3258e-09, 7.6920e-03, 9.8080e-01,
        8.6611e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.172

[Epoch: 8, batch: 45/226] total loss per batch: 0.393
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.0705e-07, 1.1419e-09, 1.0000e+00, 6.3210e-08, 1.1591e-08, 6.0427e-11,
        4.7519e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 8, batch: 90/226] total loss per batch: 0.369
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.7347e-02, 3.8026e-10, 2.0138e-02, 4.2640e-07, 3.4680e-01, 5.7952e-01,
        1.6191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.022

[Epoch: 8, batch: 135/226] total loss per batch: 0.401
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5947e-01, 6.4802e-03, 5.5527e-03, 3.8857e-08, 3.9520e-07, 8.5400e-03,
        1.9957e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.065

[Epoch: 8, batch: 180/226] total loss per batch: 0.394
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0031, 0.0085, 0.0040, 0.0027, 0.0116, 0.0028, 0.9673],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.014

[Epoch: 8, batch: 225/226] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0322e-08, 2.3461e-03, 9.0157e-03, 6.4872e-09, 3.7134e-03, 9.8492e-01,
        9.4660e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.178

[Epoch: 9, batch: 45/226] total loss per batch: 0.392
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.2611e-07, 2.3320e-09, 1.0000e+00, 1.1641e-07, 8.9831e-09, 6.5521e-11,
        4.6345e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 9, batch: 90/226] total loss per batch: 0.366
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.3996e-03, 6.3888e-11, 4.0343e-03, 1.1520e-07, 3.8948e-02, 9.4883e-01,
        2.7884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.024

[Epoch: 9, batch: 135/226] total loss per batch: 0.395
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7154e-01, 3.9145e-03, 6.6759e-03, 3.8663e-08, 4.8330e-07, 6.5281e-03,
        1.1337e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.058

[Epoch: 9, batch: 180/226] total loss per batch: 0.389
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0057, 0.0030, 0.0014, 0.0041, 0.0016, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 9, batch: 225/226] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0772e-08, 4.1268e-03, 9.6665e-03, 9.2401e-09, 7.4300e-03, 9.7878e-01,
        8.8758e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.185

[Epoch: 10, batch: 45/226] total loss per batch: 0.389
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5339e-07, 1.4182e-09, 1.0000e+00, 4.4739e-08, 1.2041e-08, 5.2073e-11,
        4.4560e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 10, batch: 90/226] total loss per batch: 0.364
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2750e-02, 1.9229e-10, 1.0874e-02, 2.3226e-07, 1.5196e-01, 8.1769e-01,
        6.7251e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.018

[Epoch: 10, batch: 135/226] total loss per batch: 0.392
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6366e-01, 7.2176e-03, 3.9169e-03, 3.6711e-08, 4.3505e-07, 4.1978e-03,
        2.1004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.058

[Epoch: 10, batch: 180/226] total loss per batch: 0.384
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0022, 0.0089, 0.0028, 0.0019, 0.0052, 0.0023, 0.9766],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.022

[Epoch: 10, batch: 225/226] total loss per batch: 0.370
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.1119e-08, 3.8128e-03, 8.2980e-03, 6.9895e-09, 4.7297e-03, 9.8316e-01,
        9.8987e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.175

[Epoch: 11, batch: 45/226] total loss per batch: 0.387
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9797e-07, 1.1558e-09, 1.0000e+00, 8.8495e-08, 9.2181e-09, 8.4763e-11,
        3.1283e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 11, batch: 90/226] total loss per batch: 0.363
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5811e-02, 2.7009e-10, 1.1168e-02, 4.5093e-07, 2.5179e-01, 7.1223e-01,
        8.9940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 11, batch: 135/226] total loss per batch: 0.390
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6269e-01, 4.1808e-03, 6.5857e-03, 4.2086e-08, 5.1038e-07, 5.0082e-03,
        2.1536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.051

[Epoch: 11, batch: 180/226] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0033, 0.0074, 0.0035, 0.0025, 0.0068, 0.0023, 0.9742],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 11, batch: 225/226] total loss per batch: 0.368
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3383e-07, 5.1569e-03, 9.3838e-03, 1.8992e-08, 5.7180e-03, 9.7974e-01,
        2.6218e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.167

[Epoch: 12, batch: 45/226] total loss per batch: 0.385
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.3848e-07, 2.9421e-09, 1.0000e+00, 7.7278e-08, 7.9194e-09, 9.7937e-11,
        4.9534e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 12, batch: 90/226] total loss per batch: 0.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.3417e-03, 2.1526e-10, 7.8896e-03, 2.3882e-07, 1.2239e-01, 8.5876e-01,
        3.6191e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 12, batch: 135/226] total loss per batch: 0.390
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7106e-01, 5.6909e-03, 4.3291e-03, 3.9366e-08, 4.7495e-07, 3.7809e-03,
        1.5141e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.052

[Epoch: 12, batch: 180/226] total loss per batch: 0.381
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0031, 0.0084, 0.0023, 0.0030, 0.0044, 0.0026, 0.9761],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 12, batch: 225/226] total loss per batch: 0.369
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0692e-08, 3.2420e-03, 6.1378e-03, 1.0906e-08, 4.0448e-03, 9.8658e-01,
        1.0976e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.194

[Epoch: 13, batch: 45/226] total loss per batch: 0.384
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8071e-07, 8.2439e-10, 1.0000e+00, 5.7048e-08, 3.4647e-09, 4.1911e-11,
        1.4066e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 13, batch: 90/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1952e-02, 2.7111e-10, 6.2991e-03, 3.8964e-07, 3.0610e-01, 6.6444e-01,
        1.1213e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 13, batch: 135/226] total loss per batch: 0.389
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4693e-01, 4.1930e-03, 3.8490e-03, 1.1414e-07, 9.5075e-07, 5.6437e-03,
        3.9379e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.057

[Epoch: 13, batch: 180/226] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.0819e-03, 7.1198e-03, 1.9877e-03, 8.6343e-04, 3.4212e-03, 1.0767e-03,
        9.8445e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.035

[Epoch: 13, batch: 225/226] total loss per batch: 0.368
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.7189e-07, 5.0944e-03, 8.3590e-03, 2.7023e-08, 5.7391e-03, 9.8081e-01,
        7.9653e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 14, batch: 45/226] total loss per batch: 0.383
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.2434e-07, 2.4919e-09, 1.0000e+00, 1.2928e-07, 1.0128e-08, 1.3535e-10,
        8.8445e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 14, batch: 90/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([6.9419e-03, 1.6851e-10, 5.4743e-03, 2.4161e-07, 8.8829e-02, 8.9576e-01,
        2.9948e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 14, batch: 135/226] total loss per batch: 0.388
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5807e-01, 5.9319e-03, 3.9106e-03, 6.5566e-08, 3.6031e-07, 5.0751e-03,
        2.7012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.050

[Epoch: 14, batch: 180/226] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0089, 0.0017, 0.0013, 0.0023, 0.0015, 0.9827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 14, batch: 225/226] total loss per batch: 0.365
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0933e-07, 2.7343e-03, 7.5955e-03, 1.3540e-08, 3.8952e-03, 9.8577e-01,
        3.8887e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.188

[Epoch: 15, batch: 45/226] total loss per batch: 0.384
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.7409e-07, 2.2408e-09, 1.0000e+00, 1.4433e-07, 1.2602e-08, 1.1082e-10,
        1.2609e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 1.000

[Epoch: 15, batch: 90/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2465e-02, 3.8674e-10, 8.0724e-03, 2.8195e-07, 2.3437e-01, 7.3675e-01,
        8.3439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.034

[Epoch: 15, batch: 135/226] total loss per batch: 0.394
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.8052e-01, 2.4565e-03, 1.6853e-03, 5.0680e-08, 2.2690e-07, 2.4101e-03,
        1.2925e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 15, batch: 180/226] total loss per batch: 0.391
Policy (actual, predicted): 6 3
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.0942e-03, 4.2266e-03, 2.0083e-03, 9.8389e-01, 7.8615e-04, 1.3008e-03,
        5.6940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 15, batch: 225/226] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0277e-07, 7.5342e-03, 5.0604e-03, 1.0194e-08, 5.0185e-03, 9.8239e-01,
        4.7187e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.105

[Epoch: 16, batch: 45/226] total loss per batch: 0.395
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3822e-07, 5.0850e-10, 1.0000e+00, 2.4185e-08, 1.8539e-08, 4.3388e-11,
        8.2004e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 16, batch: 90/226] total loss per batch: 0.370
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([6.5638e-03, 1.7866e-10, 3.6374e-03, 7.3690e-08, 1.3330e-01, 8.5436e-01,
        2.1367e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 16, batch: 135/226] total loss per batch: 0.397
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.1240e-01, 1.1264e-02, 7.2796e-03, 9.8093e-08, 5.4216e-07, 7.2263e-03,
        6.1825e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.068

[Epoch: 16, batch: 180/226] total loss per batch: 0.391
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0211, 0.0188, 0.0186, 0.0216, 0.0378, 0.0023, 0.8798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.025

[Epoch: 16, batch: 225/226] total loss per batch: 0.387
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.3359e-08, 2.2644e-03, 6.7178e-03, 1.1498e-08, 2.3981e-03, 9.8862e-01,
        5.8274e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.139

[Epoch: 17, batch: 45/226] total loss per batch: 0.393
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.6891e-07, 2.0922e-09, 1.0000e+00, 2.9455e-08, 6.7720e-09, 2.4331e-10,
        3.3993e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 17, batch: 90/226] total loss per batch: 0.373
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7506e-02, 5.3387e-10, 9.3453e-03, 1.3807e-06, 4.1759e-01, 5.4601e-01,
        9.5452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.030

[Epoch: 17, batch: 135/226] total loss per batch: 0.401
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5266e-01, 7.7123e-03, 2.6389e-03, 2.5420e-07, 5.5298e-07, 1.1519e-02,
        2.5468e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.047

[Epoch: 17, batch: 180/226] total loss per batch: 0.404
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0125, 0.0176, 0.0120, 0.0128, 0.0059, 0.0010, 0.9383],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.031

[Epoch: 17, batch: 225/226] total loss per batch: 0.386
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.4448e-07, 9.8317e-03, 5.3967e-03, 2.2036e-08, 3.4614e-03, 9.8131e-01,
        4.0436e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.115

[Epoch: 18, batch: 45/226] total loss per batch: 0.397
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3167e-07, 1.0476e-09, 1.0000e+00, 4.0009e-08, 9.5894e-09, 2.1567e-11,
        1.8584e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 18, batch: 90/226] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1771e-03, 8.8884e-11, 5.0174e-03, 4.0139e-08, 2.2816e-02, 9.6403e-01,
        5.9636e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 18, batch: 135/226] total loss per batch: 0.399
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6791e-01, 4.1135e-03, 4.0726e-03, 1.8533e-07, 5.2720e-07, 7.5791e-03,
        1.6321e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.058

[Epoch: 18, batch: 180/226] total loss per batch: 0.389
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0123, 0.0139, 0.0101, 0.0140, 0.0047, 0.0015, 0.9436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.022

[Epoch: 18, batch: 225/226] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.8294e-07, 3.6634e-03, 4.8657e-03, 1.1480e-07, 3.4636e-03, 9.8801e-01,
        3.8147e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.189

[Epoch: 19, batch: 45/226] total loss per batch: 0.389
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4536e-07, 1.7016e-09, 1.0000e+00, 5.0503e-08, 4.9386e-09, 1.2976e-10,
        1.3837e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 19, batch: 90/226] total loss per batch: 0.366
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([6.9263e-03, 5.0985e-10, 6.4226e-03, 1.8766e-07, 1.6560e-01, 8.1165e-01,
        9.4007e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 19, batch: 135/226] total loss per batch: 0.393
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5266e-01, 7.1454e-03, 2.5939e-03, 2.2879e-07, 9.6321e-07, 4.7222e-03,
        3.2874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.065

[Epoch: 19, batch: 180/226] total loss per batch: 0.382
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0171, 0.0123, 0.0082, 0.0196, 0.0046, 0.0012, 0.9371],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.037

[Epoch: 19, batch: 225/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.5238e-07, 6.9078e-03, 4.2826e-03, 5.7215e-08, 2.4264e-03, 9.8638e-01,
        1.0730e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.170

[Epoch: 20, batch: 45/226] total loss per batch: 0.384
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.9391e-07, 5.4988e-09, 1.0000e+00, 6.4607e-08, 5.5244e-09, 1.5694e-10,
        3.5629e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 1.000

[Epoch: 20, batch: 90/226] total loss per batch: 0.363
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.3879e-03, 3.2270e-10, 2.0582e-02, 1.7690e-07, 1.6116e-01, 8.0822e-01,
        4.6502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 20, batch: 135/226] total loss per batch: 0.387
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7767e-01, 3.9542e-03, 3.2065e-03, 2.4410e-07, 3.9532e-07, 3.5945e-03,
        1.1570e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.050

[Epoch: 20, batch: 180/226] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0098, 0.0091, 0.0062, 0.0086, 0.0058, 0.0011, 0.9596],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.027

[Epoch: 20, batch: 225/226] total loss per batch: 0.365
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.5690e-07, 4.7882e-03, 3.8661e-03, 9.6456e-08, 2.3805e-03, 9.8896e-01,
        5.6509e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.204

[Epoch: 21, batch: 45/226] total loss per batch: 0.381
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.3359e-07, 5.3731e-09, 1.0000e+00, 7.5033e-08, 1.7891e-08, 5.2052e-10,
        1.0659e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 1.000

[Epoch: 21, batch: 90/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.1926e-03, 6.5538e-10, 5.5389e-03, 2.3881e-07, 1.9856e-01, 7.8345e-01,
        5.2564e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 21, batch: 135/226] total loss per batch: 0.395
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4908e-01, 8.5014e-03, 8.0709e-03, 2.8476e-07, 9.1183e-07, 4.3429e-03,
        3.0004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.055

[Epoch: 21, batch: 180/226] total loss per batch: 0.384
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0103, 0.0134, 0.0062, 0.0096, 0.0063, 0.0012, 0.9529],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.022

[Epoch: 21, batch: 225/226] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.9415e-07, 8.9810e-03, 5.5742e-03, 2.1907e-08, 3.0967e-03, 9.8235e-01,
        6.5979e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 22, batch: 45/226] total loss per batch: 0.394
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1548e-06, 2.3086e-08, 1.0000e+00, 1.4109e-07, 3.9880e-08, 9.7711e-10,
        3.5825e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 1.000

[Epoch: 22, batch: 90/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([8.2876e-03, 6.4584e-10, 9.5494e-03, 5.0124e-07, 1.7802e-01, 7.9950e-01,
        4.6467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.032

[Epoch: 22, batch: 135/226] total loss per batch: 0.397
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5487e-01, 8.4681e-03, 4.6264e-03, 3.7066e-07, 1.8651e-06, 3.6476e-03,
        2.8386e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.025

[Epoch: 22, batch: 180/226] total loss per batch: 0.406
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0121, 0.0123, 0.0094, 0.0089, 0.0084, 0.0012, 0.9477],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.034

[Epoch: 22, batch: 225/226] total loss per batch: 0.386
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.4022e-07, 1.6155e-02, 1.4288e-02, 6.4898e-08, 3.8983e-03, 9.6566e-01,
        9.6170e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.135

[Epoch: 23, batch: 45/226] total loss per batch: 0.394
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.7100e-06, 4.0967e-07, 9.9999e-01, 2.9764e-07, 2.3144e-07, 2.4581e-08,
        5.3862e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 23, batch: 90/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1704e-02, 5.7132e-10, 4.4736e-03, 4.4517e-07, 1.8728e-01, 7.9285e-01,
        3.6876e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.147

[Epoch: 23, batch: 135/226] total loss per batch: 0.397
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5494e-01, 7.4573e-03, 4.7947e-03, 1.7224e-07, 9.2065e-07, 3.9557e-03,
        2.8855e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 23, batch: 180/226] total loss per batch: 0.392
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([7.1451e-03, 5.9366e-03, 3.1356e-03, 8.4222e-03, 4.2842e-03, 7.1143e-04,
        9.7036e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 23, batch: 225/226] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.6780e-07, 6.7877e-03, 4.8002e-03, 3.9268e-08, 2.8183e-03, 9.8559e-01,
        3.0391e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.194

[Epoch: 24, batch: 45/226] total loss per batch: 0.392
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5908e-06, 2.0996e-08, 1.0000e+00, 9.8847e-08, 4.4350e-08, 1.0580e-08,
        1.8726e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.994

[Epoch: 24, batch: 90/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([6.4043e-03, 9.8493e-11, 1.1747e-02, 1.9548e-07, 1.2820e-01, 8.5137e-01,
        2.2809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.051

[Epoch: 24, batch: 135/226] total loss per batch: 0.394
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7530e-01, 3.2847e-03, 2.1737e-03, 1.1263e-07, 6.2283e-07, 4.3591e-03,
        1.4879e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.025

[Epoch: 24, batch: 180/226] total loss per batch: 0.388
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0058, 0.0078, 0.0027, 0.0046, 0.0090, 0.0010, 0.9692],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.027

[Epoch: 24, batch: 225/226] total loss per batch: 0.380
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.1546e-07, 2.7527e-03, 2.4709e-03, 2.5301e-08, 1.5893e-03, 9.9319e-01,
        3.7482e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.186

[Epoch: 25, batch: 45/226] total loss per batch: 0.386
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3377e-06, 6.2561e-09, 1.0000e+00, 8.9740e-08, 5.7174e-09, 1.8689e-09,
        6.2664e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.985

[Epoch: 25, batch: 90/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.3661e-02, 7.1427e-10, 5.8945e-03, 3.9525e-07, 2.5753e-01, 7.1859e-01,
        4.3281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 25, batch: 135/226] total loss per batch: 0.388
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6584e-01, 5.0676e-03, 2.8321e-03, 6.8395e-08, 6.8522e-07, 4.0279e-03,
        2.2229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.024

[Epoch: 25, batch: 180/226] total loss per batch: 0.379
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0081, 0.0082, 0.0044, 0.0035, 0.0108, 0.0016, 0.9634],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.024

[Epoch: 25, batch: 225/226] total loss per batch: 0.371
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.8469e-07, 2.8637e-03, 2.6149e-03, 7.8446e-09, 1.8236e-03, 9.9270e-01,
        2.0319e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.198

[Epoch: 26, batch: 45/226] total loss per batch: 0.381
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1964e-06, 1.8083e-09, 1.0000e+00, 6.8326e-08, 7.0333e-09, 1.6011e-09,
        4.7762e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.990

[Epoch: 26, batch: 90/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.1204e-03, 1.1279e-09, 4.1008e-03, 3.3860e-07, 1.8994e-01, 7.9396e-01,
        4.8744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 26, batch: 135/226] total loss per batch: 0.384
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4645e-01, 6.6573e-03, 3.1335e-03, 3.0683e-07, 1.0646e-06, 6.0885e-03,
        3.7668e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.028

[Epoch: 26, batch: 180/226] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0066, 0.0073, 0.0041, 0.0046, 0.0087, 0.0014, 0.9673],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 26, batch: 225/226] total loss per batch: 0.363
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.1498e-07, 4.1574e-03, 2.4949e-03, 1.1214e-08, 1.8797e-03, 9.9147e-01,
        5.7126e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.185

[Epoch: 27, batch: 45/226] total loss per batch: 0.379
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2728e-06, 2.9624e-09, 1.0000e+00, 4.9491e-08, 5.8520e-09, 1.3547e-09,
        2.2333e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.991

[Epoch: 27, batch: 90/226] total loss per batch: 0.355
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.2560e-03, 7.0761e-10, 5.2840e-03, 4.2520e-07, 1.9912e-01, 7.8561e-01,
        4.7259e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 27, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4825e-01, 4.6278e-03, 3.5326e-03, 2.5041e-07, 1.3835e-06, 6.1507e-03,
        3.7440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 27, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0059, 0.0075, 0.0037, 0.0047, 0.0060, 0.0011, 0.9711],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.010

[Epoch: 27, batch: 225/226] total loss per batch: 0.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.6688e-07, 2.8862e-03, 1.9302e-03, 1.3877e-08, 1.9682e-03, 9.9321e-01,
        3.8121e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 28, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4835e-06, 3.1221e-09, 1.0000e+00, 5.3121e-08, 4.8825e-09, 1.4941e-09,
        1.8817e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.991

[Epoch: 28, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.6410e-03, 9.3644e-10, 4.1742e-03, 5.7934e-07, 2.2229e-01, 7.6334e-01,
        4.5527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 28, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6497e-01, 5.0355e-03, 3.0825e-03, 5.6252e-07, 1.7722e-06, 3.3669e-03,
        2.3540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.029

[Epoch: 28, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0059, 0.0071, 0.0038, 0.0044, 0.0064, 0.0013, 0.9711],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.007

[Epoch: 28, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9252e-07, 3.1548e-03, 2.5579e-03, 1.2616e-08, 1.6750e-03, 9.9261e-01,
        3.1493e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 29, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0688e-06, 1.6138e-09, 1.0000e+00, 3.3752e-08, 3.2967e-09, 7.1594e-10,
        2.4812e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.993

[Epoch: 29, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.8352e-03, 3.3422e-10, 3.8216e-03, 2.8767e-07, 1.9410e-01, 7.9453e-01,
        3.7122e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 29, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4493e-01, 5.5739e-03, 2.7735e-03, 1.6416e-07, 1.1057e-06, 3.8839e-03,
        4.2836e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.029

[Epoch: 29, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0052, 0.0063, 0.0029, 0.0045, 0.0048, 0.0013, 0.9751],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.007

[Epoch: 29, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.5582e-07, 2.6851e-03, 1.9490e-03, 1.3081e-08, 1.6193e-03, 9.9375e-01,
        4.5758e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.162

[Epoch: 30, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1577e-06, 1.5765e-09, 1.0000e+00, 2.7133e-08, 2.3757e-09, 6.5201e-10,
        1.7037e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.993

[Epoch: 30, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.7990e-03, 1.0997e-09, 3.7524e-03, 5.9153e-07, 2.3952e-01, 7.4826e-01,
        3.6698e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 30, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6022e-01, 4.8716e-03, 3.7570e-03, 3.7270e-07, 1.2575e-06, 3.5298e-03,
        2.7616e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 30, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0051, 0.0062, 0.0030, 0.0040, 0.0048, 0.0011, 0.9757],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.006

[Epoch: 30, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.2593e-07, 2.4992e-03, 2.0580e-03, 1.1910e-08, 1.7443e-03, 9.9370e-01,
        1.6931e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.163

[Epoch: 31, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1225e-06, 1.6336e-09, 1.0000e+00, 3.2887e-08, 3.0729e-09, 1.0530e-09,
        1.7289e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.995

[Epoch: 31, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.3436e-03, 4.0204e-10, 3.7478e-03, 2.8653e-07, 1.8723e-01, 8.0316e-01,
        3.5195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 31, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4802e-01, 4.8867e-03, 2.0997e-03, 1.9232e-07, 9.6399e-07, 3.5389e-03,
        4.1458e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.027

[Epoch: 31, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0037, 0.0051, 0.0032, 0.0050, 0.0040, 0.0012, 0.9778],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 31, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.5199e-07, 2.8689e-03, 2.5453e-03, 1.7156e-08, 2.4262e-03, 9.9216e-01,
        5.0647e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.138

[Epoch: 32, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3727e-06, 1.8813e-09, 1.0000e+00, 3.3782e-08, 4.2587e-09, 1.0329e-09,
        3.0003e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.994

[Epoch: 32, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.0427e-03, 9.7043e-10, 4.4313e-03, 5.9876e-07, 2.3059e-01, 7.5754e-01,
        3.4006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 32, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6123e-01, 5.2366e-03, 2.9634e-03, 3.7016e-07, 1.3738e-06, 4.0583e-03,
        2.6507e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.027

[Epoch: 32, batch: 180/226] total loss per batch: 0.375
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0043, 0.0065, 0.0029, 0.0041, 0.0040, 0.0012, 0.9770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.010

[Epoch: 32, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9485e-07, 3.0383e-03, 2.2520e-03, 1.8685e-08, 1.8176e-03, 9.9289e-01,
        3.2669e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 33, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5117e-06, 2.1529e-09, 1.0000e+00, 4.1227e-08, 2.8029e-09, 1.1233e-09,
        1.7861e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.995

[Epoch: 33, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.6708e-03, 5.9104e-10, 3.3942e-03, 3.8914e-07, 2.0410e-01, 7.8639e-01,
        3.4364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 33, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4162e-01, 7.2401e-03, 3.6423e-03, 2.6584e-07, 7.5895e-07, 3.6434e-03,
        4.3855e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 33, batch: 180/226] total loss per batch: 0.375
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0037, 0.0055, 0.0041, 0.0050, 0.0035, 0.0013, 0.9769],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.012

[Epoch: 33, batch: 225/226] total loss per batch: 0.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.7760e-07, 2.0385e-03, 2.8591e-03, 2.0176e-08, 2.0654e-03, 9.9304e-01,
        8.4868e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.194

[Epoch: 34, batch: 45/226] total loss per batch: 0.379
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4744e-06, 2.6804e-09, 1.0000e+00, 4.5917e-08, 4.7571e-09, 1.2663e-09,
        2.8695e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.994

[Epoch: 34, batch: 90/226] total loss per batch: 0.355
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.7437e-03, 8.0387e-10, 3.6736e-03, 4.3143e-07, 1.7369e-01, 8.1569e-01,
        3.2090e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 34, batch: 135/226] total loss per batch: 0.384
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7252e-01, 3.7647e-03, 2.7147e-03, 5.5173e-07, 1.1081e-06, 3.3822e-03,
        1.7614e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 34, batch: 180/226] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0036, 0.0060, 0.0016, 0.0045, 0.0031, 0.0011, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 34, batch: 225/226] total loss per batch: 0.362
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.0923e-07, 2.5296e-03, 3.4287e-03, 4.4861e-08, 1.9165e-03, 9.9212e-01,
        6.7026e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.144

[Epoch: 35, batch: 45/226] total loss per batch: 0.379
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9594e-06, 5.4397e-09, 1.0000e+00, 7.0612e-08, 7.7023e-09, 1.8334e-09,
        1.6868e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.994

[Epoch: 35, batch: 90/226] total loss per batch: 0.356
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.9982e-03, 1.1721e-09, 5.7139e-03, 8.9008e-07, 2.7682e-01, 7.0989e-01,
        3.5781e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 35, batch: 135/226] total loss per batch: 0.384
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4135e-01, 6.7835e-03, 5.4068e-03, 1.0091e-06, 9.7980e-07, 4.1089e-03,
        4.2350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 35, batch: 180/226] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0034, 0.0057, 0.0043, 0.0044, 0.0027, 0.0013, 0.9780],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 35, batch: 225/226] total loss per batch: 0.363
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.5863e-07, 2.1543e-03, 2.9009e-03, 4.3666e-08, 1.5444e-03, 9.9340e-01,
        4.7398e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 36, batch: 45/226] total loss per batch: 0.380
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4920e-06, 4.0080e-09, 1.0000e+00, 6.2050e-08, 1.2903e-08, 3.6191e-09,
        1.3316e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 36, batch: 90/226] total loss per batch: 0.356
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.6479e-03, 2.1146e-09, 3.7065e-03, 6.1723e-07, 2.2029e-01, 7.6939e-01,
        2.9636e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 36, batch: 135/226] total loss per batch: 0.384
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5319e-01, 5.7970e-03, 4.4640e-03, 9.5360e-07, 1.3101e-06, 2.6565e-03,
        3.3886e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 36, batch: 180/226] total loss per batch: 0.380
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0067, 0.0083, 0.0029, 0.0073, 0.0040, 0.0023, 0.9685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 36, batch: 225/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.9659e-06, 7.1290e-03, 4.8276e-03, 1.2305e-08, 2.8100e-03, 9.8523e-01,
        4.3583e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.126

[Epoch: 37, batch: 45/226] total loss per batch: 0.386
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3920e-06, 1.6409e-09, 1.0000e+00, 2.2512e-08, 1.1390e-08, 1.5528e-09,
        3.0336e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.993

[Epoch: 37, batch: 90/226] total loss per batch: 0.365
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.8818e-03, 4.1414e-10, 3.9891e-03, 8.3293e-07, 1.6391e-01, 8.2691e-01,
        2.3046e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 37, batch: 135/226] total loss per batch: 0.389
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5759e-01, 6.8194e-03, 3.2299e-03, 4.0862e-07, 8.1338e-07, 6.4804e-03,
        2.5874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.054

[Epoch: 37, batch: 180/226] total loss per batch: 0.386
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0054, 0.0043, 0.0070, 0.0036, 0.0022, 0.9759],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 37, batch: 225/226] total loss per batch: 0.382
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1731e-06, 5.4267e-03, 3.5695e-03, 6.7048e-08, 2.4337e-03, 9.8857e-01,
        5.4244e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.161

[Epoch: 38, batch: 45/226] total loss per batch: 0.395
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4876e-06, 2.1937e-09, 1.0000e+00, 1.9842e-08, 3.9124e-09, 7.7426e-10,
        1.4703e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.993

[Epoch: 38, batch: 90/226] total loss per batch: 0.364
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([9.9001e-03, 2.1763e-09, 2.6999e-03, 7.8473e-07, 2.4924e-01, 7.3391e-01,
        4.2532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 38, batch: 135/226] total loss per batch: 0.392
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6024e-01, 6.9161e-03, 2.1087e-03, 7.7318e-07, 4.3918e-07, 4.7665e-03,
        2.5966e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.020

[Epoch: 38, batch: 180/226] total loss per batch: 0.388
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0047, 0.0108, 0.0056, 0.0062, 0.0078, 0.0026, 0.9623],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.008

[Epoch: 38, batch: 225/226] total loss per batch: 0.375
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.2989e-07, 2.9127e-03, 1.0136e-03, 2.7176e-08, 2.4735e-03, 9.9360e-01,
        2.2509e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.184

[Epoch: 39, batch: 45/226] total loss per batch: 0.398
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0390e-06, 2.3951e-09, 1.0000e+00, 1.8964e-07, 1.1648e-08, 1.7171e-08,
        3.4867e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 39, batch: 90/226] total loss per batch: 0.365
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1397e-02, 5.0421e-10, 4.0703e-03, 1.9952e-06, 1.1804e-01, 8.6342e-01,
        3.0617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.024

[Epoch: 39, batch: 135/226] total loss per batch: 0.409
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6135e-01, 5.9210e-03, 1.1985e-02, 5.6829e-06, 9.7149e-07, 6.4784e-03,
        1.4261e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 39, batch: 180/226] total loss per batch: 0.404
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0064, 0.0150, 0.0060, 0.0014, 0.0011, 0.9681],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 39, batch: 225/226] total loss per batch: 0.385
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.6423e-07, 3.6159e-03, 2.4126e-03, 5.2306e-08, 3.1584e-03, 9.9081e-01,
        1.5998e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.209

[Epoch: 40, batch: 45/226] total loss per batch: 0.396
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.3533e-07, 7.8468e-10, 1.0000e+00, 3.5228e-08, 1.8068e-09, 2.6107e-09,
        2.8135e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 40, batch: 90/226] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.9625e-03, 1.7167e-09, 5.8484e-03, 8.4837e-07, 1.8472e-01, 7.9651e-01,
        4.9522e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.034

[Epoch: 40, batch: 135/226] total loss per batch: 0.412
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3135e-01, 5.6116e-03, 4.4709e-03, 5.4656e-07, 2.6617e-07, 7.1753e-03,
        5.1392e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.049

[Epoch: 40, batch: 180/226] total loss per batch: 0.392
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([9.1366e-04, 1.7375e-02, 3.7120e-03, 1.1102e-02, 6.1537e-04, 1.8224e-03,
        9.6446e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.016

[Epoch: 40, batch: 225/226] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.7999e-07, 5.2820e-03, 2.7092e-03, 1.7215e-07, 3.1313e-02, 9.6069e-01,
        7.5379e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.138

[Epoch: 41, batch: 45/226] total loss per batch: 0.398
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5053e-06, 1.2461e-08, 1.0000e+00, 7.5263e-08, 7.4828e-09, 2.7951e-09,
        5.2899e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 41, batch: 90/226] total loss per batch: 0.372
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([8.4152e-03, 9.3222e-10, 6.4533e-03, 9.1769e-07, 2.6821e-01, 7.1420e-01,
        2.7233e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 41, batch: 135/226] total loss per batch: 0.394
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7260e-01, 3.2337e-03, 6.3111e-03, 8.1628e-07, 2.7590e-07, 4.8644e-03,
        1.2985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.030

[Epoch: 41, batch: 180/226] total loss per batch: 0.383
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0101, 0.0044, 0.0100, 0.0015, 0.0020, 0.9705],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.015

[Epoch: 41, batch: 225/226] total loss per batch: 0.369
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.5474e-06, 3.3464e-03, 4.0676e-03, 1.9664e-07, 4.4953e-03, 9.8809e-01,
        1.4952e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.091

[Epoch: 42, batch: 45/226] total loss per batch: 0.390
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0510e-06, 1.9561e-09, 1.0000e+00, 6.4529e-08, 2.6049e-09, 2.8899e-09,
        4.2460e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.993

[Epoch: 42, batch: 90/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.4317e-03, 9.8170e-10, 4.0687e-03, 7.4645e-07, 1.7095e-01, 8.1348e-01,
        4.0676e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 42, batch: 135/226] total loss per batch: 0.386
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5070e-01, 7.5653e-03, 4.5544e-03, 8.2709e-07, 2.8138e-07, 1.0302e-02,
        2.6875e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.030

[Epoch: 42, batch: 180/226] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0146, 0.0045, 0.0083, 0.0015, 0.0018, 0.9675],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.025

[Epoch: 42, batch: 225/226] total loss per batch: 0.362
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.5311e-06, 3.3330e-03, 1.8728e-03, 1.4490e-07, 3.8215e-03, 9.9097e-01,
        4.1757e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.165

[Epoch: 43, batch: 45/226] total loss per batch: 0.380
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0540e-06, 9.3181e-10, 1.0000e+00, 2.3594e-08, 5.4719e-10, 1.2876e-09,
        2.1567e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.995

[Epoch: 43, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([7.1427e-03, 1.3991e-09, 3.7912e-03, 1.0332e-06, 2.4078e-01, 7.4511e-01,
        3.1780e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.024

[Epoch: 43, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6867e-01, 3.7950e-03, 3.5953e-03, 5.7231e-07, 2.7008e-07, 5.4041e-03,
        1.8535e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.021

[Epoch: 43, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0022, 0.0132, 0.0042, 0.0071, 0.0017, 0.0020, 0.9695],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.019

[Epoch: 43, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0996e-06, 2.2549e-03, 1.9420e-03, 9.6283e-08, 2.7121e-03, 9.9309e-01,
        2.5104e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 44, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.5016e-07, 1.1387e-09, 1.0000e+00, 1.9401e-08, 7.9233e-10, 1.1588e-09,
        3.1950e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.995

[Epoch: 44, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.8737e-03, 8.5116e-10, 4.2609e-03, 7.6335e-07, 2.1416e-01, 7.7236e-01,
        3.3458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 44, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5365e-01, 6.0326e-03, 5.0952e-03, 5.7534e-07, 2.7677e-07, 5.8627e-03,
        2.9362e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.027

[Epoch: 44, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0103, 0.0038, 0.0060, 0.0018, 0.0019, 0.9743],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.016

[Epoch: 44, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.3741e-07, 2.2167e-03, 2.0069e-03, 7.7360e-08, 2.8908e-03, 9.9288e-01,
        1.6005e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 45, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.7199e-07, 7.8615e-10, 1.0000e+00, 1.4670e-08, 6.6485e-10, 9.9871e-10,
        3.6100e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 45, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.3439e-03, 7.5724e-10, 2.9995e-03, 7.0126e-07, 2.0409e-01, 7.8555e-01,
        3.0197e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 45, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5771e-01, 4.8420e-03, 3.5884e-03, 5.8347e-07, 3.8087e-07, 5.0754e-03,
        2.8784e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.026

[Epoch: 45, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0098, 0.0037, 0.0060, 0.0016, 0.0017, 0.9755],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 45, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.2353e-07, 2.3092e-03, 1.8756e-03, 7.4178e-08, 2.4541e-03, 9.9336e-01,
        1.2392e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 46, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.9911e-07, 6.9980e-10, 1.0000e+00, 1.1418e-08, 4.2675e-10, 8.2195e-10,
        3.7414e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 46, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.2754e-03, 6.9554e-10, 2.9298e-03, 8.1086e-07, 2.4100e-01, 7.4885e-01,
        2.9430e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 46, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5534e-01, 4.7892e-03, 3.8614e-03, 5.9382e-07, 3.2236e-07, 4.8002e-03,
        3.1211e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.026

[Epoch: 46, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0082, 0.0028, 0.0047, 0.0016, 0.0016, 0.9794],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.008

[Epoch: 46, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.4251e-07, 1.7031e-03, 1.8430e-03, 6.4535e-08, 2.5001e-03, 9.9395e-01,
        1.0839e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 47, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.3413e-07, 6.0834e-10, 1.0000e+00, 1.1430e-08, 5.2391e-10, 9.0880e-10,
        2.0353e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 47, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.6596e-03, 6.1047e-10, 2.9357e-03, 6.5284e-07, 1.8072e-01, 8.1007e-01,
        2.6103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.018

[Epoch: 47, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5710e-01, 4.2999e-03, 3.3890e-03, 6.7489e-07, 3.8839e-07, 3.7016e-03,
        3.1512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.024

[Epoch: 47, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0092, 0.0029, 0.0051, 0.0018, 0.0019, 0.9773],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.011

[Epoch: 47, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.4827e-07, 2.4096e-03, 1.9634e-03, 6.3324e-08, 2.5360e-03, 9.9309e-01,
        1.0344e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 48, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.5504e-07, 4.7438e-10, 1.0000e+00, 7.6299e-09, 2.8018e-10, 6.7251e-10,
        3.6950e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 48, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.3566e-03, 7.2477e-10, 2.4499e-03, 7.0899e-07, 2.5895e-01, 7.3163e-01,
        2.6106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 48, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5067e-01, 4.8243e-03, 3.3901e-03, 6.9777e-07, 3.8648e-07, 4.8334e-03,
        3.6281e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.028

[Epoch: 48, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0086, 0.0025, 0.0045, 0.0015, 0.0014, 0.9798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.009

[Epoch: 48, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.3234e-07, 1.5333e-03, 1.9597e-03, 6.1006e-08, 2.1564e-03, 9.9435e-01,
        1.1937e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.147

[Epoch: 49, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.1733e-07, 7.2352e-10, 1.0000e+00, 1.6081e-08, 5.4471e-10, 1.2209e-09,
        2.3030e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 49, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.3225e-03, 7.1419e-10, 3.2343e-03, 6.9673e-07, 1.9330e-01, 7.9702e-01,
        3.1175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.033

[Epoch: 49, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6007e-01, 4.3522e-03, 3.0696e-03, 5.3009e-07, 3.6109e-07, 3.2580e-03,
        2.9246e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.022

[Epoch: 49, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0090, 0.0031, 0.0042, 0.0020, 0.0019, 0.9780],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.006

[Epoch: 49, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0767e-06, 2.3285e-03, 2.3195e-03, 7.2251e-08, 2.3582e-03, 9.9299e-01,
        6.7194e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 50, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.7168e-07, 6.0437e-10, 1.0000e+00, 9.2859e-09, 2.6138e-10, 9.1538e-10,
        5.1136e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 50, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.8678e-03, 1.0978e-09, 2.7085e-03, 5.9439e-07, 2.4514e-01, 7.4502e-01,
        2.2654e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 50, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5623e-01, 5.4509e-03, 2.6290e-03, 9.8309e-07, 4.6813e-07, 3.4863e-03,
        3.2202e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 50, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0081, 0.0027, 0.0043, 0.0018, 0.0018, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 50, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3564e-06, 2.0010e-03, 1.9982e-03, 1.5686e-07, 2.0752e-03, 9.9392e-01,
        8.4437e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.148

[Epoch: 51, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3609e-06, 1.3668e-09, 1.0000e+00, 2.3671e-08, 1.0990e-09, 1.7003e-09,
        4.7042e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 51, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.0363e-03, 1.3329e-09, 2.9165e-03, 1.0990e-06, 1.6813e-01, 8.2352e-01,
        2.3967e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.029

[Epoch: 51, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5041e-01, 4.2037e-03, 3.0444e-03, 1.3080e-06, 5.5971e-07, 4.7013e-03,
        3.7638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.027

[Epoch: 51, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0090, 0.0025, 0.0051, 0.0022, 0.0018, 0.9774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 51, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0289e-06, 2.3063e-03, 2.1731e-03, 8.0352e-08, 2.0466e-03, 9.9347e-01,
        1.1764e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.164

[Epoch: 52, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0496e-06, 4.9437e-10, 1.0000e+00, 1.2094e-08, 2.3241e-10, 7.4791e-10,
        6.0668e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 52, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.7499e-03, 1.3789e-09, 3.7360e-03, 1.0790e-06, 2.6051e-01, 7.2768e-01,
        3.3189e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 52, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6039e-01, 4.5718e-03, 2.3596e-03, 7.1516e-07, 3.4589e-07, 3.3885e-03,
        2.9293e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.029

[Epoch: 52, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0060, 0.0023, 0.0042, 0.0017, 0.0017, 0.9826],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 52, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1356e-06, 1.7861e-03, 2.3320e-03, 1.1721e-07, 2.0652e-03, 9.9382e-01,
        1.1024e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 53, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0492e-06, 1.1608e-09, 1.0000e+00, 1.7483e-08, 9.3082e-10, 1.2267e-09,
        6.5025e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 53, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.2535e-03, 1.3044e-09, 2.7725e-03, 9.4385e-07, 1.9747e-01, 7.9417e-01,
        2.3352e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.023

[Epoch: 53, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5478e-01, 4.0703e-03, 2.6534e-03, 9.6479e-07, 5.4436e-07, 3.2121e-03,
        3.5279e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 53, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0090, 0.0026, 0.0039, 0.0023, 0.0019, 0.9783],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 53, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3331e-06, 2.2609e-03, 1.8338e-03, 6.3901e-08, 1.7564e-03, 9.9415e-01,
        6.6801e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 54, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.8442e-07, 5.9467e-10, 1.0000e+00, 1.2586e-08, 3.3010e-10, 9.0115e-10,
        3.5049e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 54, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.5943e-03, 8.6152e-10, 3.2405e-03, 7.1007e-07, 2.1319e-01, 7.7834e-01,
        2.6369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.011

[Epoch: 54, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5573e-01, 5.2143e-03, 2.6602e-03, 7.3440e-07, 4.4162e-07, 3.3112e-03,
        3.3084e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.032

[Epoch: 54, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0060, 0.0017, 0.0036, 0.0016, 0.0017, 0.9839],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 54, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.5173e-07, 1.7948e-03, 2.1249e-03, 9.3620e-08, 1.8025e-03, 9.9428e-01,
        7.3684e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 55, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.6151e-07, 7.2753e-10, 1.0000e+00, 1.5834e-08, 5.9636e-10, 1.1374e-09,
        6.3098e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 55, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.1146e-03, 1.3680e-09, 2.3767e-03, 1.1031e-06, 2.2821e-01, 7.6360e-01,
        2.6989e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.019

[Epoch: 55, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5539e-01, 3.7195e-03, 2.9280e-03, 7.1210e-07, 5.8621e-07, 2.8060e-03,
        3.5152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.032

[Epoch: 55, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0068, 0.0021, 0.0037, 0.0018, 0.0016, 0.9824],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 55, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3845e-06, 2.1386e-03, 2.1969e-03, 6.9728e-08, 1.7939e-03, 9.9387e-01,
        9.0215e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.165

[Epoch: 56, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.8772e-07, 5.2648e-10, 1.0000e+00, 1.0755e-08, 2.2595e-10, 1.0353e-09,
        4.2186e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 56, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0317e-03, 8.4443e-10, 2.3631e-03, 4.6752e-07, 2.0655e-01, 7.8639e-01,
        2.6723e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 56, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5679e-01, 5.0369e-03, 2.2375e-03, 7.3793e-07, 4.2228e-07, 2.9300e-03,
        3.3009e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.032

[Epoch: 56, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0021, 0.0087, 0.0024, 0.0039, 0.0022, 0.0016, 0.9790],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 56, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0063e-06, 2.0266e-03, 1.7910e-03, 7.4301e-08, 1.5962e-03, 9.9458e-01,
        6.4970e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.141

[Epoch: 57, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3316e-06, 7.4746e-10, 1.0000e+00, 1.7715e-08, 6.0812e-10, 1.1682e-09,
        1.1070e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 57, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.3030e-03, 1.2591e-09, 1.6951e-03, 1.0460e-06, 2.0363e-01, 7.8868e-01,
        2.6937e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 57, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5742e-01, 5.5270e-03, 2.2682e-03, 1.6204e-06, 7.1035e-07, 2.4415e-03,
        3.2342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 57, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0022, 0.0062, 0.0023, 0.0044, 0.0024, 0.0011, 0.9814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 57, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0763e-07, 1.9993e-03, 1.8610e-03, 8.6253e-08, 1.6170e-03, 9.9452e-01,
        1.9591e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.184

[Epoch: 58, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.6927e-07, 1.1032e-09, 1.0000e+00, 2.4907e-08, 5.5219e-10, 1.4338e-09,
        3.1532e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 58, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.3892e-03, 8.1970e-10, 2.2734e-03, 6.0697e-07, 1.9458e-01, 7.9866e-01,
        2.1038e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 58, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6403e-01, 4.1202e-03, 3.1784e-03, 1.3574e-06, 4.1879e-07, 3.2545e-03,
        2.5413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 58, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0078, 0.0025, 0.0033, 0.0021, 0.0019, 0.9807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 58, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.6280e-07, 1.5244e-03, 1.9200e-03, 7.6980e-08, 2.1828e-03, 9.9437e-01,
        7.8971e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.135

[Epoch: 59, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.8554e-06, 9.3186e-10, 1.0000e+00, 3.0542e-08, 7.8945e-10, 1.6067e-09,
        8.4087e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 59, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.9911e-03, 8.7271e-10, 1.7602e-03, 1.1702e-06, 2.3697e-01, 7.5652e-01,
        1.7662e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 59, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4397e-01, 4.4393e-03, 2.2199e-03, 1.5166e-06, 1.2740e-06, 2.9265e-03,
        4.6442e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 59, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0027, 0.0064, 0.0017, 0.0030, 0.0030, 0.0017, 0.9816],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 59, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0277e-06, 2.3553e-03, 2.8525e-03, 1.1422e-07, 1.7960e-03, 9.9299e-01,
        1.7030e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 60, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1452e-06, 1.6742e-09, 1.0000e+00, 2.7491e-08, 5.2204e-10, 2.5676e-09,
        3.7308e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 60, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.7330e-03, 7.7920e-10, 3.2365e-03, 1.0426e-06, 2.2738e-01, 7.6437e-01,
        2.2713e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 60, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6115e-01, 3.7951e-03, 2.6453e-03, 1.1322e-06, 6.2731e-07, 2.5522e-03,
        2.9854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 60, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0022, 0.0085, 0.0025, 0.0029, 0.0022, 0.0013, 0.9804],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 60, batch: 225/226] total loss per batch: 0.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.9158e-06, 2.2267e-03, 1.9801e-03, 9.1991e-08, 1.9717e-03, 9.9382e-01,
        1.2019e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 61, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4596e-06, 1.1967e-09, 1.0000e+00, 2.6672e-08, 1.3363e-09, 2.0845e-09,
        2.6550e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 61, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0221e-03, 9.5220e-10, 1.7321e-03, 1.3415e-06, 1.5564e-01, 8.3841e-01,
        2.1924e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 61, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5404e-01, 7.8658e-03, 2.4055e-03, 3.0450e-06, 6.3600e-07, 3.8066e-03,
        3.1878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 61, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0022, 0.0062, 0.0017, 0.0026, 0.0018, 0.0015, 0.9841],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 61, batch: 225/226] total loss per batch: 0.361
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([3.0555e-06, 2.3324e-03, 2.5313e-03, 1.0633e-07, 2.0656e-03, 9.9307e-01,
        7.0872e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 62, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.9393e-07, 1.8904e-09, 1.0000e+00, 2.2657e-08, 2.4149e-09, 1.4229e-09,
        2.2317e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 62, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([5.7354e-03, 1.2565e-09, 5.1235e-03, 1.4408e-06, 2.8794e-01, 6.9792e-01,
        3.2874e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 62, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6788e-01, 2.8538e-03, 1.6721e-03, 8.6057e-07, 4.2494e-07, 2.8457e-03,
        2.4746e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.019

[Epoch: 62, batch: 180/226] total loss per batch: 0.375
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0057, 0.0035, 0.0043, 0.0030, 0.0017, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 62, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.0858e-07, 1.6059e-03, 2.1615e-03, 7.4995e-08, 1.3294e-03, 9.9490e-01,
        8.5471e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.127

[Epoch: 63, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8028e-06, 3.8741e-09, 1.0000e+00, 3.4127e-08, 2.0902e-09, 3.4043e-09,
        6.5596e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 63, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.5195e-03, 3.7565e-10, 3.7138e-03, 1.4476e-06, 1.7906e-01, 8.1265e-01,
        2.0586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 63, batch: 135/226] total loss per batch: 0.382
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5582e-01, 5.6891e-03, 2.0856e-03, 3.1535e-06, 4.6435e-07, 4.2680e-03,
        3.2132e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 63, batch: 180/226] total loss per batch: 0.375
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0060, 0.0032, 0.0033, 0.0019, 0.0018, 0.9824],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.014

[Epoch: 63, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.4677e-06, 2.0798e-03, 3.2308e-03, 1.3218e-07, 1.7364e-03, 9.9295e-01,
        7.6388e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.163

[Epoch: 64, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9127e-06, 1.6347e-09, 1.0000e+00, 2.3965e-08, 5.5025e-10, 1.5973e-09,
        1.5335e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 64, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.6850e-03, 9.5052e-10, 2.9373e-03, 1.6166e-06, 2.1814e-01, 7.7282e-01,
        2.4148e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.019

[Epoch: 64, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6041e-01, 5.5833e-03, 1.6449e-03, 1.1922e-06, 5.1610e-07, 2.1418e-03,
        3.0218e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 64, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0025, 0.0065, 0.0027, 0.0041, 0.0027, 0.0023, 0.9790],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 64, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.9830e-06, 2.1427e-03, 1.7468e-03, 6.4767e-08, 2.3560e-03, 9.9375e-01,
        6.6346e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.157

[Epoch: 65, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3143e-06, 1.6003e-09, 1.0000e+00, 3.5566e-08, 8.2502e-10, 2.0780e-09,
        4.7735e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 65, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.9678e-03, 1.0087e-09, 3.6100e-03, 1.6735e-06, 2.0887e-01, 7.8199e-01,
        2.5623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.018

[Epoch: 65, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5233e-01, 5.3990e-03, 3.1271e-03, 1.3931e-06, 6.5124e-07, 4.0203e-03,
        3.5125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 65, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0026, 0.0047, 0.0025, 0.0031, 0.0035, 0.0018, 0.9817],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 65, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.8851e-06, 1.9048e-03, 2.2847e-03, 8.7476e-08, 1.8636e-03, 9.9394e-01,
        1.5834e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.149

[Epoch: 66, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2405e-06, 2.2280e-09, 1.0000e+00, 3.3914e-08, 1.7293e-09, 2.0744e-09,
        4.4302e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 66, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.3713e-03, 3.1332e-09, 3.0025e-03, 2.0965e-06, 2.4839e-01, 7.4167e-01,
        2.5619e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 66, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4975e-01, 5.1528e-03, 2.1432e-03, 1.3305e-06, 6.8336e-07, 2.9819e-03,
        3.9973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 66, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0021, 0.0076, 0.0033, 0.0035, 0.0029, 0.0023, 0.9782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.008

[Epoch: 66, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.4586e-06, 1.5051e-03, 1.9275e-03, 9.2184e-08, 1.5367e-03, 9.9503e-01,
        1.4595e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 67, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1636e-06, 5.8553e-09, 1.0000e+00, 4.9169e-08, 1.5794e-09, 1.8071e-09,
        5.4568e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 67, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6501e-03, 1.2219e-09, 2.6013e-03, 1.9025e-06, 1.8319e-01, 8.1044e-01,
        2.1103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 67, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6156e-01, 4.0744e-03, 2.3139e-03, 8.4740e-07, 8.2090e-07, 2.7092e-03,
        2.9341e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.035

[Epoch: 67, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0023, 0.0067, 0.0025, 0.0053, 0.0028, 0.0023, 0.9781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 67, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([2.1547e-06, 2.4564e-03, 2.7544e-03, 1.1306e-07, 1.9546e-03, 9.9283e-01,
        7.0189e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.172

[Epoch: 68, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.5515e-06, 3.3568e-09, 1.0000e+00, 2.6047e-08, 1.9767e-09, 1.8237e-09,
        8.8674e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 68, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.7159e-03, 1.7913e-09, 3.1494e-03, 1.4954e-06, 2.3914e-01, 7.5252e-01,
        2.4723e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 68, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5241e-01, 5.7706e-03, 2.6457e-03, 1.4127e-06, 4.1002e-07, 2.8321e-03,
        3.6342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 68, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0056, 0.0021, 0.0031, 0.0022, 0.0019, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 68, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.8000e-06, 1.8629e-03, 2.1075e-03, 1.1599e-07, 1.6900e-03, 9.9434e-01,
        1.0602e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.146

[Epoch: 69, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6471e-06, 5.1828e-09, 1.0000e+00, 5.1297e-08, 3.1630e-09, 6.9218e-09,
        4.1151e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 69, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0200e-03, 1.9818e-09, 2.2747e-03, 2.0498e-06, 1.9364e-01, 7.9942e-01,
        2.6479e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 69, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6054e-01, 5.7081e-03, 2.1021e-03, 1.3536e-06, 8.8235e-07, 2.5115e-03,
        2.9135e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 69, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0021, 0.0041, 0.0023, 0.0032, 0.0024, 0.0018, 0.9840],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 69, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.6205e-06, 1.6899e-03, 3.7025e-03, 9.6420e-08, 1.6311e-03, 9.9297e-01,
        9.3676e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 70, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4787e-06, 2.3700e-09, 1.0000e+00, 3.2338e-08, 1.8655e-09, 2.0862e-09,
        4.8760e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 70, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.6224e-03, 1.2283e-09, 2.9085e-03, 1.5392e-06, 2.3585e-01, 7.5619e-01,
        2.4336e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 70, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5562e-01, 5.1609e-03, 2.4092e-03, 2.3717e-06, 6.6277e-07, 2.8805e-03,
        3.3930e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 70, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([9.5928e-04, 7.6302e-03, 1.6044e-03, 3.4483e-03, 1.9615e-03, 1.9475e-03,
        9.8245e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 70, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0527e-06, 1.9756e-03, 1.6075e-03, 4.7281e-08, 1.5840e-03, 9.9483e-01,
        1.8625e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 71, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4604e-06, 1.3206e-09, 1.0000e+00, 1.2059e-08, 1.2060e-09, 1.3390e-09,
        5.7741e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 71, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.2712e-03, 8.7791e-10, 2.6162e-03, 1.2610e-06, 1.8182e-01, 8.1106e-01,
        2.2328e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 71, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6134e-01, 3.3190e-03, 2.7311e-03, 1.5023e-06, 6.8153e-07, 2.6514e-03,
        2.9959e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 71, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0080, 0.0018, 0.0033, 0.0026, 0.0019, 0.9807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 71, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3696e-06, 1.5843e-03, 2.7161e-03, 7.1407e-08, 1.3250e-03, 9.9437e-01,
        9.0562e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.165

[Epoch: 72, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7969e-06, 3.8735e-09, 1.0000e+00, 2.2644e-08, 2.3540e-09, 2.7632e-09,
        4.2452e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 72, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8177e-03, 8.9622e-10, 1.8437e-03, 1.2143e-06, 2.4438e-01, 7.4955e-01,
        2.4006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 72, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5643e-01, 6.7286e-03, 2.1011e-03, 2.6154e-06, 1.1811e-06, 2.2596e-03,
        3.2476e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 72, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0044, 0.0017, 0.0029, 0.0023, 0.0012, 0.9856],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 72, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([3.1543e-06, 2.4224e-03, 2.8168e-03, 1.8216e-07, 1.7944e-03, 9.9296e-01,
        1.2030e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.141

[Epoch: 73, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4868e-06, 4.4519e-09, 1.0000e+00, 4.1920e-08, 3.2465e-09, 3.9598e-09,
        1.3594e-07], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 73, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.1453e-03, 1.9815e-09, 3.7798e-03, 1.7855e-06, 2.1511e-01, 7.7440e-01,
        3.5615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 73, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5790e-01, 5.2151e-03, 2.2608e-03, 2.6330e-06, 9.9409e-07, 2.4485e-03,
        3.2169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.030

[Epoch: 73, batch: 180/226] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0031, 0.0075, 0.0026, 0.0030, 0.0031, 0.0015, 0.9793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 73, batch: 225/226] total loss per batch: 0.377
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([3.0090e-07, 1.9562e-03, 1.3089e-03, 2.1984e-08, 1.8615e-03, 9.9487e-01,
        1.0591e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.119

[Epoch: 74, batch: 45/226] total loss per batch: 0.419
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.3309e-07, 4.4665e-09, 1.0000e+00, 2.6952e-08, 4.3706e-10, 1.4524e-09,
        3.9164e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 74, batch: 90/226] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1812e-03, 2.8837e-10, 2.0580e-03, 1.3208e-06, 2.2229e-01, 7.7215e-01,
        1.3145e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 74, batch: 135/226] total loss per batch: 0.403
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7063e-01, 2.5387e-03, 2.8180e-03, 6.3946e-06, 6.3026e-07, 4.2023e-03,
        1.9799e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 74, batch: 180/226] total loss per batch: 0.408
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0023, 0.0112, 0.0011, 0.0037, 0.0084, 0.0047, 0.9685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.055

[Epoch: 74, batch: 225/226] total loss per batch: 0.381
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.4274e-06, 5.1554e-03, 3.1234e-03, 3.3264e-07, 3.1421e-03, 9.8858e-01,
        3.8819e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.120

[Epoch: 75, batch: 45/226] total loss per batch: 0.395
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7803e-07, 2.8018e-09, 1.0000e+00, 2.7933e-08, 3.8719e-10, 7.0110e-09,
        1.3320e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 75, batch: 90/226] total loss per batch: 0.367
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.2103e-03, 3.2226e-10, 3.1803e-03, 2.1061e-06, 2.4917e-01, 7.4379e-01,
        1.6470e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.035

[Epoch: 75, batch: 135/226] total loss per batch: 0.393
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4806e-01, 1.2484e-02, 3.8285e-03, 3.9471e-06, 5.6897e-07, 4.9044e-03,
        3.0716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.051

[Epoch: 75, batch: 180/226] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0026, 0.0023, 0.0024, 0.0019, 0.0029, 0.0032, 0.9847],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.013

[Epoch: 75, batch: 225/226] total loss per batch: 0.364
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3843e-06, 1.3944e-03, 4.3871e-03, 1.5686e-07, 2.0027e-03, 9.9221e-01,
        3.2332e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 76, batch: 45/226] total loss per batch: 0.381
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.5412e-07, 4.8257e-09, 1.0000e+00, 4.4662e-08, 1.7284e-09, 3.1235e-09,
        5.3276e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 76, batch: 90/226] total loss per batch: 0.355
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.7214e-03, 4.0039e-10, 4.6344e-03, 2.2348e-06, 2.1699e-01, 7.7280e-01,
        2.8545e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 76, batch: 135/226] total loss per batch: 0.381
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5754e-01, 8.4728e-03, 4.5209e-03, 3.4054e-06, 1.7205e-06, 4.7362e-03,
        2.4728e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.055

[Epoch: 76, batch: 180/226] total loss per batch: 0.374
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0058, 0.3226, 0.0061, 0.0084, 0.0210, 0.0142, 0.6218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 76, batch: 225/226] total loss per batch: 0.360
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.9369e-07, 1.2780e-03, 2.8606e-03, 1.3149e-07, 1.9503e-03, 9.9391e-01,
        3.8308e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 77, batch: 45/226] total loss per batch: 0.377
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.5944e-07, 5.0171e-09, 1.0000e+00, 5.4414e-08, 1.4188e-09, 4.1057e-09,
        1.4676e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 77, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8710e-03, 3.3597e-10, 3.6734e-03, 1.6506e-06, 1.9903e-01, 7.9343e-01,
        1.9963e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 77, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6154e-01, 6.9436e-03, 3.9271e-03, 2.2336e-06, 1.3075e-06, 3.4757e-03,
        2.4113e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 77, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.9062e-03, 1.3154e-02, 7.0474e-04, 2.2675e-03, 3.8047e-03, 3.4786e-03,
        9.7468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.005

[Epoch: 77, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.6193e-07, 1.3493e-03, 2.7353e-03, 1.1066e-07, 1.9952e-03, 9.9392e-01,
        2.7688e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.139

[Epoch: 78, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.1319e-07, 3.2794e-09, 1.0000e+00, 3.6346e-08, 9.1224e-10, 2.8134e-09,
        1.1975e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 78, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8429e-03, 3.0991e-10, 3.5570e-03, 1.6017e-06, 2.2622e-01, 7.6626e-01,
        2.1111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 78, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5683e-01, 6.4743e-03, 3.8010e-03, 2.0680e-06, 1.2884e-06, 3.1961e-03,
        2.9694e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.045

[Epoch: 78, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.6919e-03, 1.1265e-02, 6.7059e-04, 2.3830e-03, 3.4814e-03, 3.1997e-03,
        9.7731e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.005

[Epoch: 78, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.4879e-07, 1.3981e-03, 2.6082e-03, 1.0232e-07, 2.0280e-03, 9.9396e-01,
        2.5781e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.144

[Epoch: 79, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9256e-07, 3.0644e-09, 1.0000e+00, 3.3178e-08, 8.3396e-10, 2.7364e-09,
        9.9322e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 79, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6286e-03, 2.6176e-10, 3.4241e-03, 1.3536e-06, 2.1487e-01, 7.7812e-01,
        1.9629e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 79, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5533e-01, 5.8977e-03, 3.4011e-03, 1.9273e-06, 1.2162e-06, 2.8512e-03,
        3.2512e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.045

[Epoch: 79, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.6786e-03, 9.9943e-03, 6.8384e-04, 2.3101e-03, 3.0473e-03, 3.0041e-03,
        9.7928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.003

[Epoch: 79, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.3778e-07, 1.4172e-03, 2.4015e-03, 8.8389e-08, 1.9253e-03, 9.9426e-01,
        2.5234e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.149

[Epoch: 80, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3470e-07, 2.5213e-09, 1.0000e+00, 2.6810e-08, 6.6550e-10, 2.3336e-09,
        1.1347e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 80, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6766e-03, 2.3336e-10, 3.2510e-03, 1.2688e-06, 2.2043e-01, 7.7270e-01,
        1.9479e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 80, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5692e-01, 5.5355e-03, 3.0675e-03, 1.6176e-06, 1.1504e-06, 2.6469e-03,
        3.1831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.045

[Epoch: 80, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.5460e-03, 9.1131e-03, 7.0891e-04, 2.4491e-03, 3.0316e-03, 2.9271e-03,
        9.8022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 80, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.8036e-07, 1.4134e-03, 2.3012e-03, 8.0084e-08, 1.9875e-03, 9.9430e-01,
        2.0813e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 81, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0117e-07, 2.1603e-09, 1.0000e+00, 2.2547e-08, 5.9036e-10, 2.0527e-09,
        8.1584e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 81, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5060e-03, 1.9968e-10, 2.9712e-03, 1.0238e-06, 2.1531e-01, 7.7833e-01,
        1.8789e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 81, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5521e-01, 5.1423e-03, 2.9664e-03, 1.4916e-06, 1.0446e-06, 2.4652e-03,
        3.4214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 81, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.4745e-03, 8.6769e-03, 7.1134e-04, 2.3514e-03, 2.5026e-03, 2.8072e-03,
        9.8148e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.002

[Epoch: 81, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.1181e-07, 1.4463e-03, 2.2333e-03, 7.3113e-08, 1.9350e-03, 9.9438e-01,
        2.1864e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 82, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.7165e-07, 1.7814e-09, 1.0000e+00, 1.8973e-08, 4.7921e-10, 1.7681e-09,
        8.6764e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 82, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5703e-03, 1.8048e-10, 2.9646e-03, 1.0223e-06, 2.1865e-01, 7.7494e-01,
        1.8740e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 82, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6010e-01, 5.0861e-03, 2.6244e-03, 1.3640e-06, 1.0395e-06, 2.2902e-03,
        2.9893e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 82, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.5565e-03, 7.9793e-03, 7.7018e-04, 2.5400e-03, 2.7527e-03, 2.6714e-03,
        9.8173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 82, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.7060e-07, 1.4084e-03, 2.1135e-03, 6.5977e-08, 1.9725e-03, 9.9450e-01,
        1.8884e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 83, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5948e-07, 1.5809e-09, 1.0000e+00, 1.6965e-08, 4.3551e-10, 1.6116e-09,
        6.4619e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 83, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4659e-03, 1.6584e-10, 2.6325e-03, 8.6915e-07, 2.2171e-01, 7.7239e-01,
        1.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 83, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5435e-01, 4.5942e-03, 2.6567e-03, 1.3124e-06, 9.5294e-07, 2.1922e-03,
        3.6209e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 83, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.3233e-03, 8.4294e-03, 7.9817e-04, 2.4194e-03, 2.2567e-03, 2.6367e-03,
        9.8214e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 83, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.5517e-07, 1.5249e-03, 2.0417e-03, 6.3050e-08, 1.9399e-03, 9.9449e-01,
        1.9107e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 84, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3533e-07, 1.2775e-09, 1.0000e+00, 1.5239e-08, 3.7161e-10, 1.4723e-09,
        5.8618e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 84, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6094e-03, 1.6593e-10, 2.8384e-03, 9.0291e-07, 2.1210e-01, 7.8167e-01,
        1.7888e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 84, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6158e-01, 5.4714e-03, 2.3619e-03, 1.2197e-06, 9.7655e-07, 2.2191e-03,
        2.8361e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 84, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.7646e-03, 7.2563e-03, 9.1816e-04, 2.7429e-03, 2.5867e-03, 2.4350e-03,
        9.8230e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 84, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9420e-07, 1.5664e-03, 2.2491e-03, 7.1062e-08, 1.9215e-03, 9.9426e-01,
        3.5072e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 85, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9981e-07, 1.8786e-09, 1.0000e+00, 1.8199e-08, 5.3928e-10, 1.9229e-09,
        1.1276e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 85, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5471e-03, 1.6723e-10, 2.5096e-03, 8.0893e-07, 2.2931e-01, 7.6482e-01,
        1.8077e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 85, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4995e-01, 4.6649e-03, 2.4257e-03, 1.1364e-06, 7.3301e-07, 2.0926e-03,
        4.0862e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 85, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([1.3629e-03, 9.8174e-03, 8.7483e-04, 2.6439e-03, 2.5359e-03, 2.6428e-03,
        9.8012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 85, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.0979e-07, 1.5207e-03, 1.7493e-03, 6.3799e-08, 1.8929e-03, 9.9484e-01,
        1.8922e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 86, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5026e-07, 1.0648e-09, 1.0000e+00, 1.4635e-08, 3.3082e-10, 1.3200e-09,
        2.7412e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 86, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6216e-03, 1.8852e-10, 2.3504e-03, 8.4763e-07, 1.9965e-01, 7.9442e-01,
        1.9554e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 86, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6719e-01, 5.5104e-03, 2.1750e-03, 1.5376e-06, 1.1662e-06, 2.6911e-03,
        2.2429e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 86, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0082, 0.0012, 0.0027, 0.0024, 0.0021, 0.9818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 86, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.7014e-07, 1.9690e-03, 2.3710e-03, 7.7395e-08, 1.8994e-03, 9.9376e-01,
        6.3708e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.157

[Epoch: 87, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.8484e-07, 2.6884e-09, 1.0000e+00, 2.5173e-08, 6.5998e-10, 2.9580e-09,
        9.9617e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 87, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8015e-03, 2.1846e-10, 2.9779e-03, 9.9273e-07, 2.3670e-01, 7.5611e-01,
        2.4139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 87, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4591e-01, 4.8230e-03, 2.7845e-03, 7.9942e-07, 8.9846e-07, 2.2434e-03,
        4.4239e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.032

[Epoch: 87, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0079, 0.0011, 0.0042, 0.0026, 0.0028, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 87, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.4696e-07, 1.8107e-03, 1.7702e-03, 8.3845e-08, 1.7276e-03, 9.9469e-01,
        2.2331e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.138

[Epoch: 88, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.8985e-07, 1.7348e-09, 1.0000e+00, 2.2718e-08, 5.5200e-10, 2.1383e-09,
        4.9883e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 88, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1337e-03, 2.7011e-10, 1.9749e-03, 9.5381e-07, 1.5816e-01, 8.3726e-01,
        1.4726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 88, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6953e-01, 4.4085e-03, 1.6871e-03, 1.3514e-06, 1.1866e-06, 2.2125e-03,
        2.2157e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 88, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0013, 0.0068, 0.0011, 0.0025, 0.0029, 0.0018, 0.9836],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.000

[Epoch: 88, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.1222e-07, 1.4111e-03, 2.7696e-03, 8.0441e-08, 1.7100e-03, 9.9411e-01,
        5.7172e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 89, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.8820e-07, 1.9667e-09, 1.0000e+00, 2.1868e-08, 3.5808e-10, 2.3632e-09,
        6.8723e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 89, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.8638e-03, 3.9235e-10, 2.4006e-03, 1.2501e-06, 3.0338e-01, 6.8910e-01,
        2.2523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 89, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5159e-01, 5.3488e-03, 2.1429e-03, 1.1208e-06, 8.9498e-07, 2.2365e-03,
        3.8681e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 89, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0112, 0.0017, 0.0043, 0.0023, 0.0029, 0.9758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 89, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2222e-06, 2.3267e-03, 2.1872e-03, 1.2655e-07, 1.4363e-03, 9.9405e-01,
        8.7825e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 90, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([9.8649e-07, 6.1675e-09, 1.0000e+00, 6.1573e-08, 1.7245e-09, 8.1768e-09,
        3.5605e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 90, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6075e-03, 7.5331e-10, 3.3513e-03, 1.5568e-06, 1.9137e-01, 8.0098e-01,
        2.6946e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 90, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6279e-01, 4.9896e-03, 2.1472e-03, 1.7454e-06, 1.3551e-06, 2.2546e-03,
        2.7820e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.029

[Epoch: 90, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0063, 0.0013, 0.0024, 0.0026, 0.0019, 0.9841],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 90, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1117e-06, 1.7387e-03, 2.8252e-03, 1.3079e-07, 2.1682e-03, 9.9327e-01,
        6.1471e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.147

[Epoch: 91, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.5359e-07, 2.6965e-09, 1.0000e+00, 1.8701e-08, 4.5687e-10, 2.0732e-09,
        3.7342e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 91, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8785e-03, 5.0545e-10, 2.9680e-03, 1.0662e-06, 2.2223e-01, 7.7087e-01,
        2.0527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 91, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5471e-01, 4.6278e-03, 2.2199e-03, 1.5032e-06, 1.2292e-06, 2.3334e-03,
        3.6108e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 91, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0074, 0.0016, 0.0029, 0.0025, 0.0020, 0.9822],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 91, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0027e-06, 2.6958e-03, 2.8752e-03, 1.5003e-07, 1.5566e-03, 9.9287e-01,
        1.4734e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 92, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1538e-06, 5.1750e-09, 1.0000e+00, 3.9787e-08, 9.5375e-10, 4.2732e-09,
        4.2582e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 92, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9710e-03, 1.1380e-09, 2.6451e-03, 1.5336e-06, 2.1265e-01, 7.8025e-01,
        2.4800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 92, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4985e-01, 7.9936e-03, 2.0946e-03, 1.4699e-06, 1.5549e-06, 3.2487e-03,
        3.6807e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 92, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0083, 0.0014, 0.0026, 0.0023, 0.0025, 0.9810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 92, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.5232e-07, 2.2205e-03, 2.1139e-03, 9.3148e-08, 2.1965e-03, 9.9347e-01,
        1.0883e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.161

[Epoch: 93, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.6468e-07, 2.9542e-09, 1.0000e+00, 4.4211e-08, 8.3350e-10, 3.6877e-09,
        1.1651e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 93, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5217e-03, 4.0727e-10, 2.6160e-03, 9.6830e-07, 2.1787e-01, 7.7592e-01,
        2.0718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 93, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6867e-01, 4.6953e-03, 1.7849e-03, 9.4633e-07, 1.4456e-06, 1.7315e-03,
        2.3111e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.031

[Epoch: 93, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0013, 0.0060, 0.0017, 0.0033, 0.0023, 0.0020, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.001

[Epoch: 93, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2538e-06, 2.6395e-03, 2.5393e-03, 1.3447e-07, 1.2749e-03, 9.9354e-01,
        1.0848e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.148

[Epoch: 94, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.2588e-06, 7.2840e-09, 1.0000e+00, 6.9838e-08, 9.4874e-10, 4.4148e-09,
        3.7246e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 94, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7347e-03, 1.7411e-09, 2.5829e-03, 1.3208e-06, 2.1494e-01, 7.7861e-01,
        2.1362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 94, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3734e-01, 6.3849e-03, 2.3211e-03, 1.2089e-06, 2.2079e-06, 3.0922e-03,
        5.0862e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 94, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0094, 0.0020, 0.0029, 0.0022, 0.0025, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 94, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0516e-06, 2.4967e-03, 2.2893e-03, 1.0869e-07, 1.5069e-03, 9.9371e-01,
        9.8312e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 95, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4760e-06, 4.1515e-09, 1.0000e+00, 5.0531e-08, 1.8515e-09, 7.1118e-09,
        7.0142e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 95, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5702e-03, 3.8981e-10, 2.0269e-03, 1.0582e-06, 2.2619e-01, 7.6856e-01,
        1.6522e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 95, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7722e-01, 3.0868e-03, 1.6238e-03, 5.5339e-07, 6.0261e-07, 1.4420e-03,
        1.6630e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 95, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0065, 0.0021, 0.0045, 0.0021, 0.0021, 0.9812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.004

[Epoch: 95, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2138e-06, 1.7911e-03, 2.9453e-03, 1.1605e-07, 1.7092e-03, 9.9355e-01,
        6.4743e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.181

[Epoch: 96, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.4488e-06, 5.8926e-09, 1.0000e+00, 7.1944e-08, 1.5872e-09, 4.8514e-09,
        8.7850e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 96, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.2421e-03, 1.4695e-09, 3.4107e-03, 1.3817e-06, 2.0515e-01, 7.8632e-01,
        2.8743e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 96, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3790e-01, 7.1666e-03, 2.4669e-03, 9.8136e-07, 1.0096e-06, 3.4636e-03,
        4.8998e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 96, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0023, 0.0072, 0.0019, 0.0023, 0.0018, 0.0020, 0.9826],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 96, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0570e-06, 1.7257e-03, 2.2692e-03, 9.8054e-08, 1.6656e-03, 9.9434e-01,
        1.3555e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 97, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.8407e-07, 1.7586e-09, 1.0000e+00, 1.7621e-08, 5.9146e-10, 1.9724e-09,
        1.7287e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 97, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7339e-03, 8.1525e-10, 1.6414e-03, 1.0481e-06, 2.0828e-01, 7.8648e-01,
        1.8705e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 97, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6531e-01, 5.3194e-03, 1.8605e-03, 9.5162e-07, 1.0864e-06, 1.5650e-03,
        2.5948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.029

[Epoch: 97, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0083, 0.0022, 0.0046, 0.0022, 0.0019, 0.9793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.005

[Epoch: 97, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.8350e-07, 1.9142e-03, 2.0515e-03, 1.4280e-07, 2.3096e-03, 9.9372e-01,
        1.5769e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.136

[Epoch: 98, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.1506e-07, 4.2538e-09, 1.0000e+00, 2.7319e-08, 1.0414e-09, 2.5715e-09,
        1.4865e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 98, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8481e-03, 1.4123e-09, 2.6671e-03, 9.2731e-07, 2.0998e-01, 7.8348e-01,
        2.0275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 98, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6151e-01, 4.0013e-03, 1.7775e-03, 7.4329e-07, 1.0752e-06, 1.8484e-03,
        3.0865e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 98, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0024, 0.0078, 0.0019, 0.0034, 0.0029, 0.0025, 0.9789],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 98, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1609e-06, 2.3681e-03, 2.4513e-03, 1.9111e-07, 3.3419e-03, 9.9184e-01,
        1.2864e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.168

[Epoch: 99, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.0752e-06, 3.0576e-09, 1.0000e+00, 2.4377e-08, 8.1526e-10, 3.3625e-09,
        4.1206e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.996

[Epoch: 99, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1271e-03, 5.6194e-10, 1.5190e-03, 1.0465e-06, 1.9972e-01, 7.9504e-01,
        1.6000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 99, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4907e-01, 7.6100e-03, 3.2644e-03, 8.7728e-07, 1.7219e-06, 2.1344e-03,
        3.7917e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 99, batch: 180/226] total loss per batch: 0.373
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0054, 0.0015, 0.0037, 0.0016, 0.0014, 0.9849],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 99, batch: 225/226] total loss per batch: 0.363
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0442e-07, 3.2408e-03, 1.3887e-03, 1.0410e-07, 2.0339e-03, 9.9334e-01,
        3.1045e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.198

[Epoch: 100, batch: 45/226] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4870e-06, 1.2982e-08, 1.0000e+00, 4.5583e-08, 9.6858e-10, 1.2440e-08,
        1.5821e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 100, batch: 90/226] total loss per batch: 0.386
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.5516e-03, 7.8315e-10, 4.4069e-03, 2.2295e-07, 2.0586e-01, 7.8491e-01,
        2.2756e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.029

[Epoch: 100, batch: 135/226] total loss per batch: 0.419
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4747e-01, 1.1061e-02, 4.2960e-03, 2.8155e-06, 1.6989e-06, 3.3845e-03,
        3.3786e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.014

[Epoch: 100, batch: 180/226] total loss per batch: 0.387
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0155, 0.0349, 0.0378, 0.0044, 0.2481, 0.0071, 0.6521],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 0.007

[Epoch: 100, batch: 225/226] total loss per batch: 0.376
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.5080e-06, 3.8128e-03, 1.9235e-03, 5.4304e-08, 4.3176e-03, 9.8994e-01,
        7.1444e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.105

[Epoch: 101, batch: 45/226] total loss per batch: 0.394
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.7372e-07, 2.7964e-09, 1.0000e+00, 4.1764e-08, 7.9526e-10, 1.3839e-09,
        6.7972e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 101, batch: 90/226] total loss per batch: 0.364
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.3471e-03, 3.3047e-10, 3.6518e-03, 2.6711e-07, 1.9912e-01, 7.9420e-01,
        1.6743e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 101, batch: 135/226] total loss per batch: 0.388
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4913e-01, 1.3723e-02, 4.2274e-03, 3.0030e-06, 1.3325e-06, 5.7547e-03,
        2.7164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 101, batch: 180/226] total loss per batch: 0.377
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([6.4474e-03, 1.3594e-02, 1.4230e-02, 4.8986e-03, 4.6534e-04, 5.1692e-03,
        9.5520e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 101, batch: 225/226] total loss per batch: 0.364
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0066e-06, 1.4953e-03, 9.9285e-04, 3.6354e-08, 2.1381e-03, 9.9537e-01,
        8.6803e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.167

[Epoch: 102, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.7871e-07, 1.6793e-09, 1.0000e+00, 3.2506e-08, 6.1029e-10, 1.7745e-09,
        4.1067e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 102, batch: 90/226] total loss per batch: 0.354
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1398e-03, 7.7024e-10, 3.3464e-03, 3.6929e-07, 2.0110e-01, 7.9161e-01,
        1.8090e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 102, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5608e-01, 8.5376e-03, 3.2242e-03, 1.8608e-06, 7.4180e-07, 3.6091e-03,
        2.8551e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 102, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([4.9698e-03, 1.0346e-02, 9.2784e-03, 5.2111e-03, 5.8448e-04, 4.7449e-03,
        9.6487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 102, batch: 225/226] total loss per batch: 0.359
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2141e-06, 2.0646e-03, 9.8283e-04, 4.1431e-08, 2.3741e-03, 9.9458e-01,
        6.5953e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.166

[Epoch: 103, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8480e-07, 2.2710e-09, 1.0000e+00, 3.3310e-08, 7.7644e-10, 1.9205e-09,
        3.8370e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 103, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9608e-03, 6.5377e-10, 3.1799e-03, 3.9023e-07, 2.0445e-01, 7.8888e-01,
        1.5269e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 103, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5561e-01, 7.8754e-03, 3.0860e-03, 1.7259e-06, 7.6951e-07, 3.2688e-03,
        3.0160e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 103, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([4.2746e-03, 9.5641e-03, 7.4921e-03, 4.8051e-03, 6.0646e-04, 4.1556e-03,
        9.6910e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 103, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1097e-06, 2.1777e-03, 9.8931e-04, 3.6081e-08, 2.1828e-03, 9.9465e-01,
        8.7189e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 104, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6354e-07, 2.1624e-09, 1.0000e+00, 3.2839e-08, 7.6382e-10, 1.6954e-09,
        4.4540e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 104, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8992e-03, 6.2638e-10, 2.9883e-03, 3.6708e-07, 2.0872e-01, 7.8484e-01,
        1.5585e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 104, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5516e-01, 7.3859e-03, 2.9997e-03, 1.5602e-06, 6.7123e-07, 2.8314e-03,
        3.1620e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.045

[Epoch: 104, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([3.8537e-03, 8.9657e-03, 6.3859e-03, 4.5123e-03, 6.1119e-04, 3.8472e-03,
        9.7182e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.005

[Epoch: 104, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.0404e-06, 2.2331e-03, 1.0284e-03, 3.3445e-08, 2.2134e-03, 9.9452e-01,
        8.4639e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 105, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4810e-07, 1.9406e-09, 1.0000e+00, 2.9098e-08, 7.0436e-10, 1.5454e-09,
        4.9639e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 105, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8594e-03, 5.3610e-10, 2.8581e-03, 3.5723e-07, 2.1180e-01, 7.8196e-01,
        1.5200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 105, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5615e-01, 6.3304e-03, 2.8610e-03, 1.4959e-06, 7.1054e-07, 2.5919e-03,
        3.2064e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 105, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([3.5300e-03, 8.3474e-03, 5.4695e-03, 4.3672e-03, 6.3400e-04, 3.6035e-03,
        9.7405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 105, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.8246e-07, 2.2540e-03, 1.0333e-03, 3.0117e-08, 2.1633e-03, 9.9455e-01,
        7.7731e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 106, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1351e-07, 1.6512e-09, 1.0000e+00, 2.5999e-08, 6.1761e-10, 1.2974e-09,
        3.8276e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 106, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8117e-03, 4.9730e-10, 2.7647e-03, 3.2225e-07, 2.1660e-01, 7.7728e-01,
        1.5380e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 106, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5578e-01, 5.8816e-03, 2.7837e-03, 1.3781e-06, 5.9877e-07, 2.3852e-03,
        3.3170e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 106, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([3.2789e-03, 7.9850e-03, 4.9186e-03, 4.2479e-03, 6.4653e-04, 3.3582e-03,
        9.7556e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 106, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0689e-07, 2.2373e-03, 1.0789e-03, 2.7564e-08, 2.1682e-03, 9.9451e-01,
        8.2842e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 107, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9386e-07, 1.5347e-09, 1.0000e+00, 2.4177e-08, 5.7174e-10, 1.2099e-09,
        3.9767e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 107, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7995e-03, 4.4063e-10, 2.6720e-03, 3.2590e-07, 2.1733e-01, 7.7670e-01,
        1.4987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 107, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5756e-01, 5.2208e-03, 2.5882e-03, 1.3565e-06, 6.9163e-07, 2.1917e-03,
        3.2436e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 107, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([3.0868e-03, 7.5469e-03, 4.3121e-03, 4.0588e-03, 6.6233e-04, 3.1889e-03,
        9.7714e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 107, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.8071e-07, 2.2408e-03, 1.0681e-03, 2.6037e-08, 2.0698e-03, 9.9462e-01,
        7.5667e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 108, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.6638e-07, 1.2970e-09, 1.0000e+00, 2.0827e-08, 4.8908e-10, 1.0388e-09,
        3.0861e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 108, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6858e-03, 4.1674e-10, 2.5767e-03, 2.8831e-07, 2.1776e-01, 7.7643e-01,
        1.5476e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 108, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5682e-01, 5.4742e-03, 2.5848e-03, 1.1728e-06, 5.8085e-07, 2.1057e-03,
        3.3013e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 108, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.9614e-03, 7.4251e-03, 3.9674e-03, 3.9911e-03, 7.0307e-04, 3.1012e-03,
        9.7785e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 108, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.1764e-07, 2.1981e-03, 1.1240e-03, 2.3138e-08, 2.0843e-03, 9.9459e-01,
        8.4482e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 109, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5434e-07, 1.1364e-09, 1.0000e+00, 1.9952e-08, 4.5547e-10, 9.4131e-10,
        4.1052e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 109, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7002e-03, 3.7239e-10, 2.4326e-03, 2.8076e-07, 2.1769e-01, 7.7668e-01,
        1.4975e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 109, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5780e-01, 4.9583e-03, 2.4488e-03, 1.1177e-06, 6.0141e-07, 2.0273e-03,
        3.2761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 109, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.8262e-03, 7.1513e-03, 3.6228e-03, 3.9380e-03, 7.3191e-04, 2.9340e-03,
        9.7880e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 109, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.4672e-07, 2.1643e-03, 1.1423e-03, 2.0753e-08, 2.0151e-03, 9.9468e-01,
        6.9281e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 110, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2261e-07, 1.0098e-09, 1.0000e+00, 1.7024e-08, 3.9016e-10, 8.2661e-10,
        2.9267e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 110, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6584e-03, 3.3501e-10, 2.4084e-03, 2.5375e-07, 2.1889e-01, 7.7552e-01,
        1.5258e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 110, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5898e-01, 5.0744e-03, 2.3669e-03, 9.7185e-07, 4.9632e-07, 1.8595e-03,
        3.1718e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 110, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.7812e-03, 6.8656e-03, 3.3999e-03, 4.0328e-03, 7.5521e-04, 2.7931e-03,
        9.7937e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 110, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.1657e-07, 2.1601e-03, 1.2010e-03, 1.8831e-08, 2.0535e-03, 9.9458e-01,
        7.6736e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 111, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1374e-07, 8.1142e-10, 1.0000e+00, 1.6150e-08, 3.5764e-10, 7.2381e-10,
        3.0497e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 111, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6282e-03, 3.1190e-10, 2.1727e-03, 2.4744e-07, 2.1733e-01, 7.7741e-01,
        1.4562e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 111, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5688e-01, 4.8404e-03, 2.2398e-03, 9.8120e-07, 5.3436e-07, 1.7858e-03,
        3.4250e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 111, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.5073e-03, 6.7907e-03, 3.1818e-03, 3.5732e-03, 7.5100e-04, 2.6636e-03,
        9.8053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 111, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3852e-07, 2.0748e-03, 1.1998e-03, 1.6974e-08, 1.9911e-03, 9.9473e-01,
        7.0253e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.149

[Epoch: 112, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.9063e-07, 7.8516e-10, 1.0000e+00, 1.3746e-08, 3.0313e-10, 6.5024e-10,
        2.5596e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 112, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5863e-03, 2.8513e-10, 2.2574e-03, 2.0992e-07, 2.1532e-01, 7.7934e-01,
        1.4915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 112, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5863e-01, 4.6436e-03, 2.1987e-03, 8.8778e-07, 4.8563e-07, 1.8138e-03,
        3.2712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 112, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.7259e-03, 6.4052e-03, 3.0130e-03, 4.0343e-03, 8.0322e-04, 2.4750e-03,
        9.8054e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 112, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.7236e-07, 2.1408e-03, 1.3155e-03, 1.5948e-08, 1.9186e-03, 9.9462e-01,
        5.9658e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.162

[Epoch: 113, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.7103e-07, 6.0662e-10, 1.0000e+00, 1.3388e-08, 2.8718e-10, 5.6636e-10,
        2.9715e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 113, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6359e-03, 2.7449e-10, 2.1652e-03, 2.1474e-07, 2.1435e-01, 7.8044e-01,
        1.4041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 113, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5811e-01, 4.8169e-03, 2.0383e-03, 9.0603e-07, 4.1440e-07, 1.8235e-03,
        3.3214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.046

[Epoch: 113, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.3122e-03, 6.2379e-03, 2.9156e-03, 3.3079e-03, 8.8769e-04, 2.4641e-03,
        9.8187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 113, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.4543e-07, 2.1429e-03, 1.2976e-03, 1.9605e-08, 1.9444e-03, 9.9461e-01,
        7.8827e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.139

[Epoch: 114, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3035e-07, 9.5403e-10, 1.0000e+00, 1.5784e-08, 3.1136e-10, 6.8093e-10,
        2.2482e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 114, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7339e-03, 4.2870e-10, 2.2315e-03, 2.4404e-07, 2.2977e-01, 7.6453e-01,
        1.7350e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 114, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5690e-01, 5.0822e-03, 1.8916e-03, 8.8843e-07, 5.0242e-07, 1.7811e-03,
        3.4344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 114, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([2.6694e-03, 5.2533e-03, 3.1168e-03, 4.1862e-03, 8.2947e-04, 2.1875e-03,
        9.8176e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 114, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.1187e-07, 2.4705e-03, 1.4568e-03, 1.7206e-08, 1.9881e-03, 9.9408e-01,
        6.3991e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.172

[Epoch: 115, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.8957e-07, 6.8560e-10, 1.0000e+00, 1.4319e-08, 3.9114e-10, 5.4059e-10,
        2.8681e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 115, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6423e-03, 1.9804e-10, 2.1348e-03, 1.9440e-07, 1.9950e-01, 7.9518e-01,
        1.5380e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 115, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5851e-01, 4.6005e-03, 2.4812e-03, 6.2747e-07, 4.0865e-07, 1.4224e-03,
        3.2988e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.047

[Epoch: 115, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0023, 0.0074, 0.0022, 0.0043, 0.0011, 0.0026, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 115, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.1755e-07, 1.9635e-03, 1.4535e-03, 2.2777e-08, 1.8713e-03, 9.9471e-01,
        7.4895e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.134

[Epoch: 116, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3164e-07, 8.3428e-10, 1.0000e+00, 1.3206e-08, 2.4726e-10, 7.5664e-10,
        3.2439e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 116, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5715e-03, 4.4313e-10, 1.9543e-03, 2.4718e-07, 2.3153e-01, 7.6326e-01,
        1.6902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 116, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5582e-01, 5.8668e-03, 2.0807e-03, 1.2131e-06, 6.2274e-07, 1.8228e-03,
        3.4406e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 116, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0028, 0.0064, 0.0041, 0.0044, 0.0016, 0.0025, 0.9782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 116, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.1386e-07, 2.3250e-03, 1.4753e-03, 1.9455e-08, 1.6430e-03, 9.9456e-01,
        7.4391e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.173

[Epoch: 117, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3366e-07, 7.7791e-10, 1.0000e+00, 1.3375e-08, 4.0691e-10, 5.9340e-10,
        1.9565e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 117, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5053e-03, 1.9702e-10, 2.1520e-03, 2.3006e-07, 2.2340e-01, 7.7137e-01,
        1.5758e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 117, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6372e-01, 4.2244e-03, 1.9165e-03, 9.7109e-07, 5.3294e-07, 1.4778e-03,
        2.8662e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.050

[Epoch: 117, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0045, 0.0021, 0.0027, 0.0013, 0.0020, 0.9855],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 117, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1543e-06, 1.9419e-03, 1.8853e-03, 2.6939e-08, 1.8426e-03, 9.9433e-01,
        4.8419e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.128

[Epoch: 118, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8226e-07, 1.0167e-09, 1.0000e+00, 2.2141e-08, 4.1560e-10, 1.0376e-09,
        3.0923e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 118, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9332e-03, 4.9414e-10, 1.9998e-03, 3.1842e-07, 2.2555e-01, 7.6907e-01,
        1.4469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 118, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5179e-01, 6.3796e-03, 2.0859e-03, 8.7334e-07, 4.3664e-07, 1.3490e-03,
        3.8396e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 118, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0024, 0.0092, 0.0034, 0.0040, 0.0017, 0.0022, 0.9772],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 118, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.3623e-07, 1.7760e-03, 1.6919e-03, 2.7691e-08, 1.4942e-03, 9.9504e-01,
        8.1304e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.172

[Epoch: 119, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3660e-07, 1.7994e-09, 1.0000e+00, 3.7550e-08, 9.6475e-10, 1.2632e-09,
        4.9613e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 119, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7825e-03, 3.7704e-10, 2.2733e-03, 3.8622e-07, 2.0845e-01, 7.8547e-01,
        2.0191e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 119, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6242e-01, 3.8398e-03, 2.0619e-03, 8.4300e-07, 4.8198e-07, 1.5665e-03,
        3.0107e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.045

[Epoch: 119, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0053, 0.0023, 0.0035, 0.0018, 0.0027, 0.9826],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 119, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2408e-06, 2.3184e-03, 2.4260e-03, 3.5362e-08, 2.1575e-03, 9.9310e-01,
        9.5046e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.130

[Epoch: 120, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4291e-07, 8.0067e-10, 1.0000e+00, 2.0680e-08, 7.0670e-10, 1.3938e-09,
        4.1786e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 120, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8493e-03, 3.4362e-10, 2.3547e-03, 3.1886e-07, 2.2033e-01, 7.7394e-01,
        1.5256e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 120, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6001e-01, 4.3644e-03, 1.9343e-03, 6.7678e-07, 5.0840e-07, 1.4462e-03,
        3.2244e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 120, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0064, 0.0029, 0.0032, 0.0012, 0.0016, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 120, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.6176e-07, 1.1506e-03, 1.2879e-03, 2.7686e-08, 1.3007e-03, 9.9626e-01,
        9.1431e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.163

[Epoch: 121, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.4833e-07, 1.8947e-09, 1.0000e+00, 4.1114e-08, 1.2743e-09, 2.7520e-09,
        5.2314e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 121, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5744e-03, 6.6533e-10, 2.0981e-03, 4.0353e-07, 2.1167e-01, 7.8246e-01,
        2.1881e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 121, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5534e-01, 4.6802e-03, 1.9309e-03, 1.2570e-06, 6.1423e-07, 1.7028e-03,
        3.6341e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.048

[Epoch: 121, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0074, 0.0032, 0.0035, 0.0018, 0.0026, 0.9796],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 121, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2742e-06, 2.2539e-03, 2.2168e-03, 3.6567e-08, 2.2346e-03, 9.9329e-01,
        2.6480e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 122, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.1200e-07, 9.3343e-10, 1.0000e+00, 1.5678e-08, 8.1142e-10, 1.4276e-09,
        6.2846e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 122, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8122e-03, 3.8226e-10, 2.4528e-03, 2.9450e-07, 2.1299e-01, 7.8085e-01,
        1.8945e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 122, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6439e-01, 4.8006e-03, 1.7365e-03, 6.0240e-07, 4.9699e-07, 1.8412e-03,
        2.7234e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.044

[Epoch: 122, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0059, 0.0024, 0.0034, 0.0015, 0.0020, 0.9828],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 122, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.8902e-07, 1.3826e-03, 1.9159e-03, 3.3284e-08, 1.4890e-03, 9.9521e-01,
        7.0524e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.138

[Epoch: 123, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3871e-07, 1.2424e-09, 1.0000e+00, 2.1843e-08, 6.7899e-10, 2.2607e-09,
        3.3939e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 123, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5210e-03, 5.1250e-10, 1.5641e-03, 4.0079e-07, 2.2237e-01, 7.7249e-01,
        2.0487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 123, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5243e-01, 4.7777e-03, 2.0755e-03, 1.1497e-06, 6.7271e-07, 1.7257e-03,
        3.8991e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.050

[Epoch: 123, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0023, 0.0072, 0.0026, 0.0040, 0.0013, 0.0018, 0.9807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 123, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1980e-06, 2.2050e-03, 1.7555e-03, 3.5586e-08, 1.6134e-03, 9.9442e-01,
        1.7832e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.176

[Epoch: 124, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.6131e-07, 1.1855e-09, 1.0000e+00, 1.5012e-08, 1.0416e-09, 1.6355e-09,
        6.7704e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 124, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8605e-03, 7.4845e-10, 2.4710e-03, 3.9175e-07, 2.2029e-01, 7.7301e-01,
        2.3668e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 124, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5474e-01, 5.4133e-03, 2.1431e-03, 1.0309e-06, 4.6211e-07, 2.0026e-03,
        3.5699e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 124, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0060, 0.0016, 0.0032, 0.0015, 0.0021, 0.9839],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 124, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.5221e-07, 2.0201e-03, 1.8338e-03, 3.7002e-08, 2.0004e-03, 9.9414e-01,
        1.1098e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.124

[Epoch: 125, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6145e-07, 1.5910e-09, 1.0000e+00, 1.6824e-08, 1.3477e-09, 1.6353e-09,
        6.1137e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 125, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4614e-03, 6.2561e-10, 1.3740e-03, 4.4208e-07, 2.1692e-01, 7.7906e-01,
        1.1850e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 125, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6779e-01, 4.0992e-03, 1.6333e-03, 1.0232e-06, 6.7868e-07, 1.6121e-03,
        2.4861e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.048

[Epoch: 125, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0059, 0.0028, 0.0040, 0.0018, 0.0019, 0.9818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.000

[Epoch: 125, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3287e-06, 1.5245e-03, 1.6995e-03, 4.2398e-08, 1.3983e-03, 9.9538e-01,
        1.2899e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.171

[Epoch: 126, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.9748e-07, 1.3890e-09, 1.0000e+00, 2.3145e-08, 9.1318e-10, 1.3333e-09,
        5.7533e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 126, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8911e-03, 1.2489e-09, 2.1477e-03, 4.4709e-07, 2.1086e-01, 7.8273e-01,
        2.3677e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 126, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.3720e-01, 1.3472e-02, 2.5174e-03, 1.0074e-06, 1.0244e-06, 2.5095e-03,
        4.4299e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 126, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0070, 0.0020, 0.0038, 0.0015, 0.0020, 0.9817],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 126, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3093e-06, 2.2423e-03, 1.8532e-03, 6.0234e-08, 2.3729e-03, 9.9353e-01,
        2.2158e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.132

[Epoch: 127, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.5241e-07, 2.1032e-09, 1.0000e+00, 2.0165e-08, 1.1323e-09, 1.9691e-09,
        4.4012e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 127, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4682e-03, 6.9830e-10, 2.5484e-03, 4.5773e-07, 2.0921e-01, 7.8565e-01,
        1.1159e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 127, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7732e-01, 2.1792e-03, 1.6941e-03, 1.9616e-06, 7.2480e-07, 1.2611e-03,
        1.7542e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 127, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0079, 0.0021, 0.0039, 0.0019, 0.0019, 0.9809],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 127, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.3286e-06, 1.7057e-03, 1.8833e-03, 4.5705e-08, 1.6116e-03, 9.9480e-01,
        3.2640e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.171

[Epoch: 128, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.1907e-07, 9.6449e-10, 1.0000e+00, 2.0273e-08, 7.7144e-10, 2.8787e-09,
        1.6320e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 128, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.3387e-03, 1.7345e-09, 1.3910e-03, 5.0288e-07, 2.4824e-01, 7.4548e-01,
        2.5480e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 128, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5176e-01, 3.5977e-03, 2.0897e-03, 1.1024e-06, 1.1028e-06, 2.3802e-03,
        4.0172e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.043

[Epoch: 128, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0062, 0.0021, 0.0032, 0.0016, 0.0019, 0.9832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 128, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.2387e-06, 2.8703e-03, 1.8660e-03, 6.7800e-08, 2.0854e-03, 9.9318e-01,
        2.1725e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.142

[Epoch: 129, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.2287e-07, 1.8370e-09, 1.0000e+00, 2.0486e-08, 1.1976e-09, 1.8790e-09,
        2.6018e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 129, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2784e-03, 5.5549e-10, 4.8434e-03, 5.3209e-07, 1.6191e-01, 8.3076e-01,
        1.2092e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.039

[Epoch: 129, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5710e-01, 5.1181e-03, 2.0661e-03, 1.0926e-06, 1.0780e-06, 2.0433e-03,
        3.3672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 129, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0080, 0.0024, 0.0036, 0.0019, 0.0017, 0.9804],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 129, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.3031e-07, 1.4582e-03, 1.7574e-03, 3.1531e-08, 1.3711e-03, 9.9541e-01,
        1.2222e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 130, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.1997e-07, 1.6444e-09, 1.0000e+00, 2.7182e-08, 2.1040e-09, 3.3556e-09,
        1.6686e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.997

[Epoch: 130, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.7686e-03, 1.8368e-09, 2.7859e-03, 4.1842e-07, 2.6045e-01, 7.3129e-01,
        2.7073e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 130, batch: 135/226] total loss per batch: 0.383
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5601e-01, 4.2990e-03, 1.2079e-03, 7.7726e-07, 9.6878e-07, 1.3429e-03,
        3.7135e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 130, batch: 180/226] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0020, 0.0099, 0.0025, 0.0023, 0.0020, 0.0031, 0.9782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.003

[Epoch: 130, batch: 225/226] total loss per batch: 0.362
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.0028e-07, 2.5981e-03, 1.8220e-03, 5.2876e-08, 1.4435e-03, 9.9414e-01,
        2.9002e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.133

[Epoch: 131, batch: 45/226] total loss per batch: 0.384
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.3290e-06, 3.4830e-09, 1.0000e+00, 3.9455e-08, 2.9004e-09, 5.5871e-09,
        1.6261e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 131, batch: 90/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5589e-03, 3.1633e-09, 1.5805e-03, 8.0476e-07, 1.6982e-01, 8.2451e-01,
        2.5293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 131, batch: 135/226] total loss per batch: 0.388
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6136e-01, 5.6964e-03, 2.5105e-03, 1.0988e-06, 1.0745e-06, 4.1701e-03,
        2.6260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 131, batch: 180/226] total loss per batch: 0.376
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0011, 0.0117, 0.0042, 0.0041, 0.0013, 0.0016, 0.9759],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.021

[Epoch: 131, batch: 225/226] total loss per batch: 0.362
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.3354e-07, 2.7630e-03, 1.5137e-03, 2.6874e-08, 9.9684e-04, 9.9473e-01,
        1.2485e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.162

[Epoch: 132, batch: 45/226] total loss per batch: 0.378
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([1.1888e-06, 4.3674e-09, 1.0000e+00, 4.2739e-08, 2.7008e-09, 1.0136e-08,
        2.8720e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 132, batch: 90/226] total loss per batch: 0.353
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([4.2967e-03, 2.3486e-09, 2.5122e-03, 1.4024e-06, 2.4469e-01, 7.4428e-01,
        4.2123e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 132, batch: 135/226] total loss per batch: 0.380
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6207e-01, 5.6853e-03, 2.6111e-03, 1.8014e-06, 1.3623e-06, 4.3713e-03,
        2.5258e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 132, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0011, 0.0098, 0.0028, 0.0030, 0.0011, 0.0012, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.022

[Epoch: 132, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0087e-07, 3.2661e-03, 1.9520e-03, 4.7933e-08, 1.4395e-03, 9.9334e-01,
        1.4149e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 133, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.0366e-07, 3.1573e-09, 1.0000e+00, 2.8421e-08, 2.3788e-09, 6.1343e-09,
        3.1003e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 133, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.4506e-03, 1.5491e-09, 1.6841e-03, 7.3052e-07, 1.9830e-01, 7.9521e-01,
        2.3611e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 133, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5884e-01, 5.0030e-03, 2.6001e-03, 1.1746e-06, 1.1650e-06, 3.9896e-03,
        2.9564e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 133, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0087, 0.0023, 0.0029, 0.0014, 0.0013, 0.9821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.021

[Epoch: 133, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.3069e-07, 2.7214e-03, 2.0666e-03, 6.3311e-08, 1.2886e-03, 9.9392e-01,
        1.3702e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 134, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.9986e-07, 3.2109e-09, 1.0000e+00, 3.0567e-08, 3.2352e-09, 7.9750e-09,
        1.9527e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 134, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.5490e-03, 1.4393e-09, 1.7481e-03, 6.9634e-07, 2.1949e-01, 7.7439e-01,
        1.8251e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 134, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6136e-01, 4.4076e-03, 2.2759e-03, 7.9748e-07, 9.8742e-07, 3.2336e-03,
        2.8721e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 134, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0080, 0.0024, 0.0032, 0.0015, 0.0014, 0.9821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 134, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.1549e-07, 2.6350e-03, 1.8746e-03, 4.6952e-08, 1.3557e-03, 9.9413e-01,
        1.1580e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 135, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.9268e-07, 2.2319e-09, 1.0000e+00, 2.2925e-08, 2.1822e-09, 5.3725e-09,
        1.3537e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 135, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.3020e-03, 1.2933e-09, 1.8181e-03, 6.6666e-07, 2.1664e-01, 7.7737e-01,
        1.8669e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 135, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5696e-01, 4.4896e-03, 2.2452e-03, 7.6471e-07, 9.5627e-07, 3.0856e-03,
        3.3222e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 135, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0071, 0.0022, 0.0032, 0.0014, 0.0013, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 135, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3876e-07, 2.3007e-03, 1.9054e-03, 4.3174e-08, 1.3962e-03, 9.9440e-01,
        1.1496e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 136, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.4318e-07, 1.9712e-09, 1.0000e+00, 2.0664e-08, 2.0132e-09, 4.6822e-09,
        1.1315e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 136, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1774e-03, 1.1166e-09, 1.7605e-03, 5.7703e-07, 2.1577e-01, 7.7845e-01,
        1.8406e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 136, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5660e-01, 4.4652e-03, 2.1212e-03, 6.9234e-07, 9.0284e-07, 2.9326e-03,
        3.3880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 136, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0070, 0.0021, 0.0030, 0.0015, 0.0013, 0.9839],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.017

[Epoch: 136, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.0401e-07, 2.2391e-03, 1.8867e-03, 3.8011e-08, 1.4190e-03, 9.9445e-01,
        1.0494e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 137, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.4961e-07, 1.6602e-09, 1.0000e+00, 1.7926e-08, 1.6418e-09, 4.1004e-09,
        1.0559e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 137, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0963e-03, 1.0118e-09, 1.6871e-03, 5.2475e-07, 2.2109e-01, 7.7339e-01,
        1.7319e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 137, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5790e-01, 4.5591e-03, 2.0518e-03, 6.2673e-07, 8.2716e-07, 2.6012e-03,
        3.2889e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 137, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0069, 0.0021, 0.0032, 0.0014, 0.0013, 0.9837],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 137, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.5156e-07, 2.1072e-03, 1.8674e-03, 3.6623e-08, 1.4590e-03, 9.9457e-01,
        1.0408e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.157

[Epoch: 138, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7459e-07, 1.3797e-09, 1.0000e+00, 1.5166e-08, 1.3321e-09, 3.1526e-09,
        9.0154e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 138, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9707e-03, 7.9709e-10, 1.7546e-03, 4.4926e-07, 2.2050e-01, 7.7401e-01,
        1.7610e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 138, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5542e-01, 4.6397e-03, 2.0084e-03, 6.1015e-07, 7.9690e-07, 2.7173e-03,
        3.5213e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 138, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0066, 0.0019, 0.0032, 0.0016, 0.0013, 0.9840],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 138, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.4639e-07, 2.1697e-03, 1.8894e-03, 3.3823e-08, 1.4996e-03, 9.9444e-01,
        9.1115e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 139, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.4933e-07, 1.3238e-09, 1.0000e+00, 1.5096e-08, 1.2288e-09, 3.3580e-09,
        8.3051e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 139, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9703e-03, 8.9151e-10, 1.6931e-03, 4.6555e-07, 2.1763e-01, 7.7702e-01,
        1.6924e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 139, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6119e-01, 4.6150e-03, 1.9548e-03, 5.2728e-07, 6.9378e-07, 2.4057e-03,
        2.9832e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 139, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0070, 0.0020, 0.0032, 0.0015, 0.0013, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 139, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.7014e-07, 1.9345e-03, 1.8188e-03, 2.7600e-08, 1.5200e-03, 9.9473e-01,
        9.2341e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.157

[Epoch: 140, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5705e-07, 9.4265e-10, 1.0000e+00, 1.0786e-08, 9.2442e-10, 2.2308e-09,
        7.0971e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 140, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8369e-03, 6.4771e-10, 1.8029e-03, 3.5578e-07, 2.1801e-01, 7.7667e-01,
        1.6837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 140, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5211e-01, 4.8467e-03, 1.9125e-03, 5.1702e-07, 7.1206e-07, 2.4744e-03,
        3.8655e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 140, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0064, 0.0019, 0.0034, 0.0017, 0.0013, 0.9837],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 140, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.9335e-07, 2.1874e-03, 1.8660e-03, 3.0052e-08, 1.5592e-03, 9.9439e-01,
        9.1275e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 141, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5661e-07, 1.0274e-09, 1.0000e+00, 1.1785e-08, 9.3478e-10, 2.8714e-09,
        6.2663e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 141, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7377e-03, 6.2446e-10, 1.6084e-03, 3.6667e-07, 2.1666e-01, 7.7843e-01,
        1.5678e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 141, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6496e-01, 4.4144e-03, 1.7752e-03, 4.5692e-07, 6.4620e-07, 2.1903e-03,
        2.6656e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 141, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0068, 0.0018, 0.0032, 0.0016, 0.0014, 0.9836],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 141, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([3.9437e-07, 1.8519e-03, 1.7207e-03, 2.3425e-08, 1.5820e-03, 9.9484e-01,
        1.1576e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 142, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.4682e-07, 6.3924e-10, 1.0000e+00, 8.1413e-09, 6.4939e-10, 1.4521e-09,
        6.0665e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 142, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1107e-03, 7.3425e-10, 2.1252e-03, 3.5820e-07, 2.3167e-01, 7.6215e-01,
        1.9403e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 142, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4927e-01, 5.6706e-03, 1.7304e-03, 4.4579e-07, 5.5532e-07, 2.2790e-03,
        4.1053e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 142, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0068, 0.0021, 0.0035, 0.0022, 0.0016, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 142, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.0162e-07, 2.0318e-03, 1.7102e-03, 3.0779e-08, 1.5351e-03, 9.9472e-01,
        6.2684e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 143, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.4661e-07, 1.4446e-09, 1.0000e+00, 1.9296e-08, 1.3927e-09, 4.8036e-09,
        5.2406e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 143, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.3345e-03, 7.5997e-10, 1.5499e-03, 4.4026e-07, 2.0002e-01, 7.9571e-01,
        1.3845e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 143, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6897e-01, 4.2194e-03, 1.7758e-03, 4.6557e-07, 7.5073e-07, 2.0970e-03,
        2.2932e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 143, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0013, 0.0061, 0.0015, 0.0033, 0.0013, 0.0012, 0.9853],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 143, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.0216e-07, 2.3763e-03, 2.0311e-03, 4.0789e-08, 2.0187e-03, 9.9357e-01,
        2.1234e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.148

[Epoch: 144, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.1300e-07, 9.4206e-10, 1.0000e+00, 1.6584e-08, 1.3137e-09, 2.4904e-09,
        1.6247e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 144, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.2255e-03, 8.6348e-10, 2.7503e-03, 6.2143e-07, 2.3684e-01, 7.5583e-01,
        2.3510e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 144, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4340e-01, 5.3223e-03, 1.9102e-03, 7.1456e-07, 9.5692e-07, 2.7991e-03,
        4.6568e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 144, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0072, 0.0021, 0.0043, 0.0030, 0.0019, 0.9796],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.018

[Epoch: 144, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.2125e-07, 1.8673e-03, 2.1750e-03, 4.4001e-08, 1.8616e-03, 9.9410e-01,
        1.2062e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 145, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.5729e-07, 1.7195e-09, 1.0000e+00, 4.4530e-08, 2.6849e-09, 5.1613e-09,
        6.2243e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 145, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4913e-03, 2.0794e-09, 1.6511e-03, 5.0514e-07, 1.8471e-01, 8.1090e-01,
        1.2464e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 145, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7237e-01, 3.3624e-03, 1.9689e-03, 9.1256e-07, 7.1968e-07, 2.2205e-03,
        2.0076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 145, batch: 180/226] total loss per batch: 0.372
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0055, 0.0018, 0.0041, 0.0019, 0.0015, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 145, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.9163e-07, 1.8398e-03, 1.7118e-03, 5.4649e-08, 1.7054e-03, 9.9474e-01,
        2.4095e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.164

[Epoch: 146, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.8226e-07, 1.0359e-09, 1.0000e+00, 1.7082e-08, 1.1184e-09, 2.9636e-09,
        6.9440e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 146, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9985e-03, 7.9605e-10, 2.2921e-03, 6.3371e-07, 2.2868e-01, 7.6501e-01,
        2.0193e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 146, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5146e-01, 6.1487e-03, 1.7182e-03, 4.7839e-07, 7.7551e-07, 2.5324e-03,
        3.8142e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 146, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0069, 0.0013, 0.0047, 0.0019, 0.0019, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 146, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3523e-07, 2.0703e-03, 1.8212e-03, 4.1077e-08, 1.9131e-03, 9.9419e-01,
        2.5522e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.147

[Epoch: 147, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.7784e-07, 9.0904e-10, 1.0000e+00, 1.7048e-08, 7.7456e-10, 2.1565e-09,
        1.0858e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 147, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7006e-03, 1.4125e-09, 2.4047e-03, 4.2637e-07, 2.1091e-01, 7.8359e-01,
        1.3957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 147, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5961e-01, 5.2129e-03, 1.9358e-03, 6.2186e-07, 7.4559e-07, 1.9363e-03,
        3.1300e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 147, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0070, 0.0015, 0.0036, 0.0016, 0.0014, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.001

[Epoch: 147, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.3079e-07, 1.5453e-03, 1.8049e-03, 5.1172e-08, 1.7870e-03, 9.9486e-01,
        1.8248e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 148, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.3008e-07, 1.8103e-09, 1.0000e+00, 3.1958e-08, 2.2033e-09, 5.0878e-09,
        6.7621e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 148, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8729e-03, 7.8278e-10, 2.1245e-03, 5.3500e-07, 2.2134e-01, 7.7307e-01,
        1.5950e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 148, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5748e-01, 5.3238e-03, 1.3935e-03, 4.9292e-07, 7.9472e-07, 2.2081e-03,
        3.3594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 148, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0057, 0.0014, 0.0040, 0.0026, 0.0016, 0.9829],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 148, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.8176e-07, 1.7001e-03, 1.8319e-03, 3.7920e-08, 1.7038e-03, 9.9476e-01,
        1.4986e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.167

[Epoch: 149, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.6295e-07, 1.8921e-09, 1.0000e+00, 2.1453e-08, 1.5064e-09, 3.7938e-09,
        5.5874e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 149, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9897e-03, 1.6482e-09, 2.6810e-03, 5.9992e-07, 2.1686e-01, 7.7642e-01,
        2.0512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 149, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5656e-01, 4.8142e-03, 1.8469e-03, 6.7750e-07, 7.4248e-07, 2.6547e-03,
        3.4123e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 149, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0083, 0.0014, 0.0030, 0.0020, 0.0017, 0.9818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 149, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.0011e-07, 2.1145e-03, 1.4540e-03, 4.5352e-08, 1.5889e-03, 9.9484e-01,
        2.5281e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 150, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([6.0733e-07, 1.3847e-09, 1.0000e+00, 2.2476e-08, 1.9474e-09, 3.2460e-09,
        6.2376e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 150, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0459e-03, 1.4656e-09, 1.8909e-03, 5.2905e-07, 2.3245e-01, 7.6160e-01,
        2.0136e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 150, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5612e-01, 5.0590e-03, 1.7428e-03, 7.1387e-07, 7.4159e-07, 2.5663e-03,
        3.4515e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 150, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0052, 0.0019, 0.0043, 0.0021, 0.0015, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.004

[Epoch: 150, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0262e-07, 1.8592e-03, 1.8658e-03, 4.4647e-08, 1.9175e-03, 9.9436e-01,
        3.4932e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.151

[Epoch: 151, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.2561e-07, 1.9896e-09, 1.0000e+00, 3.5118e-08, 1.5493e-09, 4.2914e-09,
        1.0565e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 151, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6123e-03, 1.2062e-09, 2.1758e-03, 4.4645e-07, 1.8415e-01, 8.1009e-01,
        1.9756e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 151, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6575e-01, 4.4158e-03, 1.5911e-03, 5.4818e-07, 5.0244e-07, 1.5734e-03,
        2.6673e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 151, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0072, 0.0015, 0.0032, 0.0017, 0.0018, 0.9829],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 151, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3957e-07, 1.6765e-03, 1.6549e-03, 3.5672e-08, 1.5362e-03, 9.9513e-01,
        1.5663e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 152, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.3707e-07, 1.2356e-09, 1.0000e+00, 1.6656e-08, 1.4698e-09, 3.4302e-09,
        6.7380e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 152, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([3.1597e-03, 1.8892e-09, 3.6115e-03, 5.2798e-07, 2.7333e-01, 7.1730e-01,
        2.6035e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 152, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5674e-01, 4.8454e-03, 1.6747e-03, 4.9146e-07, 4.7418e-07, 1.8486e-03,
        3.4888e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 152, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0059, 0.0015, 0.0028, 0.0020, 0.0014, 0.9847],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 152, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.4248e-07, 1.7787e-03, 1.6428e-03, 2.6938e-08, 1.5120e-03, 9.9507e-01,
        1.4259e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 153, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.2655e-07, 1.2487e-09, 1.0000e+00, 1.9230e-08, 1.1406e-09, 3.0894e-09,
        1.2055e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 153, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4382e-03, 8.6667e-10, 1.9249e-03, 1.6324e-07, 1.7997e-01, 8.1511e-01,
        1.5556e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 153, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5874e-01, 5.2906e-03, 1.6884e-03, 4.7518e-07, 4.1780e-07, 1.9304e-03,
        3.2353e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 153, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0070, 0.0015, 0.0032, 0.0017, 0.0015, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 153, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.7049e-07, 1.6605e-03, 1.7323e-03, 2.9845e-08, 1.4599e-03, 9.9515e-01,
        1.1968e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.148

[Epoch: 154, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.9200e-07, 1.1680e-09, 1.0000e+00, 1.6814e-08, 1.1140e-09, 3.4605e-09,
        8.3590e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 154, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7201e-03, 8.5965e-10, 2.2588e-03, 2.0466e-07, 2.1663e-01, 7.7772e-01,
        1.6671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 154, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5866e-01, 4.7877e-03, 1.7015e-03, 4.5975e-07, 4.3338e-07, 1.6827e-03,
        3.3164e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 154, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0067, 0.0015, 0.0030, 0.0018, 0.0014, 0.9837],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 154, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.4336e-07, 1.7829e-03, 1.6110e-03, 3.1601e-08, 1.7609e-03, 9.9484e-01,
        2.2866e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.162

[Epoch: 155, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.6904e-07, 1.2590e-09, 1.0000e+00, 1.8549e-08, 1.0826e-09, 3.1693e-09,
        1.0434e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 155, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5994e-03, 8.9701e-10, 1.7502e-03, 1.8000e-07, 2.2665e-01, 7.6853e-01,
        1.4779e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 155, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5888e-01, 4.7085e-03, 1.6371e-03, 3.6933e-07, 3.3866e-07, 1.8318e-03,
        3.2944e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 155, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0065, 0.0017, 0.0033, 0.0017, 0.0018, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 155, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.2837e-07, 1.6890e-03, 1.6998e-03, 2.0995e-08, 1.5465e-03, 9.9506e-01,
        1.3933e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.146

[Epoch: 156, batch: 45/226] total loss per batch: 0.374
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.6763e-07, 8.3727e-10, 1.0000e+00, 1.3351e-08, 8.5193e-10, 2.4023e-09,
        9.3708e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 156, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6848e-03, 7.2814e-10, 2.0338e-03, 1.6795e-07, 2.1577e-01, 7.7875e-01,
        1.7609e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 156, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5869e-01, 4.6269e-03, 1.6003e-03, 3.0571e-07, 3.3700e-07, 1.6107e-03,
        3.3470e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 156, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0076, 0.0018, 0.0035, 0.0019, 0.0016, 0.9816],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 156, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.0658e-07, 1.4616e-03, 1.5277e-03, 2.6245e-08, 1.5234e-03, 9.9549e-01,
        1.0736e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 157, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0987e-07, 7.8424e-10, 1.0000e+00, 1.0493e-08, 6.0875e-10, 1.7666e-09,
        7.8434e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 157, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8337e-03, 8.2063e-10, 1.7995e-03, 1.7300e-07, 2.2044e-01, 7.7429e-01,
        1.6360e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 157, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6011e-01, 4.9011e-03, 1.4975e-03, 3.7153e-07, 4.0469e-07, 1.8705e-03,
        3.1617e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 157, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0061, 0.0015, 0.0040, 0.0019, 0.0018, 0.9827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 157, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.0997e-07, 2.2328e-03, 1.5021e-03, 2.5678e-08, 1.6059e-03, 9.9466e-01,
        2.6093e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.149

[Epoch: 158, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5955e-07, 8.0921e-10, 1.0000e+00, 1.4529e-08, 9.8599e-10, 2.3200e-09,
        9.0101e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 158, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.3450e-03, 8.1461e-10, 1.8523e-03, 1.3668e-07, 1.8462e-01, 8.1079e-01,
        1.3894e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 158, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4927e-01, 5.5144e-03, 1.8455e-03, 5.0918e-07, 4.3057e-07, 1.9452e-03,
        4.1425e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 158, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0071, 0.0022, 0.0035, 0.0023, 0.0019, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 158, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0809e-07, 1.3355e-03, 1.3993e-03, 2.5871e-08, 1.6755e-03, 9.9559e-01,
        4.8664e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 159, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.7718e-07, 1.2844e-09, 1.0000e+00, 2.0866e-08, 8.8854e-10, 2.1145e-09,
        8.2537e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 159, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.6335e-03, 8.3867e-10, 2.3209e-03, 2.5891e-07, 2.4815e-01, 7.4506e-01,
        1.8358e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.011

[Epoch: 159, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7285e-01, 4.5543e-03, 1.2738e-03, 5.6888e-07, 3.8225e-07, 2.1157e-03,
        1.9207e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 159, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0021, 0.0073, 0.0017, 0.0036, 0.0018, 0.0015, 0.9821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 159, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([1.1165e-06, 2.5635e-03, 1.9545e-03, 4.5222e-08, 2.0525e-03, 9.9343e-01,
        2.6992e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 160, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([8.5580e-07, 2.1367e-09, 1.0000e+00, 5.4303e-08, 3.8076e-09, 1.0992e-08,
        1.6274e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 160, batch: 90/226] total loss per batch: 0.352
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2892e-03, 2.2358e-09, 1.3724e-03, 2.5553e-07, 2.1827e-01, 7.7796e-01,
        1.1050e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 160, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4542e-01, 4.8296e-03, 2.5787e-03, 6.1758e-07, 6.0093e-07, 2.3440e-03,
        4.4824e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 160, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0057, 0.0018, 0.0025, 0.0017, 0.0016, 0.9847],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 160, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([3.9952e-07, 1.5670e-03, 1.4148e-03, 1.6655e-08, 1.5922e-03, 9.9543e-01,
        9.2869e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 161, batch: 45/226] total loss per batch: 0.376
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.8134e-07, 1.0779e-09, 1.0000e+00, 1.0405e-08, 1.0130e-09, 2.2797e-09,
        5.1765e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 161, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9316e-03, 1.4347e-09, 1.8132e-03, 2.5604e-07, 2.1424e-01, 7.8036e-01,
        1.6609e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 161, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6425e-01, 4.8371e-03, 1.7695e-03, 6.9757e-07, 5.6272e-07, 1.5448e-03,
        2.7596e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 161, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0063, 0.0019, 0.0033, 0.0020, 0.0020, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.002

[Epoch: 161, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3948e-07, 1.9118e-03, 1.9231e-03, 3.7241e-08, 1.9475e-03, 9.9422e-01,
        2.8722e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.146

[Epoch: 162, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([5.9695e-07, 3.0963e-09, 1.0000e+00, 3.1998e-08, 1.8449e-09, 7.3259e-09,
        1.1387e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 162, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.0179e-03, 1.9284e-09, 1.6361e-03, 2.8461e-07, 2.2165e-01, 7.7321e-01,
        1.4836e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 162, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5958e-01, 4.4279e-03, 2.4100e-03, 5.5029e-07, 8.3982e-07, 2.0585e-03,
        3.1526e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 162, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0064, 0.0023, 0.0032, 0.0019, 0.0017, 0.9827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 162, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0518e-07, 1.7149e-03, 1.6829e-03, 2.6012e-08, 1.8099e-03, 9.9479e-01,
        2.0899e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.166

[Epoch: 163, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.5763e-07, 1.4078e-09, 1.0000e+00, 1.5299e-08, 1.0883e-09, 3.7770e-09,
        1.1324e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 163, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9719e-03, 1.8277e-09, 2.0225e-03, 2.3729e-07, 2.1838e-01, 7.7602e-01,
        1.6017e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 163, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5473e-01, 5.3534e-03, 2.0280e-03, 3.8992e-07, 5.2915e-07, 1.6476e-03,
        3.6236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 163, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0062, 0.0018, 0.0030, 0.0016, 0.0016, 0.9842],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 163, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3602e-07, 1.7921e-03, 2.0277e-03, 2.5713e-08, 1.8291e-03, 9.9435e-01,
        9.6580e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.145

[Epoch: 164, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.8432e-07, 1.7959e-09, 1.0000e+00, 1.8054e-08, 1.6953e-09, 5.7032e-09,
        6.3003e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 164, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8942e-03, 1.4658e-09, 2.1345e-03, 3.0264e-07, 2.1120e-01, 7.8326e-01,
        1.5070e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 164, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6473e-01, 4.6794e-03, 1.8983e-03, 5.2466e-07, 6.0164e-07, 1.3820e-03,
        2.7308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 164, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0069, 0.0018, 0.0031, 0.0014, 0.0018, 0.9832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 164, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.4898e-07, 1.8755e-03, 1.8752e-03, 2.4961e-08, 1.7086e-03, 9.9454e-01,
        2.0015e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.161

[Epoch: 165, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.4828e-07, 1.3189e-09, 1.0000e+00, 1.6190e-08, 1.0041e-09, 3.5501e-09,
        1.1473e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 165, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7196e-03, 1.2389e-09, 1.7614e-03, 1.8814e-07, 2.2084e-01, 7.7421e-01,
        1.4709e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 165, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5114e-01, 5.3113e-03, 1.7777e-03, 3.6903e-07, 5.7612e-07, 1.7157e-03,
        4.0053e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 165, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0054, 0.0016, 0.0029, 0.0015, 0.0016, 0.9854],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 165, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.4239e-07, 1.6601e-03, 2.0260e-03, 2.1487e-08, 1.7263e-03, 9.9459e-01,
        1.4116e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 166, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4517e-07, 1.1925e-09, 1.0000e+00, 1.2404e-08, 1.0317e-09, 3.5528e-09,
        1.4640e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 166, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7770e-03, 1.3413e-09, 2.0522e-03, 2.8581e-07, 2.2038e-01, 7.7429e-01,
        1.5073e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 166, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6638e-01, 4.6101e-03, 1.9608e-03, 4.5419e-07, 5.8116e-07, 1.4748e-03,
        2.5573e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 166, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0068, 0.0018, 0.0034, 0.0016, 0.0017, 0.9827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 166, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0275e-07, 1.8219e-03, 1.7601e-03, 2.6104e-08, 1.6796e-03, 9.9474e-01,
        1.9189e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 167, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.8271e-07, 1.4969e-09, 1.0000e+00, 1.6012e-08, 1.2780e-09, 4.5869e-09,
        1.2620e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 167, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6741e-03, 1.5364e-09, 1.9014e-03, 2.0858e-07, 2.1974e-01, 7.7505e-01,
        1.6283e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 167, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4595e-01, 6.5935e-03, 2.0373e-03, 4.3897e-07, 5.9418e-07, 2.0069e-03,
        4.3411e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 167, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0058, 0.0016, 0.0028, 0.0016, 0.0017, 0.9848],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 167, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9650e-07, 1.5948e-03, 1.7952e-03, 2.1164e-08, 1.7736e-03, 9.9484e-01,
        2.2175e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.144

[Epoch: 168, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.3896e-07, 7.1953e-10, 1.0000e+00, 8.2443e-09, 7.1948e-10, 1.9326e-09,
        1.2689e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 168, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7224e-03, 1.3080e-09, 1.7558e-03, 2.5324e-07, 2.1672e-01, 7.7834e-01,
        1.4653e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 168, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.7250e-01, 3.2992e-03, 1.4825e-03, 2.5069e-07, 3.6538e-07, 1.2057e-03,
        2.1511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 168, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0085, 0.0019, 0.0036, 0.0017, 0.0016, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 168, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.2698e-07, 1.7812e-03, 2.0068e-03, 2.3080e-08, 1.7861e-03, 9.9443e-01,
        1.2739e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.159

[Epoch: 169, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8813e-07, 1.4522e-09, 1.0000e+00, 1.7791e-08, 1.1662e-09, 3.5227e-09,
        2.3679e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 169, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5329e-03, 1.4817e-09, 1.6429e-03, 1.8037e-07, 2.2106e-01, 7.7384e-01,
        1.9239e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 169, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4011e-01, 6.1180e-03, 2.1767e-03, 5.4315e-07, 6.5034e-07, 2.2109e-03,
        4.9387e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.035

[Epoch: 169, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0065, 0.0016, 0.0033, 0.0017, 0.0018, 0.9832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 169, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.2869e-07, 1.5543e-03, 1.9123e-03, 3.2208e-08, 1.5430e-03, 9.9499e-01,
        2.0838e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 170, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.6757e-07, 1.3267e-09, 1.0000e+00, 1.3817e-08, 7.4774e-10, 2.8408e-09,
        1.1729e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 170, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8884e-03, 1.3450e-09, 1.5962e-03, 1.9600e-07, 2.1711e-01, 7.7758e-01,
        1.8265e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 170, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6274e-01, 4.7657e-03, 1.8187e-03, 3.4672e-07, 5.1138e-07, 1.9405e-03,
        2.8736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 170, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0052, 0.0016, 0.0035, 0.0021, 0.0015, 0.9843],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 170, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.3620e-07, 1.8094e-03, 1.7339e-03, 3.3653e-08, 1.9337e-03, 9.9452e-01,
        2.4941e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.135

[Epoch: 171, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.6269e-07, 9.8372e-10, 1.0000e+00, 1.1859e-08, 8.5521e-10, 2.1216e-09,
        1.3224e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 171, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2821e-03, 1.2283e-09, 1.5691e-03, 1.6460e-07, 2.1232e-01, 7.8334e-01,
        1.4819e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 171, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6234e-01, 4.5492e-03, 1.8137e-03, 6.6367e-07, 5.5615e-07, 1.6928e-03,
        2.9599e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.033

[Epoch: 171, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0095, 0.0017, 0.0030, 0.0014, 0.0022, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 171, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.8151e-07, 1.8864e-03, 2.4700e-03, 3.9391e-08, 2.1911e-03, 9.9345e-01,
        2.4133e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.178

[Epoch: 172, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([7.3898e-07, 2.2093e-09, 1.0000e+00, 2.6395e-08, 1.8263e-09, 3.9532e-09,
        1.7910e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 172, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6037e-03, 1.8841e-09, 1.8964e-03, 3.2390e-07, 2.3315e-01, 7.6175e-01,
        1.6026e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 172, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5079e-01, 5.2548e-03, 2.1090e-03, 5.6273e-07, 7.0820e-07, 2.4812e-03,
        3.9362e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.048

[Epoch: 172, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0049, 0.0019, 0.0033, 0.0020, 0.0014, 0.9852],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 172, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.3230e-07, 1.5019e-03, 1.7596e-03, 2.8220e-08, 1.5193e-03, 9.9522e-01,
        1.0759e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.141

[Epoch: 173, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5093e-07, 1.3438e-09, 1.0000e+00, 1.3399e-08, 1.0285e-09, 4.6689e-09,
        1.8214e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 173, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5693e-03, 1.1410e-09, 2.1876e-03, 2.4042e-07, 2.1336e-01, 7.8142e-01,
        1.4646e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 173, batch: 135/226] total loss per batch: 0.379
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6363e-01, 4.6423e-03, 1.9355e-03, 3.1268e-07, 4.6209e-07, 1.5356e-03,
        2.8251e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 173, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0079, 0.0016, 0.0033, 0.0017, 0.0020, 0.9819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 173, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9653e-07, 1.7960e-03, 2.4615e-03, 4.0615e-08, 1.7161e-03, 9.9403e-01,
        3.4981e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 174, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.9220e-07, 1.4777e-09, 1.0000e+00, 2.0851e-08, 1.4803e-09, 4.5936e-09,
        2.2889e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 174, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6070e-03, 1.2309e-09, 1.4863e-03, 2.2900e-07, 2.1753e-01, 7.7793e-01,
        1.4447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 174, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5575e-01, 5.1416e-03, 1.7471e-03, 5.0810e-07, 6.4423e-07, 2.0146e-03,
        3.5345e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 174, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0059, 0.0017, 0.0035, 0.0019, 0.0017, 0.9836],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 174, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.5754e-07, 1.7368e-03, 1.7360e-03, 2.3436e-08, 1.7831e-03, 9.9474e-01,
        1.2781e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.142

[Epoch: 175, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4402e-07, 1.1398e-09, 1.0000e+00, 1.4271e-08, 5.5819e-10, 2.7637e-09,
        1.2099e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 175, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5681e-03, 1.0652e-09, 1.9715e-03, 1.9537e-07, 2.1704e-01, 7.7744e-01,
        1.9886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 175, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5778e-01, 4.8733e-03, 1.8276e-03, 3.5034e-07, 5.2420e-07, 1.8048e-03,
        3.3717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 175, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0069, 0.0016, 0.0031, 0.0015, 0.0016, 0.9837],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 175, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.2425e-07, 1.7651e-03, 1.9716e-03, 4.3805e-08, 1.6728e-03, 9.9459e-01,
        1.7622e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.161

[Epoch: 176, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8812e-07, 1.3151e-09, 1.0000e+00, 1.6557e-08, 8.2560e-10, 3.8855e-09,
        2.7819e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 176, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6641e-03, 1.3400e-09, 1.6261e-03, 2.2033e-07, 2.2036e-01, 7.7461e-01,
        1.7452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 176, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5699e-01, 5.0489e-03, 1.8176e-03, 4.4086e-07, 5.5920e-07, 1.9481e-03,
        3.4190e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 176, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0076, 0.0018, 0.0034, 0.0019, 0.0020, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 176, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.6814e-07, 1.7399e-03, 1.9411e-03, 2.8227e-08, 1.6796e-03, 9.9464e-01,
        4.5985e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 177, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.8036e-07, 1.5670e-09, 1.0000e+00, 2.3833e-08, 1.3046e-09, 3.3912e-09,
        2.1499e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 177, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7518e-03, 1.0344e-09, 1.6058e-03, 1.7903e-07, 2.2226e-01, 7.7243e-01,
        1.9489e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 177, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5859e-01, 5.0766e-03, 1.7965e-03, 4.1268e-07, 4.2444e-07, 2.0152e-03,
        3.2524e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 177, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0062, 0.0018, 0.0033, 0.0018, 0.0017, 0.9836],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 177, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.5784e-07, 1.6149e-03, 1.8920e-03, 2.8005e-08, 1.7470e-03, 9.9475e-01,
        1.6683e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.161

[Epoch: 178, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.5218e-07, 1.0095e-09, 1.0000e+00, 1.5187e-08, 8.2829e-10, 4.0536e-09,
        2.0603e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 178, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.3967e-03, 1.3630e-09, 1.7418e-03, 2.1099e-07, 2.1160e-01, 7.8373e-01,
        1.5325e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 178, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5907e-01, 4.5566e-03, 1.4728e-03, 3.3896e-07, 5.1284e-07, 1.6408e-03,
        3.3260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 178, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0082, 0.0018, 0.0036, 0.0018, 0.0018, 0.9810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 178, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.5696e-07, 1.8088e-03, 1.9200e-03, 3.6981e-08, 1.5151e-03, 9.9476e-01,
        3.0430e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.139

[Epoch: 179, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8893e-07, 1.0487e-09, 1.0000e+00, 1.3750e-08, 7.3367e-10, 1.9353e-09,
        1.5868e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 179, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6731e-03, 9.3741e-10, 1.5812e-03, 1.5520e-07, 2.2376e-01, 7.7105e-01,
        1.9414e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 179, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5708e-01, 5.9908e-03, 1.9515e-03, 4.3964e-07, 4.7066e-07, 1.8622e-03,
        3.3116e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 179, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0057, 0.0017, 0.0036, 0.0019, 0.0020, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 179, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.9510e-07, 1.5800e-03, 1.8187e-03, 2.7181e-08, 1.6014e-03, 9.9500e-01,
        3.0086e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 180, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0276e-07, 9.6375e-10, 1.0000e+00, 1.6797e-08, 7.6882e-10, 2.5407e-09,
        1.9224e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 180, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.5528e-03, 5.5826e-10, 2.0989e-03, 1.6886e-07, 2.0544e-01, 7.8922e-01,
        1.6866e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 180, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6179e-01, 4.4114e-03, 1.4705e-03, 2.9530e-07, 3.7194e-07, 1.4522e-03,
        3.0874e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.032

[Epoch: 180, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0012, 0.0068, 0.0017, 0.0030, 0.0013, 0.0014, 0.9844],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.019

[Epoch: 180, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.0185e-07, 1.8600e-03, 1.8367e-03, 3.5754e-08, 1.7756e-03, 9.9453e-01,
        3.4348e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 181, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4346e-07, 1.2218e-09, 1.0000e+00, 1.5627e-08, 6.1046e-10, 2.0130e-09,
        1.6784e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 181, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7516e-03, 1.4212e-09, 1.6681e-03, 1.9023e-07, 2.2400e-01, 7.7081e-01,
        1.7689e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 181, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5317e-01, 5.2611e-03, 1.8612e-03, 5.0148e-07, 4.5322e-07, 2.0235e-03,
        3.7687e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 181, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0071, 0.0020, 0.0045, 0.0026, 0.0021, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 181, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.0700e-07, 1.6232e-03, 1.8407e-03, 1.8623e-08, 1.3350e-03, 9.9520e-01,
        1.5033e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.150

[Epoch: 182, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.0587e-07, 1.2855e-09, 1.0000e+00, 1.6475e-08, 1.1863e-09, 3.3526e-09,
        2.1985e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 182, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.9693e-03, 1.0400e-09, 1.6846e-03, 1.9054e-07, 2.0693e-01, 7.8772e-01,
        1.6988e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 182, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6456e-01, 3.8710e-03, 1.6079e-03, 2.8875e-07, 2.5411e-07, 1.8138e-03,
        2.8150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.030

[Epoch: 182, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0060, 0.0019, 0.0029, 0.0016, 0.0016, 0.9844],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 182, batch: 225/226] total loss per batch: 0.358
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.4730e-07, 1.6757e-03, 1.7528e-03, 2.8542e-08, 1.7697e-03, 9.9480e-01,
        1.0858e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 183, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.3317e-07, 1.2240e-09, 1.0000e+00, 1.5335e-08, 7.8557e-10, 2.7550e-09,
        9.7952e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 183, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7873e-03, 1.3499e-09, 1.9035e-03, 2.2139e-07, 2.2551e-01, 7.6880e-01,
        2.0021e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 183, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4879e-01, 6.5604e-03, 2.4896e-03, 3.9984e-07, 5.4015e-07, 2.1352e-03,
        4.0019e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.041

[Epoch: 183, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0013, 0.0058, 0.0014, 0.0029, 0.0014, 0.0016, 0.9856],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 183, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.1942e-07, 1.7379e-03, 1.5995e-03, 2.2486e-08, 1.3649e-03, 9.9530e-01,
        2.5685e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.149

[Epoch: 184, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.2056e-07, 1.2183e-09, 1.0000e+00, 2.3556e-08, 1.2679e-09, 3.4407e-09,
        1.5497e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 184, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7799e-03, 2.0687e-09, 1.7062e-03, 2.3431e-07, 2.2045e-01, 7.7443e-01,
        1.6312e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 184, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5899e-01, 4.9454e-03, 1.7695e-03, 5.8671e-07, 4.1542e-07, 2.1438e-03,
        3.2153e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 184, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0074, 0.0017, 0.0031, 0.0016, 0.0020, 0.9822],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.006

[Epoch: 184, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3041e-07, 1.8518e-03, 1.7173e-03, 2.1573e-08, 1.6654e-03, 9.9476e-01,
        2.1047e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.155

[Epoch: 185, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1909e-07, 1.1966e-09, 1.0000e+00, 1.6792e-08, 1.0327e-09, 2.1622e-09,
        1.6871e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 185, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7065e-03, 1.1654e-09, 1.7948e-03, 2.2200e-07, 2.2430e-01, 7.7043e-01,
        1.7704e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 185, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6060e-01, 4.8243e-03, 1.8153e-03, 3.6051e-07, 4.0539e-07, 1.8328e-03,
        3.0931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 185, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0014, 0.0063, 0.0014, 0.0026, 0.0014, 0.0014, 0.9855],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 185, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9387e-07, 1.6313e-03, 1.5849e-03, 1.8167e-08, 1.4914e-03, 9.9529e-01,
        1.6209e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.156

[Epoch: 186, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.0242e-07, 6.3480e-10, 1.0000e+00, 1.3194e-08, 6.1118e-10, 2.0656e-09,
        1.5285e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 186, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6408e-03, 1.4015e-09, 1.8589e-03, 2.3180e-07, 2.1039e-01, 7.8436e-01,
        1.7471e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 186, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5761e-01, 5.2481e-03, 1.7807e-03, 4.5921e-07, 3.4271e-07, 1.6603e-03,
        3.3702e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 186, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0072, 0.0017, 0.0036, 0.0018, 0.0020, 0.9819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.009

[Epoch: 186, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([5.9130e-07, 1.9584e-03, 1.6774e-03, 2.1529e-08, 1.6250e-03, 9.9474e-01,
        1.5447e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.147

[Epoch: 187, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1220e-07, 8.5380e-10, 1.0000e+00, 1.3639e-08, 8.6745e-10, 1.4946e-09,
        9.4561e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 187, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6117e-03, 1.2706e-09, 1.6884e-03, 1.6484e-07, 2.1732e-01, 7.7752e-01,
        1.8623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 187, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6015e-01, 4.8111e-03, 1.6471e-03, 3.2763e-07, 3.5492e-07, 1.7252e-03,
        3.1666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 187, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0069, 0.0015, 0.0034, 0.0016, 0.0017, 0.9832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 187, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.0711e-07, 2.0687e-03, 1.8965e-03, 2.4326e-08, 1.8378e-03, 9.9420e-01,
        2.3483e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.160

[Epoch: 188, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.7791e-07, 1.0362e-09, 1.0000e+00, 1.9901e-08, 1.0078e-09, 2.2273e-09,
        2.4895e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 188, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8867e-03, 9.2253e-10, 1.9817e-03, 1.9197e-07, 2.2745e-01, 7.6663e-01,
        2.0527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 188, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5582e-01, 5.4481e-03, 1.9965e-03, 4.8088e-07, 3.4544e-07, 1.7849e-03,
        3.4947e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 188, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0017, 0.0054, 0.0013, 0.0027, 0.0014, 0.0016, 0.9859],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 188, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.4988e-07, 1.8670e-03, 1.7505e-03, 2.4554e-08, 1.7441e-03, 9.9464e-01,
        1.4957e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.141

[Epoch: 189, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.2474e-07, 9.0931e-10, 1.0000e+00, 1.4320e-08, 9.1364e-10, 1.6572e-09,
        7.9166e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 189, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4662e-03, 1.1699e-09, 1.4782e-03, 1.6179e-07, 2.0902e-01, 7.8656e-01,
        1.4772e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 189, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5750e-01, 5.6359e-03, 1.9627e-03, 3.4814e-07, 4.5103e-07, 1.8698e-03,
        3.3030e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.038

[Epoch: 189, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0078, 0.0015, 0.0031, 0.0016, 0.0015, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 189, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.7595e-07, 1.7085e-03, 1.7935e-03, 1.7548e-08, 1.8059e-03, 9.9469e-01,
        2.5597e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 190, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.4271e-07, 1.4257e-09, 1.0000e+00, 2.3472e-08, 1.6837e-09, 3.1610e-09,
        2.9923e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 190, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8968e-03, 9.8059e-10, 1.7189e-03, 2.0451e-07, 2.3530e-01, 7.5931e-01,
        1.7721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 190, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6227e-01, 3.9593e-03, 1.5755e-03, 6.0144e-07, 3.6975e-07, 1.8922e-03,
        3.0301e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 190, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0065, 0.0019, 0.0036, 0.0018, 0.0020, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.007

[Epoch: 190, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.2634e-07, 1.6606e-03, 1.9015e-03, 2.4523e-08, 1.8391e-03, 9.9460e-01,
        1.6283e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.158

[Epoch: 191, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3200e-07, 1.2276e-09, 1.0000e+00, 2.3247e-08, 1.4193e-09, 2.2595e-09,
        1.1196e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 191, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7893e-03, 1.2191e-09, 1.7792e-03, 1.9638e-07, 2.0505e-01, 7.8931e-01,
        2.0733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 191, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5361e-01, 5.6829e-03, 1.8181e-03, 5.2163e-07, 3.7184e-07, 1.9339e-03,
        3.6955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 191, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0063, 0.0018, 0.0030, 0.0014, 0.0015, 0.9846],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.013

[Epoch: 191, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([4.0982e-07, 1.3941e-03, 1.6038e-03, 1.8308e-08, 1.5597e-03, 9.9544e-01,
        2.3994e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 192, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5583e-07, 1.2427e-09, 1.0000e+00, 1.3940e-08, 1.2357e-09, 2.3880e-09,
        2.5044e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 192, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6782e-03, 8.1647e-10, 1.5314e-03, 1.6252e-07, 2.2237e-01, 7.7288e-01,
        1.5425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 192, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6360e-01, 4.2841e-03, 1.3854e-03, 4.3152e-07, 4.4268e-07, 1.5442e-03,
        2.9187e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.040

[Epoch: 192, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0080, 0.0022, 0.0037, 0.0019, 0.0020, 0.9807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.010

[Epoch: 192, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.9058e-07, 1.9784e-03, 1.4020e-03, 2.0689e-08, 1.6096e-03, 9.9501e-01,
        1.2417e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.170

[Epoch: 193, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([4.4038e-07, 1.2240e-09, 1.0000e+00, 2.4982e-08, 2.2481e-09, 2.4697e-09,
        3.2641e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 193, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6543e-03, 8.3552e-10, 2.1030e-03, 2.1299e-07, 2.2163e-01, 7.7268e-01,
        1.9395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 193, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5749e-01, 4.8426e-03, 1.6328e-03, 6.2324e-07, 2.8458e-07, 2.0736e-03,
        3.3964e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.034

[Epoch: 193, batch: 180/226] total loss per batch: 0.371
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0066, 0.0017, 0.0034, 0.0016, 0.0019, 0.9832],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.008

[Epoch: 193, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.9503e-07, 1.6036e-03, 1.8797e-03, 1.9867e-08, 1.7769e-03, 9.9474e-01,
        2.5976e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.139

[Epoch: 194, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.5739e-07, 1.1042e-09, 1.0000e+00, 1.1615e-08, 6.7463e-10, 2.2292e-09,
        1.0446e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.998

[Epoch: 194, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.8968e-03, 9.6265e-10, 1.9583e-03, 2.0652e-07, 2.1648e-01, 7.7755e-01,
        2.1161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 194, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5789e-01, 5.0581e-03, 1.8491e-03, 4.1790e-07, 3.7688e-07, 1.7898e-03,
        3.3415e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 194, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0059, 0.0020, 0.0033, 0.0017, 0.0015, 0.9842],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 194, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.7717e-07, 1.6285e-03, 1.6745e-03, 2.2646e-08, 1.7671e-03, 9.9493e-01,
        3.1904e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.154

[Epoch: 195, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.1084e-07, 1.3513e-09, 1.0000e+00, 1.7594e-08, 1.2361e-09, 2.4636e-09,
        3.7070e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 195, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.2396e-03, 1.0899e-09, 1.5253e-03, 1.9018e-07, 2.0544e-01, 7.9034e-01,
        1.4560e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 195, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5484e-01, 5.2049e-03, 1.5736e-03, 6.3922e-07, 3.6135e-07, 1.7859e-03,
        3.6598e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.036

[Epoch: 195, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0018, 0.0081, 0.0016, 0.0032, 0.0020, 0.0021, 0.9812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 195, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([9.0253e-07, 1.8367e-03, 2.2530e-03, 3.4969e-08, 2.0030e-03, 9.9391e-01,
        3.0876e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 196, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.9478e-07, 1.1506e-09, 1.0000e+00, 1.4863e-08, 8.7403e-10, 1.8527e-09,
        6.4815e-09], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 196, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([2.1071e-03, 1.1265e-09, 2.1947e-03, 2.4293e-07, 2.4442e-01, 7.4899e-01,
        2.2918e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 196, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6615e-01, 4.4000e-03, 1.5233e-03, 5.0255e-07, 3.4037e-07, 1.6194e-03,
        2.6304e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.037

[Epoch: 196, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0063, 0.0017, 0.0033, 0.0014, 0.0018, 0.9839],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.016

[Epoch: 196, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.8011e-07, 1.4601e-03, 1.7652e-03, 2.1983e-08, 1.7510e-03, 9.9502e-01,
        2.7453e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 197, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.3505e-07, 1.4028e-09, 1.0000e+00, 1.2306e-08, 1.0932e-09, 2.3244e-09,
        4.7982e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 197, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.1612e-03, 9.8933e-10, 1.5685e-03, 1.9192e-07, 2.0360e-01, 7.9228e-01,
        1.3869e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 197, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.4786e-01, 5.8751e-03, 2.0491e-03, 8.0559e-07, 4.2483e-07, 1.9355e-03,
        4.2278e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 197, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0016, 0.0073, 0.0019, 0.0036, 0.0016, 0.0019, 0.9821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.012

[Epoch: 197, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([7.6772e-07, 1.8668e-03, 1.8961e-03, 3.2256e-08, 1.9234e-03, 9.9431e-01,
        1.8197e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.148

[Epoch: 198, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.1738e-07, 1.0625e-09, 1.0000e+00, 1.1274e-08, 1.0634e-09, 1.5551e-09,
        1.7025e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 198, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.7859e-03, 7.0285e-10, 1.8231e-03, 1.5382e-07, 2.3117e-01, 7.6361e-01,
        1.6117e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 198, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6387e-01, 4.7946e-03, 1.3568e-03, 6.5446e-07, 3.3232e-07, 1.5785e-03,
        2.8403e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.042

[Epoch: 198, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0067, 0.0015, 0.0032, 0.0019, 0.0021, 0.9831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.014

[Epoch: 198, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([8.0222e-07, 1.6199e-03, 1.8931e-03, 2.8268e-08, 1.6900e-03, 9.9480e-01,
        4.5105e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.153

[Epoch: 199, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([3.2141e-07, 1.5339e-09, 1.0000e+00, 1.4634e-08, 9.3134e-10, 1.8336e-09,
        3.4253e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 199, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.6092e-03, 8.0332e-10, 2.0975e-03, 1.8312e-07, 2.1524e-01, 7.7916e-01,
        1.8865e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 199, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.5568e-01, 5.3751e-03, 1.7784e-03, 4.2806e-07, 3.2984e-07, 1.8492e-03,
        3.5318e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 199, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0015, 0.0065, 0.0017, 0.0029, 0.0015, 0.0014, 0.9845],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.011

[Epoch: 199, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.2898e-07, 1.7782e-03, 1.8418e-03, 2.4812e-08, 1.7297e-03, 9.9465e-01,
        2.1486e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.152

[Epoch: 200, batch: 45/226] total loss per batch: 0.375
Policy (actual, predicted): 2 2
Policy data: tensor([0., 0., 1., 0., 0., 0., 0.])
Policy pred: tensor([2.8403e-07, 1.3629e-09, 1.0000e+00, 1.6486e-08, 7.6077e-10, 2.2015e-09,
        1.2673e-08], grad_fn=<SelectBackward>)Value (actual, predicted): 0.999 0.999

[Epoch: 200, batch: 90/226] total loss per batch: 0.351
Policy (actual, predicted): 5 5
Policy data: tensor([0.0017, 0.0000, 0.0017, 0.0000, 0.2183, 0.7767, 0.0017])
Policy pred: tensor([1.4903e-03, 9.5528e-10, 1.6183e-03, 1.8249e-07, 2.1487e-01, 7.8044e-01,
        1.5836e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 200, batch: 135/226] total loss per batch: 0.378
Policy (actual, predicted): 0 0
Policy data: tensor([0.9583, 0.0050, 0.0017, 0.0000, 0.0000, 0.0017, 0.0333])
Policy pred: tensor([9.6010e-01, 5.0224e-03, 1.4097e-03, 4.1700e-07, 3.8325e-07, 1.5783e-03,
        3.1884e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.038 0.039

[Epoch: 200, batch: 180/226] total loss per batch: 0.370
Policy (actual, predicted): 6 6
Policy data: tensor([0.0017, 0.0067, 0.0017, 0.0033, 0.0017, 0.0017, 0.9833])
Policy pred: tensor([0.0019, 0.0068, 0.0019, 0.0035, 0.0020, 0.0020, 0.9819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.012 -0.015

[Epoch: 200, batch: 225/226] total loss per batch: 0.357
Policy (actual, predicted): 5 5
Policy data: tensor([0.0000, 0.0017, 0.0017, 0.0000, 0.0017, 0.9950, 0.0000])
Policy pred: tensor([6.3499e-07, 1.5701e-03, 1.8304e-03, 2.2903e-08, 1.6372e-03, 9.9496e-01,
        1.6843e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.154 -0.167

