Training set samples: 5967
Batch size: 32
[Epoch: 1, batch: 37/187] total loss per batch: 1.343
Policy (actual, predicted): 0 3
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0334e-01, 3.8405e-01, 1.3765e-03, 5.0901e-01, 4.1430e-05, 6.3466e-06,
        2.1798e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 -0.000

[Epoch: 1, batch: 74/187] total loss per batch: 1.319
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.7800e-02, 8.4206e-02, 7.6669e-05, 1.1522e-05, 6.0035e-01, 2.3859e-01,
        1.8970e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.027

[Epoch: 1, batch: 111/187] total loss per batch: 1.259
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([2.5394e-02, 3.3786e-01, 5.9903e-05, 1.6983e-01, 1.0992e-01, 2.9212e-01,
        6.4817e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 1, batch: 148/187] total loss per batch: 1.216
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0083, 0.1811, 0.0060, 0.0698, 0.0023, 0.7192, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 1, batch: 185/187] total loss per batch: 1.334
Policy (actual, predicted): 1 0
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.3339, 0.1973, 0.1132, 0.0278, 0.1215, 0.1453, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.003

[Epoch: 2, batch: 37/187] total loss per batch: 1.062
Policy (actual, predicted): 0 3
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9145e-01, 2.3819e-01, 1.3549e-03, 4.6844e-01, 1.0833e-04, 7.1710e-06,
        4.4615e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.035

[Epoch: 2, batch: 74/187] total loss per batch: 1.067
Policy (actual, predicted): 1 5
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.2914e-02, 1.7300e-01, 3.0111e-05, 8.5551e-06, 3.5422e-01, 3.8029e-01,
        2.9539e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.240

[Epoch: 2, batch: 111/187] total loss per batch: 1.019
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([2.5103e-02, 4.7372e-01, 1.4151e-05, 9.5378e-02, 5.9905e-02, 2.6980e-01,
        7.6074e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.003

[Epoch: 2, batch: 148/187] total loss per batch: 0.986
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0068, 0.2254, 0.0028, 0.1106, 0.0012, 0.6399, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 2, batch: 185/187] total loss per batch: 1.093
Policy (actual, predicted): 1 0
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.3779, 0.3049, 0.0439, 0.0405, 0.0843, 0.1213, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 3, batch: 37/187] total loss per batch: 0.947
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8301e-01, 6.9869e-02, 1.3226e-03, 3.4553e-01, 1.9405e-05, 1.3993e-06,
        2.5138e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.431

[Epoch: 3, batch: 74/187] total loss per batch: 0.971
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.3142e-02, 2.1350e-01, 1.5531e-05, 7.2790e-06, 4.9573e-01, 1.8788e-01,
        3.9718e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.631

[Epoch: 3, batch: 111/187] total loss per batch: 0.916
Policy (actual, predicted): 1 3
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([3.6548e-02, 2.5149e-01, 2.7997e-05, 3.1872e-01, 1.2208e-01, 1.5844e-01,
        1.1269e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.003

[Epoch: 3, batch: 148/187] total loss per batch: 0.929
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0027, 0.1658, 0.0045, 0.0513, 0.0022, 0.7667, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.005

[Epoch: 3, batch: 185/187] total loss per batch: 0.996
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0781, 0.7027, 0.0627, 0.0300, 0.0494, 0.0541, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 4, batch: 37/187] total loss per batch: 0.926
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0548e-01, 5.4555e-02, 1.4849e-04, 3.9704e-02, 1.2825e-05, 3.3467e-07,
        9.8415e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.293

[Epoch: 4, batch: 74/187] total loss per batch: 0.945
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.8422e-02, 5.9074e-01, 4.2975e-05, 1.6170e-05, 1.8307e-01, 6.8938e-02,
        9.8770e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.589

[Epoch: 4, batch: 111/187] total loss per batch: 0.893
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.8037e-02, 5.8022e-01, 3.3650e-05, 8.7959e-02, 6.1511e-02, 1.9884e-01,
        5.3399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.002

[Epoch: 4, batch: 148/187] total loss per batch: 0.897
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0041, 0.1598, 0.0033, 0.0681, 0.0027, 0.7514, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.003

[Epoch: 4, batch: 185/187] total loss per batch: 0.954
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.1097, 0.6943, 0.0140, 0.0276, 0.0522, 0.0815, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 5, batch: 37/187] total loss per batch: 0.892
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4889e-01, 6.3419e-02, 4.2766e-04, 8.7036e-02, 2.0043e-05, 2.7554e-06,
        2.0769e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.334

[Epoch: 5, batch: 74/187] total loss per batch: 0.919
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.8659e-02, 3.4871e-01, 1.4879e-05, 6.3327e-06, 3.8575e-01, 2.2208e-01,
        2.4789e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.323

[Epoch: 5, batch: 111/187] total loss per batch: 0.859
Policy (actual, predicted): 1 6
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([3.5442e-03, 2.5964e-01, 3.1102e-05, 1.8980e-01, 3.2428e-02, 2.1346e-01,
        3.0109e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 5, batch: 148/187] total loss per batch: 0.865
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0039, 0.2418, 0.0060, 0.0771, 0.0063, 0.6581, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 5, batch: 185/187] total loss per batch: 0.918
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.1205, 0.6031, 0.0265, 0.0391, 0.0610, 0.1301, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 6, batch: 37/187] total loss per batch: 0.881
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.9608e-01, 1.3589e-01, 4.5742e-04, 1.6706e-01, 3.1873e-05, 8.8337e-06,
        4.6799e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.429

[Epoch: 6, batch: 74/187] total loss per batch: 0.894
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([3.4705e-02, 7.2046e-01, 5.4251e-05, 7.2482e-06, 1.7730e-01, 3.6156e-02,
        3.1312e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.543

[Epoch: 6, batch: 111/187] total loss per batch: 0.840
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.0341e-03, 6.8638e-01, 1.9568e-05, 6.4688e-02, 6.3545e-02, 1.3199e-01,
        4.6350e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 6, batch: 148/187] total loss per batch: 0.845
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2753, 0.0055, 0.0783, 0.0029, 0.6230, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 6, batch: 185/187] total loss per batch: 0.903
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0494, 0.7125, 0.0231, 0.0579, 0.0387, 0.0405, 0.0779],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 7, batch: 37/187] total loss per batch: 0.857
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1591e-01, 4.8739e-02, 3.0449e-04, 3.4412e-02, 1.5167e-05, 6.7857e-06,
        6.1129e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.254

[Epoch: 7, batch: 74/187] total loss per batch: 0.875
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([2.9829e-02, 2.9822e-01, 6.4715e-05, 5.1669e-06, 5.5275e-01, 6.2438e-02,
        5.6694e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.718

[Epoch: 7, batch: 111/187] total loss per batch: 0.823
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.0283e-02, 3.1478e-01, 2.2439e-04, 2.1751e-01, 2.4230e-02, 2.6066e-01,
        1.3231e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 7, batch: 148/187] total loss per batch: 0.831
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0076, 0.3069, 0.0117, 0.0842, 0.0042, 0.5694, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 7, batch: 185/187] total loss per batch: 0.887
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0845, 0.6061, 0.0329, 0.0846, 0.0694, 0.1043, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 8, batch: 37/187] total loss per batch: 0.852
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5550e-01, 1.0212e-01, 2.2681e-04, 4.1933e-02, 1.2647e-05, 8.5753e-07,
        2.0505e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.390

[Epoch: 8, batch: 74/187] total loss per batch: 0.869
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([2.9299e-02, 7.1604e-01, 1.9035e-05, 1.0576e-05, 1.6730e-01, 6.4590e-02,
        2.2738e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.676

[Epoch: 8, batch: 111/187] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([9.2006e-03, 5.0071e-01, 5.8399e-05, 1.2525e-01, 4.0576e-02, 1.8326e-01,
        1.4094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 8, batch: 148/187] total loss per batch: 0.824
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0049, 0.1833, 0.0033, 0.0534, 0.0028, 0.7455, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 8, batch: 185/187] total loss per batch: 0.881
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0461, 0.7782, 0.0162, 0.0227, 0.0211, 0.0771, 0.0386],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 9, batch: 37/187] total loss per batch: 0.844
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4283e-01, 4.4181e-02, 6.0286e-04, 1.1179e-01, 8.6193e-05, 1.0761e-05,
        4.9930e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.122

[Epoch: 9, batch: 74/187] total loss per batch: 0.863
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.8650e-02, 3.7255e-01, 1.2924e-04, 4.9786e-05, 4.6628e-01, 6.4439e-02,
        3.7903e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.372

[Epoch: 9, batch: 111/187] total loss per batch: 0.814
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([2.2994e-02, 3.4752e-01, 3.9833e-05, 2.7878e-01, 5.0428e-02, 1.3367e-01,
        1.6656e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 9, batch: 148/187] total loss per batch: 0.820
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0052, 0.2993, 0.0076, 0.0622, 0.0038, 0.6138, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 9, batch: 185/187] total loss per batch: 0.875
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.1134, 0.6346, 0.0145, 0.0666, 0.0240, 0.1242, 0.0227],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 10, batch: 37/187] total loss per batch: 0.841
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6610e-01, 8.2372e-02, 1.0758e-04, 5.1200e-02, 2.2045e-05, 1.6489e-06,
        1.9202e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.424

[Epoch: 10, batch: 74/187] total loss per batch: 0.859
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.5522e-01, 5.7209e-01, 2.3683e-05, 7.5745e-06, 1.7263e-01, 7.3514e-02,
        2.6510e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.808

[Epoch: 10, batch: 111/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.0882e-03, 4.8790e-01, 3.2813e-05, 9.7803e-02, 2.2991e-02, 2.9702e-01,
        8.7167e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 10, batch: 148/187] total loss per batch: 0.816
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0053, 0.2339, 0.0062, 0.1081, 0.0065, 0.6305, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 10, batch: 185/187] total loss per batch: 0.867
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0544, 0.7989, 0.0217, 0.0274, 0.0244, 0.0386, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 11, batch: 37/187] total loss per batch: 0.827
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9694e-01, 6.1464e-02, 3.6178e-04, 1.4078e-01, 3.2438e-05, 6.8374e-06,
        4.1092e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.189

[Epoch: 11, batch: 74/187] total loss per batch: 0.846
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.8157e-02, 7.0123e-01, 1.6090e-05, 4.6764e-06, 2.0530e-01, 5.8729e-02,
        1.6559e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.409

[Epoch: 11, batch: 111/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([2.1893e-02, 4.3287e-01, 4.4006e-05, 2.0461e-01, 1.4701e-02, 1.3233e-01,
        1.9356e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 11, batch: 148/187] total loss per batch: 0.803
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.3434, 0.0060, 0.0516, 0.0042, 0.5832, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 11, batch: 185/187] total loss per batch: 0.853
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0694, 0.6561, 0.0218, 0.0481, 0.0103, 0.1358, 0.0584],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 12, batch: 37/187] total loss per batch: 0.806
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9012e-01, 5.3236e-02, 7.9726e-05, 5.6274e-02, 1.9485e-05, 2.7874e-06,
        2.7047e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.199

[Epoch: 12, batch: 74/187] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([3.4675e-02, 5.5549e-01, 1.2615e-05, 6.8064e-06, 3.4895e-01, 4.2176e-02,
        1.8686e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.745

[Epoch: 12, batch: 111/187] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.2936e-03, 5.2979e-01, 7.6386e-05, 9.5176e-02, 2.7650e-02, 1.9109e-01,
        1.4892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 12, batch: 148/187] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2869, 0.0062, 0.0610, 0.0054, 0.6281, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 12, batch: 185/187] total loss per batch: 0.833
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0623, 0.7297, 0.0249, 0.0398, 0.0275, 0.0645, 0.0512],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 13, batch: 37/187] total loss per batch: 0.795
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6208e-01, 5.7514e-02, 2.7290e-04, 7.9764e-02, 2.7136e-05, 3.6466e-06,
        3.4178e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.367

[Epoch: 13, batch: 74/187] total loss per batch: 0.816
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([3.1364e-02, 5.5215e-01, 5.5899e-06, 3.0113e-06, 3.4767e-01, 6.0712e-02,
        8.0987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.560

[Epoch: 13, batch: 111/187] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.3276e-02, 4.1495e-01, 4.0083e-05, 1.9984e-01, 1.0658e-02, 1.7531e-01,
        1.8593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.002

[Epoch: 13, batch: 148/187] total loss per batch: 0.778
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0042, 0.2738, 0.0049, 0.0600, 0.0045, 0.6478, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 13, batch: 185/187] total loss per batch: 0.826
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0591, 0.7543, 0.0118, 0.0296, 0.0142, 0.0845, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 14, batch: 37/187] total loss per batch: 0.793
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1119e-01, 3.9573e-02, 6.4148e-05, 4.8987e-02, 1.9240e-05, 2.9461e-06,
        1.6215e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.232

[Epoch: 14, batch: 74/187] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([2.3321e-02, 5.9554e-01, 1.1312e-05, 5.5128e-06, 3.1615e-01, 4.5300e-02,
        1.9675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.311

[Epoch: 14, batch: 111/187] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.0435e-02, 6.5006e-01, 2.7616e-05, 1.2560e-01, 1.5458e-02, 1.2181e-01,
        7.6608e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 14, batch: 148/187] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0044, 0.2392, 0.0042, 0.0599, 0.0057, 0.6803, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 14, batch: 185/187] total loss per batch: 0.826
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0552, 0.7647, 0.0162, 0.0291, 0.0145, 0.0713, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 15, batch: 37/187] total loss per batch: 0.796
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9271e-01, 4.4603e-02, 1.4842e-04, 6.2366e-02, 1.7519e-05, 2.0638e-06,
        1.5135e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.316

[Epoch: 15, batch: 74/187] total loss per batch: 0.817
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([2.2446e-02, 3.6446e-01, 4.2927e-06, 6.6719e-06, 5.3378e-01, 6.3180e-02,
        1.6116e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.753

[Epoch: 15, batch: 111/187] total loss per batch: 0.776
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.0709e-02, 3.8356e-01, 1.9236e-05, 1.2466e-01, 1.6288e-02, 2.2493e-01,
        2.3982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 15, batch: 148/187] total loss per batch: 0.782
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0033, 0.3400, 0.0038, 0.0710, 0.0028, 0.5742, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 15, batch: 185/187] total loss per batch: 0.830
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0834, 0.6340, 0.0232, 0.0864, 0.0142, 0.0935, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 16, batch: 37/187] total loss per batch: 0.796
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3188e-01, 8.5128e-02, 1.0347e-04, 8.2470e-02, 2.0638e-05, 3.8629e-06,
        3.9520e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.229

[Epoch: 16, batch: 74/187] total loss per batch: 0.821
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([2.7416e-02, 7.0546e-01, 1.0180e-05, 3.5040e-06, 2.0391e-01, 5.0966e-02,
        1.2233e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.544

[Epoch: 16, batch: 111/187] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1131e-02, 4.3851e-01, 2.8732e-05, 2.1112e-01, 3.3147e-02, 2.1915e-01,
        8.6912e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 16, batch: 148/187] total loss per batch: 0.787
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2493, 0.0081, 0.0596, 0.0057, 0.6647, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 16, batch: 185/187] total loss per batch: 0.833
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0417, 0.7336, 0.0076, 0.0272, 0.0174, 0.1316, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 17, batch: 37/187] total loss per batch: 0.800
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8268e-01, 5.0292e-02, 1.6186e-04, 6.6700e-02, 2.2342e-05, 2.4351e-06,
        1.3827e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.287

[Epoch: 17, batch: 74/187] total loss per batch: 0.821
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.9377e-02, 6.0331e-01, 1.0235e-05, 9.8815e-06, 3.2985e-01, 3.9301e-02,
        8.1376e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.509

[Epoch: 17, batch: 111/187] total loss per batch: 0.783
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.0766e-02, 6.4777e-01, 2.7311e-05, 9.6583e-02, 7.5040e-03, 1.6775e-01,
        6.9591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 17, batch: 148/187] total loss per batch: 0.789
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.2217, 0.0061, 0.0707, 0.0081, 0.6837, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 17, batch: 185/187] total loss per batch: 0.839
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0714, 0.7471, 0.0177, 0.0471, 0.0223, 0.0543, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 18, batch: 37/187] total loss per batch: 0.805
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1528e-01, 3.4570e-02, 3.6253e-05, 4.9972e-02, 2.4808e-05, 3.0206e-06,
        1.0964e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.356

[Epoch: 18, batch: 74/187] total loss per batch: 0.823
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.1146e-02, 4.3815e-01, 2.6970e-06, 6.0133e-06, 4.5803e-01, 7.4258e-02,
        1.8407e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.492

[Epoch: 18, batch: 111/187] total loss per batch: 0.781
Policy (actual, predicted): 1 5
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.5379e-02, 2.6259e-01, 4.1951e-05, 1.5783e-01, 7.8208e-03, 3.0777e-01,
        2.4856e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 18, batch: 148/187] total loss per batch: 0.785
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2623, 0.0065, 0.0785, 0.0040, 0.6334, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 18, batch: 185/187] total loss per batch: 0.839
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0520, 0.6945, 0.0096, 0.0346, 0.0176, 0.1220, 0.0697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 19, batch: 37/187] total loss per batch: 0.805
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3985e-01, 7.6498e-02, 1.3421e-04, 8.3202e-02, 2.3115e-05, 1.7591e-06,
        2.9485e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.153

[Epoch: 19, batch: 74/187] total loss per batch: 0.825
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.7811e-02, 6.3847e-01, 1.0338e-05, 3.9485e-06, 3.0733e-01, 2.7964e-02,
        8.4092e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.472

[Epoch: 19, batch: 111/187] total loss per batch: 0.779
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.3921e-02, 5.9078e-01, 1.1476e-04, 2.0192e-01, 1.5349e-02, 1.2319e-01,
        5.4724e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 19, batch: 148/187] total loss per batch: 0.784
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0040, 0.2758, 0.0068, 0.0931, 0.0074, 0.6068, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 19, batch: 185/187] total loss per batch: 0.834
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.1131, 0.6943, 0.0136, 0.0562, 0.0247, 0.0625, 0.0356],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 20, batch: 37/187] total loss per batch: 0.801
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1723e-01, 4.1199e-02, 5.8279e-05, 4.1339e-02, 1.5496e-05, 3.1607e-06,
        1.5415e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.270

[Epoch: 20, batch: 74/187] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.8191e-02, 6.2312e-01, 9.0988e-06, 9.6550e-06, 3.0460e-01, 4.4174e-02,
        9.8952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.410

[Epoch: 20, batch: 111/187] total loss per batch: 0.781
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1213e-02, 4.9444e-01, 6.7106e-05, 1.3966e-01, 1.8375e-02, 1.9588e-01,
        1.4036e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 20, batch: 148/187] total loss per batch: 0.788
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2018, 0.0044, 0.0610, 0.0054, 0.7159, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 20, batch: 185/187] total loss per batch: 0.835
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0496, 0.6923, 0.0130, 0.0502, 0.0115, 0.1278, 0.0556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 21, batch: 37/187] total loss per batch: 0.799
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6503e-01, 8.4004e-02, 1.5549e-04, 5.0684e-02, 2.0397e-05, 1.4564e-06,
        1.0862e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.260

[Epoch: 21, batch: 74/187] total loss per batch: 0.818
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.3977e-02, 4.1529e-01, 1.0650e-05, 5.7441e-06, 4.9743e-01, 5.8564e-02,
        1.4722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.675

[Epoch: 21, batch: 111/187] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.2906e-02, 4.4790e-01, 3.3050e-05, 2.2330e-01, 9.5086e-03, 1.6496e-01,
        1.4139e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 21, batch: 148/187] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.3018, 0.0042, 0.0895, 0.0082, 0.5828, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 21, batch: 185/187] total loss per batch: 0.829
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0684, 0.7367, 0.0091, 0.0378, 0.0084, 0.0709, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 22, batch: 37/187] total loss per batch: 0.790
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6133e-01, 6.9960e-02, 1.5331e-04, 6.8328e-02, 2.3841e-05, 2.8741e-06,
        2.0072e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.290

[Epoch: 22, batch: 74/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0848e-02, 7.0287e-01, 3.5176e-06, 2.3552e-06, 2.4643e-01, 3.2111e-02,
        7.7392e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.572

[Epoch: 22, batch: 111/187] total loss per batch: 0.761
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1890e-02, 5.5917e-01, 4.4083e-05, 1.2655e-01, 9.1453e-03, 1.6460e-01,
        1.2861e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 22, batch: 148/187] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0047, 0.2932, 0.0050, 0.0781, 0.0048, 0.6090, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 22, batch: 185/187] total loss per batch: 0.816
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0502, 0.7327, 0.0093, 0.0554, 0.0187, 0.0863, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 23, batch: 37/187] total loss per batch: 0.785
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4505e-01, 2.7493e-02, 4.3256e-05, 2.7270e-02, 7.5183e-06, 1.3437e-06,
        1.3112e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.199

[Epoch: 23, batch: 74/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0741e-02, 5.4402e-01, 3.0299e-06, 3.5107e-06, 3.9170e-01, 4.2475e-02,
        1.1054e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.457

[Epoch: 23, batch: 111/187] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.2294e-02, 4.3009e-01, 4.1947e-05, 1.6599e-01, 7.1893e-03, 2.2591e-01,
        1.5849e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 23, batch: 148/187] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2875, 0.0050, 0.0776, 0.0061, 0.6124, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 23, batch: 185/187] total loss per batch: 0.812
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0461, 0.7367, 0.0071, 0.0536, 0.0084, 0.0971, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 24, batch: 37/187] total loss per batch: 0.783
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9721e-01, 5.6544e-02, 8.9984e-05, 4.6005e-02, 1.1126e-05, 1.6679e-06,
        1.4295e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.343

[Epoch: 24, batch: 74/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.1413e-02, 6.0555e-01, 3.0463e-06, 2.2665e-06, 3.3108e-01, 4.4161e-02,
        7.7934e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.622

[Epoch: 24, batch: 111/187] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1809e-02, 5.4177e-01, 3.8712e-05, 1.4578e-01, 9.1940e-03, 1.6263e-01,
        1.2877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 24, batch: 148/187] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.3201, 0.0059, 0.0787, 0.0057, 0.5794, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 24, batch: 185/187] total loss per batch: 0.812
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0766, 0.6976, 0.0113, 0.0532, 0.0132, 0.0879, 0.0602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 25, batch: 37/187] total loss per batch: 0.783
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0786e-01, 5.2955e-02, 5.2635e-05, 3.8983e-02, 1.1189e-05, 1.6804e-06,
        1.3760e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.281

[Epoch: 25, batch: 74/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.2142e-02, 5.6021e-01, 1.4618e-06, 1.5321e-06, 3.7446e-01, 4.2265e-02,
        1.0919e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.573

[Epoch: 25, batch: 111/187] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([9.8000e-03, 4.3063e-01, 3.7680e-05, 1.8257e-01, 6.3294e-03, 2.1890e-01,
        1.5173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 25, batch: 148/187] total loss per batch: 0.767
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0064, 0.2771, 0.0056, 0.0766, 0.0054, 0.6221, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 25, batch: 185/187] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0394, 0.7679, 0.0096, 0.0352, 0.0083, 0.0942, 0.0454],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 26, batch: 37/187] total loss per batch: 0.786
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1180e-01, 4.5190e-02, 5.3450e-05, 4.2841e-02, 8.6336e-06, 1.2066e-06,
        1.0535e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.301

[Epoch: 26, batch: 74/187] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.2509e-03, 5.9662e-01, 4.6261e-06, 4.4712e-06, 3.4571e-01, 4.0168e-02,
        1.0236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.493

[Epoch: 26, batch: 111/187] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.3259e-02, 4.7329e-01, 4.5973e-05, 1.3686e-01, 1.1578e-02, 2.1380e-01,
        1.5116e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 26, batch: 148/187] total loss per batch: 0.769
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0061, 0.2804, 0.0043, 0.0847, 0.0046, 0.6152, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 26, batch: 185/187] total loss per batch: 0.818
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0682, 0.7007, 0.0060, 0.0568, 0.0089, 0.0839, 0.0756],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 27, batch: 37/187] total loss per batch: 0.790
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8257e-01, 4.0642e-02, 1.0392e-04, 7.6452e-02, 2.5440e-05, 1.8108e-06,
        2.0956e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.228

[Epoch: 27, batch: 74/187] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.2781e-02, 5.5202e-01, 1.8802e-06, 1.9801e-06, 3.6863e-01, 5.8692e-02,
        7.8707e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.442

[Epoch: 27, batch: 111/187] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.8416e-03, 4.9027e-01, 3.0268e-05, 2.3029e-01, 6.2265e-03, 1.6902e-01,
        9.5324e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 27, batch: 148/187] total loss per batch: 0.771
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.3075, 0.0052, 0.0795, 0.0059, 0.5892, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 27, batch: 185/187] total loss per batch: 0.820
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0412, 0.7390, 0.0176, 0.0513, 0.0146, 0.0916, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 28, batch: 37/187] total loss per batch: 0.791
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3997e-01, 3.5438e-02, 3.9853e-05, 2.4401e-02, 6.6258e-06, 8.2933e-07,
        1.4320e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.286

[Epoch: 28, batch: 74/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([9.4314e-03, 5.6301e-01, 1.8931e-06, 5.0402e-06, 3.7183e-01, 4.1981e-02,
        1.3739e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.554

[Epoch: 28, batch: 111/187] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.1428e-03, 4.9390e-01, 4.4569e-05, 1.0740e-01, 1.0005e-02, 2.5246e-01,
        1.3104e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.002

[Epoch: 28, batch: 148/187] total loss per batch: 0.773
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0070, 0.2864, 0.0042, 0.0889, 0.0055, 0.6000, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 28, batch: 185/187] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0605, 0.6727, 0.0058, 0.0636, 0.0125, 0.1080, 0.0769],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 29, batch: 37/187] total loss per batch: 0.791
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5443e-01, 7.5965e-02, 1.4392e-04, 6.9242e-02, 1.7154e-05, 2.2658e-06,
        2.0090e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.246

[Epoch: 29, batch: 74/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([9.1178e-03, 6.7310e-01, 1.7593e-06, 2.0036e-06, 2.7268e-01, 3.9175e-02,
        5.9223e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.554

[Epoch: 29, batch: 111/187] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.5697e-02, 4.4027e-01, 2.6305e-05, 2.0224e-01, 6.6012e-03, 1.6155e-01,
        1.7362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 29, batch: 148/187] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0046, 0.2797, 0.0056, 0.0644, 0.0048, 0.6366, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 29, batch: 185/187] total loss per batch: 0.823
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0412, 0.7545, 0.0121, 0.0429, 0.0095, 0.0799, 0.0600],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 30, batch: 37/187] total loss per batch: 0.791
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2471e-01, 3.9006e-02, 2.6524e-05, 3.6122e-02, 2.4690e-05, 1.9693e-06,
        1.0731e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.310

[Epoch: 30, batch: 74/187] total loss per batch: 0.811
Policy (actual, predicted): 1 4
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0544e-02, 4.4671e-01, 2.4035e-06, 3.1337e-06, 4.7727e-01, 4.7208e-02,
        1.8263e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.589

[Epoch: 30, batch: 111/187] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.7488e-03, 5.0450e-01, 2.2534e-05, 2.1816e-01, 5.7954e-03, 1.2019e-01,
        1.4458e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 30, batch: 148/187] total loss per batch: 0.772
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0061, 0.2974, 0.0047, 0.0762, 0.0055, 0.6044, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 30, batch: 185/187] total loss per batch: 0.823
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0618, 0.6559, 0.0160, 0.0675, 0.0162, 0.1045, 0.0781],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 31, batch: 37/187] total loss per batch: 0.789
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8602e-01, 9.5949e-02, 7.5371e-05, 1.1768e-01, 1.1880e-05, 3.1062e-06,
        2.5931e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.287

[Epoch: 31, batch: 74/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.5756e-02, 6.6812e-01, 3.1805e-06, 2.7919e-06, 2.5842e-01, 4.9124e-02,
        8.5784e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.481

[Epoch: 31, batch: 111/187] total loss per batch: 0.765
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([9.2124e-03, 4.4836e-01, 2.8397e-05, 1.0988e-01, 6.9291e-03, 3.0037e-01,
        1.2523e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 31, batch: 148/187] total loss per batch: 0.770
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2726, 0.0060, 0.0874, 0.0075, 0.6109, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 31, batch: 185/187] total loss per batch: 0.819
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0456, 0.7451, 0.0112, 0.0616, 0.0066, 0.0902, 0.0397],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 32, batch: 37/187] total loss per batch: 0.782
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3143e-01, 4.1200e-02, 4.5264e-05, 2.7213e-02, 7.6585e-06, 1.6192e-06,
        1.0410e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.237

[Epoch: 32, batch: 74/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.2640e-02, 5.5948e-01, 3.0628e-06, 1.7117e-06, 3.7430e-01, 4.6378e-02,
        7.1916e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.468

[Epoch: 32, batch: 111/187] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.6034e-03, 5.3803e-01, 3.4701e-05, 1.7811e-01, 7.0095e-03, 1.4509e-01,
        1.2313e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 32, batch: 148/187] total loss per batch: 0.762
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2961, 0.0042, 0.0772, 0.0040, 0.6071, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 32, batch: 185/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0590, 0.7240, 0.0129, 0.0485, 0.0145, 0.0888, 0.0523],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 33, batch: 37/187] total loss per batch: 0.778
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2653e-01, 4.2825e-02, 5.4245e-05, 3.0494e-02, 5.4413e-06, 1.3613e-06,
        8.9397e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.280

[Epoch: 33, batch: 74/187] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0786e-02, 5.5125e-01, 2.4987e-06, 1.8152e-06, 3.7915e-01, 5.0535e-02,
        8.2710e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.610

[Epoch: 33, batch: 111/187] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([9.6821e-03, 4.0621e-01, 2.1682e-05, 1.5773e-01, 8.4224e-03, 2.4016e-01,
        1.7777e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.001

[Epoch: 33, batch: 148/187] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.2577, 0.0052, 0.0906, 0.0048, 0.6294, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 33, batch: 185/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0404, 0.6923, 0.0120, 0.0733, 0.0107, 0.1059, 0.0654],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 34, batch: 37/187] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9322e-01, 6.2682e-02, 7.3301e-05, 4.3902e-02, 1.0810e-05, 1.2115e-06,
        1.0968e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.237

[Epoch: 34, batch: 74/187] total loss per batch: 0.796
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([9.2050e-03, 5.9068e-01, 1.8281e-06, 1.8075e-06, 3.4162e-01, 4.9741e-02,
        8.7472e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.464

[Epoch: 34, batch: 111/187] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.8666e-03, 5.8030e-01, 2.0515e-05, 1.5220e-01, 5.9879e-03, 1.3881e-01,
        1.1582e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 34, batch: 148/187] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.2854, 0.0049, 0.0766, 0.0039, 0.6180, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 34, batch: 185/187] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0401, 0.7460, 0.0111, 0.0453, 0.0083, 0.0953, 0.0540],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 35, batch: 37/187] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2098e-01, 3.9480e-02, 2.6295e-05, 3.9430e-02, 6.1121e-06, 1.2593e-06,
        7.5880e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.306

[Epoch: 35, batch: 74/187] total loss per batch: 0.796
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.9518e-03, 5.9414e-01, 1.2084e-06, 1.0849e-06, 3.4818e-01, 4.4397e-02,
        5.3266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.570

[Epoch: 35, batch: 111/187] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.9200e-03, 4.0123e-01, 1.9060e-05, 1.6578e-01, 8.2598e-03, 2.4027e-01,
        1.7852e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 35, batch: 148/187] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0062, 0.2875, 0.0049, 0.0835, 0.0056, 0.6057, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 35, batch: 185/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0445, 0.7457, 0.0077, 0.0525, 0.0076, 0.0942, 0.0478],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 36, batch: 37/187] total loss per batch: 0.778
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2152e-01, 5.0334e-02, 3.9486e-05, 2.8032e-02, 6.3646e-06, 8.6535e-07,
        6.6366e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.231

[Epoch: 36, batch: 74/187] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.7437e-03, 5.6134e-01, 5.0754e-07, 4.6850e-07, 3.8355e-01, 3.9200e-02,
        8.1649e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.572

[Epoch: 36, batch: 111/187] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([9.7097e-03, 5.4375e-01, 1.9243e-05, 1.7069e-01, 6.7837e-03, 1.5602e-01,
        1.1303e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 36, batch: 148/187] total loss per batch: 0.761
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0047, 0.3081, 0.0046, 0.0756, 0.0041, 0.5972, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 36, batch: 185/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0493, 0.6949, 0.0106, 0.0555, 0.0070, 0.1024, 0.0802],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 37, batch: 37/187] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9998e-01, 5.2509e-02, 4.6335e-05, 4.7366e-02, 8.5164e-06, 1.4363e-06,
        9.0149e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.305

[Epoch: 37, batch: 74/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.9306e-03, 6.1278e-01, 9.5970e-07, 1.0184e-06, 3.2583e-01, 4.9778e-02,
        5.6840e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.483

[Epoch: 37, batch: 111/187] total loss per batch: 0.757
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.1681e-03, 4.9362e-01, 1.5101e-05, 1.3438e-01, 7.4772e-03, 2.2205e-01,
        1.3529e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 37, batch: 148/187] total loss per batch: 0.764
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2822, 0.0042, 0.0852, 0.0044, 0.6125, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 37, batch: 185/187] total loss per batch: 0.812
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0387, 0.8099, 0.0073, 0.0417, 0.0074, 0.0564, 0.0385],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 38, batch: 37/187] total loss per batch: 0.782
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2712e-01, 3.7606e-02, 3.0871e-05, 3.5183e-02, 6.4290e-06, 8.7458e-07,
        5.6645e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.221

[Epoch: 38, batch: 74/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0188e-02, 5.4334e-01, 1.0761e-06, 1.3983e-06, 3.9609e-01, 4.1787e-02,
        8.5853e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.536

[Epoch: 38, batch: 111/187] total loss per batch: 0.759
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.0515e-02, 3.6913e-01, 2.1227e-05, 2.0782e-01, 8.1875e-03, 2.0216e-01,
        2.0217e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 38, batch: 148/187] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0051, 0.2947, 0.0042, 0.0768, 0.0054, 0.6057, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.002

[Epoch: 38, batch: 185/187] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0555, 0.6314, 0.0142, 0.0647, 0.0088, 0.1513, 0.0740],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 39, batch: 37/187] total loss per batch: 0.784
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9568e-01, 6.0849e-02, 1.1243e-04, 4.3243e-02, 1.4019e-05, 1.4525e-06,
        9.7589e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.294

[Epoch: 39, batch: 74/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.0255e-03, 6.1184e-01, 1.1580e-06, 6.4335e-07, 3.2320e-01, 5.1076e-02,
        6.8545e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.572

[Epoch: 39, batch: 111/187] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.8171e-03, 5.7197e-01, 2.6124e-05, 1.2428e-01, 9.1821e-03, 1.7758e-01,
        1.0914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 39, batch: 148/187] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0052, 0.3096, 0.0061, 0.0741, 0.0047, 0.5944, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 39, batch: 185/187] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0421, 0.8112, 0.0081, 0.0446, 0.0051, 0.0479, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 40, batch: 37/187] total loss per batch: 0.784
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2203e-01, 3.5433e-02, 3.3864e-05, 4.2375e-02, 4.4121e-06, 1.3876e-06,
        1.2440e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.313

[Epoch: 40, batch: 74/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.4408e-03, 5.9528e-01, 2.5429e-06, 1.5046e-06, 3.5296e-01, 3.8644e-02,
        8.6707e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.517

[Epoch: 40, batch: 111/187] total loss per batch: 0.760
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1888e-02, 4.3118e-01, 1.2254e-05, 1.9345e-01, 6.4461e-03, 2.1377e-01,
        1.4325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.001

[Epoch: 40, batch: 148/187] total loss per batch: 0.768
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.3199, 0.0061, 0.0844, 0.0060, 0.5703, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 40, batch: 185/187] total loss per batch: 0.813
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0575, 0.6853, 0.0080, 0.0592, 0.0123, 0.1182, 0.0595],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 41, batch: 37/187] total loss per batch: 0.783
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0221e-01, 6.0250e-02, 7.1675e-05, 3.7395e-02, 1.1570e-05, 1.7656e-06,
        6.0269e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.208

[Epoch: 41, batch: 74/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([1.0214e-02, 5.3394e-01, 2.0690e-06, 1.2910e-06, 3.9906e-01, 4.7066e-02,
        9.7087e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.492

[Epoch: 41, batch: 111/187] total loss per batch: 0.758
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.0118e-03, 5.4011e-01, 2.3686e-05, 1.1756e-01, 8.0824e-03, 1.5422e-01,
        1.7200e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 41, batch: 148/187] total loss per batch: 0.765
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0066, 0.2817, 0.0043, 0.0785, 0.0044, 0.6187, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 41, batch: 185/187] total loss per batch: 0.812
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0572, 0.7103, 0.0109, 0.0591, 0.0067, 0.0979, 0.0577],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 42, batch: 37/187] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0668e-01, 5.2001e-02, 4.0054e-05, 4.1206e-02, 5.1456e-06, 1.2023e-06,
        7.0328e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.309

[Epoch: 42, batch: 74/187] total loss per batch: 0.796
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.0754e-03, 5.8338e-01, 1.2234e-06, 1.0867e-06, 3.5362e-01, 4.8553e-02,
        8.3639e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.528

[Epoch: 42, batch: 111/187] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.0542e-03, 4.0270e-01, 1.6545e-05, 1.7785e-01, 6.2467e-03, 2.4579e-01,
        1.5935e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 42, batch: 148/187] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0050, 0.2791, 0.0041, 0.0840, 0.0046, 0.6173, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 42, batch: 185/187] total loss per batch: 0.805
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0491, 0.7026, 0.0090, 0.0581, 0.0089, 0.1041, 0.0681],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 43, batch: 37/187] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1893e-01, 4.4961e-02, 4.0018e-05, 3.5998e-02, 6.3233e-06, 1.7123e-06,
        5.9566e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.237

[Epoch: 43, batch: 74/187] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.7872e-03, 6.0896e-01, 1.0157e-06, 8.8791e-07, 3.3793e-01, 4.0228e-02,
        7.0967e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.532

[Epoch: 43, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.2527e-03, 5.5494e-01, 1.3556e-05, 1.5284e-01, 5.1611e-03, 1.5752e-01,
        1.2227e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 43, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2874, 0.0044, 0.0798, 0.0047, 0.6125, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 43, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0398, 0.7676, 0.0070, 0.0536, 0.0064, 0.0802, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 44, batch: 37/187] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0898e-01, 4.6425e-02, 3.4280e-05, 4.4496e-02, 5.2741e-06, 1.2685e-06,
        5.5679e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.281

[Epoch: 44, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.7875e-03, 5.5622e-01, 1.0842e-06, 8.0240e-07, 3.8373e-01, 4.6802e-02,
        6.4640e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.554

[Epoch: 44, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.4995e-03, 4.3172e-01, 1.3400e-05, 1.8461e-01, 5.9269e-03, 2.0323e-01,
        1.6600e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 44, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.2874, 0.0047, 0.0848, 0.0051, 0.6061, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 44, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0477, 0.7013, 0.0085, 0.0664, 0.0085, 0.0989, 0.0688],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 45, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1221e-01, 4.7327e-02, 2.9046e-05, 4.0362e-02, 5.2587e-06, 1.0545e-06,
        6.8822e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.264

[Epoch: 45, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.8086e-03, 6.0396e-01, 7.2024e-07, 6.5778e-07, 3.4449e-01, 4.0107e-02,
        6.6316e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.480

[Epoch: 45, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.2602e-03, 5.4685e-01, 1.0054e-05, 1.4833e-01, 4.3039e-03, 1.7508e-01,
        1.1816e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 45, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2920, 0.0048, 0.0832, 0.0048, 0.6036, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 45, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0407, 0.7632, 0.0060, 0.0464, 0.0060, 0.0923, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 46, batch: 37/187] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1889e-01, 4.5278e-02, 2.2540e-05, 3.5777e-02, 3.6918e-06, 1.0899e-06,
        3.1962e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.279

[Epoch: 46, batch: 74/187] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.7502e-03, 5.7926e-01, 5.9305e-07, 4.7866e-07, 3.6151e-01, 4.5230e-02,
        7.2543e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.546

[Epoch: 46, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.3566e-03, 4.3757e-01, 1.3076e-05, 1.4605e-01, 7.1784e-03, 2.2030e-01,
        1.8053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 46, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0049, 0.2967, 0.0048, 0.0789, 0.0047, 0.6040, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 46, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0478, 0.7279, 0.0079, 0.0575, 0.0087, 0.0819, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 47, batch: 37/187] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0406e-01, 5.3862e-02, 2.4134e-05, 4.1970e-02, 4.2980e-06, 1.5407e-06,
        7.7858e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.275

[Epoch: 47, batch: 74/187] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.7826e-03, 5.2856e-01, 1.1624e-06, 1.0077e-06, 4.1339e-01, 4.5907e-02,
        6.3540e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.516

[Epoch: 47, batch: 111/187] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.1139e-03, 4.9125e-01, 9.4456e-06, 2.1385e-01, 3.7155e-03, 1.6617e-01,
        1.1789e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 47, batch: 148/187] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2680, 0.0043, 0.0956, 0.0051, 0.6153, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 47, batch: 185/187] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0510, 0.6975, 0.0066, 0.0581, 0.0057, 0.1321, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 48, batch: 37/187] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9897e-01, 6.1274e-02, 4.0537e-05, 3.9665e-02, 5.1398e-06, 1.0110e-06,
        4.2702e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.180

[Epoch: 48, batch: 74/187] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7852e-03, 6.6098e-01, 7.9719e-07, 7.6156e-07, 2.9200e-01, 3.7273e-02,
        4.9635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.459

[Epoch: 48, batch: 111/187] total loss per batch: 0.755
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.1323e-03, 5.2339e-01, 2.2778e-05, 1.0969e-01, 7.7737e-03, 2.2239e-01,
        1.3060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 48, batch: 148/187] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2971, 0.0056, 0.0817, 0.0043, 0.6000, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 48, batch: 185/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0393, 0.7907, 0.0058, 0.0454, 0.0075, 0.0566, 0.0547],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 49, batch: 37/187] total loss per batch: 0.779
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0756e-01, 4.8157e-02, 2.8651e-05, 4.4187e-02, 3.3219e-06, 2.8198e-06,
        6.4929e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.326

[Epoch: 49, batch: 74/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.9685e-03, 4.7622e-01, 2.2997e-06, 1.3226e-06, 4.4855e-01, 5.8734e-02,
        8.5250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.640

[Epoch: 49, batch: 111/187] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([1.1264e-02, 4.0983e-01, 1.8987e-05, 2.3530e-01, 4.4197e-03, 1.7786e-01,
        1.6130e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 49, batch: 148/187] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2671, 0.0036, 0.0895, 0.0046, 0.6237, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 49, batch: 185/187] total loss per batch: 0.810
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0446, 0.6633, 0.0078, 0.0792, 0.0087, 0.1258, 0.0706],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 50, batch: 37/187] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9421e-01, 6.1570e-02, 2.7331e-05, 4.4128e-02, 6.5726e-06, 7.6512e-07,
        5.7398e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.252

[Epoch: 50, batch: 74/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.2795e-03, 7.0020e-01, 5.8440e-07, 1.3329e-06, 2.5675e-01, 3.3858e-02,
        3.9072e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.433

[Epoch: 50, batch: 111/187] total loss per batch: 0.756
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([4.5169e-03, 5.6617e-01, 1.8220e-05, 1.1385e-01, 5.9104e-03, 1.9994e-01,
        1.0960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 50, batch: 148/187] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.3202, 0.0053, 0.0746, 0.0045, 0.5846, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 50, batch: 185/187] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0449, 0.7472, 0.0076, 0.0562, 0.0067, 0.0873, 0.0500],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 51, batch: 37/187] total loss per batch: 0.778
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3377e-01, 3.3065e-02, 2.3464e-05, 3.3079e-02, 4.8748e-06, 1.8405e-06,
        5.8281e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.248

[Epoch: 51, batch: 74/187] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([9.5553e-03, 4.8098e-01, 2.8150e-06, 1.5035e-06, 4.4346e-01, 5.6883e-02,
        9.1173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.612

[Epoch: 51, batch: 111/187] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([8.3644e-03, 4.1743e-01, 1.8397e-05, 1.6374e-01, 5.9366e-03, 2.1520e-01,
        1.8930e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 51, batch: 148/187] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2589, 0.0045, 0.0956, 0.0053, 0.6238, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 51, batch: 185/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0403, 0.7319, 0.0085, 0.0572, 0.0067, 0.0980, 0.0574],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 52, batch: 37/187] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9887e-01, 5.5784e-02, 2.7737e-05, 4.5240e-02, 7.6258e-06, 1.4381e-06,
        7.0876e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.256

[Epoch: 52, batch: 74/187] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.4204e-03, 5.9017e-01, 1.0804e-06, 1.4468e-06, 3.4707e-01, 4.9325e-02,
        6.0183e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.487

[Epoch: 52, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.1106e-03, 4.8988e-01, 1.2080e-05, 1.7376e-01, 4.7671e-03, 1.7353e-01,
        1.5094e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 52, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2991, 0.0050, 0.0814, 0.0043, 0.5987, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 52, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0445, 0.7193, 0.0076, 0.0537, 0.0055, 0.1047, 0.0645],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 53, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2584e-01, 3.9143e-02, 2.1873e-05, 3.4943e-02, 4.7365e-06, 1.5434e-06,
        4.5278e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.291

[Epoch: 53, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.8023e-03, 5.8243e-01, 9.7554e-07, 1.2938e-06, 3.6127e-01, 4.3663e-02,
        5.8374e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.541

[Epoch: 53, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.6310e-03, 4.9069e-01, 1.4182e-05, 1.6017e-01, 5.7164e-03, 2.0582e-01,
        1.3195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 53, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.2852, 0.0046, 0.0855, 0.0046, 0.6084, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 53, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0393, 0.7562, 0.0065, 0.0519, 0.0058, 0.0861, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 54, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0749e-01, 5.2897e-02, 1.7354e-05, 3.9535e-02, 5.5598e-06, 1.3058e-06,
        5.0278e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.278

[Epoch: 54, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.0424e-03, 5.7909e-01, 4.9906e-07, 6.8025e-07, 3.6419e-01, 4.4225e-02,
        5.4531e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.535

[Epoch: 54, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.2978e-03, 4.7261e-01, 1.2386e-05, 1.6662e-01, 4.6745e-03, 1.8965e-01,
        1.5914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 54, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2893, 0.0047, 0.0884, 0.0045, 0.6008, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 54, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0461, 0.7282, 0.0061, 0.0594, 0.0052, 0.1004, 0.0546],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 55, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2090e-01, 4.4448e-02, 1.9289e-05, 3.4583e-02, 4.5045e-06, 1.2614e-06,
        4.1990e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.269

[Epoch: 55, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.1980e-03, 5.9514e-01, 4.0679e-07, 6.1578e-07, 3.5117e-01, 4.3249e-02,
        5.2476e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.551

[Epoch: 55, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.5414e-03, 5.1225e-01, 9.7099e-06, 1.5894e-01, 5.4654e-03, 1.8910e-01,
        1.2869e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 55, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0051, 0.2880, 0.0045, 0.0788, 0.0049, 0.6129, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 55, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0457, 0.7165, 0.0082, 0.0611, 0.0058, 0.0939, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 56, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0231e-01, 5.4307e-02, 1.4933e-05, 4.3321e-02, 4.9249e-06, 1.4587e-06,
        4.4506e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.280

[Epoch: 56, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.7344e-03, 5.8001e-01, 4.0668e-07, 6.6086e-07, 3.6809e-01, 3.9013e-02,
        6.1467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.492

[Epoch: 56, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.1026e-03, 4.5056e-01, 1.0235e-05, 1.7522e-01, 4.6353e-03, 2.1912e-01,
        1.4435e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 56, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2854, 0.0050, 0.0918, 0.0046, 0.6015, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 56, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0422, 0.7561, 0.0054, 0.0497, 0.0045, 0.0945, 0.0475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 57, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3065e-01, 3.7576e-02, 1.7580e-05, 3.1719e-02, 3.4568e-06, 8.0528e-07,
        3.4927e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.264

[Epoch: 57, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.4049e-03, 5.8655e-01, 2.9422e-07, 4.9579e-07, 3.5969e-01, 4.4513e-02,
        4.8446e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.545

[Epoch: 57, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.5130e-03, 4.9099e-01, 1.1232e-05, 1.6754e-01, 6.0429e-03, 1.7721e-01,
        1.5169e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 57, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2853, 0.0043, 0.0811, 0.0048, 0.6132, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 57, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0521, 0.6613, 0.0084, 0.0813, 0.0073, 0.1073, 0.0823],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 58, batch: 37/187] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0045e-01, 5.7083e-02, 1.6354e-05, 4.2404e-02, 4.5547e-06, 1.4375e-06,
        4.4134e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.262

[Epoch: 58, batch: 74/187] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.2319e-03, 5.9618e-01, 5.8946e-07, 4.7935e-07, 3.5492e-01, 3.6820e-02,
        5.8406e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.505

[Epoch: 58, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.1855e-03, 5.0495e-01, 8.4560e-06, 1.6034e-01, 3.8605e-03, 1.9345e-01,
        1.3221e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 58, batch: 148/187] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2726, 0.0060, 0.0911, 0.0048, 0.6139, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 58, batch: 185/187] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0437, 0.7798, 0.0058, 0.0388, 0.0050, 0.0857, 0.0412],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 59, batch: 37/187] total loss per batch: 0.776
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2026e-01, 4.9946e-02, 2.0146e-05, 2.9724e-02, 5.6610e-06, 1.1176e-06,
        3.9535e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.258

[Epoch: 59, batch: 74/187] total loss per batch: 0.796
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.2521e-03, 5.6550e-01, 2.3904e-07, 6.2492e-07, 3.8415e-01, 4.2000e-02,
        4.0951e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.526

[Epoch: 59, batch: 111/187] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.2325e-03, 4.5072e-01, 1.3623e-05, 1.7521e-01, 5.7912e-03, 2.2123e-01,
        1.4081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 59, batch: 148/187] total loss per batch: 0.759
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0047, 0.2800, 0.0041, 0.0912, 0.0044, 0.6097, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 59, batch: 185/187] total loss per batch: 0.807
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0464, 0.6898, 0.0080, 0.0827, 0.0101, 0.0942, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 60, batch: 37/187] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8827e-01, 6.1731e-02, 1.4925e-05, 4.9936e-02, 3.4656e-06, 1.1400e-06,
        4.0765e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.286

[Epoch: 60, batch: 74/187] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.5056e-03, 6.2330e-01, 7.8599e-07, 9.4791e-07, 3.2248e-01, 4.0437e-02,
        7.2718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.489

[Epoch: 60, batch: 111/187] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.0990e-03, 5.0233e-01, 1.0365e-05, 1.5598e-01, 5.7706e-03, 1.6336e-01,
        1.6545e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 60, batch: 148/187] total loss per batch: 0.758
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2987, 0.0048, 0.0897, 0.0045, 0.5908, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 60, batch: 185/187] total loss per batch: 0.806
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0474, 0.7407, 0.0063, 0.0486, 0.0051, 0.1044, 0.0475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 61, batch: 37/187] total loss per batch: 0.776
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2350e-01, 4.4638e-02, 1.6287e-05, 3.1802e-02, 6.4372e-06, 1.3851e-06,
        3.4422e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.281

[Epoch: 61, batch: 74/187] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.3063e-03, 5.2924e-01, 4.2878e-07, 6.7337e-07, 4.1550e-01, 4.2121e-02,
        5.8287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.589

[Epoch: 61, batch: 111/187] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.6303e-03, 5.0571e-01, 9.7415e-06, 1.5536e-01, 4.5707e-03, 2.0188e-01,
        1.2684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 61, batch: 148/187] total loss per batch: 0.757
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0053, 0.2854, 0.0055, 0.0937, 0.0056, 0.5983, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 61, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0451, 0.7219, 0.0051, 0.0556, 0.0061, 0.1009, 0.0654],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 62, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2222e-01, 4.4389e-02, 1.5836e-05, 3.3339e-02, 4.0814e-06, 8.2051e-07,
        3.3953e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.245

[Epoch: 62, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.0570e-03, 5.9073e-01, 6.6142e-07, 8.1718e-07, 3.5359e-01, 4.2008e-02,
        6.6162e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.515

[Epoch: 62, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([7.1335e-03, 4.6062e-01, 1.2665e-05, 1.7351e-01, 6.2627e-03, 2.0703e-01,
        1.4543e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 62, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2902, 0.0047, 0.0851, 0.0050, 0.6036, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 62, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0455, 0.7361, 0.0053, 0.0552, 0.0063, 0.0942, 0.0573],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 63, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1497e-01, 4.9848e-02, 1.5036e-05, 3.5130e-02, 4.0078e-06, 1.2049e-06,
        3.3420e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.279

[Epoch: 63, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.3586e-03, 5.8244e-01, 3.7786e-07, 5.2620e-07, 3.6614e-01, 4.0415e-02,
        5.6486e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 63, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.2864e-03, 4.8969e-01, 1.0571e-05, 1.6073e-01, 5.1518e-03, 1.9667e-01,
        1.4146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 63, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2891, 0.0049, 0.0912, 0.0057, 0.5974, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 63, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0432, 0.7385, 0.0044, 0.0535, 0.0048, 0.0978, 0.0578],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 64, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1812e-01, 4.8061e-02, 1.6131e-05, 3.3768e-02, 4.2819e-06, 9.7234e-07,
        3.2900e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.256

[Epoch: 64, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.8257e-03, 5.9960e-01, 2.4632e-07, 3.9234e-07, 3.5219e-01, 3.8462e-02,
        4.9211e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.510

[Epoch: 64, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.2236e-03, 4.8301e-01, 8.9360e-06, 1.7366e-01, 5.0998e-03, 1.9070e-01,
        1.4129e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 64, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2925, 0.0046, 0.0905, 0.0051, 0.5956, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 64, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0434, 0.7296, 0.0049, 0.0615, 0.0057, 0.0949, 0.0599],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 65, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0758e-01, 5.3987e-02, 1.6351e-05, 3.8380e-02, 4.0929e-06, 1.1086e-06,
        3.4756e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.280

[Epoch: 65, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.3560e-03, 5.6134e-01, 2.8127e-07, 4.3379e-07, 3.8834e-01, 3.9409e-02,
        5.5550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.537

[Epoch: 65, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.7597e-03, 4.7436e-01, 9.5983e-06, 1.5613e-01, 5.4330e-03, 2.0041e-01,
        1.5690e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 65, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2931, 0.0049, 0.0891, 0.0050, 0.5963, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 65, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0430, 0.7402, 0.0052, 0.0577, 0.0047, 0.0896, 0.0597],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 66, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2025e-01, 4.7311e-02, 1.4196e-05, 3.2389e-02, 4.0142e-06, 9.7446e-07,
        3.4046e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.277

[Epoch: 66, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.5390e-03, 6.2034e-01, 1.7440e-07, 3.4248e-07, 3.3155e-01, 3.9039e-02,
        4.5318e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 66, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.1502e-03, 5.0846e-01, 7.6371e-06, 1.6557e-01, 5.1560e-03, 1.8095e-01,
        1.3370e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 66, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0061, 0.2839, 0.0043, 0.0889, 0.0051, 0.6061, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 66, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0469, 0.7077, 0.0056, 0.0668, 0.0061, 0.1007, 0.0661],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 67, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0848e-01, 4.8802e-02, 1.4338e-05, 4.2668e-02, 4.0397e-06, 1.0080e-06,
        3.4200e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.257

[Epoch: 67, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.3273e-03, 5.4574e-01, 3.2551e-07, 4.7857e-07, 3.9984e-01, 4.2653e-02,
        5.4360e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 67, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.3091e-03, 4.5697e-01, 8.1534e-06, 1.5210e-01, 5.9594e-03, 2.2600e-01,
        1.5366e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 67, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0053, 0.2921, 0.0054, 0.0855, 0.0046, 0.6015, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.001

[Epoch: 67, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0404, 0.7508, 0.0051, 0.0519, 0.0052, 0.0924, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 68, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2266e-01, 5.0385e-02, 1.2312e-05, 2.6910e-02, 3.5406e-06, 8.3864e-07,
        2.8558e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.291

[Epoch: 68, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7235e-03, 6.3418e-01, 2.2437e-07, 4.1072e-07, 3.1503e-01, 4.0978e-02,
        5.0885e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.526

[Epoch: 68, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.4291e-03, 5.4070e-01, 8.2786e-06, 1.6942e-01, 4.5317e-03, 1.4230e-01,
        1.3661e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 68, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0061, 0.2898, 0.0042, 0.0911, 0.0053, 0.5975, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 68, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0488, 0.7381, 0.0060, 0.0572, 0.0055, 0.0878, 0.0565],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 69, batch: 37/187] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0554e-01, 4.7826e-02, 1.4797e-05, 4.6584e-02, 3.9856e-06, 8.9156e-07,
        3.2221e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.221

[Epoch: 69, batch: 74/187] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.4812e-03, 5.4556e-01, 3.5683e-07, 5.3153e-07, 3.9908e-01, 4.3311e-02,
        5.5740e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.462

[Epoch: 69, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.1885e-03, 4.1708e-01, 6.4771e-06, 1.5698e-01, 5.2998e-03, 2.5095e-01,
        1.6450e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 69, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0046, 0.2925, 0.0052, 0.0766, 0.0045, 0.6117, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 69, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0482, 0.7005, 0.0061, 0.0659, 0.0056, 0.1036, 0.0701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 70, batch: 37/187] total loss per batch: 0.776
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.3166e-01, 4.0258e-02, 7.4110e-06, 2.8050e-02, 3.3004e-06, 7.5986e-07,
        2.5042e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.302

[Epoch: 70, batch: 74/187] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.2116e-03, 6.0703e-01, 5.5403e-07, 9.5559e-07, 3.4135e-01, 4.0164e-02,
        5.2498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.613

[Epoch: 70, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.9524e-03, 5.3664e-01, 9.3889e-06, 1.6939e-01, 5.2743e-03, 1.5361e-01,
        1.2912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 70, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0066, 0.2889, 0.0045, 0.0890, 0.0051, 0.6001, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 70, batch: 185/187] total loss per batch: 0.804
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0456, 0.7561, 0.0055, 0.0496, 0.0057, 0.0850, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 71, batch: 37/187] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9544e-01, 5.8480e-02, 1.2514e-05, 4.6020e-02, 4.6318e-06, 1.5481e-06,
        3.7571e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.262

[Epoch: 71, batch: 74/187] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([7.4199e-03, 5.9032e-01, 3.4976e-07, 3.9479e-07, 3.5178e-01, 4.4871e-02,
        5.6062e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.509

[Epoch: 71, batch: 111/187] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.1604e-03, 4.3618e-01, 7.9540e-06, 1.8308e-01, 4.8889e-03, 2.2006e-01,
        1.4962e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 71, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2882, 0.0051, 0.0852, 0.0048, 0.6062, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 71, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0495, 0.6829, 0.0063, 0.0688, 0.0055, 0.1211, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 72, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9572e-01, 5.9640e-02, 1.1905e-05, 4.4583e-02, 4.2776e-06, 1.2722e-06,
        3.9854e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.242

[Epoch: 72, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([6.2519e-03, 5.9734e-01, 4.6245e-07, 6.1572e-07, 3.5131e-01, 3.9812e-02,
        5.2903e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.497

[Epoch: 72, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.5489e-03, 4.9458e-01, 6.4209e-06, 1.5350e-01, 6.0534e-03, 1.9753e-01,
        1.4278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 72, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2868, 0.0047, 0.0847, 0.0049, 0.6076, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 72, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0447, 0.7449, 0.0050, 0.0569, 0.0058, 0.0861, 0.0566],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 73, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1297e-01, 5.0002e-02, 1.0065e-05, 3.6981e-02, 3.0414e-06, 9.2762e-07,
        3.2523e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.269

[Epoch: 73, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.8793e-03, 5.6073e-01, 3.0752e-07, 3.7873e-07, 3.8562e-01, 4.2954e-02,
        4.8144e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.549

[Epoch: 73, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.1982e-03, 4.9101e-01, 7.0753e-06, 1.6504e-01, 4.8987e-03, 1.9194e-01,
        1.4091e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 73, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2963, 0.0047, 0.0859, 0.0046, 0.5979, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 73, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0447, 0.7303, 0.0049, 0.0583, 0.0051, 0.0985, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 74, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1588e-01, 4.7399e-02, 9.1001e-06, 3.6684e-02, 2.7927e-06, 8.6775e-07,
        2.5829e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.272

[Epoch: 74, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.1901e-03, 5.8689e-01, 1.8879e-07, 3.3108e-07, 3.6359e-01, 3.9652e-02,
        4.6790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 74, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.7580e-03, 4.8211e-01, 6.1278e-06, 1.6453e-01, 5.6347e-03, 1.9211e-01,
        1.4985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 74, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2935, 0.0047, 0.0856, 0.0047, 0.6001, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 74, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0459, 0.7382, 0.0050, 0.0563, 0.0051, 0.0917, 0.0577],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 75, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1219e-01, 5.0975e-02, 9.2481e-06, 3.6790e-02, 2.8276e-06, 8.3063e-07,
        2.9053e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.273

[Epoch: 75, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.2649e-03, 5.8365e-01, 2.0098e-07, 3.3177e-07, 3.6717e-01, 3.9170e-02,
        4.7458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.546

[Epoch: 75, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.7953e-03, 4.9034e-01, 6.2456e-06, 1.5889e-01, 5.1236e-03, 1.9968e-01,
        1.4017e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 75, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2880, 0.0047, 0.0836, 0.0048, 0.6080, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 75, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0454, 0.7274, 0.0048, 0.0611, 0.0052, 0.0944, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 76, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1545e-01, 4.8596e-02, 8.0369e-06, 3.5918e-02, 2.6077e-06, 8.4184e-07,
        2.2780e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.274

[Epoch: 76, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.1738e-03, 5.8710e-01, 1.4807e-07, 2.7198e-07, 3.6314e-01, 3.9670e-02,
        4.9151e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.513

[Epoch: 76, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.6551e-03, 4.7985e-01, 5.7565e-06, 1.6452e-01, 5.4806e-03, 1.9725e-01,
        1.4723e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 76, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2903, 0.0049, 0.0863, 0.0049, 0.6020, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 76, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0459, 0.7339, 0.0052, 0.0573, 0.0047, 0.0953, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 77, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1383e-01, 4.9918e-02, 7.7908e-06, 3.6220e-02, 2.6394e-06, 7.4943e-07,
        2.5114e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.275

[Epoch: 77, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.2508e-03, 5.9050e-01, 1.4096e-07, 2.4132e-07, 3.6038e-01, 3.9356e-02,
        4.5102e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.549

[Epoch: 77, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.4657e-03, 4.9178e-01, 5.4964e-06, 1.6940e-01, 4.7912e-03, 1.9005e-01,
        1.3851e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 77, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2875, 0.0049, 0.0852, 0.0051, 0.6060, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 77, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0439, 0.7314, 0.0046, 0.0623, 0.0053, 0.0925, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 78, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1160e-01, 5.0670e-02, 8.8077e-06, 3.7699e-02, 2.5554e-06, 8.6881e-07,
        2.2749e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.260

[Epoch: 78, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.9275e-03, 5.7695e-01, 1.0271e-07, 2.1705e-07, 3.7217e-01, 4.0753e-02,
        5.2055e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.485

[Epoch: 78, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.5025e-03, 4.8477e-01, 4.8183e-06, 1.5630e-01, 5.3433e-03, 1.9763e-01,
        1.5045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 78, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2887, 0.0049, 0.0875, 0.0051, 0.6021, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 78, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0496, 0.7211, 0.0054, 0.0576, 0.0046, 0.0964, 0.0652],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 79, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2065e-01, 4.7624e-02, 6.1427e-06, 3.1695e-02, 2.3027e-06, 5.1440e-07,
        2.2153e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.277

[Epoch: 79, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.1910e-03, 5.9696e-01, 1.1561e-07, 2.2059e-07, 3.5392e-01, 3.9871e-02,
        4.0594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.578

[Epoch: 79, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([6.2275e-03, 4.6740e-01, 6.0952e-06, 1.8107e-01, 5.2830e-03, 2.0372e-01,
        1.3629e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 79, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2781, 0.0049, 0.0848, 0.0046, 0.6165, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 79, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0437, 0.7228, 0.0051, 0.0680, 0.0061, 0.0989, 0.0556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 80, batch: 37/187] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0718e-01, 4.8195e-02, 9.0746e-06, 4.4586e-02, 2.9838e-06, 1.1875e-06,
        2.3988e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.257

[Epoch: 80, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.5562e-03, 5.7456e-01, 1.1337e-07, 2.5355e-07, 3.7712e-01, 3.8440e-02,
        5.3229e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.481

[Epoch: 80, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([4.8052e-03, 5.0931e-01, 6.4207e-06, 1.4528e-01, 4.7946e-03, 1.7697e-01,
        1.5883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 80, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0054, 0.2914, 0.0046, 0.0869, 0.0048, 0.6015, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 80, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0493, 0.7306, 0.0059, 0.0501, 0.0054, 0.0942, 0.0644],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 81, batch: 37/187] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2148e-01, 4.9148e-02, 8.0088e-06, 2.9335e-02, 2.3167e-06, 6.3089e-07,
        2.7112e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.239

[Epoch: 81, batch: 74/187] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7320e-03, 5.8400e-01, 1.3354e-07, 2.9295e-07, 3.6454e-01, 4.2275e-02,
        4.4499e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.565

[Epoch: 81, batch: 111/187] total loss per batch: 0.750
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.9624e-03, 4.7736e-01, 5.5102e-06, 1.7257e-01, 4.6746e-03, 2.0483e-01,
        1.3460e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 81, batch: 148/187] total loss per batch: 0.756
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2922, 0.0053, 0.0827, 0.0042, 0.6052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 81, batch: 185/187] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0434, 0.7268, 0.0062, 0.0641, 0.0056, 0.0952, 0.0587],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 82, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1224e-01, 4.8611e-02, 7.4970e-06, 3.9107e-02, 2.4614e-06, 9.0956e-07,
        2.6207e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.276

[Epoch: 82, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.7629e-03, 5.6537e-01, 1.9673e-07, 3.4464e-07, 3.7828e-01, 4.5175e-02,
        5.4204e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.530

[Epoch: 82, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([4.9357e-03, 4.8480e-01, 5.6551e-06, 1.6197e-01, 4.9308e-03, 1.9212e-01,
        1.5124e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 82, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0052, 0.2896, 0.0045, 0.0831, 0.0045, 0.6078, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 82, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0443, 0.7378, 0.0055, 0.0551, 0.0051, 0.0940, 0.0583],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 83, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1591e-01, 4.7212e-02, 7.5895e-06, 3.6847e-02, 2.3820e-06, 8.1134e-07,
        2.3574e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.266

[Epoch: 83, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.8985e-03, 6.1917e-01, 1.1427e-07, 2.6086e-07, 3.3079e-01, 4.0304e-02,
        4.8337e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.497

[Epoch: 83, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.2703e-03, 4.8354e-01, 5.1059e-06, 1.6975e-01, 4.8892e-03, 1.9622e-01,
        1.4033e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 83, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.2910, 0.0048, 0.0839, 0.0047, 0.6046, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 83, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0443, 0.7356, 0.0051, 0.0591, 0.0049, 0.0927, 0.0583],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 84, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1421e-01, 4.9963e-02, 7.3563e-06, 3.5789e-02, 2.1708e-06, 7.8585e-07,
        2.2750e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.271

[Epoch: 84, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.3641e-03, 5.5478e-01, 1.4857e-07, 2.4013e-07, 3.9323e-01, 4.1817e-02,
        4.8042e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.544

[Epoch: 84, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.2642e-03, 4.8715e-01, 4.9323e-06, 1.6159e-01, 5.0021e-03, 1.9388e-01,
        1.4710e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 84, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2900, 0.0048, 0.0838, 0.0049, 0.6057, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 84, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0435, 0.7324, 0.0049, 0.0584, 0.0046, 0.0960, 0.0602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 85, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0882e-01, 5.3805e-02, 7.8377e-06, 3.7341e-02, 2.3147e-06, 7.5772e-07,
        2.2127e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.268

[Epoch: 85, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7390e-03, 6.0496e-01, 9.2394e-08, 2.2531e-07, 3.4672e-01, 3.9133e-02,
        4.4495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.509

[Epoch: 85, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.1191e-03, 4.8437e-01, 5.0536e-06, 1.6613e-01, 5.2456e-03, 1.9371e-01,
        1.4541e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 85, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2916, 0.0047, 0.0853, 0.0048, 0.6024, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 85, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0447, 0.7327, 0.0050, 0.0597, 0.0047, 0.0930, 0.0602],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 86, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1448e-01, 4.9438e-02, 7.4909e-06, 3.6052e-02, 2.1611e-06, 7.4307e-07,
        2.2185e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.266

[Epoch: 86, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.0132e-03, 5.7505e-01, 1.0072e-07, 1.9145e-07, 3.7493e-01, 4.0306e-02,
        4.7004e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.543

[Epoch: 86, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.3976e-03, 4.8531e-01, 4.5963e-06, 1.6675e-01, 4.8390e-03, 1.9297e-01,
        1.4474e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 86, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2916, 0.0048, 0.0846, 0.0048, 0.6032, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 86, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0439, 0.7320, 0.0048, 0.0589, 0.0046, 0.0967, 0.0589],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 87, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1290e-01, 5.1533e-02, 7.0681e-06, 3.5534e-02, 2.0799e-06, 7.2009e-07,
        2.1212e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.271

[Epoch: 87, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.9761e-03, 5.8865e-01, 8.6916e-08, 2.0656e-07, 3.6101e-01, 4.0684e-02,
        4.6763e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.507

[Epoch: 87, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([4.9508e-03, 4.8098e-01, 4.6753e-06, 1.6019e-01, 5.4360e-03, 1.9896e-01,
        1.4948e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 87, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2918, 0.0049, 0.0868, 0.0048, 0.6004, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 87, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0465, 0.7268, 0.0053, 0.0598, 0.0049, 0.0954, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 88, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1679e-01, 4.7720e-02, 6.3147e-06, 3.5462e-02, 2.0400e-06, 6.9127e-07,
        1.9340e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.259

[Epoch: 88, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.9073e-03, 5.8550e-01, 9.8482e-08, 2.0238e-07, 3.6492e-01, 3.9715e-02,
        4.9541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.541

[Epoch: 88, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.0761e-03, 4.8996e-01, 4.1174e-06, 1.7049e-01, 4.6163e-03, 1.8937e-01,
        1.4049e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 88, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0055, 0.2888, 0.0048, 0.0840, 0.0047, 0.6069, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 88, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0431, 0.7381, 0.0047, 0.0593, 0.0049, 0.0913, 0.0585],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 89, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1344e-01, 5.1731e-02, 5.9079e-06, 3.4803e-02, 1.8692e-06, 6.4024e-07,
        1.8734e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.281

[Epoch: 89, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.3063e-03, 5.9139e-01, 7.8340e-08, 1.9802e-07, 3.5715e-01, 4.1439e-02,
        4.7086e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.506

[Epoch: 89, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.0759e-03, 4.7914e-01, 3.9387e-06, 1.6014e-01, 5.5032e-03, 2.0070e-01,
        1.4943e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 89, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2892, 0.0049, 0.0854, 0.0048, 0.6046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 89, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0501, 0.7121, 0.0060, 0.0617, 0.0053, 0.1040, 0.0607],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 90, batch: 37/187] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2623e-01, 4.0799e-02, 4.6772e-06, 3.2947e-02, 1.7991e-06, 6.2389e-07,
        1.7094e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.259

[Epoch: 90, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.8524e-03, 5.6180e-01, 1.1011e-07, 2.1003e-07, 3.8582e-01, 4.2170e-02,
        5.3579e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.553

[Epoch: 90, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.4978e-03, 4.9668e-01, 3.6005e-06, 1.6285e-01, 3.9393e-03, 1.9138e-01,
        1.3965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 90, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2837, 0.0048, 0.0856, 0.0047, 0.6101, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 90, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0431, 0.7411, 0.0048, 0.0589, 0.0049, 0.0848, 0.0623],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 91, batch: 37/187] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0165e-01, 5.9119e-02, 7.0468e-06, 3.9202e-02, 1.8868e-06, 7.0322e-07,
        2.1320e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.266

[Epoch: 91, batch: 74/187] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.5290e-03, 6.1436e-01, 6.8956e-08, 1.7220e-07, 3.3724e-01, 3.7977e-02,
        4.8991e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.480

[Epoch: 91, batch: 111/187] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.4057e-03, 4.6239e-01, 4.9229e-06, 1.6708e-01, 6.2184e-03, 1.9488e-01,
        1.6402e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 91, batch: 148/187] total loss per batch: 0.755
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0059, 0.2965, 0.0048, 0.0841, 0.0045, 0.5990, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 91, batch: 185/187] total loss per batch: 0.802
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0483, 0.7206, 0.0056, 0.0612, 0.0055, 0.1041, 0.0548],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 92, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2210e-01, 4.3297e-02, 3.8891e-06, 3.4583e-02, 1.5593e-06, 5.9194e-07,
        1.7076e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.276

[Epoch: 92, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.1928e-03, 5.7986e-01, 1.2155e-07, 1.9788e-07, 3.6799e-01, 4.2068e-02,
        4.8931e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.556

[Epoch: 92, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.5530e-03, 4.9506e-01, 4.8852e-06, 1.6346e-01, 4.6464e-03, 1.9672e-01,
        1.3456e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 92, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2888, 0.0048, 0.0861, 0.0051, 0.6040, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 92, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0433, 0.7353, 0.0048, 0.0567, 0.0048, 0.0971, 0.0578],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 93, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1041e-01, 5.3104e-02, 6.1368e-06, 3.6455e-02, 1.6417e-06, 6.6357e-07,
        1.9116e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.260

[Epoch: 93, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([5.2074e-03, 5.8349e-01, 8.2863e-08, 1.5804e-07, 3.6753e-01, 3.8869e-02,
        4.9033e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.508

[Epoch: 93, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.4324e-03, 4.7662e-01, 4.4863e-06, 1.6666e-01, 5.3430e-03, 1.9737e-01,
        1.4857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 93, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2914, 0.0047, 0.0847, 0.0048, 0.6036, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 93, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0481, 0.7215, 0.0051, 0.0621, 0.0049, 0.0964, 0.0620],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 94, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1506e-01, 4.9680e-02, 5.0732e-06, 3.5234e-02, 1.4842e-06, 5.8120e-07,
        1.7815e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.270

[Epoch: 94, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7415e-03, 5.9020e-01, 7.9767e-08, 1.5447e-07, 3.6061e-01, 3.9768e-02,
        4.6768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.531

[Epoch: 94, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.5211e-03, 4.9148e-01, 4.5461e-06, 1.6123e-01, 5.0910e-03, 1.9753e-01,
        1.3914e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 94, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2884, 0.0049, 0.0864, 0.0050, 0.6043, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 94, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0428, 0.7382, 0.0048, 0.0567, 0.0046, 0.0950, 0.0580],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 95, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1325e-01, 5.1194e-02, 5.5248e-06, 3.5529e-02, 1.5690e-06, 5.9217e-07,
        1.7655e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.265

[Epoch: 95, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.6319e-03, 5.7891e-01, 6.7790e-08, 1.4205e-07, 3.7167e-01, 4.0100e-02,
        4.6866e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.528

[Epoch: 95, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.4395e-03, 4.8225e-01, 4.0702e-06, 1.6587e-01, 5.1209e-03, 1.9355e-01,
        1.4777e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 95, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.2893, 0.0047, 0.0853, 0.0048, 0.6049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 95, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0446, 0.7280, 0.0048, 0.0614, 0.0047, 0.0951, 0.0614],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 96, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1119e-01, 5.2597e-02, 4.8162e-06, 3.6193e-02, 1.4628e-06, 5.8680e-07,
        1.7339e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.275

[Epoch: 96, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7629e-03, 5.9499e-01, 6.5099e-08, 1.3792e-07, 3.5653e-01, 3.9078e-02,
        4.6386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.513

[Epoch: 96, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.2557e-03, 4.8955e-01, 3.7814e-06, 1.6331e-01, 4.9366e-03, 1.9489e-01,
        1.4205e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 96, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0056, 0.2909, 0.0049, 0.0851, 0.0049, 0.6032, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 96, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0441, 0.7352, 0.0048, 0.0577, 0.0046, 0.0939, 0.0596],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 97, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1901e-01, 4.7565e-02, 4.6997e-06, 3.3405e-02, 1.4765e-06, 5.4190e-07,
        1.6371e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.264

[Epoch: 97, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.8480e-03, 5.7473e-01, 6.5304e-08, 1.3807e-07, 3.7447e-01, 4.0961e-02,
        4.9906e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.552

[Epoch: 97, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.2058e-03, 4.8239e-01, 3.5046e-06, 1.6459e-01, 4.8324e-03, 1.9349e-01,
        1.4949e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 97, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0058, 0.2889, 0.0048, 0.0844, 0.0048, 0.6059, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 97, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0446, 0.7275, 0.0049, 0.0622, 0.0049, 0.0951, 0.0609],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 98, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1003e-01, 5.3057e-02, 4.3055e-06, 3.6895e-02, 1.4363e-06, 5.9132e-07,
        1.5962e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.273

[Epoch: 98, batch: 74/187] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.9415e-03, 6.0107e-01, 6.0454e-08, 1.2738e-07, 3.5004e-01, 3.9360e-02,
        4.5824e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.478

[Epoch: 98, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.0568e-03, 4.8741e-01, 3.3730e-06, 1.6586e-01, 4.8145e-03, 1.9627e-01,
        1.4060e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 0.000

[Epoch: 98, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0057, 0.2930, 0.0048, 0.0845, 0.0048, 0.6018, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 98, batch: 185/187] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0460, 0.7298, 0.0051, 0.0595, 0.0047, 0.0953, 0.0596],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 99, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2547e-01, 4.3212e-02, 3.9411e-06, 3.1301e-02, 1.2717e-06, 4.5982e-07,
        1.5312e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.258

[Epoch: 99, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.9206e-03, 5.6635e-01, 6.3976e-08, 1.2999e-07, 3.8178e-01, 4.1623e-02,
        5.3210e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.592

[Epoch: 99, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([5.3289e-03, 4.8531e-01, 3.2636e-06, 1.5998e-01, 4.4926e-03, 1.9689e-01,
        1.4799e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 99, batch: 148/187] total loss per batch: 0.753
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0060, 0.2814, 0.0048, 0.0850, 0.0050, 0.6126, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 99, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0441, 0.7315, 0.0047, 0.0600, 0.0051, 0.0938, 0.0606],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 100, batch: 37/187] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.9150, 0.0500, 0.0000, 0.0350, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0694e-01, 5.4005e-02, 4.6163e-06, 3.9036e-02, 1.5692e-06, 6.0742e-07,
        1.5118e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.268 0.271

[Epoch: 100, batch: 74/187] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.5850, 0.0000, 0.0000, 0.3650, 0.0400, 0.0050])
Policy pred: tensor([4.7891e-03, 5.9791e-01, 5.3834e-08, 1.1757e-07, 3.5128e-01, 4.1476e-02,
        4.5467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.523 -0.448

[Epoch: 100, batch: 111/187] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0050, 0.4850, 0.0000, 0.1650, 0.0050, 0.1950, 0.1450])
Policy pred: tensor([4.7195e-03, 4.7932e-01, 3.5629e-06, 1.7591e-01, 5.3283e-03, 1.9405e-01,
        1.4068e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.017 -0.000

[Epoch: 100, batch: 148/187] total loss per batch: 0.754
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.2900, 0.0050, 0.0850, 0.0050, 0.6050, 0.0050])
Policy pred: tensor([0.0053, 0.2918, 0.0047, 0.0851, 0.0044, 0.6034, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 100, batch: 185/187] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0450, 0.7300, 0.0050, 0.0600, 0.0050, 0.0950, 0.0600])
Policy pred: tensor([0.0499, 0.7177, 0.0056, 0.0660, 0.0049, 0.0960, 0.0599],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

