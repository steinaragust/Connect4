Training set samples: 7022
Batch size: 32
[Epoch: 1, batch: 44/220] total loss per batch: 0.907
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0138, 0.0559, 0.0107, 0.3525, 0.0173, 0.0205, 0.5294],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.014

[Epoch: 1, batch: 88/220] total loss per batch: 0.921
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([7.8531e-01, 1.3084e-02, 6.7585e-02, 1.9181e-06, 2.7094e-02, 5.2160e-02,
        5.4770e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 1, batch: 132/220] total loss per batch: 0.896
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.0593e-01, 8.3183e-06, 1.5137e-02, 7.7764e-06, 1.9893e-02, 1.8230e-02,
        8.4079e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 1, batch: 176/220] total loss per batch: 0.888
Policy (actual, predicted): 5 0
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([4.1682e-01, 1.4310e-07, 4.0466e-01, 8.5659e-07, 7.2715e-07, 1.7852e-01,
        1.3659e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 0.282

[Epoch: 1, batch: 220/220] total loss per batch: 0.920
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9838e-01, 1.1827e-01, 4.5783e-01, 8.1685e-06, 1.1765e-01, 1.0727e-01,
        5.9241e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 2, batch: 44/220] total loss per batch: 0.664
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0078, 0.0395, 0.0073, 0.2526, 0.0077, 0.0169, 0.6680],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.007

[Epoch: 2, batch: 88/220] total loss per batch: 0.667
Policy (actual, predicted): 2 6
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.6889e-01, 2.8047e-02, 2.0861e-01, 2.0305e-06, 8.9528e-02, 1.3395e-01,
        2.7097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.025

[Epoch: 2, batch: 132/220] total loss per batch: 0.651
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0170e-02, 5.2627e-06, 6.1487e-03, 4.1790e-06, 8.7865e-03, 6.5573e-03,
        9.2833e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 2, batch: 176/220] total loss per batch: 0.634
Policy (actual, predicted): 5 0
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([4.2167e-01, 4.4839e-08, 2.6387e-01, 3.3581e-07, 3.8426e-07, 3.1446e-01,
        1.8998e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 0.159

[Epoch: 2, batch: 220/220] total loss per batch: 0.669
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.6504e-02, 4.1071e-03, 9.1128e-01, 5.2568e-07, 2.5335e-02, 2.2771e-02,
        5.7593e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 3, batch: 44/220] total loss per batch: 0.557
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0058, 0.0375, 0.0045, 0.2784, 0.0106, 0.0105, 0.6527],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.027

[Epoch: 3, batch: 88/220] total loss per batch: 0.547
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([7.9454e-01, 1.0569e-02, 1.3502e-01, 2.6882e-07, 1.8560e-02, 2.0793e-02,
        2.0511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.006

[Epoch: 3, batch: 132/220] total loss per batch: 0.551
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.3057e-01, 5.5015e-06, 6.6558e-03, 4.9087e-06, 9.1704e-03, 5.9537e-03,
        8.4764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 3, batch: 176/220] total loss per batch: 0.533
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([6.1614e-02, 1.1247e-08, 5.6565e-02, 1.2265e-07, 3.0312e-07, 8.8182e-01,
        9.7276e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.195

[Epoch: 3, batch: 220/220] total loss per batch: 0.546
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.3903e-02, 9.3050e-03, 9.3794e-01, 4.8387e-07, 9.9550e-03, 8.8999e-03,
        5.3783e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 4, batch: 44/220] total loss per batch: 0.506
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0033, 0.0335, 0.0019, 0.2167, 0.0098, 0.0077, 0.7272],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.041

[Epoch: 4, batch: 88/220] total loss per batch: 0.493
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.8457e-02, 2.9122e-02, 7.5479e-01, 2.9166e-07, 5.9712e-02, 7.7220e-02,
        5.0695e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.003

[Epoch: 4, batch: 132/220] total loss per batch: 0.513
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.7166e-02, 3.0738e-06, 2.8989e-03, 5.6014e-06, 4.3417e-03, 2.4849e-03,
        9.5310e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 4, batch: 176/220] total loss per batch: 0.494
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([3.1101e-02, 3.5068e-09, 1.9918e-02, 5.9841e-08, 3.6405e-07, 9.4898e-01,
        5.0617e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.516

[Epoch: 4, batch: 220/220] total loss per batch: 0.500
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5362e-02, 6.1011e-03, 9.6501e-01, 2.0637e-07, 7.7288e-03, 5.7989e-03,
        3.3373e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.000

[Epoch: 5, batch: 44/220] total loss per batch: 0.482
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0042, 0.0337, 0.0020, 0.3261, 0.0083, 0.0077, 0.6180],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.032

[Epoch: 5, batch: 88/220] total loss per batch: 0.462
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.9964e-01, 5.1145e-02, 4.6277e-01, 9.9692e-07, 5.4000e-02, 5.3573e-02,
        7.8872e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.001

[Epoch: 5, batch: 132/220] total loss per batch: 0.491
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.8184e-02, 4.0341e-06, 1.8589e-03, 4.7050e-06, 4.8045e-03, 2.1016e-03,
        9.7304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 5, batch: 176/220] total loss per batch: 0.475
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1173e-02, 5.4576e-09, 1.3596e-02, 6.5976e-08, 6.8709e-07, 9.6523e-01,
        1.2152e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.713

[Epoch: 5, batch: 220/220] total loss per batch: 0.478
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1889e-02, 3.8354e-03, 9.7389e-01, 4.4594e-07, 6.2119e-03, 4.1747e-03,
        6.0382e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.015

[Epoch: 6, batch: 44/220] total loss per batch: 0.463
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0029, 0.0322, 0.0014, 0.1921, 0.0071, 0.0083, 0.7560],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.038

[Epoch: 6, batch: 88/220] total loss per batch: 0.452
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3202e-01, 2.0574e-02, 5.7896e-01, 1.9179e-07, 1.9820e-02, 2.3660e-02,
        2.4963e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.000

[Epoch: 6, batch: 132/220] total loss per batch: 0.480
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.5643e-02, 5.5990e-06, 3.0196e-03, 3.0845e-06, 4.7165e-03, 3.8083e-03,
        9.7280e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.014

[Epoch: 6, batch: 176/220] total loss per batch: 0.460
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([3.0609e-02, 7.2780e-09, 1.2460e-02, 5.5587e-08, 2.8164e-07, 9.5693e-01,
        1.0730e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.763

[Epoch: 6, batch: 220/220] total loss per batch: 0.461
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.4307e-02, 6.2402e-03, 9.5755e-01, 9.0027e-07, 6.8372e-03, 5.0682e-03,
        9.4568e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.023

[Epoch: 7, batch: 44/220] total loss per batch: 0.457
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0030, 0.0291, 0.0017, 0.2003, 0.0105, 0.0084, 0.7471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.034

[Epoch: 7, batch: 88/220] total loss per batch: 0.441
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5915e-01, 1.2740e-02, 4.7415e-01, 2.3393e-07, 1.2366e-02, 1.7759e-02,
        2.3831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.013

[Epoch: 7, batch: 132/220] total loss per batch: 0.467
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.2039e-02, 3.3576e-06, 2.4441e-03, 3.0996e-06, 2.7921e-03, 2.4744e-03,
        9.8024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 7, batch: 176/220] total loss per batch: 0.454
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3524e-02, 8.1947e-09, 1.2525e-02, 6.2004e-08, 7.7795e-07, 9.6395e-01,
        2.1486e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.846

[Epoch: 7, batch: 220/220] total loss per batch: 0.455
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7196e-02, 7.4749e-03, 9.6251e-01, 1.2712e-06, 5.2812e-03, 7.5293e-03,
        3.3081e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.012

[Epoch: 8, batch: 44/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0052, 0.0321, 0.0019, 0.2387, 0.0102, 0.0097, 0.7022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.031

[Epoch: 8, batch: 88/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.2943e-01, 1.3989e-02, 6.9968e-01, 3.3105e-07, 1.2318e-02, 2.0617e-02,
        2.3967e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.007

[Epoch: 8, batch: 132/220] total loss per batch: 0.460
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.0970e-02, 4.2677e-06, 2.2891e-03, 3.8989e-06, 3.4290e-03, 2.9626e-03,
        9.8034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 8, batch: 176/220] total loss per batch: 0.449
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7891e-02, 4.7978e-09, 1.0339e-02, 5.6340e-08, 2.7679e-07, 9.7177e-01,
        5.2417e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 8, batch: 220/220] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3775e-02, 6.0589e-03, 9.6781e-01, 8.5046e-07, 5.7090e-03, 6.6389e-03,
        2.4276e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 9, batch: 44/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0024, 0.0223, 0.0017, 0.0981, 0.0076, 0.0101, 0.8578],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.039

[Epoch: 9, batch: 88/220] total loss per batch: 0.426
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([6.6231e-01, 1.2073e-02, 2.7583e-01, 2.5026e-07, 8.6887e-03, 1.4933e-02,
        2.6169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.005

[Epoch: 9, batch: 132/220] total loss per batch: 0.454
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.5750e-03, 4.7978e-06, 2.2976e-03, 6.0498e-06, 2.8602e-03, 3.7575e-03,
        9.8250e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 9, batch: 176/220] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0384e-02, 6.3267e-09, 1.0195e-02, 6.2642e-08, 6.7212e-07, 9.6942e-01,
        9.9334e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.769

[Epoch: 9, batch: 220/220] total loss per batch: 0.443
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1929e-02, 5.0804e-03, 9.7254e-01, 1.4272e-06, 5.3502e-03, 5.0912e-03,
        2.3750e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 10, batch: 44/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0032, 0.0179, 0.0016, 0.2727, 0.0075, 0.0088, 0.6883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.026

[Epoch: 10, batch: 88/220] total loss per batch: 0.425
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.5847e-02, 1.0414e-02, 9.2576e-01, 5.1642e-07, 1.2006e-02, 1.1167e-02,
        1.4809e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 10, batch: 132/220] total loss per batch: 0.451
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.0509e-02, 4.1094e-06, 3.6452e-03, 6.4141e-06, 2.6162e-03, 3.3998e-03,
        9.7982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 10, batch: 176/220] total loss per batch: 0.440
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7127e-02, 9.0237e-09, 1.2414e-02, 7.0422e-08, 4.7757e-07, 9.7046e-01,
        5.9991e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.867

[Epoch: 10, batch: 220/220] total loss per batch: 0.441
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3157e-02, 1.0158e-02, 9.6475e-01, 1.1609e-06, 5.7186e-03, 6.2123e-03,
        3.2814e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 11, batch: 44/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0020, 0.0131, 0.0021, 0.1447, 0.0050, 0.0070, 0.8261],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.024

[Epoch: 11, batch: 88/220] total loss per batch: 0.422
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.2554e-01, 2.7073e-02, 3.7858e-01, 8.9800e-07, 1.1035e-02, 2.2352e-02,
        3.5419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 11, batch: 132/220] total loss per batch: 0.450
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([9.2215e-03, 6.9993e-06, 2.3140e-03, 1.2041e-05, 3.2417e-03, 4.2796e-03,
        9.8092e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 11, batch: 176/220] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3544e-02, 1.8313e-08, 1.2074e-02, 1.9731e-07, 4.8885e-07, 9.6438e-01,
        9.0188e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 11, batch: 220/220] total loss per batch: 0.440
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9254e-02, 5.4204e-03, 9.6443e-01, 2.4668e-06, 4.8478e-03, 6.0320e-03,
        8.4570e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 12, batch: 44/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0030, 0.0210, 0.0027, 0.1888, 0.0073, 0.0096, 0.7677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.017

[Epoch: 12, batch: 88/220] total loss per batch: 0.422
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.3548e-01, 1.7667e-02, 6.7827e-01, 1.0729e-06, 1.5235e-02, 2.3554e-02,
        2.9788e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 12, batch: 132/220] total loss per batch: 0.448
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.3110e-03, 5.3346e-06, 3.5542e-03, 5.5128e-06, 2.8524e-03, 4.0331e-03,
        9.8324e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 12, batch: 176/220] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4044e-02, 1.5023e-08, 2.0667e-02, 1.1344e-07, 7.5539e-07, 9.5529e-01,
        1.0792e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.881

[Epoch: 12, batch: 220/220] total loss per batch: 0.440
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.4808e-03, 4.7935e-03, 9.7655e-01, 1.2849e-06, 6.8465e-03, 5.3211e-03,
        2.8065e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 13, batch: 44/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0028, 0.0131, 0.0024, 0.1526, 0.0072, 0.0060, 0.8159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.024

[Epoch: 13, batch: 88/220] total loss per batch: 0.422
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2616e-01, 1.7980e-02, 5.7046e-01, 1.5892e-06, 1.6244e-02, 2.4851e-02,
        4.4303e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 13, batch: 132/220] total loss per batch: 0.449
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.0844e-02, 6.0760e-06, 2.9763e-03, 1.7138e-05, 3.2105e-03, 4.0438e-03,
        9.7890e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.016

[Epoch: 13, batch: 176/220] total loss per batch: 0.436
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9308e-02, 1.7071e-08, 1.2788e-02, 2.7868e-07, 1.1870e-06, 9.6790e-01,
        2.8908e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.807

[Epoch: 13, batch: 220/220] total loss per batch: 0.439
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1726e-02, 7.4794e-03, 9.6933e-01, 2.1524e-06, 5.7411e-03, 5.7149e-03,
        4.6619e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 14, batch: 44/220] total loss per batch: 0.432
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0031, 0.0153, 0.0028, 0.1928, 0.0065, 0.0077, 0.7718],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.019

[Epoch: 14, batch: 88/220] total loss per batch: 0.421
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5803e-01, 1.3038e-02, 4.7323e-01, 5.6659e-07, 1.1243e-02, 1.9312e-02,
        2.5140e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.000

[Epoch: 14, batch: 132/220] total loss per batch: 0.450
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.8023e-03, 2.5599e-06, 2.0160e-03, 1.0598e-05, 3.1339e-03, 3.2911e-03,
        9.8374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 14, batch: 176/220] total loss per batch: 0.442
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4092e-02, 1.6254e-08, 3.2833e-02, 8.4081e-08, 2.6934e-07, 9.4307e-01,
        9.4053e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.861

[Epoch: 14, batch: 220/220] total loss per batch: 0.442
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9255e-03, 7.7736e-03, 9.7598e-01, 1.5059e-06, 4.2410e-03, 5.0637e-03,
        1.0517e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.000

[Epoch: 15, batch: 44/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0039, 0.0115, 0.0017, 0.1973, 0.0060, 0.0080, 0.7716],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.014

[Epoch: 15, batch: 88/220] total loss per batch: 0.422
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2881e-01, 8.3202e-03, 6.1588e-01, 7.7789e-07, 7.2471e-03, 1.2452e-02,
        2.7290e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.002

[Epoch: 15, batch: 132/220] total loss per batch: 0.448
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.2660e-03, 3.2313e-06, 2.6449e-03, 1.3973e-05, 5.0324e-03, 4.1551e-03,
        9.8088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.016

[Epoch: 15, batch: 176/220] total loss per batch: 0.444
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.6218e-02, 3.2619e-08, 1.2347e-02, 2.5643e-07, 8.4407e-07, 9.6143e-01,
        1.1674e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.870

[Epoch: 15, batch: 220/220] total loss per batch: 0.451
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9066e-02, 1.1611e-02, 9.5563e-01, 1.2478e-06, 6.3284e-03, 7.3576e-03,
        5.1127e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 16, batch: 44/220] total loss per batch: 0.444
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0042, 0.0224, 0.0064, 0.2309, 0.0075, 0.0062, 0.7225],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.008

[Epoch: 16, batch: 88/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1358e-01, 1.2327e-02, 5.2554e-01, 7.1880e-07, 1.0126e-02, 1.9580e-02,
        1.8845e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 16, batch: 132/220] total loss per batch: 0.451
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.2472e-02, 2.1107e-05, 7.1178e-03, 1.6730e-05, 3.9706e-03, 3.9302e-03,
        9.7247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.000

[Epoch: 16, batch: 176/220] total loss per batch: 0.439
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5465e-02, 4.6445e-08, 2.1899e-02, 3.7037e-07, 6.1128e-07, 9.5263e-01,
        4.5624e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.859

[Epoch: 16, batch: 220/220] total loss per batch: 0.446
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3208e-02, 1.3173e-02, 9.6356e-01, 3.5856e-06, 6.9209e-03, 3.1278e-03,
        9.2607e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 17, batch: 44/220] total loss per batch: 0.439
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0034, 0.0074, 0.0024, 0.1600, 0.0053, 0.0080, 0.8134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.008

[Epoch: 17, batch: 88/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3653e-01, 1.1350e-02, 6.0244e-01, 8.5477e-07, 8.0046e-03, 1.5562e-02,
        2.6107e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 17, batch: 132/220] total loss per batch: 0.451
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.9289e-03, 1.7055e-05, 3.9291e-03, 9.6313e-06, 4.8360e-03, 5.6812e-03,
        9.7960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.000

[Epoch: 17, batch: 176/220] total loss per batch: 0.440
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7538e-02, 5.8786e-08, 2.1308e-02, 1.3079e-07, 9.7600e-07, 9.6115e-01,
        1.0517e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.826

[Epoch: 17, batch: 220/220] total loss per batch: 0.441
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1785e-02, 5.4871e-03, 9.6959e-01, 5.2352e-06, 7.8144e-03, 5.3081e-03,
        1.1285e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.001

[Epoch: 18, batch: 44/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0118, 0.0033, 0.1895, 0.0091, 0.0088, 0.7731],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 18, batch: 88/220] total loss per batch: 0.423
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4667e-01, 8.8912e-03, 5.9825e-01, 1.6647e-06, 6.2141e-03, 1.7963e-02,
        2.2007e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.004

[Epoch: 18, batch: 132/220] total loss per batch: 0.449
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.8973e-03, 1.8368e-05, 3.2786e-03, 1.8869e-05, 4.2115e-03, 4.0778e-03,
        9.7950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.002

[Epoch: 18, batch: 176/220] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4491e-02, 4.8955e-08, 8.4890e-03, 2.4692e-07, 6.7796e-07, 9.6702e-01,
        1.3812e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.890

[Epoch: 18, batch: 220/220] total loss per batch: 0.439
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.5316e-02, 6.8515e-03, 9.5363e-01, 5.4545e-06, 7.1624e-03, 7.0269e-03,
        9.0208e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 19, batch: 44/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0034, 0.0103, 0.0033, 0.2333, 0.0056, 0.0120, 0.7321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.013

[Epoch: 19, batch: 88/220] total loss per batch: 0.421
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.7697e-01, 8.0187e-03, 3.7690e-01, 1.1232e-06, 1.0105e-02, 1.3911e-02,
        1.4089e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.025

[Epoch: 19, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.4693e-02, 2.7952e-05, 4.8294e-03, 3.1148e-05, 3.8437e-03, 3.4090e-03,
        9.7317e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 19, batch: 176/220] total loss per batch: 0.435
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2414e-02, 1.0164e-07, 1.5042e-02, 6.3566e-07, 9.8598e-07, 9.6254e-01,
        9.0301e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.797

[Epoch: 19, batch: 220/220] total loss per batch: 0.438
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([4.6671e-03, 7.1445e-03, 9.7536e-01, 1.0420e-05, 5.9926e-03, 6.8189e-03,
        2.4603e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 20, batch: 44/220] total loss per batch: 0.431
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0026, 0.0091, 0.0045, 0.1629, 0.0069, 0.0093, 0.8047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 20, batch: 88/220] total loss per batch: 0.418
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([1.1163e-01, 1.4591e-02, 8.1992e-01, 3.2262e-06, 1.2762e-02, 1.5002e-02,
        2.6102e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.022

[Epoch: 20, batch: 132/220] total loss per batch: 0.445
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9024e-03, 2.2431e-05, 4.4638e-03, 2.2250e-05, 3.1457e-03, 2.8285e-03,
        9.8461e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 20, batch: 176/220] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3163e-02, 5.0628e-08, 1.4727e-02, 3.5177e-07, 5.1564e-07, 9.7211e-01,
        1.6715e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.818

[Epoch: 20, batch: 220/220] total loss per batch: 0.437
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2750e-02, 6.0816e-03, 9.7263e-01, 4.8051e-06, 4.0609e-03, 4.4741e-03,
        2.9573e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 21, batch: 44/220] total loss per batch: 0.429
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0041, 0.0094, 0.0043, 0.2102, 0.0060, 0.0104, 0.7556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.000

[Epoch: 21, batch: 88/220] total loss per batch: 0.417
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8553e-01, 1.0404e-02, 5.5935e-01, 3.6993e-06, 8.8897e-03, 1.4797e-02,
        2.1025e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.014

[Epoch: 21, batch: 132/220] total loss per batch: 0.444
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.5632e-03, 2.9531e-05, 3.7158e-03, 2.3683e-05, 3.2719e-03, 3.5411e-03,
        9.8285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.013

[Epoch: 21, batch: 176/220] total loss per batch: 0.433
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1894e-02, 1.4701e-07, 1.0049e-02, 3.0168e-07, 1.2128e-06, 9.6806e-01,
        1.9739e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.823

[Epoch: 21, batch: 220/220] total loss per batch: 0.435
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.4006e-03, 5.8426e-03, 9.7341e-01, 6.5453e-06, 5.5180e-03, 6.7936e-03,
        2.4350e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 22, batch: 44/220] total loss per batch: 0.427
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0042, 0.0085, 0.0034, 0.1822, 0.0051, 0.0092, 0.7875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.004

[Epoch: 22, batch: 88/220] total loss per batch: 0.417
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.0959e-01, 1.1987e-02, 6.2975e-01, 7.8127e-06, 1.3954e-02, 1.0399e-02,
        2.4315e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.007

[Epoch: 22, batch: 132/220] total loss per batch: 0.442
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4958e-03, 4.5375e-05, 3.6391e-03, 2.8818e-05, 3.3844e-03, 4.1942e-03,
        9.8421e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.011

[Epoch: 22, batch: 176/220] total loss per batch: 0.432
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1925e-02, 1.2173e-07, 1.5434e-02, 2.6763e-07, 1.2268e-06, 9.6264e-01,
        3.1406e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.890

[Epoch: 22, batch: 220/220] total loss per batch: 0.435
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1780e-02, 4.9709e-03, 9.7253e-01, 4.0761e-06, 5.6582e-03, 5.0518e-03,
        3.4044e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.027

[Epoch: 23, batch: 44/220] total loss per batch: 0.426
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0031, 0.0055, 0.0039, 0.1451, 0.0049, 0.0078, 0.8298],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.010

[Epoch: 23, batch: 88/220] total loss per batch: 0.417
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.4943e-01, 1.0012e-02, 3.9660e-01, 5.3551e-06, 1.2305e-02, 1.2005e-02,
        1.9635e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.009

[Epoch: 23, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9693e-03, 2.9346e-05, 4.5890e-03, 3.9716e-05, 3.6854e-03, 4.7550e-03,
        9.8193e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 23, batch: 176/220] total loss per batch: 0.431
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0607e-02, 1.4592e-07, 2.2524e-02, 3.2727e-07, 1.1925e-06, 9.5687e-01,
        2.3913e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.787

[Epoch: 23, batch: 220/220] total loss per batch: 0.433
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.8920e-03, 4.4314e-03, 9.7079e-01, 7.9615e-06, 5.5178e-03, 1.0355e-02,
        6.0002e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 24, batch: 44/220] total loss per batch: 0.426
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0033, 0.0086, 0.0042, 0.1864, 0.0054, 0.0073, 0.7848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 24, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.2109e-01, 1.0014e-02, 7.1115e-01, 7.9991e-06, 1.2235e-02, 1.9739e-02,
        2.5765e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 24, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9946e-03, 6.6815e-05, 4.5807e-03, 2.8519e-05, 4.0702e-03, 4.4916e-03,
        9.8177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.008

[Epoch: 24, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8866e-02, 1.0816e-07, 1.0101e-02, 1.5233e-07, 1.1145e-06, 9.7103e-01,
        4.4804e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.867

[Epoch: 24, batch: 220/220] total loss per batch: 0.432
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1212e-02, 4.8037e-03, 9.7330e-01, 6.0141e-06, 5.9691e-03, 4.6936e-03,
        1.6459e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 25, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0038, 0.0086, 0.0069, 0.1849, 0.0062, 0.0081, 0.7815],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.012

[Epoch: 25, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5151e-01, 1.0130e-02, 4.9676e-01, 4.7647e-06, 1.1268e-02, 1.2542e-02,
        1.7785e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 25, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.1160e-03, 4.5414e-05, 6.4168e-03, 2.5597e-05, 4.5469e-03, 3.8739e-03,
        9.7698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 25, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.8738e-02, 1.6510e-07, 1.6618e-02, 4.7439e-07, 6.7626e-07, 9.5464e-01,
        1.7658e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 25, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3826e-02, 7.1235e-03, 9.7019e-01, 9.0298e-06, 3.6651e-03, 5.1628e-03,
        1.9176e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 26, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0034, 0.0066, 0.0056, 0.1836, 0.0052, 0.0068, 0.7888],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 26, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.9590e-01, 8.4795e-03, 6.5801e-01, 5.7032e-06, 7.3563e-03, 1.4183e-02,
        1.6072e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.000

[Epoch: 26, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.8655e-03, 4.7854e-05, 3.7794e-03, 3.2419e-05, 4.3389e-03, 5.1393e-03,
        9.8280e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.015

[Epoch: 26, batch: 176/220] total loss per batch: 0.430
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5620e-02, 7.4105e-08, 1.4208e-02, 2.7512e-07, 2.0035e-06, 9.7017e-01,
        1.0898e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.869

[Epoch: 26, batch: 220/220] total loss per batch: 0.432
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([4.2876e-03, 4.8474e-03, 9.7842e-01, 6.9589e-06, 5.6383e-03, 6.7745e-03,
        2.5013e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 27, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0033, 0.0080, 0.0067, 0.1747, 0.0066, 0.0071, 0.7936],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.008

[Epoch: 27, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2895e-01, 1.1934e-02, 5.1056e-01, 4.0072e-06, 8.5490e-03, 1.7073e-02,
        2.2931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.004

[Epoch: 27, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.7174e-03, 2.3898e-05, 5.6745e-03, 3.0904e-05, 5.0183e-03, 3.6602e-03,
        9.7687e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 27, batch: 176/220] total loss per batch: 0.430
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7226e-02, 1.5264e-07, 1.4656e-02, 3.4016e-07, 1.0430e-06, 9.6812e-01,
        2.5878e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.876

[Epoch: 27, batch: 220/220] total loss per batch: 0.432
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5223e-02, 6.9025e-03, 9.6563e-01, 7.2559e-06, 6.3430e-03, 5.7740e-03,
        1.1929e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 28, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0067, 0.0072, 0.1677, 0.0068, 0.0059, 0.8008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 28, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8507e-01, 8.4280e-03, 5.6973e-01, 8.5786e-06, 7.5617e-03, 1.2817e-02,
        1.6378e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.000

[Epoch: 28, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5687e-03, 8.0747e-05, 6.3402e-03, 2.6470e-05, 3.3264e-03, 4.1952e-03,
        9.8146e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.007

[Epoch: 28, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2706e-02, 1.6046e-07, 1.8661e-02, 4.9493e-07, 1.1604e-06, 9.5863e-01,
        1.0384e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.843

[Epoch: 28, batch: 220/220] total loss per batch: 0.433
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.8116e-03, 4.1312e-03, 9.7459e-01, 9.6675e-06, 7.9507e-03, 7.4662e-03,
        4.1591e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 29, batch: 44/220] total loss per batch: 0.426
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0045, 0.0105, 0.0052, 0.1993, 0.0067, 0.0060, 0.7678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 29, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5257e-01, 1.0774e-02, 5.9032e-01, 4.7107e-06, 9.8909e-03, 1.5811e-02,
        2.0635e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.000

[Epoch: 29, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2998e-03, 2.0808e-05, 4.4095e-03, 3.9374e-05, 4.8600e-03, 6.7946e-03,
        9.7858e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 29, batch: 176/220] total loss per batch: 0.430
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1311e-02, 9.2349e-08, 9.6650e-03, 5.1121e-07, 8.3806e-07, 9.6902e-01,
        1.0018e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.762

[Epoch: 29, batch: 220/220] total loss per batch: 0.434
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3595e-02, 1.3444e-02, 9.5701e-01, 8.1237e-06, 4.5366e-03, 1.1371e-02,
        3.3220e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 30, batch: 44/220] total loss per batch: 0.427
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0046, 0.0069, 0.0105, 0.1877, 0.0078, 0.0087, 0.7737],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 30, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5862e-01, 7.6081e-03, 4.8757e-01, 7.2434e-06, 1.1755e-02, 1.7519e-02,
        1.6924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.002

[Epoch: 30, batch: 132/220] total loss per batch: 0.442
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.2071e-03, 1.2737e-04, 5.9470e-03, 4.5321e-05, 7.0691e-03, 5.6562e-03,
        9.7495e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.012

[Epoch: 30, batch: 176/220] total loss per batch: 0.432
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3805e-02, 1.2990e-07, 1.5029e-02, 7.7361e-07, 3.0360e-06, 9.7116e-01,
        6.2413e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.832

[Epoch: 30, batch: 220/220] total loss per batch: 0.437
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([3.3232e-03, 1.4194e-03, 9.8861e-01, 2.5295e-06, 3.3408e-03, 3.2952e-03,
        1.3468e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 31, batch: 44/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0087, 0.0181, 0.0062, 0.2559, 0.0081, 0.0051, 0.6979],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.013

[Epoch: 31, batch: 88/220] total loss per batch: 0.455
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.6178e-01, 1.1238e-02, 3.8227e-01, 5.9830e-07, 5.6151e-03, 2.7939e-02,
        1.1157e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.021

[Epoch: 31, batch: 132/220] total loss per batch: 0.515
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.1229e-02, 6.3100e-03, 2.8359e-02, 4.8112e-05, 2.5937e-02, 2.0489e-01,
        7.2322e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 31, batch: 176/220] total loss per batch: 0.540
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([4.3881e-02, 9.2363e-09, 1.2850e-02, 6.4373e-07, 2.1939e-06, 9.4327e-01,
        2.0692e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.879

[Epoch: 31, batch: 220/220] total loss per batch: 0.530
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.9745e-02, 6.2492e-03, 9.4020e-01, 2.8953e-05, 6.7204e-03, 1.6381e-02,
        6.7730e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 32, batch: 44/220] total loss per batch: 0.500
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0026, 0.0127, 0.0094, 0.1800, 0.0279, 0.0207, 0.7466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 32, batch: 88/220] total loss per batch: 0.488
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([7.2448e-02, 1.9117e-02, 8.1014e-01, 3.6682e-06, 2.5763e-02, 1.1794e-02,
        6.0733e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.025

[Epoch: 32, batch: 132/220] total loss per batch: 0.517
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.7128e-02, 3.0952e-05, 8.2841e-03, 2.6397e-05, 1.6850e-02, 7.7483e-03,
        9.4993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.039

[Epoch: 32, batch: 176/220] total loss per batch: 0.490
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7860e-02, 6.6471e-07, 5.7006e-02, 7.7637e-07, 1.8647e-05, 9.2511e-01,
        1.2002e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.697

[Epoch: 32, batch: 220/220] total loss per batch: 0.485
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.5949e-03, 6.7216e-03, 9.7177e-01, 1.2765e-05, 2.9091e-03, 9.9526e-03,
        3.9803e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.034

[Epoch: 33, batch: 44/220] total loss per batch: 0.473
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0011, 0.0114, 0.0197, 0.0704, 0.0105, 0.0085, 0.8785],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 33, batch: 88/220] total loss per batch: 0.455
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.7397e-01, 1.0174e-02, 6.3800e-01, 3.6717e-06, 1.0276e-02, 1.4904e-02,
        5.2674e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.043

[Epoch: 33, batch: 132/220] total loss per batch: 0.474
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([2.1802e-02, 3.5437e-05, 3.6859e-03, 1.5663e-05, 1.3119e-02, 1.2059e-02,
        9.4928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 33, batch: 176/220] total loss per batch: 0.451
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([9.0150e-02, 1.9418e-07, 6.6576e-02, 1.5639e-06, 1.7577e-05, 8.4325e-01,
        2.1767e-07], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.910

[Epoch: 33, batch: 220/220] total loss per batch: 0.449
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.7890e-02, 4.0240e-03, 9.5392e-01, 6.8456e-06, 5.8469e-03, 8.1869e-03,
        1.2808e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 34, batch: 44/220] total loss per batch: 0.442
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0026, 0.0054, 0.0076, 0.2149, 0.0048, 0.0119, 0.7527],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 34, batch: 88/220] total loss per batch: 0.423
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.1466e-01, 9.0314e-03, 4.4353e-01, 2.1680e-06, 6.9131e-03, 9.0894e-03,
        1.6770e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.031

[Epoch: 34, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.2947e-02, 2.4888e-05, 6.0296e-03, 1.6737e-05, 1.6489e-02, 8.1545e-03,
        9.5634e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 34, batch: 176/220] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([9.0688e-03, 1.5355e-07, 6.9961e-03, 3.5942e-07, 1.5833e-06, 9.8393e-01,
        6.1222e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.873

[Epoch: 34, batch: 220/220] total loss per batch: 0.437
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6966e-02, 6.1595e-03, 9.6491e-01, 7.7384e-06, 5.5366e-03, 6.4002e-03,
        2.2163e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 35, batch: 44/220] total loss per batch: 0.428
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0019, 0.0049, 0.0050, 0.1382, 0.0057, 0.0100, 0.8343],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 35, batch: 88/220] total loss per batch: 0.416
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.0394e-01, 1.1763e-02, 6.4251e-01, 2.3433e-06, 7.8667e-03, 1.0786e-02,
        2.3134e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.028

[Epoch: 35, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.4039e-03, 1.3215e-05, 5.5231e-03, 1.3507e-05, 1.1848e-02, 6.4184e-03,
        9.6778e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 35, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([9.8932e-03, 1.1151e-07, 7.7762e-03, 3.1667e-07, 1.3531e-06, 9.8233e-01,
        9.2726e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.860

[Epoch: 35, batch: 220/220] total loss per batch: 0.433
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8688e-03, 2.9128e-03, 9.7447e-01, 7.9792e-06, 5.1461e-03, 7.5672e-03,
        2.4173e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 36, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0024, 0.0061, 0.0091, 0.2107, 0.0086, 0.0102, 0.7528],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 36, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6121e-01, 9.5116e-03, 4.9654e-01, 1.5394e-06, 6.3660e-03, 8.9884e-03,
        1.7382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.030

[Epoch: 36, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.6380e-03, 1.5521e-05, 4.8599e-03, 1.2834e-05, 1.1225e-02, 7.1102e-03,
        9.6814e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 36, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.2004e-02, 1.0065e-07, 9.0120e-03, 2.5912e-07, 1.1318e-06, 9.7898e-01,
        5.4576e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 36, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4910e-03, 3.3077e-03, 9.7694e-01, 7.4920e-06, 4.9208e-03, 5.2943e-03,
        3.4614e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 37, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0020, 0.0061, 0.0071, 0.1650, 0.0067, 0.0096, 0.8035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 37, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.7949e-01, 1.0203e-02, 6.7132e-01, 1.9860e-06, 7.3158e-03, 1.1104e-02,
        2.0562e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.034

[Epoch: 37, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.3775e-03, 1.3072e-05, 5.7837e-03, 1.3723e-05, 9.4964e-03, 6.6843e-03,
        9.6963e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.031

[Epoch: 37, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3266e-02, 1.0950e-07, 1.0268e-02, 2.5629e-07, 8.6352e-07, 9.7646e-01,
        3.3869e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 37, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0347e-02, 4.3255e-03, 9.7295e-01, 9.3169e-06, 5.9030e-03, 6.4359e-03,
        2.4413e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 38, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0022, 0.0057, 0.0079, 0.1954, 0.0064, 0.0089, 0.7736],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 38, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.0006e-01, 7.0879e-03, 4.6355e-01, 1.0988e-06, 5.3678e-03, 9.7189e-03,
        1.4214e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.030

[Epoch: 38, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.4009e-03, 1.2558e-05, 4.7088e-03, 1.0531e-05, 9.2127e-03, 6.5484e-03,
        9.7211e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.034

[Epoch: 38, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5328e-02, 8.3082e-08, 1.0664e-02, 1.9773e-07, 7.0100e-07, 9.7401e-01,
        3.2766e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.851

[Epoch: 38, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.0939e-03, 3.6952e-03, 9.7814e-01, 6.6245e-06, 4.5716e-03, 4.4717e-03,
        2.2677e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 39, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0026, 0.0058, 0.0075, 0.1525, 0.0065, 0.0078, 0.8173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 39, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.5202e-01, 8.2995e-03, 7.0221e-01, 1.5895e-06, 6.5128e-03, 1.1216e-02,
        1.9742e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.032

[Epoch: 39, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.1987e-03, 1.0574e-05, 4.9479e-03, 9.5139e-06, 7.6053e-03, 6.3489e-03,
        9.7288e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.033

[Epoch: 39, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5803e-02, 7.4495e-08, 1.1346e-02, 2.1373e-07, 7.6708e-07, 9.7285e-01,
        2.8611e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.845

[Epoch: 39, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0877e-02, 4.6051e-03, 9.7296e-01, 8.6777e-06, 5.7221e-03, 5.8064e-03,
        2.0705e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 40, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0028, 0.0054, 0.0087, 0.2290, 0.0068, 0.0080, 0.7394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 40, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2671e-01, 7.4495e-03, 5.2853e-01, 1.1934e-06, 5.6873e-03, 1.3889e-02,
        1.7730e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.031

[Epoch: 40, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.8835e-03, 1.2566e-05, 4.5490e-03, 1.0194e-05, 7.5905e-03, 6.9652e-03,
        9.7399e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.037

[Epoch: 40, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9649e-02, 7.8509e-08, 1.2706e-02, 2.3926e-07, 6.8005e-07, 9.6764e-01,
        2.1101e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.849

[Epoch: 40, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.7936e-03, 3.7488e-03, 9.7787e-01, 5.8634e-06, 4.9669e-03, 4.6042e-03,
        1.1256e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 41, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0031, 0.0054, 0.0067, 0.1212, 0.0054, 0.0071, 0.8510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 41, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.1291e-01, 7.1740e-03, 6.4239e-01, 1.4792e-06, 6.1638e-03, 1.3757e-02,
        1.7606e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.029

[Epoch: 41, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.3004e-03, 7.3788e-06, 4.3844e-03, 9.2181e-06, 6.4660e-03, 6.0345e-03,
        9.7580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.030

[Epoch: 41, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7286e-02, 6.7108e-08, 1.2558e-02, 1.8388e-07, 6.7100e-07, 9.7015e-01,
        2.6988e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 41, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1233e-02, 5.5302e-03, 9.7258e-01, 1.0304e-05, 5.0948e-03, 5.5315e-03,
        1.7895e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 42, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0036, 0.0055, 0.0113, 0.2461, 0.0080, 0.0080, 0.7176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 42, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5814e-01, 7.3503e-03, 4.9902e-01, 1.3917e-06, 5.5672e-03, 1.5792e-02,
        1.4125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.031

[Epoch: 42, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.2927e-03, 1.6382e-05, 4.7928e-03, 1.2496e-05, 6.9464e-03, 7.2474e-03,
        9.7469e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.037

[Epoch: 42, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6663e-02, 1.0289e-07, 1.5569e-02, 3.7407e-07, 9.9126e-07, 9.6777e-01,
        1.3548e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.838

[Epoch: 42, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0081e-02, 5.3365e-03, 9.7339e-01, 8.9823e-06, 6.4416e-03, 4.7367e-03,
        7.9996e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 43, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0031, 0.0038, 0.0058, 0.1565, 0.0044, 0.0059, 0.8205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 43, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3472e-01, 7.0056e-03, 6.2251e-01, 1.7119e-06, 5.8150e-03, 1.2066e-02,
        1.7875e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.028

[Epoch: 43, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.8185e-03, 1.0013e-05, 4.2698e-03, 1.0406e-05, 5.3471e-03, 5.7527e-03,
        9.7779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 43, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2742e-02, 1.0499e-07, 1.3653e-02, 2.1340e-07, 7.1907e-07, 9.6360e-01,
        4.0196e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 43, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.9921e-03, 4.0815e-03, 9.7606e-01, 9.4439e-06, 5.1462e-03, 5.6903e-03,
        2.2617e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 44, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0037, 0.0046, 0.0065, 0.1735, 0.0056, 0.0068, 0.7993],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 44, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6487e-01, 8.1458e-03, 5.9046e-01, 2.1888e-06, 6.6332e-03, 1.4666e-02,
        1.5231e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.019

[Epoch: 44, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.6131e-03, 2.1595e-05, 5.2942e-03, 1.6399e-05, 7.8630e-03, 7.6233e-03,
        9.7157e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.033

[Epoch: 44, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.2649e-02, 1.2445e-07, 1.4609e-02, 3.2988e-07, 6.4507e-07, 9.7274e-01,
        1.5034e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.841

[Epoch: 44, batch: 220/220] total loss per batch: 0.430
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3629e-02, 8.1646e-03, 9.6658e-01, 8.7293e-06, 7.2364e-03, 4.3700e-03,
        1.1283e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 45, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0067, 0.0097, 0.2519, 0.0102, 0.0078, 0.7084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.009

[Epoch: 45, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.3507e-01, 6.4872e-03, 5.2063e-01, 1.7161e-06, 6.4219e-03, 1.6241e-02,
        1.5144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.032

[Epoch: 45, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.2518e-03, 1.6436e-05, 5.0952e-03, 1.6387e-05, 5.2204e-03, 6.5809e-03,
        9.7682e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.028

[Epoch: 45, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7086e-02, 1.6342e-07, 1.1957e-02, 4.4293e-07, 1.6332e-06, 9.7095e-01,
        1.5770e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 45, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.6379e-03, 4.4340e-03, 9.7407e-01, 1.7057e-05, 5.6844e-03, 6.1436e-03,
        9.4940e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 46, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0025, 0.0032, 0.0037, 0.0786, 0.0027, 0.0050, 0.9043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 46, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.0221e-01, 1.0731e-02, 6.4867e-01, 2.6641e-06, 7.7596e-03, 1.4219e-02,
        1.6407e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.023

[Epoch: 46, batch: 132/220] total loss per batch: 0.439
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.6906e-03, 1.7631e-05, 4.5152e-03, 1.4821e-05, 6.8423e-03, 6.5212e-03,
        9.7640e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.034

[Epoch: 46, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6840e-02, 1.9734e-07, 1.9855e-02, 2.4677e-07, 6.8527e-07, 9.6330e-01,
        2.7716e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.843

[Epoch: 46, batch: 220/220] total loss per batch: 0.432
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.5587e-03, 5.1514e-03, 9.7707e-01, 6.1929e-06, 4.4960e-03, 4.7108e-03,
        4.5134e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 47, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0060, 0.0064, 0.0092, 0.2565, 0.0064, 0.0070, 0.7086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 47, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.0890e-01, 6.7293e-03, 4.3821e-01, 4.3797e-06, 7.2193e-03, 1.9480e-02,
        1.9459e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.016

[Epoch: 47, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.8235e-03, 2.4034e-05, 4.5657e-03, 2.2955e-05, 6.7781e-03, 7.5122e-03,
        9.7527e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 47, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6803e-02, 1.5278e-07, 1.3245e-02, 3.9880e-07, 1.7583e-06, 9.6995e-01,
        1.5338e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.841

[Epoch: 47, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2977e-02, 2.8694e-03, 9.7546e-01, 1.8986e-05, 4.8436e-03, 3.8123e-03,
        1.3918e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 48, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0045, 0.0040, 0.0082, 0.1816, 0.0053, 0.0069, 0.7894],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 48, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([1.9656e-01, 7.2707e-03, 7.6202e-01, 1.9524e-06, 5.1528e-03, 1.4449e-02,
        1.4540e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.027

[Epoch: 48, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.5418e-03, 2.6448e-05, 5.1879e-03, 1.0564e-05, 4.8703e-03, 6.1466e-03,
        9.7622e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.034

[Epoch: 48, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3052e-02, 1.1145e-07, 1.4521e-02, 1.8726e-07, 2.2405e-07, 9.6243e-01,
        4.2609e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.818

[Epoch: 48, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2874e-02, 7.4693e-03, 9.7006e-01, 7.7221e-06, 4.0158e-03, 5.5599e-03,
        1.0214e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 49, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0042, 0.0041, 0.0063, 0.1634, 0.0049, 0.0080, 0.8091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 49, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([6.2014e-01, 5.8792e-03, 3.3742e-01, 4.0756e-06, 6.8358e-03, 1.5360e-02,
        1.4356e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.025

[Epoch: 49, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2329e-03, 2.0005e-05, 5.0145e-03, 2.1183e-05, 4.5057e-03, 6.7675e-03,
        9.7944e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.032

[Epoch: 49, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0032e-02, 2.1666e-07, 1.1132e-02, 5.7458e-07, 1.1349e-06, 9.6883e-01,
        3.4036e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 49, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.5436e-03, 3.0274e-03, 9.7953e-01, 1.7948e-05, 5.3142e-03, 3.5506e-03,
        1.1530e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.030

[Epoch: 50, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0050, 0.0075, 0.0080, 0.1828, 0.0061, 0.0078, 0.7828],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.008

[Epoch: 50, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.4228e-01, 7.0493e-03, 6.8274e-01, 5.9339e-06, 1.1320e-02, 1.4958e-02,
        4.1643e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.024

[Epoch: 50, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.6717e-03, 4.2373e-05, 4.9409e-03, 1.2616e-05, 5.9631e-03, 6.3859e-03,
        9.7598e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 50, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7905e-02, 3.4839e-07, 1.9858e-02, 3.2734e-07, 4.2745e-07, 9.6224e-01,
        7.4189e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.843

[Epoch: 50, batch: 220/220] total loss per batch: 0.432
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.9277e-02, 1.2396e-02, 9.4925e-01, 1.7236e-05, 1.1485e-02, 7.5515e-03,
        2.0921e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 51, batch: 44/220] total loss per batch: 0.428
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0075, 0.0056, 0.0083, 0.1787, 0.0068, 0.0062, 0.7869],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 51, batch: 88/220] total loss per batch: 0.416
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5527e-01, 5.7245e-03, 5.9731e-01, 9.5759e-06, 8.8476e-03, 1.3297e-02,
        1.9544e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 51, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.8102e-03, 1.9214e-05, 5.4891e-03, 7.1904e-05, 6.9069e-03, 9.6570e-03,
        9.7105e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 51, batch: 176/220] total loss per batch: 0.430
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3907e-02, 1.2332e-07, 1.0542e-02, 6.2487e-07, 4.4272e-07, 9.7555e-01,
        8.9724e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.878

[Epoch: 51, batch: 220/220] total loss per batch: 0.434
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.3156e-03, 3.0557e-03, 9.7834e-01, 9.9958e-06, 4.2650e-03, 6.0094e-03,
        1.5817e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 52, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0038, 0.0056, 0.0067, 0.1783, 0.0080, 0.0072, 0.7905],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.010

[Epoch: 52, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.3800e-01, 4.4208e-03, 4.2814e-01, 5.5904e-06, 7.0299e-03, 1.2389e-02,
        1.0008e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.021

[Epoch: 52, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.1189e-03, 1.4105e-05, 5.1886e-03, 3.7471e-05, 5.6107e-03, 7.9819e-03,
        9.7405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 52, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3764e-02, 1.8248e-07, 1.5512e-02, 6.2650e-07, 6.0134e-07, 9.6072e-01,
        5.1427e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.868

[Epoch: 52, batch: 220/220] total loss per batch: 0.430
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0189e-02, 6.3651e-03, 9.7585e-01, 1.3377e-05, 3.5260e-03, 4.0474e-03,
        5.0004e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 53, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0067, 0.0074, 0.1913, 0.0047, 0.0078, 0.7773],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 53, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([1.5920e-01, 6.8028e-03, 7.8624e-01, 1.0483e-05, 1.1250e-02, 1.4706e-02,
        2.1793e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.024

[Epoch: 53, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.1428e-03, 1.9452e-05, 4.8181e-03, 2.8372e-05, 4.7198e-03, 5.9573e-03,
        9.7831e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.034

[Epoch: 53, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4772e-02, 1.6521e-07, 1.2730e-02, 4.5342e-07, 4.2015e-07, 9.6250e-01,
        6.6709e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 53, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.6027e-03, 3.8414e-03, 9.7981e-01, 9.2302e-06, 3.6240e-03, 4.1025e-03,
        7.8503e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 54, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0076, 0.0109, 0.1941, 0.0070, 0.0066, 0.7690],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.015

[Epoch: 54, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.4549e-01, 7.3842e-03, 5.1029e-01, 4.6211e-06, 5.8392e-03, 1.1015e-02,
        1.9974e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 54, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5966e-03, 1.5654e-05, 5.7118e-03, 3.0203e-05, 6.2610e-03, 7.5858e-03,
        9.7480e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 54, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7228e-02, 9.6323e-08, 1.3969e-02, 3.4845e-07, 7.1398e-07, 9.6880e-01,
        1.6885e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 54, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0133e-02, 5.0108e-03, 9.7672e-01, 1.4527e-05, 4.2722e-03, 3.8471e-03,
        2.0648e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 55, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0022, 0.0050, 0.0038, 0.1281, 0.0064, 0.0051, 0.8493],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 55, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7762e-01, 5.8336e-03, 5.8334e-01, 7.5519e-06, 5.8696e-03, 9.9028e-03,
        1.7419e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 55, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2614e-03, 2.1814e-05, 4.9589e-03, 1.4276e-05, 4.4727e-03, 6.7073e-03,
        9.7956e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.030

[Epoch: 55, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0329e-02, 1.6911e-07, 1.7596e-02, 5.1010e-07, 2.8509e-07, 9.6207e-01,
        6.3049e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.829

[Epoch: 55, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2406e-02, 4.5823e-03, 9.7208e-01, 6.7766e-06, 4.4324e-03, 6.4918e-03,
        3.3895e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 56, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0073, 0.0091, 0.0098, 0.1943, 0.0054, 0.0057, 0.7684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.018

[Epoch: 56, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0360e-01, 6.7398e-03, 5.5209e-01, 9.2833e-06, 6.0168e-03, 1.2817e-02,
        1.8730e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.001

[Epoch: 56, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5357e-03, 2.1348e-05, 5.4925e-03, 1.8654e-05, 4.5056e-03, 5.5918e-03,
        9.7983e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.030

[Epoch: 56, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9285e-02, 1.1180e-07, 1.3650e-02, 4.0217e-07, 5.5685e-07, 9.6706e-01,
        4.1031e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 56, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4212e-03, 4.3720e-03, 9.7858e-01, 8.7859e-06, 5.3232e-03, 4.2969e-03,
        2.7118e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 57, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0036, 0.0056, 0.0096, 0.2145, 0.0067, 0.0050, 0.7551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.010

[Epoch: 57, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7915e-01, 7.7609e-03, 5.8661e-01, 9.8816e-06, 5.7890e-03, 9.1250e-03,
        1.1554e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 57, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.1068e-03, 2.3580e-05, 6.3647e-03, 2.0177e-05, 5.7353e-03, 5.8411e-03,
        9.7591e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.032

[Epoch: 57, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.7461e-02, 2.5573e-07, 1.8010e-02, 6.3701e-07, 6.6624e-07, 9.5453e-01,
        1.0172e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 57, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0668e-02, 5.8004e-03, 9.7298e-01, 7.8437e-06, 5.0182e-03, 5.5176e-03,
        4.9472e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 58, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0061, 0.0062, 0.0097, 0.1511, 0.0050, 0.0054, 0.8166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.009

[Epoch: 58, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6187e-01, 7.7321e-03, 5.9341e-01, 9.9184e-06, 6.0757e-03, 1.2048e-02,
        1.8858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 58, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.3436e-03, 2.2671e-05, 5.7983e-03, 2.6837e-05, 5.1524e-03, 5.5749e-03,
        9.7808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 58, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9590e-02, 1.3682e-07, 1.5357e-02, 4.5669e-07, 5.8758e-07, 9.6505e-01,
        2.9483e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 58, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0142e-02, 4.3478e-03, 9.7465e-01, 1.2207e-05, 6.2503e-03, 4.5905e-03,
        3.4149e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 59, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0061, 0.0116, 0.1856, 0.0058, 0.0052, 0.7807],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 59, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1190e-01, 8.1011e-03, 5.4632e-01, 1.3653e-05, 5.6791e-03, 1.4084e-02,
        1.3899e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 59, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8964e-03, 2.2126e-05, 4.8535e-03, 2.1772e-05, 5.4635e-03, 5.9892e-03,
        9.7875e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 59, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0371e-02, 1.7783e-07, 1.0553e-02, 4.7281e-07, 6.1311e-07, 9.6907e-01,
        3.3587e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 59, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0571e-02, 5.7640e-03, 9.7363e-01, 1.1034e-05, 4.8900e-03, 5.1316e-03,
        5.2350e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 60, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0047, 0.0067, 0.1640, 0.0049, 0.0056, 0.8094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 60, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2650e-01, 5.6817e-03, 6.3454e-01, 1.0489e-05, 5.4695e-03, 1.2516e-02,
        1.5284e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.001

[Epoch: 60, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.6934e-03, 2.6510e-05, 5.1675e-03, 2.2644e-05, 4.4250e-03, 4.4050e-03,
        9.8026e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 60, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8113e-02, 1.3757e-07, 1.7263e-02, 2.8426e-07, 4.8091e-07, 9.6462e-01,
        5.6426e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.826

[Epoch: 60, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.7511e-03, 4.2797e-03, 9.7835e-01, 1.0307e-05, 4.4451e-03, 4.1636e-03,
        4.7803e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 61, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0075, 0.0075, 0.0158, 0.2261, 0.0066, 0.0057, 0.7309],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 61, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6576e-01, 6.7770e-03, 4.9648e-01, 8.2156e-06, 4.3153e-03, 1.1909e-02,
        1.4751e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 61, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7457e-03, 1.9081e-05, 5.2696e-03, 1.8486e-05, 4.8497e-03, 5.9041e-03,
        9.7919e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.038

[Epoch: 61, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.7877e-02, 2.7859e-07, 1.4060e-02, 6.9331e-07, 8.2393e-07, 9.5806e-01,
        1.0396e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.818

[Epoch: 61, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1334e-02, 5.8260e-03, 9.7300e-01, 1.3557e-05, 4.5670e-03, 5.2582e-03,
        5.3610e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 62, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0049, 0.0064, 0.1559, 0.0044, 0.0068, 0.8171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 62, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.8951e-01, 7.1783e-03, 6.6276e-01, 9.1105e-06, 7.3643e-03, 1.5253e-02,
        1.7931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 62, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.4463e-03, 2.7279e-05, 4.8613e-03, 2.0663e-05, 5.8607e-03, 5.1478e-03,
        9.7764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 62, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.2039e-02, 1.2220e-07, 1.6921e-02, 3.1490e-07, 5.3641e-07, 9.7104e-01,
        3.1201e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.892

[Epoch: 62, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.6939e-03, 4.6513e-03, 9.7789e-01, 1.0565e-05, 4.8606e-03, 5.8906e-03,
        7.4302e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 63, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0064, 0.0059, 0.0072, 0.1683, 0.0061, 0.0056, 0.8006],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 63, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.7223e-01, 5.7451e-03, 4.8313e-01, 7.5068e-06, 6.4887e-03, 1.5441e-02,
        1.6962e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 63, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6664e-03, 1.9359e-05, 4.7230e-03, 1.4397e-05, 4.6781e-03, 3.9731e-03,
        9.8193e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.039

[Epoch: 63, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7994e-02, 1.7825e-07, 1.1273e-02, 2.8805e-07, 2.3491e-07, 9.7073e-01,
        6.5159e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.867

[Epoch: 63, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.8641e-02, 4.9626e-03, 9.6837e-01, 8.1315e-06, 4.0175e-03, 4.0026e-03,
        2.3822e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 64, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0062, 0.0089, 0.1890, 0.0044, 0.0060, 0.7806],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 64, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.8224e-01, 6.8373e-03, 6.7058e-01, 8.7433e-06, 8.2107e-03, 1.4297e-02,
        1.7830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.018

[Epoch: 64, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7775e-03, 2.8716e-05, 5.2765e-03, 1.5274e-05, 4.7351e-03, 6.1395e-03,
        9.7903e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 64, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3642e-02, 3.3555e-07, 1.9676e-02, 7.9555e-07, 1.1228e-06, 9.5668e-01,
        5.8524e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.793

[Epoch: 64, batch: 220/220] total loss per batch: 0.430
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.7517e-03, 4.5068e-03, 9.7362e-01, 1.1133e-05, 7.2055e-03, 8.8776e-03,
        3.0901e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 65, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0045, 0.0048, 0.0107, 0.1950, 0.0070, 0.0058, 0.7721],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.011

[Epoch: 65, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.7038e-01, 6.7364e-03, 4.8018e-01, 7.2131e-06, 6.4647e-03, 1.7112e-02,
        1.9124e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 65, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.6616e-03, 2.8738e-05, 5.2072e-03, 9.3989e-06, 3.4283e-03, 3.7323e-03,
        9.8393e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 65, batch: 176/220] total loss per batch: 0.432
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3376e-02, 1.0005e-06, 1.5859e-02, 1.0411e-06, 4.6495e-06, 9.7076e-01,
        3.1938e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.823

[Epoch: 65, batch: 220/220] total loss per batch: 0.436
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.9864e-03, 3.8067e-03, 9.7841e-01, 1.3146e-05, 3.3815e-03, 6.3866e-03,
        1.2610e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 66, batch: 44/220] total loss per batch: 0.429
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0073, 0.0069, 0.0061, 0.1819, 0.0034, 0.0062, 0.7882],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 66, batch: 88/220] total loss per batch: 0.418
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.1404e-01, 4.3909e-03, 6.4087e-01, 1.1028e-05, 6.2434e-03, 1.8389e-02,
        1.6053e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.026

[Epoch: 66, batch: 132/220] total loss per batch: 0.446
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.6402e-03, 4.1799e-05, 6.0179e-03, 2.3518e-05, 7.5265e-03, 4.4995e-03,
        9.7425e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 66, batch: 176/220] total loss per batch: 0.447
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3091e-02, 6.6842e-08, 1.4395e-02, 8.2338e-08, 6.0369e-08, 9.7251e-01,
        1.3199e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.889

[Epoch: 66, batch: 220/220] total loss per batch: 0.459
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.1916e-03, 3.2915e-03, 9.7849e-01, 9.0998e-06, 4.5711e-03, 5.3790e-03,
        6.5616e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.030

[Epoch: 67, batch: 44/220] total loss per batch: 0.446
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0059, 0.0124, 0.0083, 0.1981, 0.0032, 0.0107, 0.7614],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.015

[Epoch: 67, batch: 88/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4986e-01, 5.4082e-03, 5.9393e-01, 5.2912e-05, 8.7352e-03, 1.8918e-02,
        2.3097e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 67, batch: 132/220] total loss per batch: 0.468
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.0578e-03, 1.1100e-04, 5.0478e-03, 2.2174e-05, 2.5292e-03, 7.3086e-03,
        9.7892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 67, batch: 176/220] total loss per batch: 0.445
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([4.7523e-02, 3.9480e-07, 1.8101e-02, 3.8901e-07, 1.0455e-06, 9.3437e-01,
        1.2521e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.878

[Epoch: 67, batch: 220/220] total loss per batch: 0.448
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([4.8282e-03, 1.2165e-02, 9.7788e-01, 4.4973e-06, 1.3280e-03, 3.7866e-03,
        1.0567e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 68, batch: 44/220] total loss per batch: 0.433
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0055, 0.0083, 0.0115, 0.1802, 0.0046, 0.0085, 0.7813],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 68, batch: 88/220] total loss per batch: 0.419
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.2575e-01, 4.3316e-03, 4.4093e-01, 9.8401e-06, 5.4700e-03, 7.9077e-03,
        1.5595e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 -0.005

[Epoch: 68, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.0998e-03, 5.6946e-05, 4.4853e-03, 2.8423e-05, 2.4774e-03, 4.2931e-03,
        9.8256e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 68, batch: 176/220] total loss per batch: 0.434
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9895e-02, 2.1104e-07, 1.5703e-02, 1.3490e-07, 1.3008e-06, 9.6440e-01,
        3.8971e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.848

[Epoch: 68, batch: 220/220] total loss per batch: 0.437
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5391e-02, 9.5411e-03, 9.6922e-01, 5.4785e-06, 2.1405e-03, 3.6683e-03,
        3.5872e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 69, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0077, 0.0117, 0.2208, 0.0048, 0.0068, 0.7434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.010

[Epoch: 69, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4956e-01, 4.5039e-03, 6.1294e-01, 8.4428e-06, 7.5674e-03, 7.7977e-03,
        1.7622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.002

[Epoch: 69, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0901e-03, 5.9002e-05, 5.0700e-03, 6.1590e-05, 2.1600e-03, 3.5101e-03,
        9.8405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.016

[Epoch: 69, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.6912e-02, 3.1852e-07, 1.4375e-02, 3.2462e-07, 3.6739e-06, 9.5871e-01,
        2.7064e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 69, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.6345e-03, 6.0312e-03, 9.7870e-01, 5.7705e-06, 2.5343e-03, 4.0801e-03,
        1.6366e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 70, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0046, 0.0067, 0.0105, 0.1714, 0.0044, 0.0065, 0.7958],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.009

[Epoch: 70, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8834e-01, 4.6914e-03, 5.7542e-01, 1.0330e-05, 5.8633e-03, 8.8462e-03,
        1.6831e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 70, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.9811e-03, 3.9177e-05, 5.0220e-03, 4.3891e-05, 2.7259e-03, 4.6311e-03,
        9.8156e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.018

[Epoch: 70, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8261e-02, 2.5626e-07, 1.5307e-02, 1.7894e-07, 2.1614e-06, 9.6643e-01,
        1.0082e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 70, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.1349e-03, 5.5029e-03, 9.7820e-01, 4.8051e-06, 2.8882e-03, 4.2571e-03,
        9.9289e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 71, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0065, 0.0085, 0.1806, 0.0046, 0.0065, 0.7885],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.009

[Epoch: 71, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7530e-01, 4.4375e-03, 5.8784e-01, 8.9016e-06, 5.7497e-03, 9.7075e-03,
        1.6959e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 71, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.8226e-03, 3.9339e-05, 4.9288e-03, 4.2478e-05, 2.9148e-03, 4.3758e-03,
        9.8088e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 71, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8041e-02, 2.0768e-07, 1.4721e-02, 1.5652e-07, 2.0942e-06, 9.6723e-01,
        1.2626e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 71, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1077e-02, 5.6297e-03, 9.7562e-01, 4.6940e-06, 3.1576e-03, 4.5024e-03,
        7.3673e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 72, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0057, 0.0099, 0.1768, 0.0048, 0.0063, 0.7918],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.007

[Epoch: 72, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8789e-01, 4.7491e-03, 5.7321e-01, 7.1173e-06, 5.7086e-03, 1.0671e-02,
        1.7768e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 72, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.2255e-03, 2.9389e-05, 4.8681e-03, 2.8637e-05, 2.7614e-03, 4.4810e-03,
        9.8161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 72, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9823e-02, 1.5996e-07, 1.5107e-02, 1.2456e-07, 1.3798e-06, 9.6507e-01,
        9.4352e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 72, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.9913e-03, 4.5965e-03, 9.7797e-01, 4.3524e-06, 3.5291e-03, 4.9029e-03,
        9.4319e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 73, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0063, 0.0094, 0.1818, 0.0050, 0.0062, 0.7867],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 73, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7062e-01, 4.9782e-03, 5.9006e-01, 6.4436e-06, 5.4537e-03, 1.1559e-02,
        1.7320e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 73, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.6661e-03, 2.3272e-05, 5.1140e-03, 2.2684e-05, 3.2647e-03, 4.8824e-03,
        9.8003e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 73, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9505e-02, 1.4860e-07, 1.4554e-02, 1.0830e-07, 1.0626e-06, 9.6594e-01,
        8.5371e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 73, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0058e-02, 5.1368e-03, 9.7615e-01, 3.9006e-06, 3.8420e-03, 4.8062e-03,
        6.5293e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 74, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0054, 0.0088, 0.1795, 0.0050, 0.0059, 0.7906],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 74, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9203e-01, 4.9176e-03, 5.6881e-01, 5.2089e-06, 5.4467e-03, 1.1809e-02,
        1.6985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 74, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.1461e-03, 2.1480e-05, 5.0151e-03, 2.2986e-05, 3.0520e-03, 4.7352e-03,
        9.8101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 74, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9697e-02, 1.2693e-07, 1.5974e-02, 9.9522e-08, 1.0236e-06, 9.6433e-01,
        5.5799e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 74, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0930e-02, 4.6190e-03, 9.7532e-01, 3.5912e-06, 4.1237e-03, 4.9945e-03,
        5.3263e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 75, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0052, 0.0098, 0.1801, 0.0053, 0.0059, 0.7895],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.004

[Epoch: 75, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7709e-01, 5.2131e-03, 5.8234e-01, 4.8857e-06, 5.3089e-03, 1.3384e-02,
        1.6650e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 75, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.8881e-03, 1.9999e-05, 5.4696e-03, 1.7010e-05, 3.2206e-03, 5.2605e-03,
        9.8012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 75, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9041e-02, 1.1307e-07, 1.3314e-02, 7.6303e-08, 6.5028e-07, 9.6764e-01,
        5.9059e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.861

[Epoch: 75, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.4225e-03, 5.1582e-03, 9.7689e-01, 3.7504e-06, 4.5795e-03, 4.9356e-03,
        6.1022e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 76, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0054, 0.0098, 0.1871, 0.0055, 0.0054, 0.7817],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 76, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7594e-01, 5.3078e-03, 5.8207e-01, 4.6806e-06, 5.3706e-03, 1.3846e-02,
        1.7451e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 76, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.4080e-03, 1.6834e-05, 4.9806e-03, 1.6800e-05, 3.6404e-03, 5.3286e-03,
        9.7961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 76, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0843e-02, 1.1893e-07, 1.8140e-02, 9.8051e-08, 9.8153e-07, 9.6102e-01,
        4.3054e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.840

[Epoch: 76, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2583e-02, 4.7526e-03, 9.7260e-01, 3.6719e-06, 4.8806e-03, 5.1777e-03,
        3.4358e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 77, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0043, 0.0052, 0.0087, 0.1728, 0.0053, 0.0059, 0.7978],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 77, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8011e-01, 5.6713e-03, 5.7769e-01, 4.2668e-06, 5.8598e-03, 1.4442e-02,
        1.6227e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 77, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.0120e-03, 1.9184e-05, 5.3525e-03, 1.7150e-05, 3.7414e-03, 5.6609e-03,
        9.7820e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 77, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6082e-02, 1.1426e-07, 1.0940e-02, 8.7874e-08, 5.3105e-07, 9.7298e-01,
        3.8766e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.868

[Epoch: 77, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.9696e-03, 5.4100e-03, 9.7804e-01, 4.1984e-06, 4.8675e-03, 4.7013e-03,
        4.2384e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 78, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0042, 0.0114, 0.1832, 0.0055, 0.0053, 0.7852],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 78, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1144e-01, 5.8294e-03, 5.4624e-01, 6.0759e-06, 5.9389e-03, 1.4408e-02,
        1.6136e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 78, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5923e-03, 1.9566e-05, 5.0587e-03, 2.2550e-05, 3.5637e-03, 5.9685e-03,
        9.8077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 78, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5540e-02, 1.3984e-07, 2.0786e-02, 1.4226e-07, 8.7088e-07, 9.5367e-01,
        2.7339e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 78, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2981e-02, 4.6029e-03, 9.7258e-01, 4.8587e-06, 4.7005e-03, 5.1213e-03,
        6.9773e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 79, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0039, 0.0063, 0.0092, 0.1740, 0.0057, 0.0063, 0.7946],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 79, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2868e-01, 5.8171e-03, 6.2812e-01, 8.3645e-06, 5.8922e-03, 1.4562e-02,
        1.6922e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 79, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.3670e-03, 1.9947e-05, 4.7597e-03, 1.9683e-05, 4.0209e-03, 6.2973e-03,
        9.7752e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 79, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5983e-02, 1.9462e-07, 1.0542e-02, 1.7721e-07, 1.1140e-06, 9.7347e-01,
        2.3108e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.859

[Epoch: 79, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.2056e-03, 5.4546e-03, 9.7731e-01, 6.3199e-06, 5.6925e-03, 5.3199e-03,
        1.1022e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 80, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0083, 0.0047, 0.0119, 0.1909, 0.0067, 0.0060, 0.7716],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 80, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.4062e-01, 6.4033e-03, 5.1150e-01, 8.7067e-06, 6.3975e-03, 1.7501e-02,
        1.7574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 80, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3221e-03, 3.5359e-05, 3.9333e-03, 2.2243e-05, 4.0756e-03, 4.0510e-03,
        9.8356e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 80, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4015e-02, 1.8091e-07, 2.2853e-02, 1.4283e-07, 7.4748e-07, 9.5313e-01,
        4.0074e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 80, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2982e-02, 6.1496e-03, 9.7014e-01, 6.7951e-06, 5.4807e-03, 5.2320e-03,
        1.0637e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 81, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0051, 0.0083, 0.1717, 0.0050, 0.0053, 0.8002],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 81, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2695e-01, 5.3739e-03, 6.2888e-01, 8.4161e-06, 6.8725e-03, 1.4691e-02,
        1.7223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 81, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.6522e-03, 1.8252e-05, 4.7132e-03, 3.2737e-05, 4.6464e-03, 5.4833e-03,
        9.7845e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 81, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8716e-02, 1.7538e-07, 1.2746e-02, 1.4227e-07, 5.6643e-07, 9.6854e-01,
        5.8416e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.873

[Epoch: 81, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8033e-03, 5.1758e-03, 9.7679e-01, 5.2274e-06, 4.8709e-03, 5.3467e-03,
        1.1522e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 82, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0064, 0.0049, 0.0086, 0.1613, 0.0057, 0.0063, 0.8068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 82, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6972e-01, 4.7593e-03, 4.8856e-01, 6.9716e-06, 5.0217e-03, 1.4738e-02,
        1.7189e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 82, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1942e-03, 2.7914e-05, 4.4666e-03, 2.3221e-05, 4.2901e-03, 5.7987e-03,
        9.8020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 82, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3540e-02, 3.2781e-07, 1.2371e-02, 1.8285e-07, 6.5278e-07, 9.7409e-01,
        6.0268e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.832

[Epoch: 82, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5183e-03, 5.1900e-03, 9.7646e-01, 6.5383e-06, 5.5084e-03, 5.2952e-03,
        2.0724e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 83, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0057, 0.0066, 0.0135, 0.2134, 0.0077, 0.0054, 0.7479],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 83, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.8920e-01, 6.4201e-03, 6.5548e-01, 9.5302e-06, 7.0907e-03, 2.0146e-02,
        2.1652e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 83, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5468e-03, 2.4619e-05, 5.3146e-03, 4.5595e-05, 6.6778e-03, 6.1122e-03,
        9.7628e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.030

[Epoch: 83, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([3.0882e-02, 3.7036e-07, 2.0712e-02, 2.8963e-07, 5.9877e-07, 9.4840e-01,
        7.9950e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.835

[Epoch: 83, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0017e-02, 4.8541e-03, 9.7306e-01, 7.6726e-06, 6.3898e-03, 5.6619e-03,
        1.1604e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 84, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0043, 0.0043, 0.0084, 0.1452, 0.0046, 0.0050, 0.8282],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 84, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.7295e-01, 5.0262e-03, 4.8685e-01, 4.1939e-06, 5.6057e-03, 1.4932e-02,
        1.4632e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.023

[Epoch: 84, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4018e-03, 1.8650e-05, 4.2202e-03, 2.8138e-05, 3.9688e-03, 6.7833e-03,
        9.8058e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 84, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.2491e-02, 1.3468e-07, 1.5405e-02, 1.1491e-07, 3.3723e-07, 9.7210e-01,
        1.7827e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.867

[Epoch: 84, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1011e-02, 6.8668e-03, 9.7004e-01, 8.9497e-06, 6.3597e-03, 5.6936e-03,
        1.6465e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 85, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0070, 0.0065, 0.0114, 0.2085, 0.0085, 0.0073, 0.7508],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 85, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.7948e-01, 6.2589e-03, 6.8542e-01, 8.3783e-06, 5.6179e-03, 1.0404e-02,
        1.2810e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 85, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.7643e-03, 2.5384e-05, 3.6390e-03, 2.2321e-05, 5.0416e-03, 4.4982e-03,
        9.8101e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 85, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9140e-02, 6.3320e-07, 1.5243e-02, 3.7768e-07, 5.5634e-07, 9.6562e-01,
        1.4085e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.833

[Epoch: 85, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9336e-03, 4.3588e-03, 9.7601e-01, 1.0888e-05, 5.4046e-03, 4.2519e-03,
        3.1385e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 86, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0055, 0.0050, 0.0079, 0.1728, 0.0052, 0.0061, 0.7976],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 86, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.2741e-01, 6.2830e-03, 4.2784e-01, 7.5974e-06, 4.9661e-03, 1.8424e-02,
        1.5069e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.018

[Epoch: 86, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.9478e-03, 3.1253e-05, 5.1590e-03, 3.0403e-05, 8.4128e-03, 1.2222e-02,
        9.6820e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.014

[Epoch: 86, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1261e-02, 9.0401e-07, 2.0028e-02, 3.2830e-07, 7.2207e-07, 9.5871e-01,
        5.2924e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 86, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2812e-02, 4.4466e-03, 9.7074e-01, 8.4459e-06, 5.2177e-03, 6.6352e-03,
        1.4132e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 87, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0055, 0.0091, 0.1645, 0.0048, 0.0057, 0.8039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 87, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.4408e-01, 6.8947e-03, 7.0296e-01, 9.5296e-06, 6.6011e-03, 1.5013e-02,
        2.4442e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.018

[Epoch: 87, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.8377e-03, 2.7752e-05, 4.8126e-03, 2.8066e-05, 4.9267e-03, 2.9417e-03,
        9.8043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.016

[Epoch: 87, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8133e-02, 3.7533e-07, 1.3482e-02, 3.6532e-07, 5.8232e-07, 9.6838e-01,
        6.2592e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.786

[Epoch: 87, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.3693e-03, 4.7815e-03, 9.7775e-01, 1.0545e-05, 4.5924e-03, 6.4731e-03,
        2.5449e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 88, batch: 44/220] total loss per batch: 0.423
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0077, 0.0112, 0.2044, 0.0077, 0.0064, 0.7562],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 88, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.3551e-01, 5.2621e-03, 5.1772e-01, 1.3629e-05, 7.6774e-03, 1.6218e-02,
        1.7601e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.020

[Epoch: 88, batch: 132/220] total loss per batch: 0.439
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.4117e-03, 2.0633e-05, 3.8334e-03, 1.5000e-05, 2.7821e-03, 4.1367e-03,
        9.8380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 88, batch: 176/220] total loss per batch: 0.431
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3609e-02, 9.3671e-07, 1.7245e-02, 3.4970e-07, 1.1178e-07, 9.5914e-01,
        1.6928e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.893

[Epoch: 88, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.6688e-02, 6.3940e-03, 9.6700e-01, 6.2219e-06, 4.6060e-03, 5.0690e-03,
        2.3855e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 89, batch: 44/220] total loss per batch: 0.428
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0045, 0.0051, 0.1447, 0.0058, 0.0047, 0.8297],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 89, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8042e-01, 4.8953e-03, 5.8670e-01, 1.2050e-06, 2.7655e-03, 1.2021e-02,
        1.3197e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 89, batch: 132/220] total loss per batch: 0.439
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.7919e-03, 1.7739e-05, 4.2805e-03, 1.4927e-05, 3.9102e-03, 4.8908e-03,
        9.8309e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 89, batch: 176/220] total loss per batch: 0.429
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5686e-02, 4.0143e-07, 1.3195e-02, 1.9828e-07, 2.5307e-07, 9.7112e-01,
        2.2968e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.861

[Epoch: 89, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0327e-02, 4.3989e-03, 9.7049e-01, 5.7645e-06, 5.9373e-03, 8.7741e-03,
        6.9495e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 90, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0133, 0.0050, 0.0780, 0.0066, 0.0060, 0.8845],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 90, batch: 88/220] total loss per batch: 0.413
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8750e-01, 5.1153e-03, 5.7937e-01, 5.4781e-06, 4.7405e-03, 7.7625e-03,
        1.5506e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 90, batch: 132/220] total loss per batch: 0.444
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.2218e-03, 4.6923e-06, 2.7106e-03, 1.0859e-05, 3.8024e-03, 6.5247e-03,
        9.7972e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 90, batch: 176/220] total loss per batch: 0.431
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5106e-02, 1.2415e-07, 1.5960e-02, 1.9916e-07, 3.1821e-08, 9.5893e-01,
        2.2539e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.823

[Epoch: 90, batch: 220/220] total loss per batch: 0.440
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.1064e-03, 4.9529e-03, 9.7091e-01, 1.2980e-05, 8.4088e-03, 1.0604e-02,
        3.2168e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 91, batch: 44/220] total loss per batch: 0.427
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0068, 0.0058, 0.2322, 0.0101, 0.0079, 0.7323],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.009

[Epoch: 91, batch: 88/220] total loss per batch: 0.420
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5648e-01, 1.0201e-02, 5.9425e-01, 3.4539e-06, 5.4917e-03, 1.3944e-02,
        1.9634e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.028

[Epoch: 91, batch: 132/220] total loss per batch: 0.445
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([8.5226e-03, 3.1893e-05, 3.1167e-03, 2.2369e-05, 5.8907e-03, 4.7749e-03,
        9.7764e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.009

[Epoch: 91, batch: 176/220] total loss per batch: 0.432
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.0240e-02, 1.2699e-07, 9.5096e-03, 2.5424e-07, 2.2197e-07, 9.8025e-01,
        1.0616e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.874

[Epoch: 91, batch: 220/220] total loss per batch: 0.436
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.5227e-02, 1.1133e-02, 9.6303e-01, 5.7760e-06, 3.7569e-03, 5.6424e-03,
        1.2067e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 92, batch: 44/220] total loss per batch: 0.426
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0021, 0.0040, 0.0056, 0.1670, 0.0067, 0.0053, 0.8092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.012

[Epoch: 92, batch: 88/220] total loss per batch: 0.419
Policy (actual, predicted): 2 0
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([5.0825e-01, 4.2485e-03, 4.4603e-01, 3.0666e-06, 2.2064e-03, 1.8149e-02,
        2.1116e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 92, batch: 132/220] total loss per batch: 0.440
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5250e-03, 7.3627e-05, 3.7374e-03, 2.0798e-05, 6.8257e-03, 6.2477e-03,
        9.7857e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.002

[Epoch: 92, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.1108e-02, 1.6048e-07, 9.1733e-03, 1.4907e-07, 7.7493e-08, 9.7972e-01,
        1.8319e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.876

[Epoch: 92, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0622e-02, 5.1575e-03, 9.6784e-01, 5.1976e-06, 8.3797e-03, 7.9353e-03,
        6.1013e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 93, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0031, 0.0057, 0.0085, 0.1892, 0.0087, 0.0063, 0.7785],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.012

[Epoch: 93, batch: 88/220] total loss per batch: 0.414
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.3829e-01, 1.2993e-02, 6.7883e-01, 5.0425e-06, 6.6304e-03, 1.8778e-02,
        4.4480e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.030

[Epoch: 93, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5903e-03, 1.0705e-05, 3.7928e-03, 8.4183e-06, 6.0348e-03, 3.6357e-03,
        9.8093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 93, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6030e-02, 2.6163e-07, 1.4807e-02, 2.3138e-07, 1.2410e-07, 9.6916e-01,
        5.3775e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.862

[Epoch: 93, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.9949e-03, 4.9388e-03, 9.7787e-01, 4.8252e-06, 4.3254e-03, 4.7657e-03,
        9.9680e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 94, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0032, 0.0060, 0.0080, 0.2035, 0.0097, 0.0071, 0.7625],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.007

[Epoch: 94, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6872e-01, 1.0503e-02, 4.8478e-01, 1.7014e-06, 4.8572e-03, 1.5896e-02,
        1.5242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.018

[Epoch: 94, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.8467e-03, 1.9462e-05, 3.6375e-03, 1.6861e-05, 4.4915e-03, 5.2117e-03,
        9.8278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.013

[Epoch: 94, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9638e-02, 1.8833e-07, 1.4545e-02, 1.9874e-07, 1.0567e-07, 9.6582e-01,
        4.4587e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.872

[Epoch: 94, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0078e-02, 4.8720e-03, 9.7575e-01, 4.7140e-06, 4.3289e-03, 4.9193e-03,
        4.7380e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 95, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0029, 0.0055, 0.0074, 0.1850, 0.0077, 0.0072, 0.7844],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.007

[Epoch: 95, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0237e-01, 7.8033e-03, 5.5438e-01, 1.8778e-06, 5.3292e-03, 1.6712e-02,
        1.3406e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 95, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3825e-03, 1.4926e-05, 4.3413e-03, 9.1027e-06, 4.3554e-03, 4.5126e-03,
        9.8238e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.015

[Epoch: 95, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9466e-02, 1.5513e-07, 1.4481e-02, 1.8568e-07, 9.5307e-08, 9.6605e-01,
        3.9488e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.869

[Epoch: 95, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9261e-03, 4.8317e-03, 9.7547e-01, 4.8589e-06, 4.7717e-03, 4.9448e-03,
        4.6428e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 96, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0028, 0.0051, 0.0080, 0.1813, 0.0080, 0.0065, 0.7883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.008

[Epoch: 96, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8177e-01, 6.7899e-03, 5.7413e-01, 1.7741e-06, 5.6012e-03, 1.5297e-02,
        1.6412e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 96, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1286e-03, 1.5395e-05, 4.4551e-03, 1.1405e-05, 4.7299e-03, 5.5257e-03,
        9.8013e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.013

[Epoch: 96, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9678e-02, 1.4196e-07, 1.4916e-02, 1.4745e-07, 7.0738e-08, 9.6541e-01,
        5.4192e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.859

[Epoch: 96, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0223e-02, 4.7865e-03, 9.7490e-01, 4.3272e-06, 4.9756e-03, 5.0580e-03,
        5.0629e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 97, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0028, 0.0052, 0.0073, 0.1756, 0.0064, 0.0058, 0.7969],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 97, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8408e-01, 6.7421e-03, 5.7615e-01, 1.5868e-06, 5.4500e-03, 1.4200e-02,
        1.3382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.020

[Epoch: 97, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8296e-03, 9.7256e-06, 4.4233e-03, 6.9029e-06, 4.9062e-03, 5.3132e-03,
        9.8051e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.013

[Epoch: 97, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9457e-02, 1.2257e-07, 1.4709e-02, 1.4794e-07, 7.0739e-08, 9.6583e-01,
        3.3956e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.861

[Epoch: 97, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.7672e-03, 5.1526e-03, 9.7463e-01, 4.0950e-06, 4.9029e-03, 5.4951e-03,
        4.4204e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 98, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0032, 0.0053, 0.0102, 0.1874, 0.0076, 0.0063, 0.7801],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.004

[Epoch: 98, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8455e-01, 6.1162e-03, 5.7263e-01, 1.3781e-06, 5.1789e-03, 1.5169e-02,
        1.6351e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 98, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1723e-03, 1.4764e-05, 4.8310e-03, 9.5788e-06, 4.5929e-03, 5.2035e-03,
        9.8018e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.014

[Epoch: 98, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2409e-02, 1.1329e-07, 1.6285e-02, 1.2773e-07, 6.1889e-08, 9.6131e-01,
        4.8792e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.849

[Epoch: 98, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0127e-02, 4.9781e-03, 9.7580e-01, 3.6484e-06, 4.5110e-03, 4.5603e-03,
        2.2436e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 99, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0032, 0.0046, 0.0065, 0.1694, 0.0056, 0.0055, 0.8053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.005

[Epoch: 99, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8160e-01, 6.3273e-03, 5.7839e-01, 1.4735e-06, 5.5739e-03, 1.4287e-02,
        1.3824e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.019

[Epoch: 99, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7570e-03, 9.1504e-06, 4.9126e-03, 5.7019e-06, 4.6851e-03, 5.2287e-03,
        9.8040e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.012

[Epoch: 99, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7087e-02, 1.1237e-07, 1.4931e-02, 1.2371e-07, 4.6949e-08, 9.6798e-01,
        3.8079e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.863

[Epoch: 99, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.6867e-03, 4.7392e-03, 9.7452e-01, 3.8653e-06, 5.5591e-03, 5.4390e-03,
        5.1533e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 100, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0034, 0.0056, 0.0107, 0.1991, 0.0076, 0.0066, 0.7670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 100, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7208e-01, 5.1405e-03, 5.8974e-01, 1.3609e-06, 4.8277e-03, 1.3921e-02,
        1.4291e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 100, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.4296e-03, 1.1781e-05, 5.0149e-03, 1.0582e-05, 4.9801e-03, 5.3705e-03,
        9.7918e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.016

[Epoch: 100, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3301e-02, 1.0267e-07, 1.3478e-02, 9.0556e-08, 4.9568e-08, 9.6322e-01,
        4.4053e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 100, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0124e-02, 5.2301e-03, 9.7455e-01, 3.2364e-06, 4.4615e-03, 5.6176e-03,
        1.6678e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 101, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0035, 0.0044, 0.0078, 0.1663, 0.0051, 0.0053, 0.8075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 101, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0371e-01, 6.3135e-03, 5.5501e-01, 1.4262e-06, 4.9951e-03, 1.4319e-02,
        1.5649e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.016

[Epoch: 101, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4743e-03, 9.6274e-06, 4.9151e-03, 6.7730e-06, 4.8868e-03, 5.6242e-03,
        9.8008e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.012

[Epoch: 101, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5968e-02, 1.1306e-07, 1.5969e-02, 1.6742e-07, 5.6948e-08, 9.6806e-01,
        4.1037e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.849

[Epoch: 101, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0147e-02, 5.2339e-03, 9.7473e-01, 4.4247e-06, 5.4562e-03, 4.3795e-03,
        4.7994e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 102, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0040, 0.0050, 0.0081, 0.1818, 0.0062, 0.0055, 0.7895],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 102, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7880e-01, 5.6772e-03, 5.7853e-01, 1.7352e-06, 5.5717e-03, 1.5120e-02,
        1.6304e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 102, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1287e-03, 9.8350e-06, 5.1210e-03, 9.1317e-06, 5.1600e-03, 4.8606e-03,
        9.7971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 102, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5176e-02, 1.3114e-07, 1.7296e-02, 1.1500e-07, 6.0315e-08, 9.5753e-01,
        6.6659e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 102, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0891e-02, 5.3594e-03, 9.7181e-01, 3.6707e-06, 5.7376e-03, 6.1844e-03,
        1.2961e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 103, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0037, 0.0047, 0.0099, 0.1828, 0.0053, 0.0054, 0.7882],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 103, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7186e-01, 5.3289e-03, 5.9020e-01, 1.4946e-06, 4.8669e-03, 1.2934e-02,
        1.4808e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 103, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4026e-03, 1.1496e-05, 5.0898e-03, 7.4181e-06, 4.5426e-03, 5.4250e-03,
        9.8052e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.018

[Epoch: 103, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5948e-02, 8.5545e-08, 1.2534e-02, 1.2152e-07, 6.7382e-08, 9.7152e-01,
        4.1083e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 103, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.3362e-03, 5.1049e-03, 9.7823e-01, 3.1168e-06, 4.0405e-03, 4.2592e-03,
        2.2957e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 104, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0039, 0.0044, 0.0083, 0.1750, 0.0058, 0.0052, 0.7974],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 104, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9800e-01, 6.0094e-03, 5.6238e-01, 1.5138e-06, 4.9420e-03, 1.4330e-02,
        1.4342e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 104, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0191e-03, 7.2232e-06, 4.5725e-03, 6.0098e-06, 4.8997e-03, 4.5393e-03,
        9.8096e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 104, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4919e-02, 1.0419e-07, 1.6758e-02, 1.2402e-07, 5.5327e-08, 9.5832e-01,
        7.1379e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 104, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1076e-02, 4.5325e-03, 9.7373e-01, 3.3786e-06, 5.5460e-03, 5.1037e-03,
        1.1916e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 105, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0052, 0.0056, 0.0104, 0.1978, 0.0058, 0.0054, 0.7697],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 105, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7152e-01, 5.3903e-03, 5.8686e-01, 1.3851e-06, 5.3109e-03, 1.3988e-02,
        1.6920e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 105, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1245e-03, 1.1459e-05, 5.4002e-03, 7.6832e-06, 4.9785e-03, 4.9120e-03,
        9.7957e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.014

[Epoch: 105, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7532e-02, 1.1407e-07, 1.8064e-02, 1.2232e-07, 5.5870e-08, 9.6440e-01,
        3.1223e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.869

[Epoch: 105, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.2272e-03, 5.2762e-03, 9.7637e-01, 2.2375e-06, 4.1199e-03, 4.9881e-03,
        1.4215e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 106, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0040, 0.0050, 0.0086, 0.1710, 0.0057, 0.0053, 0.8005],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 106, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8474e-01, 5.3266e-03, 5.7312e-01, 2.1337e-06, 5.9809e-03, 1.5197e-02,
        1.5633e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.015

[Epoch: 106, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3501e-03, 7.9285e-06, 5.3428e-03, 7.7305e-06, 4.2463e-03, 5.7504e-03,
        9.8029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 106, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7180e-02, 7.2454e-08, 1.1504e-02, 1.4408e-07, 4.1013e-08, 9.7132e-01,
        2.5043e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.860

[Epoch: 106, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.7442e-03, 4.2618e-03, 9.7577e-01, 4.0189e-06, 5.3425e-03, 4.8698e-03,
        7.3683e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 107, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0039, 0.0040, 0.0095, 0.1789, 0.0047, 0.0052, 0.7938],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 107, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9238e-01, 5.2288e-03, 5.7157e-01, 1.5548e-06, 4.6463e-03, 1.3314e-02,
        1.2858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 107, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0542e-03, 1.2746e-05, 4.5617e-03, 7.1546e-06, 5.9347e-03, 4.8860e-03,
        9.7954e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.011

[Epoch: 107, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4505e-02, 8.8428e-08, 1.5305e-02, 1.4737e-07, 6.8491e-08, 9.6019e-01,
        1.0562e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.852

[Epoch: 107, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9841e-03, 5.9216e-03, 9.7525e-01, 3.8012e-06, 4.7372e-03, 4.0728e-03,
        2.8108e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 108, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0060, 0.0068, 0.0089, 0.1956, 0.0070, 0.0053, 0.7704],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.006

[Epoch: 108, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7800e-01, 6.1122e-03, 5.7495e-01, 3.5068e-06, 6.2772e-03, 1.5631e-02,
        1.9027e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 108, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8929e-03, 7.4437e-06, 4.4514e-03, 6.2387e-06, 3.6739e-03, 5.3294e-03,
        9.8164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 108, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6585e-02, 1.6288e-07, 1.5001e-02, 1.6977e-07, 8.6247e-08, 9.6841e-01,
        8.3181e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.842

[Epoch: 108, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0265e-02, 5.1171e-03, 9.7322e-01, 6.7479e-06, 5.2024e-03, 6.1699e-03,
        1.5146e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 109, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0047, 0.0113, 0.1736, 0.0051, 0.0064, 0.7938],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.008

[Epoch: 109, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8816e-01, 8.2743e-03, 5.6118e-01, 2.2796e-06, 6.1015e-03, 1.8522e-02,
        1.7757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.016

[Epoch: 109, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0813e-03, 1.8872e-05, 4.9547e-03, 1.0267e-05, 5.9912e-03, 5.8691e-03,
        9.7807e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.032

[Epoch: 109, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5821e-02, 1.2664e-07, 2.2989e-02, 1.8838e-07, 6.9638e-08, 9.5119e-01,
        2.9226e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.883

[Epoch: 109, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9524e-03, 3.8461e-03, 9.7635e-01, 2.7845e-06, 5.1854e-03, 4.6616e-03,
        6.4895e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 110, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0043, 0.0061, 0.1848, 0.0043, 0.0074, 0.7882],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.000

[Epoch: 110, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6151e-01, 4.5001e-03, 6.0377e-01, 3.5122e-06, 5.0110e-03, 1.3680e-02,
        1.1528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.001

[Epoch: 110, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.3501e-03, 9.4238e-06, 6.2417e-03, 9.2949e-06, 4.9408e-03, 4.7336e-03,
        9.7672e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 110, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5815e-02, 1.4038e-07, 1.3054e-02, 3.0592e-07, 1.8668e-07, 9.7113e-01,
        3.9613e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.837

[Epoch: 110, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9554e-03, 6.3164e-03, 9.7384e-01, 7.1430e-06, 5.7314e-03, 4.1285e-03,
        1.6889e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 111, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0072, 0.0069, 0.0190, 0.1740, 0.0148, 0.0093, 0.7688],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 111, batch: 88/220] total loss per batch: 0.419
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1686e-01, 4.6374e-03, 5.4152e-01, 2.5237e-06, 5.7644e-03, 1.2702e-02,
        1.8511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 111, batch: 132/220] total loss per batch: 0.449
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([1.4029e-02, 2.9125e-05, 7.3489e-03, 1.5171e-05, 7.4516e-03, 4.4061e-03,
        9.6672e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.010

[Epoch: 111, batch: 176/220] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.5311e-02, 2.2275e-07, 2.0584e-02, 5.4476e-07, 1.2148e-07, 9.5410e-01,
        6.1931e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 111, batch: 220/220] total loss per batch: 0.442
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.7489e-02, 7.8652e-03, 9.6674e-01, 2.8174e-06, 5.2559e-03, 2.6369e-03,
        6.1214e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 112, batch: 44/220] total loss per batch: 0.431
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0085, 0.0158, 0.0053, 0.1537, 0.0118, 0.0561, 0.7488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 112, batch: 88/220] total loss per batch: 0.424
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7093e-01, 4.9316e-03, 5.7801e-01, 3.6066e-06, 4.6147e-03, 2.8946e-02,
        1.2574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 112, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.9336e-03, 1.6147e-05, 3.5067e-03, 9.4323e-06, 4.6544e-03, 3.5589e-03,
        9.8432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.014

[Epoch: 112, batch: 176/220] total loss per batch: 0.437
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4047e-02, 1.5177e-07, 2.0937e-02, 1.7382e-07, 8.4180e-08, 9.5502e-01,
        3.0324e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.850

[Epoch: 112, batch: 220/220] total loss per batch: 0.431
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0828e-02, 6.2293e-03, 9.7182e-01, 6.5212e-06, 7.0991e-03, 4.0132e-03,
        5.6339e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 113, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0032, 0.0038, 0.0031, 0.1199, 0.0056, 0.0106, 0.8538],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 113, batch: 88/220] total loss per batch: 0.416
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7773e-01, 6.1342e-03, 5.8450e-01, 2.1444e-06, 3.7082e-03, 1.6874e-02,
        1.1049e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.022

[Epoch: 113, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5872e-03, 3.3652e-05, 4.1708e-03, 1.0410e-05, 5.1438e-03, 4.1240e-03,
        9.8193e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 113, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6264e-02, 1.9467e-07, 1.4099e-02, 1.7331e-07, 1.9276e-07, 9.6964e-01,
        3.6010e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.846

[Epoch: 113, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0817e-02, 4.1541e-03, 9.7542e-01, 5.8655e-06, 5.7674e-03, 3.8333e-03,
        4.7050e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 114, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0039, 0.0056, 0.0044, 0.2404, 0.0068, 0.0139, 0.7250],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 114, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7676e-01, 5.1517e-03, 5.8345e-01, 1.9729e-06, 3.3690e-03, 1.7516e-02,
        1.3754e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 114, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3130e-03, 2.7610e-05, 4.5007e-03, 1.8672e-05, 4.8222e-03, 4.3805e-03,
        9.8194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 114, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8670e-02, 2.0581e-07, 1.7498e-02, 1.9748e-07, 1.7399e-07, 9.6383e-01,
        7.5726e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.835

[Epoch: 114, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0545e-02, 4.6239e-03, 9.7599e-01, 5.7982e-06, 5.2465e-03, 3.5870e-03,
        4.2718e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 115, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0041, 0.0051, 0.0047, 0.2028, 0.0066, 0.0118, 0.7650],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 115, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9100e-01, 4.9866e-03, 5.7053e-01, 1.9220e-06, 3.6758e-03, 1.5938e-02,
        1.3868e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.003

[Epoch: 115, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3149e-03, 2.7458e-05, 4.7644e-03, 1.5447e-05, 4.5385e-03, 4.3694e-03,
        9.8197e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 115, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8759e-02, 1.8519e-07, 1.6354e-02, 1.6146e-07, 1.6110e-07, 9.6489e-01,
        7.0272e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.843

[Epoch: 115, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5651e-03, 4.4520e-03, 9.7736e-01, 5.4077e-06, 5.0142e-03, 3.5989e-03,
        4.6983e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 116, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0045, 0.0048, 0.0048, 0.1926, 0.0065, 0.0102, 0.7766],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 116, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8445e-01, 5.2501e-03, 5.7573e-01, 1.8125e-06, 3.8178e-03, 1.6019e-02,
        1.4734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 116, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2492e-03, 2.7796e-05, 4.9717e-03, 1.4086e-05, 4.6673e-03, 4.3844e-03,
        9.8169e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 116, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9136e-02, 1.6407e-07, 1.5993e-02, 1.4958e-07, 1.4743e-07, 9.6487e-01,
        6.6131e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 116, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4784e-03, 4.7021e-03, 9.7720e-01, 4.8742e-06, 4.8686e-03, 3.7393e-03,
        4.5303e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 117, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0049, 0.0049, 0.1868, 0.0064, 0.0093, 0.7831],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 117, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8267e-01, 5.1666e-03, 5.7833e-01, 1.6625e-06, 3.9167e-03, 1.5589e-02,
        1.4329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 117, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2498e-03, 2.6026e-05, 5.0666e-03, 1.2675e-05, 4.6884e-03, 4.4497e-03,
        9.8151e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 117, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9130e-02, 1.4538e-07, 1.5305e-02, 1.3171e-07, 1.2351e-07, 9.6556e-01,
        6.6888e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.851

[Epoch: 117, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5652e-03, 4.7359e-03, 9.7702e-01, 4.7447e-06, 4.7941e-03, 3.8797e-03,
        4.4205e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 118, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0049, 0.0051, 0.1834, 0.0065, 0.0086, 0.7867],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 118, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7950e-01, 5.1624e-03, 5.8122e-01, 1.5239e-06, 3.9831e-03, 1.5717e-02,
        1.4413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 118, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3292e-03, 2.4257e-05, 5.2796e-03, 1.1924e-05, 4.7552e-03, 4.6298e-03,
        9.8097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 118, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9472e-02, 1.3282e-07, 1.5276e-02, 1.2146e-07, 1.1539e-07, 9.6525e-01,
        6.1205e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 118, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8155e-03, 4.8814e-03, 9.7627e-01, 4.3975e-06, 4.8916e-03, 4.1329e-03,
        4.6105e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 119, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0050, 0.0054, 0.1843, 0.0065, 0.0082, 0.7859],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 119, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8207e-01, 5.0794e-03, 5.7953e-01, 1.3452e-06, 3.9485e-03, 1.5248e-02,
        1.4124e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 119, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.3512e-03, 2.2906e-05, 5.2564e-03, 1.0942e-05, 4.8135e-03, 4.7360e-03,
        9.8081e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 119, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9698e-02, 1.1445e-07, 1.4927e-02, 1.0502e-07, 9.3633e-08, 9.6537e-01,
        5.8902e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 119, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8128e-03, 4.8298e-03, 9.7616e-01, 4.1539e-06, 4.8440e-03, 4.3484e-03,
        4.2012e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 120, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0050, 0.0058, 0.1801, 0.0065, 0.0076, 0.7902],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 120, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7998e-01, 5.1333e-03, 5.8114e-01, 1.2323e-06, 4.0527e-03, 1.5314e-02,
        1.4382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 120, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4305e-03, 2.0869e-05, 5.3749e-03, 1.0212e-05, 4.6788e-03, 4.8168e-03,
        9.8067e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 120, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9958e-02, 1.1106e-07, 1.5250e-02, 1.0004e-07, 9.0348e-08, 9.6479e-01,
        5.6214e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 120, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.7945e-03, 4.8669e-03, 9.7599e-01, 3.7496e-06, 4.9072e-03, 4.4291e-03,
        4.4315e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 121, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0050, 0.0052, 0.0065, 0.1847, 0.0065, 0.0075, 0.7845],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 121, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7909e-01, 4.9900e-03, 5.8257e-01, 1.1233e-06, 4.0482e-03, 1.5126e-02,
        1.4177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 121, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4220e-03, 2.0271e-05, 5.2832e-03, 9.2682e-06, 4.8135e-03, 4.7924e-03,
        9.8066e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 121, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9815e-02, 9.4281e-08, 1.4569e-02, 8.5348e-08, 7.6466e-08, 9.6562e-01,
        5.2121e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 121, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0117e-02, 5.0975e-03, 9.7510e-01, 3.7256e-06, 4.8716e-03, 4.8055e-03,
        3.8915e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 122, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0050, 0.0050, 0.0067, 0.1791, 0.0063, 0.0068, 0.7910],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 122, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8438e-01, 5.1500e-03, 5.7745e-01, 9.9461e-07, 4.0526e-03, 1.4939e-02,
        1.4026e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 122, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7015e-03, 1.8873e-05, 5.4112e-03, 9.0730e-06, 4.7665e-03, 5.0960e-03,
        9.8000e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 122, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0288e-02, 9.2105e-08, 1.5523e-02, 8.1920e-08, 7.1963e-08, 9.6419e-01,
        4.7100e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 122, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4895e-03, 4.4945e-03, 9.7651e-01, 3.2737e-06, 4.9270e-03, 4.5709e-03,
        3.7465e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 123, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0049, 0.0075, 0.1803, 0.0062, 0.0068, 0.7892],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 123, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7573e-01, 5.1420e-03, 5.8439e-01, 9.4827e-07, 4.1206e-03, 1.5063e-02,
        1.5557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 123, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6814e-03, 1.9005e-05, 5.3909e-03, 8.3999e-06, 4.8902e-03, 4.8580e-03,
        9.8015e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 123, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9635e-02, 8.2959e-08, 1.4377e-02, 7.3902e-08, 6.0747e-08, 9.6599e-01,
        4.9182e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 123, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1808e-02, 6.1055e-03, 9.7195e-01, 3.5429e-06, 4.8677e-03, 5.2595e-03,
        3.9864e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 124, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0057, 0.0074, 0.1867, 0.0058, 0.0068, 0.7825],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 124, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8944e-01, 4.9984e-03, 5.7335e-01, 9.3121e-07, 4.1661e-03, 1.4924e-02,
        1.3123e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 124, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7908e-03, 1.8406e-05, 5.1756e-03, 7.8880e-06, 4.8974e-03, 4.6633e-03,
        9.8045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 124, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0776e-02, 8.6675e-08, 1.5925e-02, 7.0034e-08, 6.0385e-08, 9.6330e-01,
        5.3192e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 124, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.6229e-03, 3.5236e-03, 9.7811e-01, 2.6173e-06, 5.2647e-03, 4.4751e-03,
        2.9026e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 125, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0042, 0.0081, 0.1654, 0.0059, 0.0061, 0.8053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 125, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6174e-01, 5.3393e-03, 5.9534e-01, 8.4772e-07, 4.1943e-03, 1.5618e-02,
        1.7773e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 125, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7968e-03, 1.4707e-05, 5.2714e-03, 7.3050e-06, 4.5563e-03, 5.1169e-03,
        9.8024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 125, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8182e-02, 1.0933e-07, 1.4198e-02, 9.1088e-08, 9.7429e-08, 9.6762e-01,
        4.9362e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.849

[Epoch: 125, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0463e-02, 5.9034e-03, 9.7346e-01, 3.4199e-06, 4.7620e-03, 5.4001e-03,
        4.4539e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 126, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0062, 0.0105, 0.2068, 0.0064, 0.0073, 0.7578],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 126, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1832e-01, 5.3065e-03, 5.4649e-01, 1.3850e-06, 4.2552e-03, 1.3464e-02,
        1.2163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 126, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6936e-03, 1.9392e-05, 4.9232e-03, 1.0007e-05, 5.5841e-03, 4.8190e-03,
        9.7995e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.029

[Epoch: 126, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.6291e-02, 1.0421e-07, 1.7364e-02, 1.3032e-07, 1.0316e-07, 9.5634e-01,
        9.2587e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 126, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0247e-02, 4.8514e-03, 9.7407e-01, 5.9264e-06, 6.0795e-03, 4.7400e-03,
        1.0539e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 127, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0062, 0.0062, 0.0075, 0.1685, 0.0052, 0.0069, 0.7995],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 127, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3868e-01, 4.8077e-03, 6.1893e-01, 1.4314e-06, 5.0186e-03, 1.4802e-02,
        1.7761e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 127, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2411e-03, 1.5814e-05, 5.3702e-03, 6.6178e-06, 4.6039e-03, 4.9967e-03,
        9.7977e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 127, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7382e-02, 1.4017e-07, 1.2249e-02, 1.8950e-07, 1.6655e-07, 9.7037e-01,
        6.5817e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 127, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4924e-03, 4.5329e-03, 9.7590e-01, 5.5388e-06, 5.0392e-03, 5.0277e-03,
        6.9993e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 128, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0043, 0.0102, 0.1780, 0.0050, 0.0059, 0.7917],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 128, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2642e-01, 5.8382e-03, 5.3385e-01, 1.9559e-06, 4.4517e-03, 1.4206e-02,
        1.5232e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 128, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7890e-03, 1.5278e-05, 5.1516e-03, 1.2756e-05, 5.2537e-03, 4.6593e-03,
        9.8012e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 128, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2793e-02, 1.1232e-07, 1.7728e-02, 1.6812e-07, 9.9258e-08, 9.5948e-01,
        7.9652e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 128, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.1223e-03, 7.2863e-03, 9.7403e-01, 4.8930e-06, 5.1096e-03, 5.4391e-03,
        5.6491e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 129, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0049, 0.0082, 0.1823, 0.0054, 0.0069, 0.7876],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 129, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5271e-01, 4.7319e-03, 6.0531e-01, 2.4015e-06, 5.4451e-03, 1.6891e-02,
        1.4906e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 129, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.6580e-03, 1.6737e-05, 5.8489e-03, 1.0855e-05, 5.9965e-03, 5.1178e-03,
        9.7735e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 129, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8038e-02, 1.3777e-07, 1.2845e-02, 2.3264e-07, 2.1698e-07, 9.6912e-01,
        2.6912e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.883

[Epoch: 129, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2517e-02, 3.3230e-03, 9.7315e-01, 6.2756e-06, 6.2558e-03, 4.7396e-03,
        5.6575e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 130, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0067, 0.0060, 0.0095, 0.1867, 0.0050, 0.0060, 0.7801],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 130, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0679e-01, 5.9010e-03, 5.5207e-01, 2.3517e-06, 5.1553e-03, 1.5302e-02,
        1.4785e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 130, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4018e-03, 1.5948e-05, 6.0622e-03, 1.0261e-05, 5.2524e-03, 4.8501e-03,
        9.7941e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 130, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0802e-02, 1.0895e-07, 1.4748e-02, 1.3604e-07, 1.2390e-07, 9.6445e-01,
        5.9542e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.831

[Epoch: 130, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0255e-02, 9.3060e-03, 9.7188e-01, 6.0752e-06, 4.2417e-03, 4.3060e-03,
        5.2362e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 131, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0043, 0.0086, 0.1607, 0.0052, 0.0053, 0.8111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 131, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4508e-01, 5.1122e-03, 6.0699e-01, 1.4126e-06, 5.3341e-03, 1.6679e-02,
        2.0796e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 131, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.5507e-03, 2.6456e-05, 6.4788e-03, 7.7576e-06, 7.0514e-03, 4.8813e-03,
        9.7500e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 131, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7309e-02, 1.1162e-07, 1.3972e-02, 2.4047e-07, 1.5577e-07, 9.6872e-01,
        6.6247e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 131, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.8592e-03, 2.5679e-03, 9.7910e-01, 5.2843e-06, 5.5607e-03, 4.8979e-03,
        5.3214e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 132, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0059, 0.0057, 0.0084, 0.1941, 0.0054, 0.0060, 0.7745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 132, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6390e-01, 4.7420e-03, 5.0440e-01, 1.7341e-06, 4.8334e-03, 1.2119e-02,
        1.0004e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 132, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6039e-03, 1.2651e-05, 3.9250e-03, 8.3206e-06, 4.4951e-03, 5.3182e-03,
        9.8164e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 132, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.7529e-02, 2.3457e-07, 2.1891e-02, 2.9923e-07, 1.8354e-07, 9.5058e-01,
        8.0573e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.852

[Epoch: 132, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3707e-02, 6.2574e-03, 9.6808e-01, 6.2597e-06, 6.3354e-03, 5.6058e-03,
        3.4495e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 133, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0068, 0.0064, 0.0113, 0.1955, 0.0057, 0.0063, 0.7680],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 133, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.8709e-01, 5.3644e-03, 6.6446e-01, 2.8848e-06, 5.7039e-03, 1.7857e-02,
        1.9524e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 133, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2666e-03, 1.9021e-05, 5.7286e-03, 1.4865e-05, 4.5146e-03, 4.3902e-03,
        9.8107e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 133, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6207e-02, 1.1334e-07, 1.4723e-02, 1.8828e-07, 1.5712e-07, 9.6907e-01,
        1.1533e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 133, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5366e-03, 4.7993e-03, 9.7865e-01, 5.5235e-06, 3.7836e-03, 5.2212e-03,
        6.6479e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 134, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0044, 0.0081, 0.1497, 0.0057, 0.0056, 0.8219],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.007

[Epoch: 134, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.3363e-01, 6.1896e-03, 5.2833e-01, 1.6670e-06, 5.2055e-03, 1.3516e-02,
        1.3131e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 134, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2106e-03, 1.8788e-05, 5.4254e-03, 8.9085e-06, 5.7391e-03, 6.0724e-03,
        9.7752e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 134, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7310e-02, 7.7342e-08, 1.3673e-02, 1.1217e-07, 7.2734e-08, 9.6902e-01,
        8.7931e-11], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.867

[Epoch: 134, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1186e-02, 3.9189e-03, 9.7468e-01, 4.9769e-06, 5.8058e-03, 4.3974e-03,
        2.4807e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 135, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0050, 0.0048, 0.0087, 0.1958, 0.0055, 0.0056, 0.7745],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 135, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7088e-01, 5.6981e-03, 5.9077e-01, 2.8342e-06, 5.6770e-03, 1.2835e-02,
        1.4138e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 135, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7510e-03, 1.7223e-05, 5.2666e-03, 1.0279e-05, 4.5038e-03, 5.1363e-03,
        9.8031e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 135, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4131e-02, 1.7362e-07, 1.5016e-02, 1.6663e-07, 1.3705e-07, 9.6085e-01,
        1.3670e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.843

[Epoch: 135, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.9864e-03, 4.9923e-03, 9.7592e-01, 4.0709e-06, 4.2013e-03, 4.8919e-03,
        6.3143e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 136, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0063, 0.0062, 0.0093, 0.1863, 0.0048, 0.0071, 0.7800],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 136, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5563e-01, 5.6485e-03, 6.0054e-01, 2.1732e-06, 6.2505e-03, 1.5032e-02,
        1.6895e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 136, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9776e-03, 1.9058e-05, 4.8036e-03, 1.1018e-05, 5.0076e-03, 4.6762e-03,
        9.8050e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 136, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9919e-02, 1.5930e-07, 1.9535e-02, 2.8564e-07, 1.2972e-07, 9.6055e-01,
        7.5309e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.860

[Epoch: 136, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.4111e-02, 4.4958e-03, 9.7115e-01, 6.0937e-06, 4.8256e-03, 5.4048e-03,
        7.1494e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 137, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0057, 0.0119, 0.1625, 0.0062, 0.0066, 0.8018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.013

[Epoch: 137, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2867e-01, 6.1795e-03, 5.3099e-01, 2.3761e-06, 4.6662e-03, 1.6111e-02,
        1.3384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 137, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5871e-03, 1.3443e-05, 5.1604e-03, 1.1501e-05, 5.9269e-03, 5.9490e-03,
        9.7835e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 137, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.3851e-02, 7.3013e-08, 9.8387e-03, 8.3784e-08, 6.1602e-08, 9.7631e-01,
        1.6462e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.831

[Epoch: 137, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.3759e-03, 5.5655e-03, 9.7938e-01, 3.6181e-06, 4.7847e-03, 4.8840e-03,
        3.3980e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 138, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0043, 0.0037, 0.0078, 0.1791, 0.0039, 0.0050, 0.7962],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 138, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3008e-01, 5.7880e-03, 6.2865e-01, 2.0885e-06, 5.8386e-03, 1.4770e-02,
        1.4869e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 138, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1180e-03, 1.2628e-05, 4.2820e-03, 1.0077e-05, 4.2214e-03, 4.5024e-03,
        9.8185e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 138, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.6550e-02, 3.1610e-07, 2.0966e-02, 3.5489e-07, 1.2886e-07, 9.5248e-01,
        6.4322e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 138, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0104e-02, 4.6126e-03, 9.7515e-01, 5.8041e-06, 4.9318e-03, 5.1936e-03,
        3.5432e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 139, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0067, 0.0078, 0.0121, 0.2078, 0.0066, 0.0060, 0.7529],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 139, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.4158e-01, 5.6965e-03, 5.1079e-01, 3.6387e-06, 5.2239e-03, 1.9354e-02,
        1.7349e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 139, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9273e-03, 1.3194e-05, 4.6213e-03, 9.4384e-06, 5.1227e-03, 4.9916e-03,
        9.8031e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 139, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.6388e-02, 7.7680e-08, 1.1743e-02, 1.0955e-07, 7.4782e-08, 9.7187e-01,
        2.5847e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.883

[Epoch: 139, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.5697e-02, 5.7131e-03, 9.5587e-01, 6.0182e-06, 7.9045e-03, 4.8060e-03,
        3.6218e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 140, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0040, 0.0094, 0.1462, 0.0052, 0.0056, 0.8241],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.010

[Epoch: 140, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3377e-01, 7.5080e-03, 6.2649e-01, 1.9848e-06, 6.8432e-03, 1.2800e-02,
        1.2591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 140, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.3072e-03, 1.5618e-05, 4.8553e-03, 1.5837e-05, 5.5068e-03, 5.5927e-03,
        9.7871e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.012

[Epoch: 140, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3613e-02, 8.1454e-08, 1.4408e-02, 1.0986e-07, 2.2198e-07, 9.6198e-01,
        1.2764e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 140, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([2.9652e-03, 4.1177e-03, 9.8056e-01, 2.9421e-06, 3.9862e-03, 8.3628e-03,
        1.6100e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 141, batch: 44/220] total loss per batch: 0.422
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0056, 0.0058, 0.0076, 0.1964, 0.0050, 0.0056, 0.7740],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 141, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1822e-01, 5.4529e-03, 5.3930e-01, 3.7620e-06, 5.9548e-03, 1.3509e-02,
        1.7568e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 141, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.4845e-03, 1.2308e-05, 6.7694e-03, 9.1566e-06, 4.3957e-03, 6.3106e-03,
        9.7602e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 141, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7571e-02, 2.1013e-07, 1.7387e-02, 2.5592e-07, 8.6169e-08, 9.6504e-01,
        6.0790e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 141, batch: 220/220] total loss per batch: 0.429
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.9404e-03, 9.7310e-03, 9.7083e-01, 6.3212e-06, 4.9689e-03, 5.5215e-03,
        3.8110e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 142, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0063, 0.0068, 0.0095, 0.1643, 0.0074, 0.0061, 0.7995],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 142, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7339e-01, 7.3372e-03, 5.8475e-01, 3.5591e-06, 5.2761e-03, 1.3206e-02,
        1.6035e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 142, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8896e-03, 1.4888e-05, 4.8762e-03, 8.6771e-06, 5.1838e-03, 4.6853e-03,
        9.8034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 142, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7690e-02, 1.3784e-07, 1.2590e-02, 1.1564e-07, 6.7652e-08, 9.6972e-01,
        7.9535e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.834

[Epoch: 142, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3632e-02, 5.3198e-03, 9.6720e-01, 4.3353e-06, 6.8545e-03, 6.9590e-03,
        2.6491e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.000

[Epoch: 143, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0065, 0.0063, 0.0102, 0.2003, 0.0061, 0.0061, 0.7645],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 143, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3971e-01, 4.7397e-03, 6.1554e-01, 2.4879e-06, 6.8118e-03, 1.6476e-02,
        1.6721e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 143, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2454e-03, 8.6520e-06, 4.4980e-03, 7.4779e-06, 3.3413e-03, 4.6740e-03,
        9.8323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.018

[Epoch: 143, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8222e-02, 1.3198e-07, 1.3848e-02, 2.2464e-07, 8.6833e-08, 9.6793e-01,
        6.4730e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.826

[Epoch: 143, batch: 220/220] total loss per batch: 0.430
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([6.0067e-03, 5.2427e-03, 9.7712e-01, 1.3884e-06, 6.4262e-03, 5.2008e-03,
        4.3424e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 144, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0061, 0.0059, 0.0112, 0.1751, 0.0042, 0.0050, 0.7926],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 144, batch: 88/220] total loss per batch: 0.412
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8382e-01, 5.1280e-03, 5.8389e-01, 7.0536e-06, 4.3940e-03, 1.1593e-02,
        1.1170e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.019

[Epoch: 144, batch: 132/220] total loss per batch: 0.445
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2385e-03, 8.6571e-06, 5.9222e-03, 2.2087e-05, 4.5516e-03, 4.8668e-03,
        9.7939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 144, batch: 176/220] total loss per batch: 0.428
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5134e-02, 1.0141e-06, 1.6444e-02, 1.9774e-07, 4.7612e-08, 9.6842e-01,
        1.6424e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.880

[Epoch: 144, batch: 220/220] total loss per batch: 0.436
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.2847e-03, 1.9798e-03, 9.7206e-01, 2.6946e-05, 9.5511e-03, 7.0723e-03,
        2.8036e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 145, batch: 44/220] total loss per batch: 0.425
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0073, 0.0108, 0.0064, 0.1748, 0.0063, 0.0057, 0.7886],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.000

[Epoch: 145, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6614e-01, 4.5496e-03, 5.8637e-01, 1.9249e-05, 4.9549e-03, 1.0005e-02,
        2.7957e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 145, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4415e-03, 5.9871e-05, 3.9505e-03, 3.6576e-05, 3.3218e-03, 3.5383e-03,
        9.8465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 145, batch: 176/220] total loss per batch: 0.427
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1263e-02, 6.9127e-07, 1.6166e-02, 4.3448e-07, 1.0880e-07, 9.6257e-01,
        4.2432e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.816

[Epoch: 145, batch: 220/220] total loss per batch: 0.433
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([5.9649e-03, 3.9594e-03, 9.8422e-01, 9.5038e-06, 3.3294e-03, 2.5112e-03,
        2.6944e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 146, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0068, 0.0078, 0.0074, 0.1843, 0.0113, 0.0058, 0.7767],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.012

[Epoch: 146, batch: 88/220] total loss per batch: 0.415
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.5786e-01, 4.4453e-03, 5.0891e-01, 1.2543e-05, 6.8348e-03, 1.0191e-02,
        1.1747e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.015

[Epoch: 146, batch: 132/220] total loss per batch: 0.438
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6374e-03, 2.4835e-05, 4.2741e-03, 1.5702e-05, 5.0644e-03, 5.0682e-03,
        9.8092e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 146, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0222e-02, 3.8510e-07, 1.7672e-02, 3.3438e-07, 7.9643e-08, 9.6210e-01,
        7.5602e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.802

[Epoch: 146, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8300e-03, 2.5657e-03, 9.7900e-01, 8.6222e-06, 4.1851e-03, 4.3855e-03,
        2.0459e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 147, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0074, 0.0072, 0.1614, 0.0062, 0.0050, 0.8061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.011

[Epoch: 147, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3612e-01, 4.5473e-03, 6.2591e-01, 9.4963e-06, 7.2849e-03, 1.2204e-02,
        1.3929e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 147, batch: 132/220] total loss per batch: 0.436
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.6763e-03, 2.8058e-05, 2.8024e-03, 1.6709e-05, 4.4151e-03, 3.3767e-03,
        9.8368e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 147, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9505e-02, 3.0550e-07, 1.5059e-02, 2.3739e-07, 5.4730e-08, 9.6544e-01,
        2.3665e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 147, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.8677e-03, 4.0299e-03, 9.7659e-01, 8.2653e-06, 4.3171e-03, 6.1486e-03,
        3.5828e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 148, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0072, 0.0072, 0.0071, 0.1874, 0.0061, 0.0053, 0.7797],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 148, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0674e-01, 4.5980e-03, 5.5371e-01, 9.0806e-06, 6.2520e-03, 1.3353e-02,
        1.5340e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 148, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4458e-03, 2.7171e-05, 3.4817e-03, 1.6952e-05, 4.5206e-03, 3.3718e-03,
        9.8414e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 148, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0155e-02, 3.0651e-07, 1.4171e-02, 2.1906e-07, 3.8669e-08, 9.6567e-01,
        2.9153e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.849

[Epoch: 148, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4297e-03, 4.3556e-03, 9.7645e-01, 7.9365e-06, 4.6297e-03, 5.1151e-03,
        1.1427e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 149, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0063, 0.0059, 0.0069, 0.1761, 0.0057, 0.0053, 0.7937],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 149, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7699e-01, 4.3483e-03, 5.8449e-01, 7.6084e-06, 5.9165e-03, 1.3047e-02,
        1.5194e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 149, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5414e-03, 2.2324e-05, 3.6297e-03, 1.6255e-05, 4.6296e-03, 3.8568e-03,
        9.8330e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 149, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0309e-02, 2.5012e-07, 1.5255e-02, 1.8833e-07, 3.2669e-08, 9.6444e-01,
        1.0394e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 149, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1062e-02, 4.6170e-03, 9.7366e-01, 7.6841e-06, 5.0460e-03, 5.5936e-03,
        1.1072e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 150, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0061, 0.0060, 0.0076, 0.1799, 0.0058, 0.0051, 0.7895],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 150, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8289e-01, 4.7690e-03, 5.7725e-01, 7.4196e-06, 5.9595e-03, 1.3991e-02,
        1.5125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 150, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8205e-03, 2.3819e-05, 3.9159e-03, 1.5090e-05, 5.0166e-03, 4.0152e-03,
        9.8219e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 150, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9921e-02, 2.6485e-07, 1.4560e-02, 1.4968e-07, 2.7475e-08, 9.6552e-01,
        1.9281e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 150, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.7834e-03, 6.2288e-03, 9.7405e-01, 6.4112e-06, 4.6952e-03, 5.2191e-03,
        1.4774e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 151, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0060, 0.0059, 0.0078, 0.1790, 0.0056, 0.0052, 0.7905],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 151, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7848e-01, 4.6217e-03, 5.8201e-01, 6.1883e-06, 5.7025e-03, 1.4270e-02,
        1.4911e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 151, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7490e-03, 2.0341e-05, 4.0595e-03, 1.3901e-05, 4.9382e-03, 4.2743e-03,
        9.8194e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 151, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9627e-02, 2.2033e-07, 1.5161e-02, 1.3349e-07, 2.5219e-08, 9.6521e-01,
        2.5614e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 151, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8400e-03, 5.5803e-03, 9.7489e-01, 5.9622e-06, 4.6848e-03, 4.9774e-03,
        1.7696e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 152, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0059, 0.0056, 0.0080, 0.1784, 0.0056, 0.0051, 0.7915],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 152, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7996e-01, 4.6243e-03, 5.8048e-01, 5.9206e-06, 5.6840e-03, 1.4553e-02,
        1.4691e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 152, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8203e-03, 1.8605e-05, 4.1496e-03, 1.3355e-05, 5.0007e-03, 4.2873e-03,
        9.8171e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 152, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9766e-02, 1.9188e-07, 1.4652e-02, 1.1528e-07, 2.1871e-08, 9.6558e-01,
        2.4783e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 152, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5618e-03, 5.1728e-03, 9.7558e-01, 5.5482e-06, 4.6531e-03, 5.0186e-03,
        1.0528e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 153, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0058, 0.0055, 0.0082, 0.1797, 0.0055, 0.0050, 0.7903],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 153, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8258e-01, 4.6528e-03, 5.7795e-01, 5.2900e-06, 5.4756e-03, 1.4464e-02,
        1.4873e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 153, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8876e-03, 1.7469e-05, 4.3247e-03, 1.2619e-05, 5.0770e-03, 4.4619e-03,
        9.8122e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 153, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9540e-02, 1.7198e-07, 1.4931e-02, 1.0329e-07, 1.9627e-08, 9.6553e-01,
        1.4243e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 153, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0087e-02, 5.1927e-03, 9.7481e-01, 5.3304e-06, 4.8707e-03, 5.0241e-03,
        1.1765e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 154, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0056, 0.0053, 0.0084, 0.1794, 0.0055, 0.0050, 0.7908],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 154, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7935e-01, 4.7338e-03, 5.8066e-01, 4.9882e-06, 5.4204e-03, 1.4883e-02,
        1.4955e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 154, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0522e-03, 1.6689e-05, 4.4950e-03, 1.2185e-05, 5.1807e-03, 4.5569e-03,
        9.8069e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 154, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9631e-02, 1.5039e-07, 1.4950e-02, 9.0653e-08, 1.7270e-08, 9.6542e-01,
        1.8647e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 154, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.6520e-03, 5.0395e-03, 9.7532e-01, 5.0088e-06, 4.8510e-03, 5.1152e-03,
        1.2437e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 155, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0055, 0.0053, 0.0088, 0.1808, 0.0054, 0.0050, 0.7891],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 155, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8035e-01, 4.7784e-03, 5.7951e-01, 4.4950e-06, 5.4010e-03, 1.4946e-02,
        1.5010e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 155, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9590e-03, 1.4878e-05, 4.6040e-03, 1.1513e-05, 5.1451e-03, 4.7334e-03,
        9.8053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 155, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9905e-02, 1.3438e-07, 1.4932e-02, 8.0412e-08, 1.5300e-08, 9.6516e-01,
        1.7894e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 155, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.8608e-03, 5.0794e-03, 9.7533e-01, 4.5920e-06, 4.8022e-03, 4.9140e-03,
        7.7007e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 156, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0055, 0.0052, 0.0088, 0.1788, 0.0054, 0.0048, 0.7915],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 156, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8316e-01, 4.7969e-03, 5.7735e-01, 4.1897e-06, 5.2013e-03, 1.4747e-02,
        1.4747e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 156, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1747e-03, 1.4523e-05, 4.6006e-03, 1.0921e-05, 5.2795e-03, 4.7078e-03,
        9.8021e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 156, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9571e-02, 1.1791e-07, 1.4810e-02, 7.1886e-08, 1.4132e-08, 9.6562e-01,
        1.1088e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 156, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0129e-02, 5.1119e-03, 9.7448e-01, 4.5502e-06, 5.1383e-03, 5.1216e-03,
        1.0336e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 157, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0052, 0.0090, 0.1813, 0.0054, 0.0051, 0.7886],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 157, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7647e-01, 4.7840e-03, 5.8318e-01, 3.7505e-06, 5.2593e-03, 1.5269e-02,
        1.5037e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 157, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9318e-03, 1.2429e-05, 4.8945e-03, 1.0082e-05, 5.1528e-03, 4.7804e-03,
        9.8022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 157, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9975e-02, 1.1485e-07, 1.5654e-02, 6.7911e-08, 1.2179e-08, 9.6437e-01,
        1.6187e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 157, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.6521e-03, 5.1810e-03, 9.7556e-01, 3.9114e-06, 4.5469e-03, 5.0444e-03,
        6.7716e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 158, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0051, 0.0092, 0.1775, 0.0052, 0.0049, 0.7928],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 158, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8390e-01, 4.9525e-03, 5.7645e-01, 3.5483e-06, 5.1909e-03, 1.4652e-02,
        1.4858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 158, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.4135e-03, 1.2538e-05, 4.7035e-03, 9.7736e-06, 5.3856e-03, 4.8658e-03,
        9.7961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 158, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0434e-02, 8.3527e-08, 1.3988e-02, 5.8698e-08, 1.2754e-08, 9.6558e-01,
        1.0035e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.851

[Epoch: 158, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.3708e-03, 4.5530e-03, 9.7559e-01, 4.1487e-06, 5.6590e-03, 4.8126e-03,
        1.1373e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 159, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0052, 0.0100, 0.1861, 0.0054, 0.0052, 0.7828],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 159, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7742e-01, 4.9959e-03, 5.8240e-01, 3.1681e-06, 5.0743e-03, 1.5494e-02,
        1.4611e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 159, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6713e-03, 1.2112e-05, 4.8978e-03, 9.0908e-06, 5.0769e-03, 5.3925e-03,
        9.7994e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 159, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0416e-02, 1.1499e-07, 1.6664e-02, 6.1277e-08, 1.2542e-08, 9.6292e-01,
        1.5388e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 159, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1312e-02, 5.9739e-03, 9.7348e-01, 3.3379e-06, 3.9541e-03, 5.2738e-03,
        2.4066e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 160, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0052, 0.0044, 0.0077, 0.1672, 0.0045, 0.0045, 0.8065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 160, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9103e-01, 4.9833e-03, 5.6911e-01, 3.3729e-06, 5.3081e-03, 1.3530e-02,
        1.6029e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 160, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2494e-03, 8.1647e-06, 5.1367e-03, 8.5834e-06, 5.2786e-03, 3.8682e-03,
        9.8045e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 160, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8526e-02, 7.4352e-08, 1.5207e-02, 6.3476e-08, 1.5042e-08, 9.6627e-01,
        6.6509e-10], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 160, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.0104e-03, 3.7353e-03, 9.7770e-01, 3.7438e-06, 5.3066e-03, 5.2382e-03,
        7.3922e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 161, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0057, 0.0063, 0.0124, 0.1933, 0.0060, 0.0054, 0.7709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 161, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6003e-01, 5.6880e-03, 5.9911e-01, 3.1552e-06, 4.6668e-03, 1.6322e-02,
        1.4183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 161, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.3253e-03, 1.4659e-05, 4.7499e-03, 1.5363e-05, 4.9065e-03, 5.7758e-03,
        9.7921e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 161, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.6558e-02, 1.4692e-07, 1.5749e-02, 1.0705e-07, 1.3779e-08, 9.5769e-01,
        2.2645e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 161, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2105e-02, 8.4233e-03, 9.7113e-01, 4.6568e-06, 4.3099e-03, 4.0094e-03,
        1.3209e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 162, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0046, 0.0038, 0.0069, 0.1520, 0.0039, 0.0044, 0.8245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 162, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1752e-01, 6.0326e-03, 5.4023e-01, 4.6818e-06, 5.8912e-03, 1.4328e-02,
        1.5997e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.015

[Epoch: 162, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.6446e-03, 1.6707e-05, 5.2169e-03, 1.2635e-05, 5.5760e-03, 4.0328e-03,
        9.7850e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 162, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5417e-02, 2.1854e-07, 1.5267e-02, 1.6209e-07, 1.0057e-07, 9.6932e-01,
        7.4030e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 162, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.6318e-03, 2.9700e-03, 9.7643e-01, 7.8618e-06, 5.3366e-03, 6.6067e-03,
        1.8194e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 163, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0056, 0.0059, 0.0115, 0.1981, 0.0062, 0.0056, 0.7671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 163, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4749e-01, 5.2092e-03, 6.1269e-01, 4.7829e-06, 6.0142e-03, 1.3602e-02,
        1.4985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 163, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5237e-03, 1.5064e-05, 4.8790e-03, 1.2872e-05, 4.9585e-03, 5.1498e-03,
        9.7946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 163, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0537e-02, 1.8527e-07, 1.4813e-02, 1.5064e-07, 5.0511e-08, 9.6465e-01,
        1.9584e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.838

[Epoch: 163, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.2297e-03, 6.5985e-03, 9.7384e-01, 6.2218e-06, 5.3440e-03, 4.9549e-03,
        2.9709e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 164, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0043, 0.0040, 0.0100, 0.1897, 0.0049, 0.0053, 0.7818],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 164, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1867e-01, 4.8613e-03, 5.3958e-01, 3.2558e-06, 5.1163e-03, 1.5602e-02,
        1.6171e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.015

[Epoch: 164, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5629e-03, 9.7292e-06, 5.4230e-03, 1.0636e-05, 6.0890e-03, 5.0527e-03,
        9.7785e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 164, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8850e-02, 1.3792e-07, 1.8013e-02, 1.8052e-07, 8.7197e-08, 9.6314e-01,
        6.5216e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.878

[Epoch: 164, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.4571e-03, 3.3915e-03, 9.7920e-01, 7.9877e-06, 4.2713e-03, 4.6552e-03,
        1.6183e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 165, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0051, 0.0104, 0.1752, 0.0056, 0.0050, 0.7921],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.000

[Epoch: 165, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.3502e-01, 5.4381e-03, 6.2267e-01, 3.8346e-06, 6.3401e-03, 1.4043e-02,
        1.6492e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 165, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9729e-03, 1.3539e-05, 4.9350e-03, 1.0086e-05, 4.4452e-03, 4.4883e-03,
        9.8114e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 165, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3302e-02, 1.7368e-07, 1.3691e-02, 1.3360e-07, 4.2228e-08, 9.6301e-01,
        3.1632e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 165, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.3101e-02, 5.9477e-03, 9.6913e-01, 6.2871e-06, 6.4522e-03, 5.3393e-03,
        2.0456e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 166, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0048, 0.0102, 0.1822, 0.0059, 0.0050, 0.7870],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 166, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1674e-01, 4.9874e-03, 5.4514e-01, 2.6690e-06, 5.0153e-03, 1.4457e-02,
        1.3652e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 166, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.8079e-03, 8.7609e-06, 5.6690e-03, 9.7941e-06, 5.5875e-03, 5.1975e-03,
        9.7772e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 166, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8095e-02, 1.0914e-07, 1.6005e-02, 1.4108e-07, 4.0044e-08, 9.6590e-01,
        4.7393e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.859

[Epoch: 166, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.4055e-03, 4.2486e-03, 9.8003e-01, 5.7792e-06, 3.6433e-03, 4.6304e-03,
        3.4114e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 167, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0054, 0.0093, 0.1755, 0.0049, 0.0052, 0.7943],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 167, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4585e-01, 4.6561e-03, 6.1539e-01, 3.3521e-06, 5.6101e-03, 1.4571e-02,
        1.3923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.013

[Epoch: 167, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.2764e-03, 9.6374e-06, 4.3922e-03, 7.1741e-06, 4.2884e-03, 5.2928e-03,
        9.8173e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 167, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9497e-02, 1.4446e-07, 1.3619e-02, 1.0991e-07, 4.7003e-08, 9.6688e-01,
        4.6253e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.842

[Epoch: 167, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0463e-02, 5.8490e-03, 9.7272e-01, 5.7276e-06, 5.6171e-03, 5.3362e-03,
        6.7179e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 168, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0047, 0.0109, 0.1860, 0.0061, 0.0052, 0.7817],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 168, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1798e-01, 4.6299e-03, 5.4312e-01, 3.1799e-06, 4.8747e-03, 1.3874e-02,
        1.5516e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 168, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1004e-03, 9.7804e-06, 5.3272e-03, 1.1203e-05, 4.9119e-03, 4.4205e-03,
        9.8022e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 168, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3638e-02, 1.3606e-07, 1.6254e-02, 1.5274e-07, 3.7078e-08, 9.6011e-01,
        1.0683e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 168, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5267e-03, 4.5619e-03, 9.7748e-01, 6.5163e-06, 4.2446e-03, 4.1650e-03,
        1.9519e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 169, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0055, 0.0047, 0.0085, 0.1678, 0.0045, 0.0048, 0.8041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 169, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5995e-01, 5.9798e-03, 5.9379e-01, 3.4199e-06, 5.9082e-03, 1.5907e-02,
        1.8461e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.016

[Epoch: 169, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5638e-03, 1.2933e-05, 5.7292e-03, 9.4945e-06, 5.9752e-03, 6.3344e-03,
        9.7637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.027

[Epoch: 169, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.5798e-02, 1.0764e-07, 1.3832e-02, 1.2454e-07, 6.5458e-08, 9.7037e-01,
        8.4650e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 169, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1954e-02, 4.7654e-03, 9.7072e-01, 6.4879e-06, 6.2125e-03, 6.3018e-03,
        3.7232e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 170, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0053, 0.0107, 0.1905, 0.0055, 0.0053, 0.7778],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 170, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8552e-01, 5.1197e-03, 5.7341e-01, 4.0312e-06, 5.5780e-03, 1.7062e-02,
        1.3308e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.020

[Epoch: 170, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4541e-03, 6.3441e-06, 3.6108e-03, 7.9535e-06, 4.1467e-03, 4.1437e-03,
        9.8363e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 170, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2393e-02, 1.1117e-07, 1.5515e-02, 1.0784e-07, 6.7432e-08, 9.6209e-01,
        1.1993e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.845

[Epoch: 170, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.5192e-03, 6.3243e-03, 9.7715e-01, 5.6057e-06, 4.4663e-03, 4.5168e-03,
        1.7149e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 171, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0045, 0.0106, 0.1910, 0.0048, 0.0045, 0.7797],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 171, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5901e-01, 5.3079e-03, 6.0458e-01, 2.7833e-06, 4.9308e-03, 1.1836e-02,
        1.4336e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 171, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5967e-03, 1.3142e-05, 5.1002e-03, 9.2385e-06, 4.8372e-03, 4.3841e-03,
        9.8006e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 171, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8118e-02, 1.6189e-07, 1.2281e-02, 1.8189e-07, 6.6129e-08, 9.6960e-01,
        8.4588e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 171, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0207e-02, 5.3177e-03, 9.7492e-01, 6.6271e-06, 5.3038e-03, 4.2267e-03,
        2.1618e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 172, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0061, 0.0051, 0.0095, 0.1648, 0.0056, 0.0051, 0.8038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 172, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2613e-01, 4.8256e-03, 5.3247e-01, 4.1431e-06, 4.8857e-03, 1.4920e-02,
        1.6760e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.012

[Epoch: 172, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.4451e-03, 8.8794e-06, 4.3915e-03, 9.8924e-06, 6.4017e-03, 5.7544e-03,
        9.7799e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 172, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1242e-02, 1.4417e-07, 1.9934e-02, 1.3037e-07, 5.7240e-08, 9.5882e-01,
        5.2223e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.853

[Epoch: 172, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0057e-02, 4.1092e-03, 9.7566e-01, 4.8763e-06, 4.4998e-03, 5.6475e-03,
        2.0665e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 173, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0059, 0.0044, 0.0100, 0.1804, 0.0054, 0.0053, 0.7887],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 173, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2379e-01, 4.8533e-03, 6.3134e-01, 2.5857e-06, 6.3530e-03, 1.9468e-02,
        1.4197e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 173, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6257e-03, 8.7510e-06, 6.1297e-03, 1.4468e-05, 4.7236e-03, 5.3764e-03,
        9.7912e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 173, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8483e-02, 1.2631e-07, 1.3749e-02, 1.4877e-07, 6.5940e-08, 9.6777e-01,
        1.0104e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 173, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0582e-02, 5.8882e-03, 9.7387e-01, 5.4805e-06, 5.0049e-03, 4.6267e-03,
        2.2344e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 174, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0043, 0.0045, 0.0098, 0.1818, 0.0044, 0.0050, 0.7903],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 174, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.2856e-01, 4.8408e-03, 5.3605e-01, 3.1625e-06, 4.5278e-03, 1.1322e-02,
        1.4704e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 174, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0490e-03, 7.9496e-06, 4.1395e-03, 7.2012e-06, 4.1030e-03, 4.0631e-03,
        9.8263e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.026

[Epoch: 174, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2378e-02, 2.0641e-07, 1.6273e-02, 1.7371e-07, 5.9942e-08, 9.6135e-01,
        7.0975e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.846

[Epoch: 174, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2930e-02, 5.0743e-03, 9.6990e-01, 8.0781e-06, 6.9757e-03, 5.0695e-03,
        4.2992e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 175, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0056, 0.0051, 0.0109, 0.1870, 0.0051, 0.0050, 0.7813],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.010

[Epoch: 175, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5097e-01, 5.5641e-03, 5.9916e-01, 3.1250e-06, 5.4115e-03, 1.8158e-02,
        2.0732e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 175, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.5048e-03, 9.7993e-06, 5.3238e-03, 8.0646e-06, 6.3623e-03, 5.9215e-03,
        9.7587e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 175, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8925e-02, 1.4898e-07, 1.4457e-02, 1.4239e-07, 3.2177e-08, 9.6662e-01,
        1.0478e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.863

[Epoch: 175, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.1339e-03, 4.7720e-03, 9.7924e-01, 3.5861e-06, 3.8961e-03, 4.9391e-03,
        1.4064e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 176, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0044, 0.0092, 0.1777, 0.0047, 0.0057, 0.7934],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.006

[Epoch: 176, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8730e-01, 4.6619e-03, 5.7380e-01, 3.0300e-06, 5.6299e-03, 1.4326e-02,
        1.4282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 176, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6019e-03, 6.1952e-06, 6.0657e-03, 9.4118e-06, 4.6061e-03, 5.3339e-03,
        9.7938e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 176, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.0186e-02, 1.0344e-07, 1.6352e-02, 1.1146e-07, 5.6087e-08, 9.6346e-01,
        9.1252e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 176, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0049e-02, 4.7611e-03, 9.7497e-01, 5.0721e-06, 5.1324e-03, 5.0682e-03,
        1.4932e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 177, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0065, 0.0053, 0.0102, 0.1816, 0.0054, 0.0057, 0.7853],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 177, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7531e-01, 5.0332e-03, 5.8802e-01, 3.2151e-06, 4.8045e-03, 1.3194e-02,
        1.3629e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 177, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.7516e-03, 9.6747e-06, 4.4979e-03, 8.8861e-06, 4.5288e-03, 4.8071e-03,
        9.8040e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 177, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1373e-02, 1.7183e-07, 1.3316e-02, 1.5312e-07, 4.1780e-08, 9.6531e-01,
        5.2464e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 177, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0282e-02, 5.4600e-03, 9.7487e-01, 5.1436e-06, 4.7362e-03, 4.6334e-03,
        1.4088e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 178, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0042, 0.0045, 0.0102, 0.1647, 0.0049, 0.0045, 0.8070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 178, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0303e-01, 4.4844e-03, 5.5324e-01, 2.3942e-06, 5.2413e-03, 1.6846e-02,
        1.7158e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.017

[Epoch: 178, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.1130e-03, 5.2838e-06, 5.5158e-03, 7.3755e-06, 5.5028e-03, 5.2206e-03,
        9.7864e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 178, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8403e-02, 9.8839e-08, 1.6320e-02, 1.3449e-07, 4.0913e-08, 9.6528e-01,
        1.1187e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 178, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.0117e-03, 4.5484e-03, 9.7795e-01, 3.8111e-06, 4.5085e-03, 4.9591e-03,
        1.5719e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 179, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0046, 0.0123, 0.2040, 0.0044, 0.0055, 0.7648],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 179, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.4950e-01, 6.0326e-03, 6.1078e-01, 2.8658e-06, 5.7142e-03, 1.3925e-02,
        1.4041e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.011

[Epoch: 179, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.7709e-03, 7.2008e-06, 4.5181e-03, 7.3107e-06, 4.2997e-03, 4.3411e-03,
        9.8206e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 179, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1118e-02, 1.0615e-07, 1.3737e-02, 8.2573e-08, 2.9527e-08, 9.6514e-01,
        8.5253e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.858

[Epoch: 179, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1276e-02, 4.6146e-03, 9.7413e-01, 4.8449e-06, 5.3577e-03, 4.6058e-03,
        1.0855e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 180, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0061, 0.0046, 0.0087, 0.1829, 0.0055, 0.0058, 0.7865],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 180, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.1695e-01, 5.0898e-03, 5.4227e-01, 3.1643e-06, 4.8732e-03, 1.5863e-02,
        1.4945e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 180, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.6540e-03, 7.7470e-06, 4.7854e-03, 7.9553e-06, 4.6265e-03, 5.1030e-03,
        9.7982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 180, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1305e-02, 1.2862e-07, 1.6960e-02, 1.1523e-07, 5.1042e-08, 9.6173e-01,
        4.5839e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.834

[Epoch: 180, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.2236e-02, 6.5087e-03, 9.6927e-01, 5.8742e-06, 5.6188e-03, 6.3419e-03,
        1.3931e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 181, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0046, 0.0041, 0.0087, 0.1520, 0.0050, 0.0044, 0.8213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 181, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.2017e-01, 4.6505e-03, 6.4060e-01, 2.9101e-06, 5.0501e-03, 1.3542e-02,
        1.5985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.008

[Epoch: 181, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8985e-03, 1.0475e-05, 5.2338e-03, 1.0308e-05, 5.8041e-03, 5.7522e-03,
        9.7829e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 181, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.4183e-02, 9.9000e-08, 1.3372e-02, 8.3069e-08, 2.3851e-08, 9.7245e-01,
        8.4535e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.872

[Epoch: 181, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.7708e-03, 4.8407e-03, 9.7744e-01, 3.1993e-06, 5.0972e-03, 4.8316e-03,
        1.5522e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 182, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0046, 0.0100, 0.1897, 0.0046, 0.0043, 0.7821],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.004

[Epoch: 182, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.6821e-01, 6.0006e-03, 4.8666e-01, 3.3124e-06, 5.8223e-03, 1.8068e-02,
        1.5240e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.014

[Epoch: 182, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9712e-03, 7.1251e-06, 4.3466e-03, 8.4157e-06, 4.1486e-03, 4.2800e-03,
        9.8224e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.022

[Epoch: 182, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.9380e-02, 1.1565e-07, 1.7229e-02, 9.5350e-08, 4.9475e-08, 9.5339e-01,
        4.9495e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.839

[Epoch: 182, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1435e-02, 3.9565e-03, 9.7590e-01, 4.4015e-06, 4.2577e-03, 4.4327e-03,
        1.1341e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 183, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0064, 0.0053, 0.0114, 0.2138, 0.0062, 0.0063, 0.7506],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.001

[Epoch: 183, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([2.9997e-01, 6.6195e-03, 6.5308e-01, 3.1998e-06, 5.5260e-03, 1.5801e-02,
        1.9002e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 183, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.5652e-03, 7.5475e-06, 4.9766e-03, 1.1645e-05, 4.5386e-03, 4.3592e-03,
        9.8154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 183, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7923e-02, 1.2455e-07, 1.3420e-02, 1.4096e-07, 5.0895e-08, 9.6866e-01,
        1.0542e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.860

[Epoch: 183, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.2969e-03, 5.2337e-03, 9.7424e-01, 5.6582e-06, 5.1396e-03, 6.0749e-03,
        1.0461e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 184, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0058, 0.0060, 0.0097, 0.1461, 0.0056, 0.0048, 0.8218],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.005

[Epoch: 184, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.3287e-01, 4.9023e-03, 5.2584e-01, 3.4035e-06, 4.6649e-03, 1.1978e-02,
        1.9747e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 184, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.5449e-03, 1.0052e-05, 5.0078e-03, 8.7590e-06, 4.8289e-03, 5.3984e-03,
        9.7920e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 184, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.4669e-02, 1.0104e-07, 1.4157e-02, 6.8661e-08, 3.8099e-08, 9.7117e-01,
        3.0708e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.862

[Epoch: 184, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1098e-02, 7.0101e-03, 9.7225e-01, 3.0455e-06, 5.0364e-03, 4.5816e-03,
        2.0638e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 185, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0045, 0.0036, 0.0103, 0.1797, 0.0044, 0.0048, 0.7927],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 185, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.7419e-01, 8.5880e-03, 5.8623e-01, 1.5021e-06, 5.0309e-03, 1.1944e-02,
        1.4016e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.001

[Epoch: 185, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([6.1101e-03, 1.4587e-05, 4.9707e-03, 1.0632e-05, 4.6320e-03, 4.8045e-03,
        9.7946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 185, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9252e-02, 9.9367e-08, 1.5268e-02, 5.5458e-08, 2.2887e-08, 9.6548e-01,
        2.4381e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.844

[Epoch: 185, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.7094e-03, 5.1689e-03, 9.7462e-01, 5.5216e-06, 4.5810e-03, 5.9005e-03,
        1.1200e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 186, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0066, 0.0072, 0.0108, 0.1986, 0.0064, 0.0068, 0.7635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.007

[Epoch: 186, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6573e-01, 6.0696e-03, 5.9255e-01, 3.3111e-06, 4.9709e-03, 1.5399e-02,
        1.5286e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.002

[Epoch: 186, batch: 132/220] total loss per batch: 0.437
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.4218e-03, 4.6100e-06, 4.3463e-03, 8.0985e-06, 3.1128e-03, 4.3609e-03,
        9.8375e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 186, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1046e-02, 1.0138e-07, 1.4367e-02, 5.0379e-08, 1.6529e-08, 9.6459e-01,
        8.7138e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.856

[Epoch: 186, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.5024e-03, 3.6409e-03, 9.7709e-01, 3.1402e-06, 4.7354e-03, 5.0220e-03,
        7.9623e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 187, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0042, 0.0101, 0.1603, 0.0055, 0.0056, 0.8095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 187, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9386e-01, 5.3989e-03, 5.6885e-01, 2.2668e-06, 5.0653e-03, 1.1921e-02,
        1.4898e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.004

[Epoch: 187, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([3.8778e-03, 6.2297e-06, 4.9778e-03, 5.9051e-06, 4.6336e-03, 4.1390e-03,
        9.8236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.021

[Epoch: 187, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8516e-02, 1.2039e-07, 1.5418e-02, 7.9405e-08, 2.0404e-08, 9.6607e-01,
        4.8890e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 187, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0957e-02, 4.9722e-03, 9.7352e-01, 2.8490e-06, 5.1908e-03, 5.3449e-03,
        9.5772e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 188, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0044, 0.0049, 0.0103, 0.1897, 0.0060, 0.0058, 0.7789],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 188, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6824e-01, 5.8879e-03, 5.9311e-01, 2.5891e-06, 5.1486e-03, 1.3533e-02,
        1.4079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 188, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.9706e-03, 8.1699e-06, 5.4142e-03, 9.3669e-06, 5.7492e-03, 5.2601e-03,
        9.7859e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 188, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7947e-02, 8.0475e-08, 1.4809e-02, 5.4876e-08, 1.7137e-08, 9.6724e-01,
        4.8014e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.855

[Epoch: 188, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([8.9725e-03, 5.3627e-03, 9.7668e-01, 2.4614e-06, 4.4529e-03, 4.5193e-03,
        7.2201e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 189, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0042, 0.0095, 0.1826, 0.0050, 0.0053, 0.7885],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.001

[Epoch: 189, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9972e-01, 5.0370e-03, 5.6107e-01, 2.8325e-06, 5.6409e-03, 1.3999e-02,
        1.4536e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.007

[Epoch: 189, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2473e-03, 7.7419e-06, 4.7803e-03, 8.3020e-06, 5.0910e-03, 4.7596e-03,
        9.8011e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.017

[Epoch: 189, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9350e-02, 8.7487e-08, 1.5033e-02, 6.9624e-08, 2.0335e-08, 9.6562e-01,
        3.0022e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 189, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0494e-02, 5.0106e-03, 9.7429e-01, 3.0142e-06, 5.1535e-03, 5.0401e-03,
        1.2725e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 190, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0051, 0.0047, 0.0096, 0.1773, 0.0051, 0.0052, 0.7930],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 190, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5711e-01, 5.4419e-03, 6.0002e-01, 2.4161e-06, 4.8871e-03, 1.5048e-02,
        1.7486e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 190, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.2642e-03, 5.5292e-06, 5.2919e-03, 7.1789e-06, 5.0359e-03, 5.4721e-03,
        9.7892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 190, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.3341e-02, 1.0486e-07, 1.7489e-02, 7.0322e-08, 1.6480e-08, 9.5917e-01,
        5.2243e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.854

[Epoch: 190, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4313e-03, 5.3815e-03, 9.7548e-01, 3.2881e-06, 4.8936e-03, 4.8016e-03,
        4.8903e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 191, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0049, 0.0097, 0.1790, 0.0051, 0.0055, 0.7904],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.000

[Epoch: 191, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9238e-01, 5.2748e-03, 5.6885e-01, 2.6387e-06, 5.2455e-03, 1.4486e-02,
        1.3760e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 191, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.3285e-03, 7.5382e-06, 5.0801e-03, 8.3057e-06, 5.1917e-03, 5.1213e-03,
        9.7926e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.024

[Epoch: 191, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.7102e-02, 8.4503e-08, 1.2148e-02, 5.7253e-08, 1.9351e-08, 9.7075e-01,
        3.7058e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.848

[Epoch: 191, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.3490e-03, 4.4257e-03, 9.7646e-01, 2.3650e-06, 4.8175e-03, 4.9409e-03,
        8.1202e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 192, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0049, 0.0107, 0.1809, 0.0049, 0.0051, 0.7887],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 192, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8601e-01, 5.5141e-03, 5.7185e-01, 2.4347e-06, 5.3650e-03, 1.5822e-02,
        1.5439e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 192, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.6037e-03, 5.4384e-06, 5.1795e-03, 6.8843e-06, 4.8401e-03, 4.8099e-03,
        9.8055e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.019

[Epoch: 192, batch: 176/220] total loss per batch: 0.423
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.1530e-02, 6.7329e-08, 1.5848e-02, 4.9414e-08, 1.3546e-08, 9.6262e-01,
        4.5032e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.861

[Epoch: 192, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1206e-02, 5.0275e-03, 9.7397e-01, 2.9330e-06, 5.0965e-03, 4.6983e-03,
        3.5248e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 193, batch: 44/220] total loss per batch: 0.419
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0053, 0.0049, 0.0085, 0.1837, 0.0049, 0.0049, 0.7878],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.003

[Epoch: 193, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6429e-01, 5.6709e-03, 5.9496e-01, 2.0827e-06, 5.4010e-03, 1.4936e-02,
        1.4744e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 193, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8331e-03, 5.8088e-06, 4.8953e-03, 6.8891e-06, 4.9054e-03, 5.4185e-03,
        9.7993e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 193, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8708e-02, 7.2593e-08, 1.5062e-02, 4.5075e-08, 1.5208e-08, 9.6623e-01,
        2.1697e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.872

[Epoch: 193, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.4109e-03, 5.9322e-03, 9.7377e-01, 3.0324e-06, 5.6636e-03, 5.2079e-03,
        1.6810e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 194, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0054, 0.0051, 0.0097, 0.1789, 0.0050, 0.0055, 0.7905],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.002

[Epoch: 194, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.8346e-01, 4.7323e-03, 5.7717e-01, 2.2649e-06, 5.2801e-03, 1.5065e-02,
        1.4296e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 194, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0809e-03, 4.9351e-06, 5.0774e-03, 6.0572e-06, 4.9364e-03, 5.0949e-03,
        9.7980e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 194, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.2631e-02, 8.8492e-08, 1.8227e-02, 6.9820e-08, 1.5239e-08, 9.5914e-01,
        4.8780e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.859

[Epoch: 194, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1746e-02, 4.4510e-03, 9.7325e-01, 2.8241e-06, 4.8354e-03, 5.7122e-03,
        4.2405e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 195, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0049, 0.0044, 0.0118, 0.1806, 0.0059, 0.0056, 0.7868],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.002

[Epoch: 195, batch: 88/220] total loss per batch: 0.409
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.9291e-01, 4.4434e-03, 5.6682e-01, 2.2024e-06, 5.5105e-03, 1.4122e-02,
        1.6191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.009

[Epoch: 195, batch: 132/220] total loss per batch: 0.434
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.3823e-03, 5.7195e-06, 4.1762e-03, 8.6392e-06, 4.5532e-03, 4.4539e-03,
        9.8142e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 195, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.9196e-02, 1.1960e-07, 1.1186e-02, 8.0809e-08, 2.8435e-08, 9.6962e-01,
        1.1780e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.820

[Epoch: 195, batch: 220/220] total loss per batch: 0.426
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([7.2410e-03, 4.0195e-03, 9.7919e-01, 2.1381e-06, 5.5306e-03, 3.9978e-03,
        1.4059e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 196, batch: 44/220] total loss per batch: 0.420
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0052, 0.0051, 0.0095, 0.1783, 0.0047, 0.0054, 0.7919],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.003

[Epoch: 196, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.5959e-01, 5.4929e-03, 6.0235e-01, 3.9550e-06, 5.3344e-03, 1.4918e-02,
        1.2306e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.005

[Epoch: 196, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([4.8439e-03, 8.2074e-06, 4.8476e-03, 8.2568e-06, 5.6661e-03, 5.4095e-03,
        9.7922e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 196, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.8968e-02, 8.7842e-08, 1.5099e-02, 8.7068e-08, 1.9130e-08, 9.6593e-01,
        3.0199e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.864

[Epoch: 196, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([9.6646e-03, 7.1008e-03, 9.7309e-01, 3.0443e-06, 4.3941e-03, 5.7463e-03,
        5.1558e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 197, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0069, 0.0070, 0.0103, 0.1895, 0.0067, 0.0066, 0.7730],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.004

[Epoch: 197, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0760e-01, 5.4555e-03, 5.4732e-01, 2.7656e-06, 4.8613e-03, 1.7785e-02,
        1.6976e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.006

[Epoch: 197, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.8734e-03, 1.1150e-05, 6.2890e-03, 9.0080e-06, 4.7465e-03, 5.8213e-03,
        9.7725e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.020

[Epoch: 197, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.9793e-02, 3.1126e-07, 1.4827e-02, 1.1051e-07, 2.7681e-08, 9.5538e-01,
        1.0430e-08], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.876

[Epoch: 197, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1334e-02, 3.9220e-03, 9.7356e-01, 3.1029e-06, 6.6536e-03, 4.5075e-03,
        1.7580e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 198, batch: 44/220] total loss per batch: 0.421
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0047, 0.0036, 0.0075, 0.1265, 0.0046, 0.0046, 0.8485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.000

[Epoch: 198, batch: 88/220] total loss per batch: 0.410
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([3.6163e-01, 5.4692e-03, 5.9599e-01, 3.7837e-06, 5.4026e-03, 1.3145e-02,
        1.8357e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.010

[Epoch: 198, batch: 132/220] total loss per batch: 0.435
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0312e-03, 8.1960e-06, 5.0358e-03, 9.2324e-06, 4.6776e-03, 4.9766e-03,
        9.8026e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.023

[Epoch: 198, batch: 176/220] total loss per batch: 0.424
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.4280e-02, 1.0088e-07, 1.6916e-02, 3.8591e-08, 2.5810e-08, 9.6880e-01,
        7.8852e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.847

[Epoch: 198, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0443e-02, 5.3715e-03, 9.7542e-01, 2.4111e-06, 4.3946e-03, 4.3597e-03,
        8.9661e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 199, batch: 44/220] total loss per batch: 0.424
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0048, 0.0073, 0.0224, 0.2715, 0.0077, 0.0071, 0.6791],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 0.004

[Epoch: 199, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0479e-01, 5.4551e-03, 5.5900e-01, 4.0494e-06, 5.5053e-03, 1.2676e-02,
        1.2567e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.003

[Epoch: 199, batch: 132/220] total loss per batch: 0.441
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([7.3889e-03, 4.8322e-06, 3.2145e-03, 7.3717e-06, 5.4573e-03, 3.8398e-03,
        9.8009e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.025

[Epoch: 199, batch: 176/220] total loss per batch: 0.426
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([1.4366e-02, 8.7731e-08, 1.2566e-02, 2.7534e-08, 3.9077e-08, 9.7307e-01,
        1.8996e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 199, batch: 220/220] total loss per batch: 0.428
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.1701e-02, 4.8502e-03, 9.7286e-01, 1.6643e-06, 6.0116e-03, 4.5627e-03,
        9.3567e-06], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 200, batch: 44/220] total loss per batch: 0.427
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0050, 0.0100, 0.1800, 0.0050, 0.0050, 0.7900])
Policy pred: tensor([0.0052, 0.0041, 0.0071, 0.1654, 0.0031, 0.0053, 0.8097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.001 -0.009

[Epoch: 200, batch: 88/220] total loss per batch: 0.411
Policy (actual, predicted): 2 2
Policy data: tensor([0.3800, 0.0050, 0.5800, 0.0000, 0.0050, 0.0150, 0.0150])
Policy pred: tensor([4.0095e-01, 5.1620e-03, 5.6143e-01, 8.6830e-07, 5.3344e-03, 1.2950e-02,
        1.4176e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.012 0.015

[Epoch: 200, batch: 132/220] total loss per batch: 0.447
Policy (actual, predicted): 6 6
Policy data: tensor([0.0050, 0.0000, 0.0050, 0.0000, 0.0050, 0.0050, 0.9800])
Policy pred: tensor([5.0106e-03, 3.0334e-06, 7.0407e-03, 8.7428e-06, 5.7222e-03, 6.2784e-03,
        9.7594e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.023 -0.036

[Epoch: 200, batch: 176/220] total loss per batch: 0.425
Policy (actual, predicted): 5 5
Policy data: tensor([0.0200, 0.0000, 0.0150, 0.0000, 0.0000, 0.9650, 0.0000])
Policy pred: tensor([2.4666e-02, 7.9092e-08, 1.3462e-02, 3.3793e-08, 6.6273e-08, 9.6187e-01,
        6.2187e-09], grad_fn=<SelectBackward>)Value (actual, predicted): -0.855 -0.857

[Epoch: 200, batch: 220/220] total loss per batch: 0.427
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.0050, 0.9750, 0.0000, 0.0050, 0.0050, 0.0000])
Policy pred: tensor([1.0638e-02, 3.8435e-03, 9.7880e-01, 2.2161e-06, 3.5379e-03, 3.1684e-03,
        1.3180e-05], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

