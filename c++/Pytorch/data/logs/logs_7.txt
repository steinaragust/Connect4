Training set samples: 6073
Batch size: 32
[Epoch: 1, batch: 38/190] total loss per batch: 1.748
Policy (actual, predicted): 4 3
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0739, 0.1418, 0.0828, 0.3874, 0.0983, 0.0649, 0.1509],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.032

[Epoch: 1, batch: 76/190] total loss per batch: 1.560
Policy (actual, predicted): 3 2
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0226, 0.0417, 0.3581, 0.0334, 0.0527, 0.2787, 0.2127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.217

[Epoch: 1, batch: 114/190] total loss per batch: 1.538
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0201, 0.1379, 0.0621, 0.4747, 0.1653, 0.0850, 0.0549],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.055

[Epoch: 1, batch: 152/190] total loss per batch: 1.506
Policy (actual, predicted): 1 3
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.1542, 0.0600, 0.0550, 0.3265, 0.1696, 0.1292, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.088

[Epoch: 1, batch: 190/190] total loss per batch: 1.511
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7287, 0.0528, 0.0437, 0.0643, 0.0379, 0.0436, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 2, batch: 38/190] total loss per batch: 1.245
Policy (actual, predicted): 4 1
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0682, 0.2647, 0.0659, 0.2268, 0.2329, 0.0445, 0.0970],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.034

[Epoch: 2, batch: 76/190] total loss per batch: 1.183
Policy (actual, predicted): 3 6
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0165, 0.0276, 0.0466, 0.2791, 0.0406, 0.2937, 0.2960],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.198

[Epoch: 2, batch: 114/190] total loss per batch: 1.153
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0202, 0.1522, 0.0831, 0.3362, 0.1688, 0.1446, 0.0948],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.069

[Epoch: 2, batch: 152/190] total loss per batch: 1.163
Policy (actual, predicted): 1 3
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0928, 0.0598, 0.0827, 0.3310, 0.2577, 0.0977, 0.0784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.243

[Epoch: 2, batch: 190/190] total loss per batch: 1.157
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.8112, 0.0142, 0.0346, 0.0463, 0.0318, 0.0481, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 3, batch: 38/190] total loss per batch: 1.007
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0799, 0.1915, 0.0491, 0.1215, 0.4262, 0.0444, 0.0875],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 3, batch: 76/190] total loss per batch: 0.942
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0039, 0.0067, 0.0016, 0.9385, 0.0082, 0.0252, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.324

[Epoch: 3, batch: 114/190] total loss per batch: 0.915
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0224, 0.1060, 0.1525, 0.2674, 0.1577, 0.1908, 0.1032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.068

[Epoch: 3, batch: 152/190] total loss per batch: 0.947
Policy (actual, predicted): 1 4
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0954, 0.1792, 0.0606, 0.2335, 0.2598, 0.1263, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.261

[Epoch: 3, batch: 190/190] total loss per batch: 0.916
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.8155, 0.0114, 0.0341, 0.0481, 0.0347, 0.0401, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.043

[Epoch: 4, batch: 38/190] total loss per batch: 0.898
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0751, 0.1271, 0.0148, 0.0234, 0.6837, 0.0268, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.005

[Epoch: 4, batch: 76/190] total loss per batch: 0.854
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0067, 0.0031, 0.0035, 0.9345, 0.0101, 0.0204, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.073

[Epoch: 4, batch: 114/190] total loss per batch: 0.846
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0095, 0.0205, 0.0774, 0.7052, 0.0846, 0.0843, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.079

[Epoch: 4, batch: 152/190] total loss per batch: 0.882
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0157, 0.8016, 0.0187, 0.0122, 0.0928, 0.0370, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.068

[Epoch: 4, batch: 190/190] total loss per batch: 0.856
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7832, 0.0120, 0.0376, 0.0612, 0.0452, 0.0372, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.036

[Epoch: 5, batch: 38/190] total loss per batch: 0.861
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1249, 0.1458, 0.0228, 0.0333, 0.5989, 0.0316, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 5, batch: 76/190] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0128, 0.0058, 0.0518, 0.8577, 0.0136, 0.0365, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.142

[Epoch: 5, batch: 114/190] total loss per batch: 0.823
Policy (actual, predicted): 3 5
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0220, 0.0737, 0.1683, 0.2639, 0.1337, 0.2773, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.083

[Epoch: 5, batch: 152/190] total loss per batch: 0.863
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0124, 0.9028, 0.0088, 0.0085, 0.0373, 0.0186, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.015

[Epoch: 5, batch: 190/190] total loss per batch: 0.832
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7396, 0.0164, 0.0472, 0.0825, 0.0435, 0.0431, 0.0277],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.042

[Epoch: 6, batch: 38/190] total loss per batch: 0.843
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1372, 0.2050, 0.0212, 0.0200, 0.5691, 0.0168, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.011

[Epoch: 6, batch: 76/190] total loss per batch: 0.806
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0143, 0.0064, 0.0146, 0.8976, 0.0126, 0.0276, 0.0270],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.027

[Epoch: 6, batch: 114/190] total loss per batch: 0.803
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0214, 0.0328, 0.1092, 0.6674, 0.0595, 0.0760, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.069

[Epoch: 6, batch: 152/190] total loss per batch: 0.839
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0111, 0.8719, 0.0098, 0.0087, 0.0521, 0.0218, 0.0246],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.015

[Epoch: 6, batch: 190/190] total loss per batch: 0.807
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7104, 0.0208, 0.0507, 0.0793, 0.0598, 0.0411, 0.0379],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.012

[Epoch: 7, batch: 38/190] total loss per batch: 0.825
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1703, 0.1017, 0.0348, 0.0254, 0.6002, 0.0216, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 7, batch: 76/190] total loss per batch: 0.790
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0043, 0.0040, 0.0028, 0.9427, 0.0099, 0.0197, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.246

[Epoch: 7, batch: 114/190] total loss per batch: 0.793
Policy (actual, predicted): 3 5
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0174, 0.0593, 0.2607, 0.2004, 0.1168, 0.3024, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.064

[Epoch: 7, batch: 152/190] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0109, 0.9152, 0.0083, 0.0086, 0.0262, 0.0178, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.018

[Epoch: 7, batch: 190/190] total loss per batch: 0.798
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7840, 0.0196, 0.0387, 0.0560, 0.0379, 0.0330, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.021

[Epoch: 8, batch: 38/190] total loss per batch: 0.814
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1476, 0.1536, 0.0180, 0.0169, 0.6144, 0.0200, 0.0295],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 8, batch: 76/190] total loss per batch: 0.780
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0178, 0.0127, 0.0145, 0.8476, 0.0190, 0.0370, 0.0514],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 0.012

[Epoch: 8, batch: 114/190] total loss per batch: 0.783
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0190, 0.0522, 0.0729, 0.6430, 0.0790, 0.1036, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.070

[Epoch: 8, batch: 152/190] total loss per batch: 0.815
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0100, 0.9351, 0.0057, 0.0072, 0.0144, 0.0185, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.011

[Epoch: 8, batch: 190/190] total loss per batch: 0.790
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7389, 0.0247, 0.0579, 0.0575, 0.0461, 0.0370, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 -0.004

[Epoch: 9, batch: 38/190] total loss per batch: 0.804
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1442, 0.1332, 0.0438, 0.0168, 0.6229, 0.0121, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 9, batch: 76/190] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0214, 0.0160, 0.0511, 0.8246, 0.0250, 0.0374, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.095

[Epoch: 9, batch: 114/190] total loss per batch: 0.777
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0204, 0.0546, 0.1609, 0.4421, 0.0859, 0.1991, 0.0370],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 9, batch: 152/190] total loss per batch: 0.808
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0093, 0.9229, 0.0064, 0.0071, 0.0259, 0.0151, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.071

[Epoch: 9, batch: 190/190] total loss per batch: 0.783
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7490, 0.0314, 0.0451, 0.0630, 0.0435, 0.0329, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.008

[Epoch: 10, batch: 38/190] total loss per batch: 0.799
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1953, 0.1571, 0.0281, 0.0137, 0.5745, 0.0155, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 10, batch: 76/190] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0157, 0.0134, 0.0191, 0.8825, 0.0114, 0.0261, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.024

[Epoch: 10, batch: 114/190] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0231, 0.0797, 0.1713, 0.4135, 0.0899, 0.1883, 0.0341],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.064

[Epoch: 10, batch: 152/190] total loss per batch: 0.805
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0089, 0.9401, 0.0066, 0.0071, 0.0171, 0.0125, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 10, batch: 190/190] total loss per batch: 0.781
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7453, 0.0344, 0.0490, 0.0534, 0.0401, 0.0447, 0.0331],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.031

[Epoch: 11, batch: 38/190] total loss per batch: 0.796
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1222, 0.1220, 0.0349, 0.0168, 0.6802, 0.0082, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.013

[Epoch: 11, batch: 76/190] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0032, 0.0022, 0.0066, 0.9633, 0.0032, 0.0082, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.282

[Epoch: 11, batch: 114/190] total loss per batch: 0.773
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0215, 0.0792, 0.1158, 0.4038, 0.0851, 0.2679, 0.0267],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.051

[Epoch: 11, batch: 152/190] total loss per batch: 0.805
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9368, 0.0046, 0.0054, 0.0134, 0.0177, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 11, batch: 190/190] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7170, 0.0236, 0.0505, 0.0748, 0.0578, 0.0390, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.019

[Epoch: 12, batch: 38/190] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1905, 0.1862, 0.0486, 0.0216, 0.5195, 0.0099, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.009

[Epoch: 12, batch: 76/190] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0118, 0.0115, 0.1463, 0.7711, 0.0116, 0.0238, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.013

[Epoch: 12, batch: 114/190] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0445, 0.2296, 0.4534, 0.1020, 0.1185, 0.0242],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.052

[Epoch: 12, batch: 152/190] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0117, 0.9279, 0.0055, 0.0126, 0.0166, 0.0177, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.049

[Epoch: 12, batch: 190/190] total loss per batch: 0.778
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7685, 0.0427, 0.0375, 0.0462, 0.0318, 0.0332, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.020

[Epoch: 13, batch: 38/190] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1338, 0.0885, 0.0309, 0.0217, 0.6957, 0.0116, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 13, batch: 76/190] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0117, 0.0089, 0.0076, 0.8956, 0.0175, 0.0214, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.041

[Epoch: 13, batch: 114/190] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0222, 0.0836, 0.1251, 0.4507, 0.0926, 0.2035, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 13, batch: 152/190] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0077, 0.9482, 0.0036, 0.0082, 0.0106, 0.0133, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.005

[Epoch: 13, batch: 190/190] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6918, 0.0350, 0.0554, 0.0643, 0.0544, 0.0438, 0.0554],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.032

[Epoch: 14, batch: 38/190] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1809, 0.1602, 0.0595, 0.0156, 0.5620, 0.0074, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.024

[Epoch: 14, batch: 76/190] total loss per batch: 0.762
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0102, 0.0134, 0.0118, 0.8874, 0.0131, 0.0252, 0.0388],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.064

[Epoch: 14, batch: 114/190] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0475, 0.1629, 0.4686, 0.1024, 0.1699, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.054

[Epoch: 14, batch: 152/190] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9381, 0.0038, 0.0079, 0.0166, 0.0123, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.068

[Epoch: 14, batch: 190/190] total loss per batch: 0.775
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7213, 0.0376, 0.0451, 0.0699, 0.0509, 0.0368, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.024

[Epoch: 15, batch: 38/190] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1491, 0.1245, 0.0327, 0.0237, 0.6415, 0.0075, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 15, batch: 76/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0155, 0.0152, 0.0324, 0.8588, 0.0139, 0.0303, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 15, batch: 114/190] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0193, 0.0736, 0.1430, 0.4643, 0.0914, 0.1894, 0.0190],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.059

[Epoch: 15, batch: 152/190] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0144, 0.9295, 0.0045, 0.0121, 0.0168, 0.0120, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.016

[Epoch: 15, batch: 190/190] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7234, 0.0378, 0.0484, 0.0547, 0.0460, 0.0373, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.029

[Epoch: 16, batch: 38/190] total loss per batch: 0.790
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1443, 0.1614, 0.0579, 0.0191, 0.5929, 0.0125, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 16, batch: 76/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0126, 0.0290, 0.8669, 0.0193, 0.0148, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.068

[Epoch: 16, batch: 114/190] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0243, 0.0680, 0.1968, 0.3634, 0.1156, 0.2101, 0.0218],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.036

[Epoch: 16, batch: 152/190] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0087, 0.9441, 0.0040, 0.0102, 0.0086, 0.0164, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 16, batch: 190/190] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7556, 0.0261, 0.0551, 0.0549, 0.0400, 0.0314, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.041

[Epoch: 17, batch: 38/190] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1954, 0.1135, 0.0288, 0.0216, 0.6228, 0.0056, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 17, batch: 76/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0203, 0.0143, 0.0373, 0.8298, 0.0143, 0.0296, 0.0544],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 17, batch: 114/190] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0325, 0.0595, 0.1148, 0.5421, 0.0938, 0.1360, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.053

[Epoch: 17, batch: 152/190] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0159, 0.9265, 0.0050, 0.0080, 0.0159, 0.0155, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 17, batch: 190/190] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7222, 0.0436, 0.0468, 0.0546, 0.0391, 0.0447, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.042

[Epoch: 18, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1245, 0.1680, 0.0745, 0.0244, 0.5821, 0.0094, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.039

[Epoch: 18, batch: 76/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0099, 0.0111, 0.0158, 0.9108, 0.0132, 0.0155, 0.0237],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.068

[Epoch: 18, batch: 114/190] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0202, 0.0665, 0.1447, 0.4453, 0.1067, 0.1940, 0.0226],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.054

[Epoch: 18, batch: 152/190] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0063, 0.9455, 0.0052, 0.0114, 0.0093, 0.0149, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 18, batch: 190/190] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7222, 0.0343, 0.0492, 0.0592, 0.0542, 0.0349, 0.0460],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.035

[Epoch: 19, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2243, 0.1128, 0.0323, 0.0192, 0.5951, 0.0065, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 19, batch: 76/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0175, 0.0125, 0.0252, 0.8911, 0.0126, 0.0158, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 19, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0276, 0.0705, 0.1938, 0.4125, 0.0748, 0.1956, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 19, batch: 152/190] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0112, 0.9434, 0.0060, 0.0074, 0.0132, 0.0114, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.088

[Epoch: 19, batch: 190/190] total loss per batch: 0.772
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7150, 0.0423, 0.0505, 0.0536, 0.0453, 0.0433, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 20, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1319, 0.1553, 0.0486, 0.0211, 0.6284, 0.0065, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 20, batch: 76/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0207, 0.0106, 0.0469, 0.8455, 0.0157, 0.0207, 0.0398],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.072

[Epoch: 20, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0277, 0.0480, 0.1392, 0.4672, 0.1279, 0.1670, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 20, batch: 152/190] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0127, 0.9151, 0.0092, 0.0176, 0.0120, 0.0104, 0.0230],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.001

[Epoch: 20, batch: 190/190] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7733, 0.0227, 0.0381, 0.0498, 0.0443, 0.0382, 0.0336],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.034

[Epoch: 21, batch: 38/190] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1689, 0.1239, 0.0460, 0.0257, 0.6216, 0.0062, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 21, batch: 76/190] total loss per batch: 0.762
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0173, 0.0124, 0.0283, 0.8839, 0.0101, 0.0221, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 21, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0242, 0.0785, 0.1852, 0.3875, 0.0947, 0.2120, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.049

[Epoch: 21, batch: 152/190] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0103, 0.9522, 0.0032, 0.0075, 0.0091, 0.0105, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 21, batch: 190/190] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7053, 0.0517, 0.0498, 0.0596, 0.0441, 0.0449, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 22, batch: 38/190] total loss per batch: 0.789
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2099, 0.1622, 0.0453, 0.0383, 0.5263, 0.0054, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 22, batch: 76/190] total loss per batch: 0.762
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0144, 0.0113, 0.0296, 0.8834, 0.0236, 0.0148, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.132

[Epoch: 22, batch: 114/190] total loss per batch: 0.769
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0196, 0.0629, 0.1199, 0.5416, 0.0772, 0.1621, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 22, batch: 152/190] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0113, 0.9202, 0.0055, 0.0127, 0.0208, 0.0108, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.002

[Epoch: 22, batch: 190/190] total loss per batch: 0.773
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7117, 0.0299, 0.0465, 0.0624, 0.0498, 0.0499, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 23, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0969, 0.1242, 0.0290, 0.0098, 0.7218, 0.0051, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 23, batch: 76/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0258, 0.0117, 0.0626, 0.8363, 0.0194, 0.0170, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.099

[Epoch: 23, batch: 114/190] total loss per batch: 0.770
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0249, 0.0658, 0.1621, 0.3820, 0.0908, 0.2450, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 23, batch: 152/190] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9494, 0.0034, 0.0115, 0.0094, 0.0106, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 23, batch: 190/190] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7461, 0.0395, 0.0458, 0.0494, 0.0446, 0.0390, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 24, batch: 38/190] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2093, 0.1560, 0.0614, 0.0177, 0.5405, 0.0062, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 24, batch: 76/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0148, 0.0134, 0.0180, 0.8830, 0.0111, 0.0217, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.101

[Epoch: 24, batch: 114/190] total loss per batch: 0.767
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0243, 0.0803, 0.2006, 0.4282, 0.1012, 0.1484, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.054

[Epoch: 24, batch: 152/190] total loss per batch: 0.799
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0076, 0.9527, 0.0032, 0.0089, 0.0090, 0.0087, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.010

[Epoch: 24, batch: 190/190] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7279, 0.0324, 0.0438, 0.0559, 0.0403, 0.0416, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.059

[Epoch: 25, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1459, 0.1349, 0.0394, 0.0274, 0.6390, 0.0058, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 25, batch: 76/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0189, 0.0126, 0.0162, 0.8883, 0.0133, 0.0190, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.105

[Epoch: 25, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0236, 0.0735, 0.1165, 0.4672, 0.0701, 0.2287, 0.0204],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.067

[Epoch: 25, batch: 152/190] total loss per batch: 0.800
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0179, 0.8980, 0.0055, 0.0180, 0.0222, 0.0239, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 25, batch: 190/190] total loss per batch: 0.771
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7254, 0.0401, 0.0487, 0.0588, 0.0470, 0.0394, 0.0405],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.036

[Epoch: 26, batch: 38/190] total loss per batch: 0.788
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1727, 0.1636, 0.0782, 0.0251, 0.5446, 0.0047, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 26, batch: 76/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0204, 0.0110, 0.0431, 0.8727, 0.0061, 0.0186, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.076

[Epoch: 26, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0209, 0.0609, 0.1990, 0.3828, 0.1298, 0.1866, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.029

[Epoch: 26, batch: 152/190] total loss per batch: 0.798
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0078, 0.9577, 0.0052, 0.0051, 0.0109, 0.0056, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.009

[Epoch: 26, batch: 190/190] total loss per batch: 0.770
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7503, 0.0481, 0.0468, 0.0414, 0.0345, 0.0380, 0.0409],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 27, batch: 38/190] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1335, 0.1214, 0.0294, 0.0173, 0.6839, 0.0046, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 27, batch: 76/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0153, 0.0116, 0.0381, 0.8415, 0.0217, 0.0252, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 0.011

[Epoch: 27, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0240, 0.0702, 0.1403, 0.5349, 0.0742, 0.1435, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 27, batch: 152/190] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0131, 0.9384, 0.0045, 0.0161, 0.0138, 0.0072, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.010

[Epoch: 27, batch: 190/190] total loss per batch: 0.769
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7300, 0.0180, 0.0484, 0.0779, 0.0467, 0.0369, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 28, batch: 38/190] total loss per batch: 0.786
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1673, 0.1748, 0.0441, 0.0132, 0.5773, 0.0074, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 28, batch: 76/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0179, 0.0127, 0.0234, 0.8913, 0.0186, 0.0153, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.101

[Epoch: 28, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0305, 0.0690, 0.1946, 0.3361, 0.1377, 0.2140, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 28, batch: 152/190] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0132, 0.9334, 0.0050, 0.0054, 0.0105, 0.0135, 0.0191],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.017

[Epoch: 28, batch: 190/190] total loss per batch: 0.768
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7099, 0.0401, 0.0558, 0.0485, 0.0494, 0.0513, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 29, batch: 38/190] total loss per batch: 0.785
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1892, 0.1267, 0.0545, 0.0131, 0.6036, 0.0055, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 29, batch: 76/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0122, 0.0109, 0.0256, 0.8996, 0.0111, 0.0160, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.150

[Epoch: 29, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0253, 0.0839, 0.1522, 0.4396, 0.1010, 0.1772, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 29, batch: 152/190] total loss per batch: 0.795
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0096, 0.9514, 0.0039, 0.0122, 0.0087, 0.0069, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 29, batch: 190/190] total loss per batch: 0.768
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7299, 0.0336, 0.0459, 0.0656, 0.0431, 0.0475, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.074

[Epoch: 30, batch: 38/190] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1073, 0.1924, 0.0421, 0.0234, 0.6167, 0.0058, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 30, batch: 76/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0145, 0.0102, 0.0345, 0.8705, 0.0106, 0.0234, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.126

[Epoch: 30, batch: 114/190] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0265, 0.0766, 0.1618, 0.4075, 0.1046, 0.2084, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 30, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.9338, 0.0034, 0.0120, 0.0163, 0.0094, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 30, batch: 190/190] total loss per batch: 0.768
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7193, 0.0371, 0.0493, 0.0456, 0.0426, 0.0431, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.033

[Epoch: 31, batch: 38/190] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1817, 0.1122, 0.0455, 0.0140, 0.6310, 0.0051, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 31, batch: 76/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0195, 0.0116, 0.0188, 0.8868, 0.0194, 0.0158, 0.0282],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.098

[Epoch: 31, batch: 114/190] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0361, 0.0476, 0.1621, 0.4935, 0.0992, 0.1440, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 31, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0126, 0.9343, 0.0050, 0.0094, 0.0140, 0.0097, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.006

[Epoch: 31, batch: 190/190] total loss per batch: 0.768
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7393, 0.0504, 0.0421, 0.0509, 0.0430, 0.0380, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 32, batch: 38/190] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1902, 0.1497, 0.0704, 0.0221, 0.5570, 0.0042, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 32, batch: 76/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0187, 0.0276, 0.0359, 0.8220, 0.0191, 0.0279, 0.0487],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.033

[Epoch: 32, batch: 114/190] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0218, 0.0692, 0.1331, 0.3987, 0.0738, 0.2766, 0.0269],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.062

[Epoch: 32, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0074, 0.9606, 0.0030, 0.0077, 0.0064, 0.0076, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.058

[Epoch: 32, batch: 190/190] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7658, 0.0334, 0.0425, 0.0558, 0.0342, 0.0296, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 33, batch: 38/190] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1138, 0.1531, 0.0380, 0.0167, 0.6637, 0.0040, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 33, batch: 76/190] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0130, 0.0100, 0.0455, 0.8747, 0.0155, 0.0111, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 33, batch: 114/190] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0214, 0.0548, 0.1406, 0.4634, 0.0835, 0.2134, 0.0228],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 33, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0176, 0.9360, 0.0037, 0.0098, 0.0177, 0.0079, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.003

[Epoch: 33, batch: 190/190] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7029, 0.0389, 0.0637, 0.0566, 0.0497, 0.0415, 0.0467],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.040

[Epoch: 34, batch: 38/190] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2089, 0.1413, 0.0480, 0.0259, 0.5560, 0.0069, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.042

[Epoch: 34, batch: 76/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0219, 0.0109, 0.0225, 0.8792, 0.0122, 0.0239, 0.0295],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.019

[Epoch: 34, batch: 114/190] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0266, 0.0847, 0.2402, 0.3393, 0.1049, 0.1858, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 34, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9293, 0.0046, 0.0101, 0.0190, 0.0100, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.163

[Epoch: 34, batch: 190/190] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7172, 0.0331, 0.0541, 0.0577, 0.0526, 0.0328, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.025

[Epoch: 35, batch: 38/190] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1434, 0.1410, 0.0690, 0.0119, 0.6229, 0.0054, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 35, batch: 76/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0111, 0.0195, 0.0363, 0.8844, 0.0160, 0.0126, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.153

[Epoch: 35, batch: 114/190] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0289, 0.0465, 0.0784, 0.5833, 0.0870, 0.1593, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.050

[Epoch: 35, batch: 152/190] total loss per batch: 0.796
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0160, 0.9222, 0.0037, 0.0257, 0.0099, 0.0120, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.033

[Epoch: 35, batch: 190/190] total loss per batch: 0.769
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7482, 0.0356, 0.0415, 0.0502, 0.0547, 0.0334, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.043

[Epoch: 36, batch: 38/190] total loss per batch: 0.784
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1376, 0.1183, 0.0314, 0.0210, 0.6819, 0.0045, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.001

[Epoch: 36, batch: 76/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0078, 0.0161, 0.0186, 0.8986, 0.0195, 0.0198, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.095

[Epoch: 36, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0323, 0.0883, 0.1932, 0.3293, 0.0925, 0.2402, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.052

[Epoch: 36, batch: 152/190] total loss per batch: 0.797
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.7958, 0.0092, 0.0072, 0.1469, 0.0134, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.063

[Epoch: 36, batch: 190/190] total loss per batch: 0.770
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7086, 0.0330, 0.0643, 0.0678, 0.0482, 0.0397, 0.0384],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 37, batch: 38/190] total loss per batch: 0.785
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2158, 0.1915, 0.0545, 0.0243, 0.4937, 0.0081, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.050

[Epoch: 37, batch: 76/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0216, 0.0131, 0.0168, 0.8724, 0.0242, 0.0142, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.103

[Epoch: 37, batch: 114/190] total loss per batch: 0.765
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0537, 0.1165, 0.5048, 0.1058, 0.1791, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.064

[Epoch: 37, batch: 152/190] total loss per batch: 0.801
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0069, 0.9653, 0.0052, 0.0047, 0.0111, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.036

[Epoch: 37, batch: 190/190] total loss per batch: 0.811
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.8437, 0.0462, 0.0210, 0.0226, 0.0168, 0.0308, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.225

[Epoch: 38, batch: 38/190] total loss per batch: 0.878
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1760, 0.0644, 0.0682, 0.0201, 0.6479, 0.0048, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 38, batch: 76/190] total loss per batch: 0.889
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0290, 0.0100, 0.1360, 0.7299, 0.0266, 0.0326, 0.0359],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.314

[Epoch: 38, batch: 114/190] total loss per batch: 0.895
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0272, 0.0463, 0.1770, 0.4784, 0.1009, 0.1128, 0.0572],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.036

[Epoch: 38, batch: 152/190] total loss per batch: 0.919
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0286, 0.7134, 0.0192, 0.0226, 0.0562, 0.0587, 0.1013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 0.035

[Epoch: 38, batch: 190/190] total loss per batch: 0.857
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6632, 0.0160, 0.0794, 0.0843, 0.0642, 0.0480, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.032

[Epoch: 39, batch: 38/190] total loss per batch: 0.879
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1047, 0.1831, 0.0436, 0.0276, 0.6056, 0.0024, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.001

[Epoch: 39, batch: 76/190] total loss per batch: 0.850
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0244, 0.0235, 0.0112, 0.8604, 0.0074, 0.0396, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 0.008

[Epoch: 39, batch: 114/190] total loss per batch: 0.831
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0290, 0.0680, 0.1702, 0.4241, 0.1389, 0.1369, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 39, batch: 152/190] total loss per batch: 0.844
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0179, 0.9283, 0.0077, 0.0076, 0.0270, 0.0040, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.015

[Epoch: 39, batch: 190/190] total loss per batch: 0.811
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6950, 0.0523, 0.0706, 0.0642, 0.0535, 0.0296, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.040

[Epoch: 40, batch: 38/190] total loss per batch: 0.823
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1659, 0.1722, 0.0298, 0.0248, 0.5943, 0.0055, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.002

[Epoch: 40, batch: 76/190] total loss per batch: 0.781
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0103, 0.0146, 0.0099, 0.8608, 0.0165, 0.0439, 0.0440],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.014

[Epoch: 40, batch: 114/190] total loss per batch: 0.782
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0156, 0.0643, 0.2086, 0.3836, 0.1092, 0.1853, 0.0333],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.025

[Epoch: 40, batch: 152/190] total loss per batch: 0.810
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0158, 0.9095, 0.0093, 0.0119, 0.0253, 0.0086, 0.0197],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.018

[Epoch: 40, batch: 190/190] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7341, 0.0541, 0.0410, 0.0479, 0.0395, 0.0359, 0.0475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.064

[Epoch: 41, batch: 38/190] total loss per batch: 0.793
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1368, 0.1492, 0.0478, 0.0371, 0.6126, 0.0035, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.004

[Epoch: 41, batch: 76/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0140, 0.0123, 0.0131, 0.8934, 0.0056, 0.0343, 0.0273],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.034

[Epoch: 41, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0549, 0.1557, 0.4265, 0.0931, 0.2122, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.021

[Epoch: 41, batch: 152/190] total loss per batch: 0.794
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0152, 0.9301, 0.0050, 0.0094, 0.0185, 0.0093, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 41, batch: 190/190] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7391, 0.0454, 0.0458, 0.0521, 0.0411, 0.0327, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 42, batch: 38/190] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1847, 0.1418, 0.0521, 0.0165, 0.5952, 0.0035, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.002

[Epoch: 42, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0155, 0.0116, 0.0197, 0.8893, 0.0104, 0.0232, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.025

[Epoch: 42, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0243, 0.0635, 0.1607, 0.4335, 0.0970, 0.1957, 0.0254],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.026

[Epoch: 42, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0143, 0.9347, 0.0039, 0.0105, 0.0160, 0.0093, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 42, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7094, 0.0387, 0.0540, 0.0603, 0.0476, 0.0393, 0.0506],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.058

[Epoch: 43, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1491, 0.1463, 0.0478, 0.0250, 0.6212, 0.0030, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.005

[Epoch: 43, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0156, 0.0147, 0.0230, 0.8887, 0.0107, 0.0202, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.033

[Epoch: 43, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0242, 0.0639, 0.1574, 0.4428, 0.0905, 0.1973, 0.0238],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.025

[Epoch: 43, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0121, 0.9426, 0.0037, 0.0092, 0.0146, 0.0081, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 43, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7285, 0.0379, 0.0499, 0.0561, 0.0465, 0.0366, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 44, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1684, 0.1300, 0.0484, 0.0181, 0.6260, 0.0033, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.003

[Epoch: 44, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0146, 0.0235, 0.8844, 0.0123, 0.0199, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.038

[Epoch: 44, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0251, 0.0643, 0.1596, 0.4360, 0.0957, 0.1980, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.025

[Epoch: 44, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0113, 0.9452, 0.0035, 0.0097, 0.0122, 0.0083, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 44, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7109, 0.0371, 0.0538, 0.0605, 0.0471, 0.0411, 0.0495],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.056

[Epoch: 45, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1586, 0.1507, 0.0499, 0.0230, 0.6090, 0.0033, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.005

[Epoch: 45, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0179, 0.0138, 0.0264, 0.8796, 0.0133, 0.0188, 0.0302],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.047

[Epoch: 45, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0247, 0.0669, 0.1588, 0.4380, 0.0968, 0.1940, 0.0209],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.025

[Epoch: 45, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0121, 0.9440, 0.0037, 0.0099, 0.0121, 0.0088, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 45, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7202, 0.0383, 0.0510, 0.0593, 0.0473, 0.0402, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 46, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1753, 0.1358, 0.0453, 0.0173, 0.6174, 0.0034, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.004

[Epoch: 46, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0187, 0.0146, 0.0264, 0.8776, 0.0150, 0.0189, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.056

[Epoch: 46, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0679, 0.1658, 0.4296, 0.0964, 0.1956, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.030

[Epoch: 46, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9455, 0.0037, 0.0101, 0.0110, 0.0092, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 46, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7253, 0.0368, 0.0513, 0.0575, 0.0432, 0.0401, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 47, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1409, 0.1541, 0.0502, 0.0206, 0.6259, 0.0033, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.007

[Epoch: 47, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0170, 0.0142, 0.0298, 0.8771, 0.0154, 0.0185, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.066

[Epoch: 47, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0253, 0.0664, 0.1569, 0.4507, 0.0947, 0.1863, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.031

[Epoch: 47, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0110, 0.9480, 0.0034, 0.0088, 0.0121, 0.0083, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 47, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7142, 0.0378, 0.0529, 0.0614, 0.0509, 0.0409, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 48, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2156, 0.1286, 0.0500, 0.0217, 0.5757, 0.0039, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.007

[Epoch: 48, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0153, 0.0235, 0.8776, 0.0139, 0.0169, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.071

[Epoch: 48, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0680, 0.1688, 0.4065, 0.0978, 0.2134, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.030

[Epoch: 48, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0107, 0.9405, 0.0040, 0.0120, 0.0114, 0.0099, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 48, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7440, 0.0356, 0.0452, 0.0528, 0.0411, 0.0395, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 49, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0994, 0.1423, 0.0531, 0.0191, 0.6779, 0.0032, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.007

[Epoch: 49, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0174, 0.0140, 0.0309, 0.8756, 0.0149, 0.0201, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.073

[Epoch: 49, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0248, 0.0566, 0.1533, 0.4990, 0.0885, 0.1600, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.030

[Epoch: 49, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0108, 0.9459, 0.0037, 0.0092, 0.0127, 0.0089, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 49, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7099, 0.0423, 0.0529, 0.0573, 0.0529, 0.0425, 0.0420],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 50, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2225, 0.1420, 0.0490, 0.0303, 0.5453, 0.0040, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 50, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0238, 0.0190, 0.0336, 0.8593, 0.0165, 0.0176, 0.0302],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.077

[Epoch: 50, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0258, 0.0792, 0.1782, 0.3700, 0.1153, 0.2130, 0.0185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.028

[Epoch: 50, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0109, 0.9400, 0.0040, 0.0132, 0.0116, 0.0096, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 50, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7340, 0.0341, 0.0496, 0.0560, 0.0447, 0.0374, 0.0443],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 51, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1245, 0.1248, 0.0559, 0.0141, 0.6735, 0.0029, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.010

[Epoch: 51, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0189, 0.0128, 0.0264, 0.8725, 0.0118, 0.0192, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 51, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0254, 0.0680, 0.1450, 0.4688, 0.0874, 0.1888, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.034

[Epoch: 51, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0103, 0.9442, 0.0040, 0.0099, 0.0126, 0.0085, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 51, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7125, 0.0354, 0.0508, 0.0624, 0.0518, 0.0432, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 52, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1695, 0.1659, 0.0487, 0.0205, 0.5841, 0.0049, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 52, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0139, 0.0168, 0.0302, 0.8770, 0.0148, 0.0165, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 52, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0237, 0.0635, 0.1578, 0.4520, 0.1083, 0.1791, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 52, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9440, 0.0042, 0.0106, 0.0110, 0.0101, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 52, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7561, 0.0353, 0.0480, 0.0487, 0.0398, 0.0354, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 53, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1708, 0.1328, 0.0521, 0.0208, 0.6152, 0.0035, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 53, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0196, 0.0152, 0.0327, 0.8631, 0.0171, 0.0201, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.083

[Epoch: 53, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0246, 0.0682, 0.1680, 0.4212, 0.1020, 0.2001, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.031

[Epoch: 53, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0098, 0.9492, 0.0031, 0.0084, 0.0109, 0.0090, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 53, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7041, 0.0401, 0.0526, 0.0617, 0.0512, 0.0434, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.055

[Epoch: 54, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1501, 0.1484, 0.0561, 0.0200, 0.6151, 0.0045, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 54, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0158, 0.0146, 0.0362, 0.8762, 0.0142, 0.0159, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 54, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0257, 0.0679, 0.1618, 0.4375, 0.0986, 0.1905, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.033

[Epoch: 54, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9408, 0.0042, 0.0118, 0.0120, 0.0107, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 54, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7411, 0.0357, 0.0477, 0.0542, 0.0419, 0.0378, 0.0417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 55, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1876, 0.1290, 0.0475, 0.0202, 0.6078, 0.0035, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 55, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0171, 0.0142, 0.0293, 0.8799, 0.0137, 0.0167, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 55, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0253, 0.0530, 0.1658, 0.4427, 0.1034, 0.1943, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.033

[Epoch: 55, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9461, 0.0033, 0.0105, 0.0110, 0.0102, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 55, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7244, 0.0326, 0.0534, 0.0574, 0.0449, 0.0414, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 56, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1276, 0.1600, 0.0504, 0.0190, 0.6343, 0.0033, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 56, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0158, 0.0178, 0.0261, 0.8661, 0.0179, 0.0169, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.094

[Epoch: 56, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0230, 0.0808, 0.1549, 0.4511, 0.0813, 0.1889, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 56, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9496, 0.0033, 0.0097, 0.0096, 0.0073, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 56, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6988, 0.0489, 0.0508, 0.0601, 0.0537, 0.0406, 0.0472],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 57, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2159, 0.1221, 0.0507, 0.0230, 0.5808, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.014

[Epoch: 57, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0178, 0.0160, 0.0475, 0.8485, 0.0171, 0.0192, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.075

[Epoch: 57, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0278, 0.0539, 0.1744, 0.4000, 0.1096, 0.2189, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.027

[Epoch: 57, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0094, 0.9465, 0.0035, 0.0098, 0.0092, 0.0116, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 57, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7368, 0.0295, 0.0488, 0.0529, 0.0515, 0.0403, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 58, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1150, 0.1642, 0.0595, 0.0188, 0.6311, 0.0039, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 58, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0209, 0.0141, 0.0294, 0.8711, 0.0121, 0.0176, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.083

[Epoch: 58, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0246, 0.0909, 0.1633, 0.4508, 0.0980, 0.1543, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 58, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.9256, 0.0044, 0.0175, 0.0134, 0.0136, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 58, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7565, 0.0362, 0.0464, 0.0557, 0.0368, 0.0337, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 59, batch: 38/190] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1735, 0.1439, 0.0308, 0.0229, 0.6203, 0.0026, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 59, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0093, 0.0114, 0.0149, 0.9255, 0.0104, 0.0133, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.094

[Epoch: 59, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0238, 0.0613, 0.1383, 0.4583, 0.0983, 0.2017, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.029

[Epoch: 59, batch: 152/190] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0094, 0.9425, 0.0047, 0.0098, 0.0157, 0.0074, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 59, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6881, 0.0477, 0.0691, 0.0600, 0.0503, 0.0447, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 60, batch: 38/190] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1608, 0.1249, 0.0806, 0.0151, 0.6109, 0.0045, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 60, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0199, 0.0143, 0.0424, 0.8481, 0.0157, 0.0203, 0.0393],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.069

[Epoch: 60, batch: 114/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0365, 0.0763, 0.1895, 0.4135, 0.0931, 0.1724, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 60, batch: 152/190] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0136, 0.9400, 0.0048, 0.0099, 0.0119, 0.0110, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 60, batch: 190/190] total loss per batch: 0.765
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7481, 0.0239, 0.0413, 0.0534, 0.0544, 0.0368, 0.0422],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.041

[Epoch: 61, batch: 38/190] total loss per batch: 0.782
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1621, 0.1710, 0.0429, 0.0266, 0.5863, 0.0039, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 61, batch: 76/190] total loss per batch: 0.753
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0184, 0.0161, 0.0403, 0.8468, 0.0155, 0.0177, 0.0451],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 61, batch: 114/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0248, 0.0694, 0.1760, 0.3987, 0.1071, 0.2067, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.030

[Epoch: 61, batch: 152/190] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9396, 0.0033, 0.0101, 0.0121, 0.0113, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.047

[Epoch: 61, batch: 190/190] total loss per batch: 0.765
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6839, 0.0601, 0.0503, 0.0665, 0.0385, 0.0336, 0.0671],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.028

[Epoch: 62, batch: 38/190] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1828, 0.1378, 0.0382, 0.0184, 0.6127, 0.0037, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 62, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0164, 0.0150, 0.0365, 0.8673, 0.0262, 0.0166, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 62, batch: 114/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0256, 0.0553, 0.1555, 0.4792, 0.0877, 0.1798, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 62, batch: 152/190] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0173, 0.9367, 0.0043, 0.0092, 0.0115, 0.0103, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 62, batch: 190/190] total loss per batch: 0.765
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7462, 0.0200, 0.0567, 0.0464, 0.0526, 0.0451, 0.0330],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.034

[Epoch: 63, batch: 38/190] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1624, 0.1386, 0.0553, 0.0228, 0.6113, 0.0045, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 63, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0227, 0.0166, 0.0388, 0.8569, 0.0115, 0.0171, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.077

[Epoch: 63, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0292, 0.0762, 0.1659, 0.4395, 0.0783, 0.1898, 0.0211],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.035

[Epoch: 63, batch: 152/190] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0097, 0.9460, 0.0039, 0.0071, 0.0142, 0.0115, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 63, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7298, 0.0431, 0.0481, 0.0592, 0.0403, 0.0381, 0.0414],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.063

[Epoch: 64, batch: 38/190] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1441, 0.1216, 0.0536, 0.0169, 0.6524, 0.0043, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 64, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0139, 0.0128, 0.0256, 0.8866, 0.0151, 0.0151, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.075

[Epoch: 64, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0253, 0.0599, 0.1528, 0.4237, 0.1189, 0.2045, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 64, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0156, 0.9342, 0.0053, 0.0124, 0.0113, 0.0120, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.045

[Epoch: 64, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7161, 0.0288, 0.0540, 0.0670, 0.0480, 0.0417, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.056

[Epoch: 65, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1831, 0.1711, 0.0444, 0.0298, 0.5627, 0.0041, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 65, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0118, 0.0115, 0.0204, 0.9078, 0.0114, 0.0133, 0.0238],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 65, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0252, 0.0686, 0.1607, 0.4511, 0.0992, 0.1769, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 65, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0121, 0.9316, 0.0047, 0.0116, 0.0124, 0.0134, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 65, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7305, 0.0406, 0.0473, 0.0515, 0.0481, 0.0385, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 66, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1346, 0.1196, 0.0540, 0.0150, 0.6690, 0.0035, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 66, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0237, 0.0157, 0.0390, 0.8522, 0.0142, 0.0254, 0.0299],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.080

[Epoch: 66, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0248, 0.0703, 0.1769, 0.3905, 0.1157, 0.2023, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 66, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0078, 0.9577, 0.0031, 0.0062, 0.0065, 0.0097, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 66, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7067, 0.0403, 0.0550, 0.0640, 0.0453, 0.0432, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.074

[Epoch: 67, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2354, 0.1832, 0.0559, 0.0177, 0.4997, 0.0027, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 67, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0218, 0.0151, 0.0423, 0.8509, 0.0186, 0.0214, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 67, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0237, 0.0707, 0.1400, 0.4917, 0.0906, 0.1681, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 67, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9371, 0.0033, 0.0119, 0.0140, 0.0106, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.053

[Epoch: 67, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7533, 0.0292, 0.0435, 0.0468, 0.0435, 0.0381, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.038

[Epoch: 68, batch: 38/190] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.0870, 0.1278, 0.0318, 0.0172, 0.7189, 0.0054, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 68, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0181, 0.0127, 0.0327, 0.8806, 0.0106, 0.0235, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 68, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0282, 0.0649, 0.1775, 0.4041, 0.1016, 0.2070, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 68, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0129, 0.9419, 0.0027, 0.0069, 0.0101, 0.0158, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.049

[Epoch: 68, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7143, 0.0383, 0.0512, 0.0613, 0.0509, 0.0448, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.065

[Epoch: 69, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2124, 0.1428, 0.0576, 0.0170, 0.5580, 0.0071, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.042

[Epoch: 69, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0118, 0.0142, 0.0213, 0.9000, 0.0113, 0.0117, 0.0298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.097

[Epoch: 69, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0251, 0.0639, 0.1462, 0.4440, 0.0964, 0.2057, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 69, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.9440, 0.0036, 0.0089, 0.0138, 0.0073, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.007

[Epoch: 69, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7232, 0.0426, 0.0462, 0.0567, 0.0448, 0.0386, 0.0478],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 70, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1708, 0.1363, 0.0418, 0.0182, 0.6196, 0.0062, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 70, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0206, 0.0141, 0.0341, 0.8690, 0.0153, 0.0165, 0.0304],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.095

[Epoch: 70, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0264, 0.0760, 0.1827, 0.4631, 0.0838, 0.1546, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 70, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0108, 0.9483, 0.0025, 0.0112, 0.0090, 0.0097, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.055

[Epoch: 70, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7543, 0.0300, 0.0494, 0.0513, 0.0447, 0.0340, 0.0364],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.061

[Epoch: 71, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1488, 0.1422, 0.0585, 0.0209, 0.6179, 0.0047, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 71, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0179, 0.0147, 0.0282, 0.8772, 0.0162, 0.0155, 0.0303],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 71, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0319, 0.0565, 0.1552, 0.3973, 0.1111, 0.2302, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.031

[Epoch: 71, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0111, 0.9433, 0.0033, 0.0095, 0.0116, 0.0114, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 71, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7129, 0.0383, 0.0515, 0.0570, 0.0454, 0.0438, 0.0512],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 72, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1510, 0.1415, 0.0495, 0.0237, 0.6225, 0.0073, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 72, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0168, 0.0192, 0.0487, 0.8410, 0.0193, 0.0161, 0.0390],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.113

[Epoch: 72, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0241, 0.0835, 0.1535, 0.4635, 0.0897, 0.1698, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 72, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0112, 0.9318, 0.0045, 0.0143, 0.0132, 0.0131, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 72, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7293, 0.0349, 0.0484, 0.0629, 0.0519, 0.0352, 0.0374],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.040

[Epoch: 73, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1683, 0.1515, 0.0482, 0.0186, 0.6030, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 73, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0156, 0.0157, 0.0411, 0.8504, 0.0194, 0.0230, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.073

[Epoch: 73, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0277, 0.0621, 0.1706, 0.3969, 0.1002, 0.2209, 0.0216],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 73, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0100, 0.9579, 0.0033, 0.0063, 0.0092, 0.0064, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 73, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7184, 0.0420, 0.0512, 0.0589, 0.0429, 0.0442, 0.0424],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.056

[Epoch: 74, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1819, 0.1311, 0.0503, 0.0173, 0.6067, 0.0060, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 74, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0145, 0.0135, 0.0189, 0.9002, 0.0106, 0.0174, 0.0249],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 74, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0269, 0.0616, 0.1493, 0.4705, 0.0956, 0.1839, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 74, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.9364, 0.0051, 0.0070, 0.0135, 0.0120, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.018

[Epoch: 74, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7456, 0.0234, 0.0449, 0.0514, 0.0519, 0.0355, 0.0473],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.059

[Epoch: 75, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1426, 0.1414, 0.0578, 0.0224, 0.6266, 0.0039, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 75, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0157, 0.0140, 0.0265, 0.8730, 0.0134, 0.0155, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 75, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0331, 0.0762, 0.1655, 0.4052, 0.1045, 0.1916, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 75, batch: 152/190] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0086, 0.9418, 0.0038, 0.0160, 0.0081, 0.0107, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 75, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7297, 0.0535, 0.0463, 0.0516, 0.0451, 0.0287, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.039

[Epoch: 76, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1845, 0.1414, 0.0473, 0.0139, 0.6031, 0.0048, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 76, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0212, 0.0160, 0.0307, 0.8684, 0.0156, 0.0190, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 76, batch: 114/190] total loss per batch: 0.761
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0221, 0.0585, 0.1486, 0.5290, 0.0701, 0.1580, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.036

[Epoch: 76, batch: 152/190] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.9372, 0.0041, 0.0084, 0.0132, 0.0086, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.024

[Epoch: 76, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7012, 0.0303, 0.0619, 0.0603, 0.0534, 0.0478, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.043

[Epoch: 77, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1587, 0.1375, 0.0458, 0.0174, 0.6281, 0.0058, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 77, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0181, 0.0345, 0.8501, 0.0153, 0.0234, 0.0407],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 77, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0261, 0.1028, 0.2065, 0.2946, 0.1240, 0.2277, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.027

[Epoch: 77, batch: 152/190] total loss per batch: 0.791
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0120, 0.9488, 0.0036, 0.0097, 0.0092, 0.0072, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.064

[Epoch: 77, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7031, 0.0431, 0.0511, 0.0594, 0.0499, 0.0421, 0.0515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.033

[Epoch: 78, batch: 38/190] total loss per batch: 0.779
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1748, 0.1377, 0.0601, 0.0267, 0.5891, 0.0048, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 78, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0142, 0.0161, 0.0336, 0.8786, 0.0146, 0.0190, 0.0238],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.079

[Epoch: 78, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0317, 0.0482, 0.0992, 0.4781, 0.1147, 0.2067, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.033

[Epoch: 78, batch: 152/190] total loss per batch: 0.790
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0098, 0.9508, 0.0029, 0.0093, 0.0048, 0.0120, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 78, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7691, 0.0274, 0.0469, 0.0559, 0.0329, 0.0301, 0.0378],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.064

[Epoch: 79, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1385, 0.1483, 0.0479, 0.0144, 0.6416, 0.0050, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 79, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0115, 0.0229, 0.0206, 0.8823, 0.0176, 0.0192, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.096

[Epoch: 79, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0224, 0.0629, 0.1585, 0.5021, 0.0689, 0.1621, 0.0231],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.050

[Epoch: 79, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9422, 0.0034, 0.0076, 0.0174, 0.0088, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.060

[Epoch: 79, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6974, 0.0490, 0.0619, 0.0587, 0.0469, 0.0455, 0.0408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 80, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1685, 0.1346, 0.0462, 0.0212, 0.6202, 0.0051, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.015

[Epoch: 80, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0271, 0.0153, 0.0510, 0.8297, 0.0223, 0.0219, 0.0328],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 80, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0316, 0.0735, 0.1969, 0.3661, 0.1235, 0.1876, 0.0208],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 80, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9413, 0.0034, 0.0127, 0.0098, 0.0103, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.061

[Epoch: 80, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7570, 0.0258, 0.0415, 0.0517, 0.0429, 0.0381, 0.0430],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 81, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1655, 0.1510, 0.0560, 0.0181, 0.5992, 0.0043, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 81, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0155, 0.0155, 0.0342, 0.8724, 0.0172, 0.0176, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 81, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0288, 0.0585, 0.1444, 0.4778, 0.0908, 0.1830, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.033

[Epoch: 81, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0088, 0.9394, 0.0046, 0.0150, 0.0102, 0.0097, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 81, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7058, 0.0394, 0.0542, 0.0594, 0.0543, 0.0417, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.038

[Epoch: 82, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1399, 0.1383, 0.0533, 0.0229, 0.6374, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 82, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0095, 0.0116, 0.0252, 0.9021, 0.0128, 0.0132, 0.0256],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 82, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0233, 0.0705, 0.1566, 0.4473, 0.0891, 0.1934, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 82, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0126, 0.9479, 0.0036, 0.0075, 0.0107, 0.0115, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 82, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7385, 0.0365, 0.0471, 0.0553, 0.0437, 0.0351, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 83, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1876, 0.1524, 0.0355, 0.0164, 0.5984, 0.0052, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 83, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0170, 0.0169, 0.0317, 0.8695, 0.0138, 0.0180, 0.0331],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 83, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0293, 0.0712, 0.1669, 0.3934, 0.1061, 0.2152, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.031

[Epoch: 83, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0126, 0.9359, 0.0048, 0.0124, 0.0135, 0.0095, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 83, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7087, 0.0381, 0.0484, 0.0600, 0.0463, 0.0426, 0.0558],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.027

[Epoch: 84, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1708, 0.1330, 0.0524, 0.0255, 0.6095, 0.0039, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 84, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0126, 0.0168, 0.0344, 0.8700, 0.0175, 0.0186, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.080

[Epoch: 84, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0549, 0.1651, 0.4434, 0.1030, 0.1899, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 84, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0070, 0.9529, 0.0031, 0.0086, 0.0071, 0.0092, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 84, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7671, 0.0337, 0.0433, 0.0464, 0.0444, 0.0306, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 85, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1502, 0.1318, 0.0644, 0.0211, 0.6229, 0.0053, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 85, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0226, 0.0191, 0.0289, 0.8611, 0.0161, 0.0199, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 85, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0231, 0.0757, 0.1494, 0.4779, 0.0839, 0.1713, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.034

[Epoch: 85, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0167, 0.9331, 0.0035, 0.0112, 0.0126, 0.0120, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 85, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7025, 0.0340, 0.0589, 0.0661, 0.0506, 0.0436, 0.0443],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 86, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1702, 0.1423, 0.0366, 0.0163, 0.6270, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.003

[Epoch: 86, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0240, 0.0146, 0.0375, 0.8598, 0.0178, 0.0218, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.080

[Epoch: 86, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0767, 0.1785, 0.3845, 0.1015, 0.2134, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 86, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0094, 0.9445, 0.0054, 0.0140, 0.0097, 0.0093, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 86, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7242, 0.0439, 0.0433, 0.0550, 0.0466, 0.0389, 0.0480],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 87, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1675, 0.1737, 0.0504, 0.0252, 0.5712, 0.0040, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.040

[Epoch: 87, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0125, 0.0120, 0.0284, 0.8935, 0.0100, 0.0133, 0.0304],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.097

[Epoch: 87, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0273, 0.0611, 0.1848, 0.4509, 0.1122, 0.1456, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.034

[Epoch: 87, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0074, 0.9554, 0.0038, 0.0063, 0.0070, 0.0062, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 87, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7166, 0.0362, 0.0493, 0.0593, 0.0495, 0.0422, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.069

[Epoch: 88, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1388, 0.1022, 0.0561, 0.0095, 0.6868, 0.0035, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 88, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0063, 0.0160, 0.0236, 0.9048, 0.0123, 0.0190, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 88, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0203, 0.0552, 0.1299, 0.4849, 0.0813, 0.2155, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 88, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0112, 0.9442, 0.0022, 0.0085, 0.0144, 0.0134, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 88, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7350, 0.0396, 0.0470, 0.0559, 0.0471, 0.0363, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.027

[Epoch: 89, batch: 38/190] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1599, 0.1952, 0.0464, 0.0515, 0.5301, 0.0087, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.049

[Epoch: 89, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0209, 0.0128, 0.0175, 0.8790, 0.0204, 0.0163, 0.0332],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.065

[Epoch: 89, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0875, 0.1659, 0.4118, 0.0966, 0.1934, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.049

[Epoch: 89, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0079, 0.9482, 0.0039, 0.0101, 0.0124, 0.0096, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 89, batch: 190/190] total loss per batch: 0.764
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7354, 0.0318, 0.0453, 0.0605, 0.0390, 0.0444, 0.0437],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.036

[Epoch: 90, batch: 38/190] total loss per batch: 0.781
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1965, 0.0974, 0.0479, 0.0058, 0.6464, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 90, batch: 76/190] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0191, 0.0162, 0.0426, 0.8611, 0.0118, 0.0141, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.112

[Epoch: 90, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0269, 0.0666, 0.1634, 0.4176, 0.1079, 0.1978, 0.0198],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 90, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0137, 0.9371, 0.0043, 0.0128, 0.0088, 0.0086, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.027

[Epoch: 90, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7149, 0.0384, 0.0622, 0.0511, 0.0515, 0.0409, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.066

[Epoch: 91, batch: 38/190] total loss per batch: 0.780
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1633, 0.1565, 0.0658, 0.0124, 0.5934, 0.0036, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 91, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0165, 0.0432, 0.8581, 0.0194, 0.0164, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.058

[Epoch: 91, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0300, 0.0615, 0.1538, 0.4677, 0.0971, 0.1717, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.028

[Epoch: 91, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9342, 0.0048, 0.0127, 0.0125, 0.0131, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.054

[Epoch: 91, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7578, 0.0238, 0.0468, 0.0542, 0.0459, 0.0360, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 92, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1843, 0.1388, 0.0374, 0.0210, 0.6093, 0.0045, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.035

[Epoch: 92, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0145, 0.0145, 0.0347, 0.8750, 0.0174, 0.0161, 0.0278],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.071

[Epoch: 92, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0274, 0.0811, 0.1696, 0.4058, 0.0912, 0.2062, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.051

[Epoch: 92, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0111, 0.9581, 0.0036, 0.0054, 0.0090, 0.0078, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 92, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7017, 0.0530, 0.0602, 0.0586, 0.0437, 0.0401, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 93, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1675, 0.1437, 0.0471, 0.0137, 0.6182, 0.0040, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 93, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0197, 0.0174, 0.0391, 0.8388, 0.0219, 0.0209, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.103

[Epoch: 93, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0284, 0.0452, 0.1385, 0.4603, 0.1047, 0.2053, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 93, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0125, 0.9346, 0.0066, 0.0102, 0.0142, 0.0094, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 93, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7350, 0.0283, 0.0468, 0.0551, 0.0481, 0.0393, 0.0473],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 94, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1671, 0.1483, 0.0457, 0.0229, 0.6044, 0.0054, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 94, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0195, 0.0336, 0.8611, 0.0166, 0.0184, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.083

[Epoch: 94, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0231, 0.0615, 0.1959, 0.4204, 0.0942, 0.1868, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 94, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0083, 0.9536, 0.0035, 0.0098, 0.0055, 0.0117, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 94, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7468, 0.0372, 0.0454, 0.0548, 0.0398, 0.0362, 0.0398],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.055

[Epoch: 95, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1555, 0.1455, 0.0632, 0.0222, 0.6044, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 95, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0213, 0.0147, 0.0298, 0.8703, 0.0166, 0.0163, 0.0310],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 95, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0264, 0.0716, 0.1696, 0.4346, 0.0893, 0.1859, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 95, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0101, 0.9369, 0.0042, 0.0098, 0.0155, 0.0123, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 95, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6955, 0.0369, 0.0573, 0.0641, 0.0511, 0.0451, 0.0500],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 96, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1695, 0.1426, 0.0395, 0.0166, 0.6245, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 96, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0157, 0.0267, 0.8746, 0.0155, 0.0213, 0.0305],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 96, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0267, 0.0653, 0.1616, 0.4452, 0.0912, 0.1936, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 96, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0084, 0.9526, 0.0033, 0.0125, 0.0061, 0.0100, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 96, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7337, 0.0378, 0.0498, 0.0533, 0.0450, 0.0395, 0.0409],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 97, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1587, 0.1420, 0.0511, 0.0192, 0.6223, 0.0037, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 97, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0111, 0.0137, 0.0313, 0.8964, 0.0141, 0.0122, 0.0212],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.096

[Epoch: 97, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0663, 0.1781, 0.4067, 0.1090, 0.1947, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 97, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0105, 0.9390, 0.0041, 0.0087, 0.0101, 0.0135, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 97, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7377, 0.0363, 0.0472, 0.0588, 0.0422, 0.0379, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 98, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1587, 0.1451, 0.0536, 0.0252, 0.6089, 0.0039, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 98, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0130, 0.0146, 0.0292, 0.9001, 0.0070, 0.0119, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.097

[Epoch: 98, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0285, 0.0741, 0.1442, 0.4680, 0.0901, 0.1776, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 98, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0084, 0.9517, 0.0029, 0.0107, 0.0088, 0.0078, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 98, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7299, 0.0354, 0.0477, 0.0530, 0.0492, 0.0391, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 99, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1548, 0.1211, 0.0430, 0.0117, 0.6591, 0.0071, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 99, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0210, 0.0167, 0.0357, 0.8605, 0.0176, 0.0206, 0.0280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.114

[Epoch: 99, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0259, 0.0551, 0.1816, 0.3902, 0.1130, 0.2168, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 99, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0105, 0.9471, 0.0044, 0.0102, 0.0104, 0.0108, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 99, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7014, 0.0350, 0.0554, 0.0625, 0.0546, 0.0449, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.066

[Epoch: 100, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2083, 0.1873, 0.0732, 0.0351, 0.4866, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 100, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0260, 0.0184, 0.0436, 0.8216, 0.0210, 0.0252, 0.0442],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.056

[Epoch: 100, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0242, 0.0792, 0.1523, 0.4795, 0.0829, 0.1657, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 100, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0101, 0.9440, 0.0036, 0.0070, 0.0089, 0.0130, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 100, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7518, 0.0349, 0.0420, 0.0486, 0.0441, 0.0381, 0.0405],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 101, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1372, 0.1182, 0.0433, 0.0190, 0.6672, 0.0040, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 101, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0154, 0.0180, 0.0326, 0.8685, 0.0157, 0.0161, 0.0338],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.074

[Epoch: 101, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0270, 0.0509, 0.1701, 0.3972, 0.1062, 0.2301, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 101, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0073, 0.9420, 0.0028, 0.0128, 0.0127, 0.0119, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 101, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7315, 0.0451, 0.0483, 0.0546, 0.0402, 0.0399, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 102, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1688, 0.1333, 0.0355, 0.0165, 0.6334, 0.0044, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.034

[Epoch: 102, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0123, 0.0122, 0.0261, 0.8930, 0.0143, 0.0156, 0.0265],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 102, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0235, 0.0703, 0.1525, 0.4815, 0.0846, 0.1690, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.050

[Epoch: 102, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0125, 0.9464, 0.0037, 0.0100, 0.0113, 0.0079, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 102, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6960, 0.0316, 0.0622, 0.0668, 0.0524, 0.0425, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.041

[Epoch: 103, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1658, 0.1350, 0.0451, 0.0214, 0.6211, 0.0040, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 103, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0173, 0.0174, 0.0357, 0.8635, 0.0183, 0.0157, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 103, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0295, 0.0808, 0.1751, 0.3919, 0.0998, 0.2047, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.050

[Epoch: 103, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9362, 0.0041, 0.0144, 0.0102, 0.0128, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 103, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7400, 0.0380, 0.0444, 0.0557, 0.0417, 0.0374, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 104, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1490, 0.1517, 0.0516, 0.0162, 0.6222, 0.0036, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 104, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0185, 0.0157, 0.0333, 0.8667, 0.0166, 0.0147, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 104, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0281, 0.0643, 0.1529, 0.4636, 0.0934, 0.1809, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 104, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9462, 0.0035, 0.0089, 0.0109, 0.0096, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 104, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7486, 0.0323, 0.0453, 0.0547, 0.0443, 0.0360, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 105, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1633, 0.1373, 0.0444, 0.0208, 0.6247, 0.0038, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 105, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0178, 0.0154, 0.0333, 0.8704, 0.0162, 0.0181, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 105, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0257, 0.0702, 0.1658, 0.4257, 0.0976, 0.1988, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 105, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0089, 0.9566, 0.0027, 0.0078, 0.0086, 0.0075, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 105, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7097, 0.0415, 0.0532, 0.0577, 0.0487, 0.0426, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 106, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1764, 0.1445, 0.0509, 0.0202, 0.5967, 0.0044, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 106, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0171, 0.0341, 0.8709, 0.0177, 0.0176, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.084

[Epoch: 106, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0638, 0.1635, 0.4432, 0.0941, 0.1931, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 106, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0088, 0.9505, 0.0030, 0.0110, 0.0089, 0.0092, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 106, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7169, 0.0350, 0.0505, 0.0577, 0.0496, 0.0441, 0.0462],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 107, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1589, 0.1404, 0.0518, 0.0183, 0.6216, 0.0036, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 107, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0143, 0.0355, 0.8687, 0.0177, 0.0162, 0.0309],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 107, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0276, 0.0664, 0.1696, 0.4145, 0.1054, 0.1982, 0.0184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 107, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0111, 0.9342, 0.0042, 0.0119, 0.0137, 0.0133, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 107, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7294, 0.0407, 0.0483, 0.0570, 0.0429, 0.0379, 0.0437],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 108, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1452, 0.1396, 0.0450, 0.0205, 0.6429, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 108, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0161, 0.0162, 0.0269, 0.8759, 0.0140, 0.0147, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 108, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0689, 0.1563, 0.4493, 0.0942, 0.1864, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 108, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0127, 0.9469, 0.0041, 0.0102, 0.0082, 0.0088, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 108, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7252, 0.0314, 0.0531, 0.0582, 0.0502, 0.0401, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 109, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1750, 0.1375, 0.0433, 0.0178, 0.6199, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 109, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0124, 0.0128, 0.0299, 0.8885, 0.0113, 0.0149, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 109, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0288, 0.0718, 0.1621, 0.4283, 0.0923, 0.1998, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 109, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9464, 0.0034, 0.0101, 0.0105, 0.0101, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 109, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7386, 0.0415, 0.0477, 0.0506, 0.0428, 0.0348, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 110, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1775, 0.1597, 0.0580, 0.0201, 0.5762, 0.0036, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 110, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0157, 0.0187, 0.0419, 0.8625, 0.0184, 0.0160, 0.0267],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.084

[Epoch: 110, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0246, 0.0659, 0.1610, 0.4554, 0.0875, 0.1883, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 110, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0075, 0.9620, 0.0020, 0.0078, 0.0058, 0.0069, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.048

[Epoch: 110, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7441, 0.0303, 0.0452, 0.0555, 0.0460, 0.0363, 0.0426],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.041

[Epoch: 111, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1405, 0.1344, 0.0598, 0.0249, 0.6304, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 111, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0310, 0.0192, 0.0386, 0.8331, 0.0251, 0.0216, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.099

[Epoch: 111, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0246, 0.0705, 0.1688, 0.4276, 0.0960, 0.1944, 0.0181],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 111, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0138, 0.9328, 0.0033, 0.0097, 0.0135, 0.0157, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 111, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6704, 0.0528, 0.0645, 0.0637, 0.0544, 0.0435, 0.0507],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 112, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1888, 0.1314, 0.0370, 0.0184, 0.6157, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 112, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0143, 0.0137, 0.0251, 0.8900, 0.0121, 0.0168, 0.0281],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.057

[Epoch: 112, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0241, 0.0618, 0.1568, 0.4091, 0.1144, 0.2150, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.027

[Epoch: 112, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0118, 0.9280, 0.0052, 0.0115, 0.0140, 0.0122, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.022

[Epoch: 112, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7509, 0.0273, 0.0453, 0.0537, 0.0417, 0.0418, 0.0393],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.034

[Epoch: 113, batch: 38/190] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1408, 0.1315, 0.0430, 0.0217, 0.6554, 0.0036, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 113, batch: 76/190] total loss per batch: 0.750
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0075, 0.0153, 0.0245, 0.9050, 0.0143, 0.0132, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.119

[Epoch: 113, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0287, 0.0743, 0.1400, 0.4713, 0.0945, 0.1753, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.033

[Epoch: 113, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0065, 0.9626, 0.0035, 0.0091, 0.0046, 0.0068, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 113, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7419, 0.0350, 0.0419, 0.0514, 0.0435, 0.0408, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 114, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1567, 0.1755, 0.0601, 0.0160, 0.5825, 0.0041, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 114, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0170, 0.0189, 0.0292, 0.8721, 0.0188, 0.0141, 0.0299],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.108

[Epoch: 114, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0284, 0.0736, 0.1776, 0.4056, 0.0930, 0.2041, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.035

[Epoch: 114, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0068, 0.9544, 0.0027, 0.0102, 0.0089, 0.0087, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 114, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7175, 0.0395, 0.0573, 0.0572, 0.0484, 0.0385, 0.0416],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.058

[Epoch: 115, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1764, 0.1467, 0.0522, 0.0261, 0.5884, 0.0046, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 115, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0218, 0.0118, 0.0309, 0.8734, 0.0152, 0.0170, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.052

[Epoch: 115, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0267, 0.0646, 0.1515, 0.4801, 0.0841, 0.1780, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 115, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0106, 0.9397, 0.0037, 0.0084, 0.0163, 0.0103, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 115, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7214, 0.0345, 0.0473, 0.0581, 0.0520, 0.0405, 0.0462],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 116, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1660, 0.1308, 0.0503, 0.0200, 0.6250, 0.0036, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 116, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0179, 0.0198, 0.0419, 0.8510, 0.0193, 0.0203, 0.0297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.079

[Epoch: 116, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0254, 0.0660, 0.1838, 0.3897, 0.1127, 0.2053, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 116, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9525, 0.0032, 0.0102, 0.0069, 0.0103, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 116, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7274, 0.0389, 0.0499, 0.0553, 0.0456, 0.0401, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.055

[Epoch: 117, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1542, 0.1477, 0.0490, 0.0174, 0.6235, 0.0034, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 117, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0173, 0.0162, 0.0394, 0.8606, 0.0178, 0.0196, 0.0291],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 117, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0244, 0.0712, 0.1576, 0.4478, 0.0910, 0.1914, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.047

[Epoch: 117, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0111, 0.9356, 0.0042, 0.0112, 0.0143, 0.0119, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 117, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7296, 0.0348, 0.0515, 0.0578, 0.0435, 0.0388, 0.0440],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 118, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1680, 0.1472, 0.0517, 0.0200, 0.6061, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 118, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0158, 0.0179, 0.0318, 0.8716, 0.0157, 0.0172, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 118, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0272, 0.0681, 0.1392, 0.4530, 0.0995, 0.1935, 0.0195],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.036

[Epoch: 118, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0092, 0.9524, 0.0029, 0.0109, 0.0082, 0.0082, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 118, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7199, 0.0343, 0.0542, 0.0623, 0.0480, 0.0375, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 119, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1620, 0.1489, 0.0519, 0.0220, 0.6082, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 119, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0150, 0.0155, 0.0327, 0.8657, 0.0166, 0.0163, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.062

[Epoch: 119, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0259, 0.0723, 0.1779, 0.4216, 0.0941, 0.1915, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 119, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9450, 0.0030, 0.0059, 0.0109, 0.0138, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.072

[Epoch: 119, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7524, 0.0366, 0.0433, 0.0507, 0.0401, 0.0373, 0.0396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 120, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1554, 0.1357, 0.0490, 0.0194, 0.6336, 0.0032, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 120, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0189, 0.0161, 0.0370, 0.8595, 0.0168, 0.0203, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.079

[Epoch: 120, batch: 114/190] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0270, 0.0604, 0.1610, 0.4655, 0.1004, 0.1695, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.052

[Epoch: 120, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0093, 0.9413, 0.0036, 0.0203, 0.0065, 0.0093, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.013

[Epoch: 120, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7277, 0.0365, 0.0492, 0.0598, 0.0437, 0.0390, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 121, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1811, 0.1493, 0.0474, 0.0223, 0.5914, 0.0044, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 121, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0157, 0.0145, 0.0403, 0.8696, 0.0169, 0.0165, 0.0266],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 121, batch: 114/190] total loss per batch: 0.768
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0245, 0.0693, 0.1494, 0.4152, 0.0818, 0.2415, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 121, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0124, 0.9293, 0.0034, 0.0190, 0.0141, 0.0107, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 121, batch: 190/190] total loss per batch: 0.763
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6956, 0.0376, 0.0510, 0.0706, 0.0489, 0.0452, 0.0511],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.039

[Epoch: 122, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1572, 0.1486, 0.0418, 0.0199, 0.6277, 0.0021, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 122, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0146, 0.0132, 0.0327, 0.8801, 0.0180, 0.0106, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.103

[Epoch: 122, batch: 114/190] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0300, 0.0706, 0.1594, 0.4185, 0.1004, 0.2037, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 122, batch: 152/190] total loss per batch: 0.789
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0101, 0.9425, 0.0033, 0.0073, 0.0201, 0.0095, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.052

[Epoch: 122, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7481, 0.0341, 0.0467, 0.0504, 0.0438, 0.0386, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.062

[Epoch: 123, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1473, 0.1467, 0.0562, 0.0181, 0.6255, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 123, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0178, 0.0170, 0.0245, 0.8752, 0.0182, 0.0147, 0.0326],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 123, batch: 114/190] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0306, 0.0675, 0.1732, 0.4308, 0.1002, 0.1804, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 123, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0075, 0.9377, 0.0036, 0.0087, 0.0222, 0.0100, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 123, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7459, 0.0367, 0.0455, 0.0498, 0.0467, 0.0367, 0.0388],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 124, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1817, 0.1423, 0.0540, 0.0183, 0.5974, 0.0027, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 124, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0177, 0.0275, 0.8740, 0.0176, 0.0150, 0.0316],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 124, batch: 114/190] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0268, 0.0647, 0.1694, 0.4370, 0.0985, 0.1864, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 124, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0109, 0.9443, 0.0037, 0.0112, 0.0100, 0.0099, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 124, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7254, 0.0359, 0.0503, 0.0569, 0.0498, 0.0403, 0.0413],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 125, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1690, 0.1440, 0.0487, 0.0186, 0.6127, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 125, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0169, 0.0163, 0.0275, 0.8764, 0.0154, 0.0162, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 125, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0265, 0.0642, 0.1658, 0.4352, 0.0958, 0.1961, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 125, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0092, 0.9516, 0.0034, 0.0088, 0.0094, 0.0089, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 125, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7309, 0.0382, 0.0496, 0.0539, 0.0464, 0.0392, 0.0417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 126, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1635, 0.1447, 0.0508, 0.0191, 0.6151, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 126, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0164, 0.0296, 0.8689, 0.0166, 0.0181, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.076

[Epoch: 126, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0271, 0.0657, 0.1605, 0.4349, 0.0997, 0.1960, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 126, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0110, 0.9457, 0.0041, 0.0099, 0.0097, 0.0100, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 126, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7215, 0.0364, 0.0511, 0.0579, 0.0481, 0.0416, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 127, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1626, 0.1449, 0.0495, 0.0200, 0.6164, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 127, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0154, 0.0292, 0.8748, 0.0149, 0.0174, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 127, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0277, 0.0647, 0.1651, 0.4398, 0.0949, 0.1914, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 127, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0112, 0.9426, 0.0042, 0.0105, 0.0107, 0.0106, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 127, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7314, 0.0376, 0.0480, 0.0558, 0.0451, 0.0395, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 128, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1585, 0.1431, 0.0491, 0.0204, 0.6227, 0.0032, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 128, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0163, 0.0297, 0.8744, 0.0160, 0.0167, 0.0311],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 128, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0280, 0.0677, 0.1652, 0.4298, 0.0972, 0.1957, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 128, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0101, 0.9453, 0.0041, 0.0104, 0.0102, 0.0100, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 128, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7239, 0.0357, 0.0497, 0.0582, 0.0474, 0.0407, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 129, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1651, 0.1398, 0.0509, 0.0197, 0.6183, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 129, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0179, 0.0167, 0.0329, 0.8676, 0.0164, 0.0177, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 129, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0281, 0.0667, 0.1629, 0.4409, 0.0939, 0.1915, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 129, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0098, 0.9471, 0.0040, 0.0097, 0.0098, 0.0099, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 129, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7360, 0.0356, 0.0487, 0.0544, 0.0450, 0.0384, 0.0420],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 130, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1611, 0.1475, 0.0500, 0.0216, 0.6130, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 130, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0171, 0.0165, 0.0365, 0.8689, 0.0163, 0.0164, 0.0282],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.091

[Epoch: 130, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0703, 0.1631, 0.4297, 0.0974, 0.1952, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 130, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0091, 0.9491, 0.0038, 0.0094, 0.0095, 0.0095, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 130, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7235, 0.0370, 0.0513, 0.0566, 0.0487, 0.0401, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 131, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1645, 0.1466, 0.0505, 0.0185, 0.6131, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 131, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0185, 0.0168, 0.0387, 0.8594, 0.0177, 0.0171, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.075

[Epoch: 131, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0278, 0.0598, 0.1664, 0.4413, 0.0973, 0.1909, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 131, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9490, 0.0034, 0.0097, 0.0090, 0.0097, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 131, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7236, 0.0356, 0.0486, 0.0591, 0.0463, 0.0415, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 132, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1701, 0.1468, 0.0570, 0.0218, 0.5973, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 132, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0153, 0.0146, 0.0329, 0.8720, 0.0157, 0.0178, 0.0318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.097

[Epoch: 132, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0263, 0.0701, 0.1513, 0.4394, 0.0992, 0.1965, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 132, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0091, 0.9542, 0.0023, 0.0072, 0.0095, 0.0084, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.034

[Epoch: 132, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7242, 0.0420, 0.0494, 0.0554, 0.0476, 0.0390, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.039

[Epoch: 133, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1581, 0.1256, 0.0472, 0.0164, 0.6464, 0.0033, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 133, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0132, 0.0152, 0.0254, 0.8864, 0.0162, 0.0139, 0.0297],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.065

[Epoch: 133, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0246, 0.0663, 0.1861, 0.4206, 0.0987, 0.1872, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 133, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9516, 0.0029, 0.0082, 0.0077, 0.0094, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 133, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7132, 0.0313, 0.0555, 0.0547, 0.0560, 0.0423, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.056

[Epoch: 134, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1727, 0.1601, 0.0497, 0.0274, 0.5812, 0.0042, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 134, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0150, 0.0139, 0.0252, 0.8811, 0.0151, 0.0179, 0.0318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.101

[Epoch: 134, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0256, 0.0665, 0.1535, 0.4477, 0.1010, 0.1877, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 134, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0138, 0.9168, 0.0038, 0.0156, 0.0184, 0.0137, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.028

[Epoch: 134, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7143, 0.0465, 0.0489, 0.0586, 0.0445, 0.0435, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 135, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1463, 0.1228, 0.0445, 0.0148, 0.6662, 0.0025, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 135, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0136, 0.0166, 0.0312, 0.8839, 0.0137, 0.0136, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.062

[Epoch: 135, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0267, 0.0560, 0.1546, 0.4643, 0.0985, 0.1829, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 135, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0108, 0.9528, 0.0036, 0.0107, 0.0071, 0.0097, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.061

[Epoch: 135, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7463, 0.0292, 0.0503, 0.0570, 0.0428, 0.0339, 0.0406],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.059

[Epoch: 136, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1871, 0.1445, 0.0607, 0.0200, 0.5810, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.045

[Epoch: 136, batch: 76/190] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0187, 0.0215, 0.0431, 0.8450, 0.0151, 0.0195, 0.0372],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 136, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0282, 0.0872, 0.1886, 0.3817, 0.0979, 0.1948, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 136, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0100, 0.9530, 0.0044, 0.0069, 0.0109, 0.0067, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 136, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7240, 0.0313, 0.0499, 0.0550, 0.0494, 0.0402, 0.0502],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 137, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1420, 0.1533, 0.0461, 0.0209, 0.6314, 0.0030, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 137, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0145, 0.0307, 0.8661, 0.0183, 0.0202, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.076

[Epoch: 137, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0298, 0.0634, 0.1408, 0.4665, 0.0904, 0.1929, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 137, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0082, 0.9526, 0.0037, 0.0086, 0.0067, 0.0107, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 137, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7517, 0.0350, 0.0465, 0.0519, 0.0393, 0.0377, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.055

[Epoch: 138, batch: 38/190] total loss per batch: 0.777
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1681, 0.1523, 0.0462, 0.0199, 0.6049, 0.0040, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 138, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0175, 0.0405, 0.8683, 0.0157, 0.0152, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.104

[Epoch: 138, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0226, 0.0622, 0.1786, 0.4288, 0.1015, 0.1909, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 138, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0103, 0.9500, 0.0029, 0.0087, 0.0088, 0.0091, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 138, batch: 190/190] total loss per batch: 0.761
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7191, 0.0466, 0.0521, 0.0531, 0.0469, 0.0375, 0.0447],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 139, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1801, 0.1296, 0.0662, 0.0193, 0.5938, 0.0051, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 139, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0154, 0.0172, 0.0356, 0.8699, 0.0176, 0.0171, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 139, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0218, 0.0592, 0.1537, 0.4595, 0.0891, 0.2008, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 139, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0086, 0.9516, 0.0034, 0.0076, 0.0103, 0.0090, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.052

[Epoch: 139, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6905, 0.0273, 0.0606, 0.0683, 0.0606, 0.0466, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.042

[Epoch: 140, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1514, 0.1376, 0.0429, 0.0212, 0.6384, 0.0039, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 140, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0189, 0.0157, 0.0318, 0.8712, 0.0166, 0.0159, 0.0298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.091

[Epoch: 140, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0285, 0.0773, 0.1739, 0.4036, 0.1070, 0.1907, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 140, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0130, 0.9285, 0.0048, 0.0152, 0.0132, 0.0142, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 140, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7518, 0.0387, 0.0432, 0.0494, 0.0376, 0.0383, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 141, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1571, 0.1357, 0.0451, 0.0206, 0.6352, 0.0032, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 141, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0155, 0.0302, 0.8701, 0.0149, 0.0151, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 141, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0641, 0.1559, 0.4589, 0.0918, 0.1850, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 141, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0082, 0.9524, 0.0036, 0.0078, 0.0090, 0.0094, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 141, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7565, 0.0327, 0.0458, 0.0535, 0.0368, 0.0366, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.059

[Epoch: 142, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1728, 0.1543, 0.0510, 0.0151, 0.5998, 0.0038, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 142, batch: 76/190] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0114, 0.0065, 0.0370, 0.8876, 0.0120, 0.0155, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.122

[Epoch: 142, batch: 114/190] total loss per batch: 0.762
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0300, 0.0630, 0.1693, 0.4392, 0.0864, 0.1965, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.057

[Epoch: 142, batch: 152/190] total loss per batch: 0.788
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0093, 0.9527, 0.0041, 0.0120, 0.0088, 0.0056, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.018

[Epoch: 142, batch: 190/190] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6631, 0.0454, 0.0603, 0.0696, 0.0597, 0.0462, 0.0556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.029

[Epoch: 143, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1582, 0.1430, 0.0470, 0.0300, 0.6130, 0.0043, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 143, batch: 76/190] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0215, 0.0153, 0.0366, 0.8474, 0.0182, 0.0270, 0.0340],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.084

[Epoch: 143, batch: 114/190] total loss per batch: 0.759
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0286, 0.0584, 0.1667, 0.4120, 0.1141, 0.2039, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.009

[Epoch: 143, batch: 152/190] total loss per batch: 0.787
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0117, 0.9514, 0.0027, 0.0092, 0.0096, 0.0075, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.054

[Epoch: 143, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7593, 0.0315, 0.0431, 0.0486, 0.0432, 0.0368, 0.0375],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 144, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1681, 0.1309, 0.0512, 0.0222, 0.6200, 0.0033, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 144, batch: 76/190] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0170, 0.0139, 0.0503, 0.8567, 0.0176, 0.0155, 0.0289],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.058

[Epoch: 144, batch: 114/190] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0648, 0.1597, 0.4475, 0.0864, 0.1984, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.029

[Epoch: 144, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0135, 0.9383, 0.0036, 0.0112, 0.0121, 0.0124, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 144, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7136, 0.0411, 0.0511, 0.0567, 0.0491, 0.0427, 0.0457],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 145, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1592, 0.1462, 0.0483, 0.0194, 0.6196, 0.0033, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 145, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0153, 0.0164, 0.0265, 0.8754, 0.0155, 0.0188, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.065

[Epoch: 145, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0295, 0.0671, 0.1679, 0.4332, 0.1051, 0.1802, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 145, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0122, 0.9415, 0.0039, 0.0118, 0.0095, 0.0105, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 145, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7264, 0.0373, 0.0519, 0.0583, 0.0442, 0.0387, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 146, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1687, 0.1344, 0.0467, 0.0196, 0.6238, 0.0033, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 146, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0180, 0.0161, 0.0340, 0.8667, 0.0179, 0.0165, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.071

[Epoch: 146, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0268, 0.0655, 0.1608, 0.4385, 0.0971, 0.1952, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.034

[Epoch: 146, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0113, 0.9437, 0.0037, 0.0104, 0.0100, 0.0106, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 146, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7376, 0.0345, 0.0483, 0.0563, 0.0434, 0.0369, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 147, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1623, 0.1449, 0.0513, 0.0200, 0.6145, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 147, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0176, 0.0176, 0.0329, 0.8656, 0.0173, 0.0177, 0.0312],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 147, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0271, 0.0669, 0.1661, 0.4265, 0.0990, 0.1971, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.035

[Epoch: 147, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0100, 0.9489, 0.0033, 0.0095, 0.0093, 0.0098, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 147, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7193, 0.0393, 0.0509, 0.0573, 0.0485, 0.0397, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 148, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1608, 0.1468, 0.0507, 0.0198, 0.6144, 0.0035, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 148, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0173, 0.0151, 0.0338, 0.8720, 0.0178, 0.0167, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 148, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0268, 0.0667, 0.1532, 0.4509, 0.0949, 0.1907, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 148, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0092, 0.9504, 0.0031, 0.0091, 0.0095, 0.0096, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 148, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7286, 0.0349, 0.0501, 0.0571, 0.0468, 0.0393, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 149, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1663, 0.1468, 0.0512, 0.0204, 0.6081, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 149, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0171, 0.0169, 0.0331, 0.8688, 0.0178, 0.0169, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 149, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0267, 0.0658, 0.1732, 0.4255, 0.0965, 0.1953, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.037

[Epoch: 149, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0098, 0.9483, 0.0033, 0.0094, 0.0099, 0.0090, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 149, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7202, 0.0389, 0.0513, 0.0565, 0.0488, 0.0409, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 150, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1644, 0.1393, 0.0483, 0.0206, 0.6202, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 150, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0154, 0.0320, 0.8756, 0.0163, 0.0160, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 150, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0265, 0.0652, 0.1555, 0.4492, 0.0953, 0.1920, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 150, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0105, 0.9418, 0.0037, 0.0110, 0.0106, 0.0110, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 150, batch: 190/190] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7188, 0.0368, 0.0523, 0.0592, 0.0468, 0.0420, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 151, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1569, 0.1353, 0.0477, 0.0173, 0.6364, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 151, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0161, 0.0176, 0.0300, 0.8728, 0.0166, 0.0149, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 151, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0270, 0.0683, 0.1733, 0.4168, 0.0984, 0.1987, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 151, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9446, 0.0034, 0.0106, 0.0104, 0.0106, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 151, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7285, 0.0369, 0.0493, 0.0563, 0.0460, 0.0383, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 152, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1607, 0.1430, 0.0474, 0.0221, 0.6200, 0.0033, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 152, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0175, 0.0163, 0.0326, 0.8727, 0.0155, 0.0160, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.091

[Epoch: 152, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0271, 0.0686, 0.1624, 0.4460, 0.0950, 0.1839, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 152, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0104, 0.9430, 0.0040, 0.0116, 0.0108, 0.0100, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 152, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7434, 0.0341, 0.0468, 0.0536, 0.0435, 0.0367, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 153, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1636, 0.1479, 0.0534, 0.0165, 0.6119, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.021

[Epoch: 153, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0183, 0.0166, 0.0351, 0.8634, 0.0174, 0.0174, 0.0318],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.094

[Epoch: 153, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0674, 0.1656, 0.4288, 0.0955, 0.1970, 0.0178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 153, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0091, 0.9513, 0.0032, 0.0090, 0.0092, 0.0089, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 153, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7302, 0.0363, 0.0482, 0.0565, 0.0452, 0.0405, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 154, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1724, 0.1486, 0.0523, 0.0224, 0.5969, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.022

[Epoch: 154, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0165, 0.0154, 0.0359, 0.8689, 0.0171, 0.0177, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 154, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0260, 0.0668, 0.1602, 0.4419, 0.0925, 0.1956, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 154, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0081, 0.9534, 0.0029, 0.0085, 0.0089, 0.0089, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 154, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7291, 0.0359, 0.0500, 0.0554, 0.0469, 0.0405, 0.0423],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 155, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1680, 0.1406, 0.0563, 0.0195, 0.6084, 0.0031, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.018

[Epoch: 155, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0155, 0.0144, 0.0304, 0.8771, 0.0182, 0.0168, 0.0277],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 155, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0255, 0.0604, 0.1629, 0.4500, 0.0922, 0.1934, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 155, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0102, 0.9478, 0.0027, 0.0090, 0.0096, 0.0096, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.041

[Epoch: 155, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7020, 0.0432, 0.0557, 0.0602, 0.0511, 0.0411, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 156, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1562, 0.1351, 0.0432, 0.0239, 0.6331, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 156, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0150, 0.0153, 0.0319, 0.8811, 0.0150, 0.0154, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 156, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0276, 0.0715, 0.1704, 0.4065, 0.1083, 0.1973, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 156, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0125, 0.9345, 0.0042, 0.0129, 0.0125, 0.0122, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 156, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7392, 0.0318, 0.0475, 0.0551, 0.0458, 0.0385, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 157, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1501, 0.1434, 0.0449, 0.0128, 0.6426, 0.0032, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 157, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0144, 0.0194, 0.0307, 0.8751, 0.0148, 0.0135, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 157, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0266, 0.0634, 0.1461, 0.4666, 0.0983, 0.1818, 0.0172],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 157, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9505, 0.0043, 0.0101, 0.0089, 0.0084, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 157, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7347, 0.0349, 0.0482, 0.0599, 0.0435, 0.0369, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 158, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1862, 0.1511, 0.0536, 0.0289, 0.5724, 0.0037, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.033

[Epoch: 158, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0204, 0.0207, 0.0408, 0.8459, 0.0156, 0.0186, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 158, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0266, 0.0782, 0.1811, 0.4018, 0.0974, 0.1973, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 158, batch: 152/190] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0073, 0.9511, 0.0022, 0.0095, 0.0123, 0.0078, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 158, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7373, 0.0357, 0.0491, 0.0527, 0.0427, 0.0382, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 159, batch: 38/190] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1433, 0.1354, 0.0627, 0.0150, 0.6366, 0.0038, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.015

[Epoch: 159, batch: 76/190] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0182, 0.0160, 0.0339, 0.8663, 0.0187, 0.0197, 0.0272],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 159, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0225, 0.0556, 0.1508, 0.5032, 0.0796, 0.1744, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.046

[Epoch: 159, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9525, 0.0026, 0.0081, 0.0073, 0.0107, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.050

[Epoch: 159, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.6866, 0.0450, 0.0574, 0.0618, 0.0576, 0.0432, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.056

[Epoch: 160, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1853, 0.1331, 0.0476, 0.0242, 0.6002, 0.0039, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 160, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0139, 0.0126, 0.0301, 0.8911, 0.0159, 0.0152, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 160, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0286, 0.0772, 0.1970, 0.3103, 0.1345, 0.2318, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 160, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0128, 0.9382, 0.0033, 0.0146, 0.0102, 0.0098, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.026

[Epoch: 160, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7559, 0.0272, 0.0433, 0.0507, 0.0447, 0.0377, 0.0405],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.040

[Epoch: 161, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1537, 0.1478, 0.0440, 0.0168, 0.6307, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 161, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0158, 0.0195, 0.0318, 0.8683, 0.0168, 0.0170, 0.0309],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.096

[Epoch: 161, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0692, 0.1386, 0.5226, 0.0705, 0.1567, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 161, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9504, 0.0037, 0.0078, 0.0097, 0.0096, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 161, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7247, 0.0371, 0.0533, 0.0552, 0.0446, 0.0397, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.063

[Epoch: 162, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1605, 0.1495, 0.0542, 0.0189, 0.6096, 0.0041, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 162, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0178, 0.0154, 0.0337, 0.8668, 0.0154, 0.0163, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 162, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0200, 0.0559, 0.1466, 0.4739, 0.0933, 0.1949, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 162, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0092, 0.9491, 0.0027, 0.0086, 0.0102, 0.0107, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 162, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7099, 0.0438, 0.0534, 0.0595, 0.0487, 0.0402, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 163, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1821, 0.1393, 0.0530, 0.0207, 0.5979, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 163, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0172, 0.0170, 0.0339, 0.8686, 0.0160, 0.0164, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.083

[Epoch: 163, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0237, 0.0657, 0.1681, 0.4447, 0.0960, 0.1864, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 163, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0122, 0.9409, 0.0036, 0.0116, 0.0105, 0.0103, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 163, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7439, 0.0270, 0.0469, 0.0554, 0.0475, 0.0379, 0.0414],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 164, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1555, 0.1408, 0.0460, 0.0187, 0.6317, 0.0031, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 164, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0173, 0.0188, 0.0340, 0.8670, 0.0190, 0.0182, 0.0257],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 164, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0266, 0.0680, 0.1608, 0.4322, 0.0996, 0.1961, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 164, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0091, 0.9493, 0.0034, 0.0098, 0.0089, 0.0097, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 164, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7152, 0.0463, 0.0520, 0.0547, 0.0458, 0.0431, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.054

[Epoch: 165, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1582, 0.1453, 0.0537, 0.0203, 0.6159, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 165, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0170, 0.0161, 0.0326, 0.8724, 0.0169, 0.0159, 0.0289],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.103

[Epoch: 165, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0249, 0.0673, 0.1584, 0.4402, 0.0977, 0.1940, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 165, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9448, 0.0037, 0.0103, 0.0117, 0.0108, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 165, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7413, 0.0297, 0.0464, 0.0537, 0.0475, 0.0385, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 166, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1716, 0.1411, 0.0533, 0.0223, 0.6044, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 166, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0148, 0.0153, 0.0362, 0.8702, 0.0148, 0.0142, 0.0344],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.090

[Epoch: 166, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0245, 0.0611, 0.1615, 0.4506, 0.0946, 0.1928, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 166, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0109, 0.9499, 0.0028, 0.0077, 0.0087, 0.0087, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 166, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7130, 0.0407, 0.0503, 0.0620, 0.0453, 0.0427, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 167, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1474, 0.1488, 0.0510, 0.0152, 0.6304, 0.0034, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 167, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0189, 0.0177, 0.0314, 0.8652, 0.0192, 0.0182, 0.0295],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 167, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0266, 0.0738, 0.1640, 0.4236, 0.1023, 0.1915, 0.0182],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 167, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0097, 0.9463, 0.0031, 0.0098, 0.0103, 0.0108, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.035

[Epoch: 167, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7290, 0.0334, 0.0516, 0.0535, 0.0518, 0.0392, 0.0415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 168, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1688, 0.1394, 0.0387, 0.0275, 0.6178, 0.0036, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 168, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0169, 0.0175, 0.0302, 0.8725, 0.0167, 0.0179, 0.0283],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.093

[Epoch: 168, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0263, 0.0613, 0.1601, 0.4456, 0.0995, 0.1905, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 168, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9377, 0.0044, 0.0140, 0.0118, 0.0131, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.047

[Epoch: 168, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7229, 0.0420, 0.0502, 0.0575, 0.0440, 0.0388, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 169, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1701, 0.1382, 0.0640, 0.0142, 0.6064, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 169, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0164, 0.0179, 0.0363, 0.8589, 0.0151, 0.0182, 0.0372],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.086

[Epoch: 169, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0265, 0.0705, 0.1668, 0.4317, 0.0916, 0.1961, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 169, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0120, 0.9503, 0.0039, 0.0067, 0.0104, 0.0070, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 169, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7325, 0.0299, 0.0477, 0.0583, 0.0470, 0.0414, 0.0433],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 170, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1586, 0.1428, 0.0448, 0.0205, 0.6256, 0.0033, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 170, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0134, 0.0300, 0.8842, 0.0139, 0.0142, 0.0284],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.091

[Epoch: 170, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0271, 0.0639, 0.1628, 0.4330, 0.0988, 0.1972, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 170, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0077, 0.9520, 0.0035, 0.0087, 0.0075, 0.0104, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 170, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7231, 0.0394, 0.0515, 0.0555, 0.0466, 0.0413, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 171, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1688, 0.1522, 0.0508, 0.0210, 0.5992, 0.0036, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.025

[Epoch: 171, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0160, 0.0147, 0.0315, 0.8738, 0.0161, 0.0170, 0.0309],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.103

[Epoch: 171, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0256, 0.0663, 0.1548, 0.4445, 0.1000, 0.1900, 0.0188],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 171, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0133, 0.9332, 0.0038, 0.0135, 0.0130, 0.0127, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 171, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7402, 0.0368, 0.0456, 0.0542, 0.0438, 0.0358, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 172, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1760, 0.1280, 0.0478, 0.0190, 0.6211, 0.0036, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 172, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0139, 0.0180, 0.0348, 0.8819, 0.0152, 0.0137, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.083

[Epoch: 172, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0255, 0.0618, 0.1623, 0.4548, 0.0867, 0.1926, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 172, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0076, 0.9545, 0.0033, 0.0084, 0.0080, 0.0091, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.047

[Epoch: 172, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7098, 0.0375, 0.0515, 0.0622, 0.0526, 0.0412, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 173, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1422, 0.1492, 0.0573, 0.0197, 0.6243, 0.0036, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 173, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0193, 0.0196, 0.0310, 0.8590, 0.0204, 0.0193, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.084

[Epoch: 173, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0264, 0.0708, 0.1776, 0.4281, 0.1016, 0.1795, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 173, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0113, 0.9452, 0.0036, 0.0073, 0.0122, 0.0089, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 173, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7354, 0.0346, 0.0496, 0.0522, 0.0455, 0.0416, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 174, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1707, 0.1502, 0.0468, 0.0227, 0.6013, 0.0036, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 174, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0193, 0.0169, 0.0313, 0.8652, 0.0157, 0.0181, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.080

[Epoch: 174, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0276, 0.0730, 0.1500, 0.4310, 0.0960, 0.2051, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.038

[Epoch: 174, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0114, 0.9361, 0.0045, 0.0145, 0.0101, 0.0118, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 174, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7347, 0.0374, 0.0499, 0.0543, 0.0439, 0.0378, 0.0420],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 175, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1766, 0.1371, 0.0465, 0.0166, 0.6167, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 175, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0118, 0.0146, 0.0322, 0.8834, 0.0152, 0.0156, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.095

[Epoch: 175, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0264, 0.0618, 0.1605, 0.4438, 0.0948, 0.1955, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 175, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0087, 0.9530, 0.0035, 0.0084, 0.0107, 0.0076, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 175, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7229, 0.0353, 0.0494, 0.0587, 0.0484, 0.0414, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 176, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1581, 0.1449, 0.0494, 0.0223, 0.6171, 0.0045, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.029

[Epoch: 176, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0144, 0.0348, 0.8720, 0.0162, 0.0138, 0.0325],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.099

[Epoch: 176, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0251, 0.0679, 0.1779, 0.4283, 0.0941, 0.1889, 0.0177],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 176, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0084, 0.9572, 0.0028, 0.0072, 0.0077, 0.0091, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 176, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7326, 0.0387, 0.0471, 0.0565, 0.0444, 0.0383, 0.0424],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.050

[Epoch: 177, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1562, 0.1457, 0.0572, 0.0201, 0.6130, 0.0030, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 177, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0220, 0.0186, 0.0352, 0.8497, 0.0213, 0.0175, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.088

[Epoch: 177, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0255, 0.0579, 0.1529, 0.4491, 0.0995, 0.1980, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.044

[Epoch: 177, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0119, 0.9372, 0.0035, 0.0106, 0.0108, 0.0129, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.025

[Epoch: 177, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7192, 0.0343, 0.0522, 0.0588, 0.0483, 0.0407, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 178, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1820, 0.1443, 0.0481, 0.0177, 0.6000, 0.0039, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 178, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0187, 0.0222, 0.0506, 0.8258, 0.0224, 0.0275, 0.0327],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.077

[Epoch: 178, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0269, 0.0711, 0.1678, 0.4396, 0.0973, 0.1812, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 178, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0091, 0.9480, 0.0033, 0.0103, 0.0088, 0.0092, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.049

[Epoch: 178, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7257, 0.0372, 0.0506, 0.0552, 0.0479, 0.0408, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 179, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1486, 0.1323, 0.0464, 0.0251, 0.6406, 0.0031, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 179, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0129, 0.0136, 0.0239, 0.8970, 0.0127, 0.0139, 0.0260],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 179, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0277, 0.0685, 0.1692, 0.4303, 0.0958, 0.1931, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 179, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0134, 0.9383, 0.0037, 0.0099, 0.0128, 0.0115, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 179, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7015, 0.0403, 0.0561, 0.0601, 0.0508, 0.0461, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 180, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.2548, 0.2322, 0.0775, 0.0311, 0.3941, 0.0049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.023

[Epoch: 180, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0126, 0.0145, 0.0249, 0.8900, 0.0130, 0.0129, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.142

[Epoch: 180, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0262, 0.0626, 0.1544, 0.4384, 0.1064, 0.1955, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.036

[Epoch: 180, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0086, 0.9459, 0.0041, 0.0140, 0.0087, 0.0094, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.038

[Epoch: 180, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7556, 0.0365, 0.0439, 0.0508, 0.0367, 0.0337, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 181, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1071, 0.1334, 0.0384, 0.0181, 0.6786, 0.0109, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.043

[Epoch: 181, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0168, 0.0323, 0.8730, 0.0158, 0.0165, 0.0298],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.093

[Epoch: 181, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0267, 0.0716, 0.1604, 0.4480, 0.0911, 0.1865, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 181, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0081, 0.9542, 0.0031, 0.0063, 0.0092, 0.0090, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 181, batch: 190/190] total loss per batch: 0.760
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7388, 0.0291, 0.0471, 0.0574, 0.0522, 0.0361, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 182, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1971, 0.1695, 0.0568, 0.0306, 0.5316, 0.0070, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.037

[Epoch: 182, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0193, 0.0162, 0.0383, 0.8586, 0.0177, 0.0209, 0.0289],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.072

[Epoch: 182, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0603, 0.1642, 0.4418, 0.0918, 0.1968, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 182, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0093, 0.9516, 0.0027, 0.0086, 0.0102, 0.0092, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.039

[Epoch: 182, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7046, 0.0493, 0.0542, 0.0617, 0.0458, 0.0405, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.057

[Epoch: 183, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1412, 0.1066, 0.0517, 0.0127, 0.6693, 0.0072, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.016

[Epoch: 183, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0183, 0.0158, 0.0316, 0.8748, 0.0172, 0.0151, 0.0271],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 183, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0645, 0.1602, 0.4346, 0.1004, 0.1952, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 183, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0100, 0.9437, 0.0032, 0.0097, 0.0112, 0.0112, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.040

[Epoch: 183, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7159, 0.0266, 0.0515, 0.0593, 0.0527, 0.0441, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 184, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1774, 0.1558, 0.0433, 0.0158, 0.5979, 0.0045, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 184, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0139, 0.0205, 0.0323, 0.8753, 0.0156, 0.0160, 0.0264],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 184, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0287, 0.0692, 0.1692, 0.4314, 0.0981, 0.1859, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.047

[Epoch: 184, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0140, 0.9410, 0.0039, 0.0116, 0.0092, 0.0089, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.037

[Epoch: 184, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7480, 0.0369, 0.0462, 0.0523, 0.0423, 0.0374, 0.0369],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 185, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1364, 0.1497, 0.0459, 0.0196, 0.6404, 0.0037, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 -0.002

[Epoch: 185, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0166, 0.0150, 0.0307, 0.8676, 0.0169, 0.0157, 0.0375],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.097

[Epoch: 185, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0275, 0.0628, 0.1590, 0.4382, 0.1000, 0.1956, 0.0169],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 185, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0085, 0.9504, 0.0028, 0.0081, 0.0099, 0.0112, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 185, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7358, 0.0343, 0.0483, 0.0565, 0.0440, 0.0384, 0.0426],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 186, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1739, 0.1371, 0.0544, 0.0213, 0.6034, 0.0041, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.035

[Epoch: 186, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0145, 0.0145, 0.0333, 0.8786, 0.0132, 0.0151, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.078

[Epoch: 186, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0268, 0.0730, 0.1629, 0.4287, 0.0935, 0.1989, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.048

[Epoch: 186, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9465, 0.0035, 0.0098, 0.0118, 0.0085, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.030

[Epoch: 186, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7180, 0.0404, 0.0507, 0.0558, 0.0482, 0.0389, 0.0481],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.055

[Epoch: 187, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1754, 0.1491, 0.0501, 0.0186, 0.5964, 0.0047, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 187, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0172, 0.0169, 0.0385, 0.8630, 0.0178, 0.0176, 0.0290],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.091

[Epoch: 187, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0256, 0.0614, 0.1628, 0.4516, 0.0867, 0.1955, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 187, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0095, 0.9454, 0.0037, 0.0126, 0.0085, 0.0095, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 187, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7228, 0.0356, 0.0499, 0.0610, 0.0475, 0.0406, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.048

[Epoch: 188, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1487, 0.1466, 0.0502, 0.0191, 0.6254, 0.0046, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 188, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0188, 0.0186, 0.0313, 0.8685, 0.0195, 0.0174, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 188, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0278, 0.0665, 0.1658, 0.4291, 0.1059, 0.1861, 0.0189],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.043

[Epoch: 188, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0114, 0.9489, 0.0033, 0.0076, 0.0105, 0.0095, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.045

[Epoch: 188, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7263, 0.0349, 0.0503, 0.0568, 0.0476, 0.0422, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 189, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1708, 0.1266, 0.0484, 0.0209, 0.6264, 0.0029, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.019

[Epoch: 189, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0143, 0.0155, 0.0293, 0.8822, 0.0135, 0.0153, 0.0299],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 189, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0278, 0.0677, 0.1598, 0.4329, 0.0990, 0.1965, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 189, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0125, 0.9346, 0.0040, 0.0098, 0.0135, 0.0121, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 189, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7349, 0.0390, 0.0493, 0.0529, 0.0430, 0.0381, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.052

[Epoch: 190, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1662, 0.1493, 0.0430, 0.0166, 0.6174, 0.0040, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.031

[Epoch: 190, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0165, 0.0212, 0.0340, 0.8631, 0.0153, 0.0164, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 190, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0264, 0.0658, 0.1624, 0.4354, 0.1000, 0.1943, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 190, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0076, 0.9555, 0.0032, 0.0097, 0.0065, 0.0089, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 190, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7130, 0.0371, 0.0536, 0.0610, 0.0489, 0.0391, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 191, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1518, 0.1456, 0.0542, 0.0215, 0.6183, 0.0043, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.020

[Epoch: 191, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0202, 0.0173, 0.0382, 0.8505, 0.0208, 0.0185, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.092

[Epoch: 191, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0237, 0.0672, 0.1582, 0.4524, 0.0923, 0.1902, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.040

[Epoch: 191, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0088, 0.9532, 0.0030, 0.0100, 0.0084, 0.0083, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.031

[Epoch: 191, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7365, 0.0323, 0.0456, 0.0558, 0.0490, 0.0379, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.042

[Epoch: 192, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1738, 0.1453, 0.0521, 0.0176, 0.6030, 0.0038, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.008

[Epoch: 192, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0185, 0.0141, 0.0338, 0.8765, 0.0161, 0.0163, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.082

[Epoch: 192, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0274, 0.0674, 0.1665, 0.4341, 0.0950, 0.1924, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 192, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0128, 0.9328, 0.0039, 0.0127, 0.0157, 0.0118, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.032

[Epoch: 192, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7056, 0.0445, 0.0582, 0.0573, 0.0464, 0.0441, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.044

[Epoch: 193, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1576, 0.1492, 0.0485, 0.0191, 0.6194, 0.0027, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.026

[Epoch: 193, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0121, 0.0155, 0.0259, 0.8853, 0.0148, 0.0175, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.087

[Epoch: 193, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0279, 0.0662, 0.1481, 0.4302, 0.1100, 0.1991, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 193, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0103, 0.9478, 0.0038, 0.0080, 0.0084, 0.0108, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.046

[Epoch: 193, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7521, 0.0314, 0.0443, 0.0521, 0.0406, 0.0372, 0.0423],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 194, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1565, 0.1245, 0.0460, 0.0244, 0.6407, 0.0037, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.032

[Epoch: 194, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0180, 0.0324, 0.8637, 0.0167, 0.0164, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.094

[Epoch: 194, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0270, 0.0668, 0.1594, 0.4452, 0.0977, 0.1866, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 194, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0090, 0.9494, 0.0032, 0.0127, 0.0070, 0.0093, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.036

[Epoch: 194, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7241, 0.0374, 0.0484, 0.0578, 0.0477, 0.0383, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.051

[Epoch: 195, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1607, 0.1537, 0.0483, 0.0191, 0.6097, 0.0038, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.028

[Epoch: 195, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0192, 0.0186, 0.0362, 0.8690, 0.0149, 0.0145, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.085

[Epoch: 195, batch: 114/190] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0243, 0.0714, 0.1780, 0.4257, 0.0879, 0.1957, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.045

[Epoch: 195, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0080, 0.9571, 0.0032, 0.0075, 0.0101, 0.0065, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.044

[Epoch: 195, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7204, 0.0380, 0.0505, 0.0583, 0.0486, 0.0391, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

[Epoch: 196, batch: 38/190] total loss per batch: 0.775
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1840, 0.1573, 0.0541, 0.0205, 0.5763, 0.0039, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.012

[Epoch: 196, batch: 76/190] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0159, 0.0149, 0.0320, 0.8770, 0.0169, 0.0168, 0.0265],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.084

[Epoch: 196, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0276, 0.0624, 0.1672, 0.4374, 0.0957, 0.1919, 0.0179],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 196, batch: 152/190] total loss per batch: 0.785
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0094, 0.9430, 0.0028, 0.0109, 0.0113, 0.0114, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.042

[Epoch: 196, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7407, 0.0365, 0.0500, 0.0516, 0.0428, 0.0394, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.049

[Epoch: 197, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1474, 0.1276, 0.0508, 0.0228, 0.6439, 0.0032, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.024

[Epoch: 197, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0174, 0.0172, 0.0301, 0.8668, 0.0201, 0.0168, 0.0315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.093

[Epoch: 197, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0277, 0.0672, 0.1547, 0.4507, 0.0986, 0.1846, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.032

[Epoch: 197, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0115, 0.9436, 0.0034, 0.0087, 0.0103, 0.0124, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.023

[Epoch: 197, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7120, 0.0379, 0.0529, 0.0618, 0.0491, 0.0413, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.046

[Epoch: 198, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1580, 0.1406, 0.0427, 0.0157, 0.6366, 0.0031, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.027

[Epoch: 198, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0163, 0.0169, 0.0348, 0.8706, 0.0143, 0.0165, 0.0306],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.081

[Epoch: 198, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0270, 0.0695, 0.1625, 0.4264, 0.0943, 0.2038, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.041

[Epoch: 198, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0098, 0.9505, 0.0035, 0.0096, 0.0086, 0.0083, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.029

[Epoch: 198, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7390, 0.0321, 0.0477, 0.0537, 0.0471, 0.0386, 0.0418],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.047

[Epoch: 199, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1758, 0.1522, 0.0570, 0.0224, 0.5849, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.030

[Epoch: 199, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0162, 0.0180, 0.0341, 0.8718, 0.0143, 0.0156, 0.0300],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.089

[Epoch: 199, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0255, 0.0700, 0.1703, 0.4290, 0.1004, 0.1864, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.042

[Epoch: 199, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0092, 0.9473, 0.0031, 0.0098, 0.0108, 0.0102, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.043

[Epoch: 199, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7343, 0.0388, 0.0478, 0.0525, 0.0445, 0.0385, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.045

[Epoch: 200, batch: 38/190] total loss per batch: 0.774
Policy (actual, predicted): 4 4
Policy data: tensor([0.1633, 0.1433, 0.0500, 0.0200, 0.6167, 0.0033, 0.0033])
Policy pred: tensor([0.1582, 0.1437, 0.0486, 0.0199, 0.6220, 0.0040, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.022 0.017

[Epoch: 200, batch: 76/190] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0167, 0.0167, 0.0333, 0.8700, 0.0167, 0.0167, 0.0300])
Policy pred: tensor([0.0160, 0.0153, 0.0329, 0.8741, 0.0176, 0.0147, 0.0293],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.088 -0.093

[Epoch: 200, batch: 114/190] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0667, 0.1633, 0.4367, 0.0967, 0.1933, 0.0167])
Policy pred: tensor([0.0248, 0.0605, 0.1600, 0.4397, 0.0915, 0.2069, 0.0167],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.042 0.039

[Epoch: 200, batch: 152/190] total loss per batch: 0.784
Policy (actual, predicted): 1 1
Policy data: tensor([0.0100, 0.9467, 0.0033, 0.0100, 0.0100, 0.0100, 0.0100])
Policy pred: tensor([0.0088, 0.9462, 0.0032, 0.0111, 0.0091, 0.0101, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.037 -0.033

[Epoch: 200, batch: 190/190] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.7267, 0.0367, 0.0500, 0.0567, 0.0467, 0.0400, 0.0433])
Policy pred: tensor([0.7097, 0.0370, 0.0556, 0.0624, 0.0488, 0.0414, 0.0451],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.049 0.053

