Training set samples: 6978
Batch size: 32
[Epoch: 1, batch: 43/219] total loss per batch: 1.110
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0038, 0.0169, 0.9357, 0.0247, 0.0140, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.014

[Epoch: 1, batch: 86/219] total loss per batch: 1.091
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2404e-05, 6.8823e-01, 3.1121e-01, 1.3526e-06, 7.2793e-06, 5.9329e-08,
        5.3814e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.803

[Epoch: 1, batch: 129/219] total loss per batch: 1.050
Policy (actual, predicted): 0 5
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([2.9249e-01, 9.8670e-02, 2.6521e-02, 2.0703e-05, 2.5920e-01, 3.0950e-01,
        1.3599e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.138

[Epoch: 1, batch: 172/219] total loss per batch: 1.017
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9330e-02, 2.9289e-01, 3.6166e-01, 5.7759e-06, 7.6820e-02, 1.8768e-01,
        4.1612e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.049

[Epoch: 1, batch: 215/219] total loss per batch: 1.039
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0021, 0.0106, 0.0476, 0.8434, 0.0214, 0.0726, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 2, batch: 43/219] total loss per batch: 0.844
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([6.1505e-04, 1.7976e-03, 6.6036e-03, 9.6476e-01, 1.7220e-02, 7.7070e-03,
        1.2972e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 2, batch: 86/219] total loss per batch: 0.831
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5835e-05, 6.8267e-01, 3.1721e-01, 4.2914e-07, 1.4426e-05, 5.7238e-08,
        7.5597e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.923

[Epoch: 2, batch: 129/219] total loss per batch: 0.797
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([8.3800e-01, 7.9966e-03, 6.2735e-03, 2.8123e-05, 6.9839e-02, 7.6616e-02,
        1.2498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.168

[Epoch: 2, batch: 172/219] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.3221e-02, 5.0902e-02, 7.9620e-01, 3.8885e-06, 3.1711e-02, 8.3553e-02,
        2.4408e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 2, batch: 215/219] total loss per batch: 0.795
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([7.0552e-04, 4.0299e-03, 4.1797e-02, 9.0781e-01, 8.4828e-03, 3.6434e-02,
        7.3982e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.001

[Epoch: 3, batch: 43/219] total loss per batch: 0.722
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([4.1529e-04, 1.5559e-03, 1.0583e-02, 9.6651e-01, 1.4045e-02, 5.0905e-03,
        1.8036e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 3, batch: 86/219] total loss per batch: 0.714
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8399e-05, 7.0210e-01, 2.9783e-01, 3.8876e-07, 1.1123e-05, 4.0749e-08,
        4.5531e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.950

[Epoch: 3, batch: 129/219] total loss per batch: 0.680
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.4259e-01, 3.2523e-03, 1.7789e-03, 7.0105e-06, 2.5950e-02, 2.6014e-02,
        4.0236e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.230

[Epoch: 3, batch: 172/219] total loss per batch: 0.672
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([9.1518e-03, 1.0255e-01, 8.1405e-01, 1.2696e-06, 1.2933e-02, 5.3791e-02,
        7.5168e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 3, batch: 215/219] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([5.2980e-04, 3.7174e-03, 1.8121e-02, 9.3696e-01, 8.6044e-03, 3.1204e-02,
        8.6809e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 4, batch: 43/219] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([3.5428e-04, 1.2091e-03, 1.0745e-02, 9.7072e-01, 9.8717e-03, 5.6674e-03,
        1.4344e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 4, batch: 86/219] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0491e-05, 7.8640e-01, 2.1355e-01, 1.4081e-07, 1.1848e-05, 7.3449e-08,
        3.0303e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.960

[Epoch: 4, batch: 129/219] total loss per batch: 0.649
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.4159e-01, 9.6383e-04, 2.3381e-03, 6.2813e-06, 2.0182e-02, 3.4463e-02,
        4.5886e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.193

[Epoch: 4, batch: 172/219] total loss per batch: 0.642
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1053e-03, 4.5733e-02, 9.0026e-01, 1.6905e-06, 6.0458e-03, 3.9472e-02,
        5.3803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.000

[Epoch: 4, batch: 215/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([4.3276e-04, 1.6474e-03, 1.8213e-02, 9.4617e-01, 6.7175e-03, 2.5868e-02,
        9.4732e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 5, batch: 43/219] total loss per batch: 0.673
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([7.0110e-04, 1.6512e-03, 1.4007e-02, 9.5243e-01, 2.0747e-02, 8.5292e-03,
        1.9293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 5, batch: 86/219] total loss per batch: 0.665
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7604e-06, 7.9827e-01, 2.0168e-01, 9.2093e-08, 7.5833e-06, 8.2826e-08,
        4.0276e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 5, batch: 129/219] total loss per batch: 0.643
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.2074e-01, 8.1650e-03, 2.0868e-03, 5.7531e-06, 4.6805e-02, 2.1368e-02,
        8.3201e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.222

[Epoch: 5, batch: 172/219] total loss per batch: 0.635
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.5432e-03, 8.9513e-02, 8.4571e-01, 2.9691e-06, 2.1607e-02, 3.0775e-02,
        7.8512e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.012

[Epoch: 5, batch: 215/219] total loss per batch: 0.656
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0010, 0.0041, 0.0201, 0.9101, 0.0119, 0.0514, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 6, batch: 43/219] total loss per batch: 0.670
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([9.1344e-04, 1.8946e-03, 6.3015e-03, 9.6963e-01, 1.2773e-02, 4.9568e-03,
        3.5293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.009

[Epoch: 6, batch: 86/219] total loss per batch: 0.651
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1340e-05, 7.3895e-01, 2.6094e-01, 2.4880e-07, 1.7660e-05, 1.6160e-07,
        7.8126e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 6, batch: 129/219] total loss per batch: 0.629
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.5928e-01, 2.7368e-03, 2.3222e-03, 7.3644e-06, 9.6048e-03, 2.5559e-02,
        4.8504e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.119

[Epoch: 6, batch: 172/219] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6072e-03, 6.0058e-02, 9.2091e-01, 1.8711e-06, 5.1188e-03, 7.0084e-03,
        4.3000e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 6, batch: 215/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0014, 0.0041, 0.0195, 0.9278, 0.0119, 0.0341, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 7, batch: 43/219] total loss per batch: 0.660
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0013, 0.0015, 0.0106, 0.9558, 0.0191, 0.0095, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.003

[Epoch: 7, batch: 86/219] total loss per batch: 0.642
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.2704e-06, 7.5764e-01, 2.4230e-01, 7.5302e-08, 1.1256e-05, 2.2072e-07,
        3.2807e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 7, batch: 129/219] total loss per batch: 0.617
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([7.4122e-01, 1.7574e-02, 5.3099e-03, 4.3542e-05, 9.7021e-02, 1.3766e-01,
        1.1790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.215

[Epoch: 7, batch: 172/219] total loss per batch: 0.612
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.4991e-03, 1.3818e-01, 8.4063e-01, 5.9995e-06, 7.6651e-03, 8.1064e-03,
        3.9153e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 7, batch: 215/219] total loss per batch: 0.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0010, 0.0029, 0.0161, 0.9512, 0.0074, 0.0203, 0.0010],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 8, batch: 43/219] total loss per batch: 0.652
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0015, 0.0021, 0.0098, 0.9614, 0.0180, 0.0049, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 8, batch: 86/219] total loss per batch: 0.636
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7638e-05, 7.4921e-01, 2.5072e-01, 3.0986e-07, 1.7028e-05, 1.8413e-07,
        2.4559e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 8, batch: 129/219] total loss per batch: 0.612
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.9105e-01, 7.6837e-04, 9.9924e-04, 3.4911e-06, 4.5879e-03, 2.3922e-03,
        1.9453e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.180

[Epoch: 8, batch: 172/219] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2435e-03, 2.7730e-02, 9.5349e-01, 2.1737e-06, 7.7048e-03, 5.1502e-03,
        3.6777e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.035

[Epoch: 8, batch: 215/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0015, 0.0060, 0.0214, 0.9423, 0.0071, 0.0203, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.004

[Epoch: 9, batch: 43/219] total loss per batch: 0.646
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0013, 0.0015, 0.0075, 0.9652, 0.0109, 0.0060, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 9, batch: 86/219] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6442e-05, 7.0123e-01, 2.9861e-01, 2.1063e-07, 8.3681e-06, 2.4272e-07,
        1.3369e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 9, batch: 129/219] total loss per batch: 0.611
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8765e-01, 1.6313e-03, 7.9017e-04, 2.6363e-06, 5.4550e-03, 4.2947e-03,
        1.7696e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.193

[Epoch: 9, batch: 172/219] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.5722e-03, 1.6337e-01, 8.1595e-01, 4.0619e-06, 7.3222e-03, 9.0808e-03,
        2.7029e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.047

[Epoch: 9, batch: 215/219] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0012, 0.0025, 0.0166, 0.9444, 0.0051, 0.0291, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 10, batch: 43/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0013, 0.0023, 0.0103, 0.9547, 0.0187, 0.0090, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 10, batch: 86/219] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6484e-05, 7.2487e-01, 2.7505e-01, 1.8102e-07, 1.3869e-05, 2.2434e-07,
        3.7514e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 10, batch: 129/219] total loss per batch: 0.608
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8758e-01, 7.0835e-04, 9.5210e-04, 4.9548e-06, 5.4398e-03, 5.1487e-03,
        1.6636e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.231

[Epoch: 10, batch: 172/219] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9760e-03, 2.8306e-02, 9.5695e-01, 3.1371e-06, 5.4556e-03, 3.9927e-03,
        3.3208e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 10, batch: 215/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0016, 0.0048, 0.0207, 0.9469, 0.0097, 0.0147, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 11, batch: 43/219] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0014, 0.0011, 0.0088, 0.9681, 0.0092, 0.0054, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 11, batch: 86/219] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3566e-05, 6.4838e-01, 3.5143e-01, 4.6090e-07, 1.5622e-05, 3.5502e-07,
        1.6235e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 11, batch: 129/219] total loss per batch: 0.606
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8338e-01, 2.9574e-03, 1.0900e-03, 1.0121e-05, 6.2071e-03, 5.9174e-03,
        4.3936e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.233

[Epoch: 11, batch: 172/219] total loss per batch: 0.602
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9081e-03, 7.6253e-02, 8.9803e-01, 6.7998e-06, 1.0579e-02, 7.7959e-03,
        4.4254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.037

[Epoch: 11, batch: 215/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0012, 0.0041, 0.0213, 0.9440, 0.0079, 0.0202, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.003

[Epoch: 12, batch: 43/219] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0015, 0.0016, 0.0075, 0.9584, 0.0198, 0.0080, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 12, batch: 86/219] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3540e-05, 7.1752e-01, 2.8233e-01, 4.2981e-07, 2.3096e-05, 3.6085e-07,
        8.9125e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.982

[Epoch: 12, batch: 129/219] total loss per batch: 0.604
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8263e-01, 1.4348e-03, 1.2117e-03, 6.4638e-06, 7.3681e-03, 7.0981e-03,
        2.4825e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.278

[Epoch: 12, batch: 172/219] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9830e-03, 1.0173e-01, 8.7667e-01, 5.1634e-06, 8.0696e-03, 5.3828e-03,
        5.1514e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 12, batch: 215/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0019, 0.0040, 0.0166, 0.9578, 0.0078, 0.0103, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 13, batch: 43/219] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0018, 0.0016, 0.0086, 0.9673, 0.0099, 0.0066, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 13, batch: 86/219] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3025e-05, 6.7735e-01, 3.2250e-01, 7.2465e-07, 3.3011e-05, 3.0845e-07,
        9.1220e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 13, batch: 129/219] total loss per batch: 0.604
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7759e-01, 4.5326e-03, 1.6379e-03, 1.8033e-05, 7.3387e-03, 8.4462e-03,
        4.3448e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.235

[Epoch: 13, batch: 172/219] total loss per batch: 0.598
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5562e-03, 4.9443e-02, 9.3442e-01, 5.3035e-06, 3.4270e-03, 5.6164e-03,
        4.5317e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.059

[Epoch: 13, batch: 215/219] total loss per batch: 0.621
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0018, 0.0056, 0.0254, 0.9429, 0.0069, 0.0158, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 14, batch: 43/219] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0019, 0.0014, 0.0055, 0.9641, 0.0142, 0.0083, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 14, batch: 86/219] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8502e-05, 7.4515e-01, 2.5470e-01, 5.0040e-07, 3.4247e-05, 2.9521e-07,
        6.0009e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 14, batch: 129/219] total loss per batch: 0.603
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8456e-01, 1.2506e-03, 1.1447e-03, 4.1716e-06, 5.8811e-03, 6.8261e-03,
        3.3505e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.292

[Epoch: 14, batch: 172/219] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4359e-03, 1.1422e-01, 8.6620e-01, 5.7959e-06, 9.0174e-03, 5.0537e-03,
        3.0693e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 14, batch: 215/219] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0015, 0.0038, 0.0136, 0.9521, 0.0083, 0.0194, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 15, batch: 43/219] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0016, 0.0016, 0.0072, 0.9659, 0.0133, 0.0083, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.013

[Epoch: 15, batch: 86/219] total loss per batch: 0.625
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8352e-05, 6.0221e-01, 3.9760e-01, 6.9504e-07, 2.9143e-05, 7.2329e-07,
        1.4199e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 15, batch: 129/219] total loss per batch: 0.602
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6269e-01, 7.0844e-03, 3.5452e-03, 3.9204e-05, 8.2453e-03, 1.7696e-02,
        6.9846e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.234

[Epoch: 15, batch: 172/219] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9012e-03, 7.6939e-02, 8.8789e-01, 5.0587e-06, 1.1122e-02, 7.5554e-03,
        1.2591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.055

[Epoch: 15, batch: 215/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0020, 0.0042, 0.0141, 0.9446, 0.0137, 0.0195, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 16, batch: 43/219] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0020, 0.0024, 0.0041, 0.9612, 0.0215, 0.0047, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 16, batch: 86/219] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4097e-05, 7.1186e-01, 2.8793e-01, 5.1684e-07, 2.6681e-05, 8.2062e-07,
        1.4071e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.978

[Epoch: 16, batch: 129/219] total loss per batch: 0.602
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6837e-01, 4.5036e-03, 1.8891e-03, 9.8848e-06, 1.3633e-02, 1.1166e-02,
        4.2366e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.333

[Epoch: 16, batch: 172/219] total loss per batch: 0.595
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3055e-03, 8.0881e-02, 9.0642e-01, 3.6589e-06, 3.1376e-03, 4.9067e-03,
        2.3405e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 16, batch: 215/219] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0012, 0.0035, 0.0178, 0.9590, 0.0065, 0.0107, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 17, batch: 43/219] total loss per batch: 0.639
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0016, 0.0086, 0.9607, 0.0117, 0.0114, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.015

[Epoch: 17, batch: 86/219] total loss per batch: 0.624
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7039e-05, 6.8185e-01, 3.1783e-01, 1.2977e-06, 4.6995e-05, 1.6850e-06,
        2.2066e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 17, batch: 129/219] total loss per batch: 0.603
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7418e-01, 4.5749e-03, 1.7598e-03, 3.4380e-05, 7.7643e-03, 1.1040e-02,
        6.4652e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.210

[Epoch: 17, batch: 172/219] total loss per batch: 0.595
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3069e-03, 8.3621e-02, 8.9817e-01, 2.4534e-06, 7.1355e-03, 3.9426e-03,
        4.8217e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.002

[Epoch: 17, batch: 215/219] total loss per batch: 0.622
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0016, 0.0038, 0.0122, 0.9536, 0.0097, 0.0176, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 18, batch: 43/219] total loss per batch: 0.640
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0022, 0.0018, 0.0072, 0.9606, 0.0174, 0.0048, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.015

[Epoch: 18, batch: 86/219] total loss per batch: 0.626
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0356e-05, 7.1163e-01, 2.8829e-01, 3.4510e-07, 9.1358e-06, 2.0264e-07,
        4.6981e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.984

[Epoch: 18, batch: 129/219] total loss per batch: 0.610
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.5442e-01, 8.7158e-03, 2.0816e-03, 1.0180e-04, 2.1599e-02, 1.2473e-02,
        6.1323e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.347

[Epoch: 18, batch: 172/219] total loss per batch: 0.600
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([5.6110e-03, 6.9091e-02, 9.0810e-01, 1.3645e-05, 3.6970e-03, 7.3405e-03,
        6.1423e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 18, batch: 215/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0018, 0.0036, 0.0178, 0.9551, 0.0067, 0.0127, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 19, batch: 43/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0022, 0.0022, 0.0059, 0.9467, 0.0213, 0.0156, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 19, batch: 86/219] total loss per batch: 0.627
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5956e-05, 6.2871e-01, 3.6950e-01, 1.4102e-06, 6.2873e-05, 1.5371e-06,
        1.6672e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.986

[Epoch: 19, batch: 129/219] total loss per batch: 0.618
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6197e-01, 4.7803e-03, 1.6902e-03, 5.6147e-05, 5.7784e-03, 2.4345e-02,
        1.3795e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.256

[Epoch: 19, batch: 172/219] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3309e-03, 5.7021e-02, 9.2855e-01, 2.9400e-06, 2.4133e-03, 1.5896e-03,
        7.0886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.076

[Epoch: 19, batch: 215/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0067, 0.0236, 0.9354, 0.0151, 0.0136, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.005

[Epoch: 20, batch: 43/219] total loss per batch: 0.645
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0025, 0.0049, 0.9656, 0.0110, 0.0100, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 20, batch: 86/219] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1641e-05, 8.0394e-01, 1.9487e-01, 2.0248e-06, 7.4027e-05, 1.5479e-06,
        1.0660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.966

[Epoch: 20, batch: 129/219] total loss per batch: 0.626
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6981e-01, 7.7185e-03, 2.0918e-03, 6.9180e-05, 9.5188e-03, 1.0211e-02,
        5.8198e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.287

[Epoch: 20, batch: 172/219] total loss per batch: 0.636
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5128e-02, 1.8012e-01, 7.6607e-01, 3.4540e-06, 3.7587e-03, 1.3146e-02,
        1.1776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 20, batch: 215/219] total loss per batch: 0.647
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0014, 0.0053, 0.0237, 0.9410, 0.0076, 0.0195, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 21, batch: 43/219] total loss per batch: 0.654
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0015, 0.0035, 0.9618, 0.0204, 0.0056, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 21, batch: 86/219] total loss per batch: 0.641
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3093e-05, 6.4504e-01, 3.5476e-01, 7.9466e-07, 1.6385e-05, 1.1843e-07,
        1.6740e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.994

[Epoch: 21, batch: 129/219] total loss per batch: 0.631
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6003e-01, 2.6584e-03, 2.3978e-03, 1.1229e-04, 7.9967e-03, 2.6247e-02,
        5.5743e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.191

[Epoch: 21, batch: 172/219] total loss per batch: 0.626
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4155e-03, 1.6374e-02, 9.6242e-01, 7.7151e-06, 3.8304e-03, 4.7004e-03,
        8.2529e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.158

[Epoch: 21, batch: 215/219] total loss per batch: 0.641
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0012, 0.0028, 0.0236, 0.9253, 0.0086, 0.0342, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 22, batch: 43/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([7.5743e-04, 2.3681e-03, 8.5427e-03, 9.6271e-01, 1.4319e-02, 8.5801e-03,
        2.7270e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.013

[Epoch: 22, batch: 86/219] total loss per batch: 0.645
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5611e-04, 6.4275e-01, 3.5624e-01, 1.1058e-06, 9.3533e-05, 2.6547e-06,
        7.6194e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.989

[Epoch: 22, batch: 129/219] total loss per batch: 0.620
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7561e-01, 6.6135e-03, 5.8216e-04, 1.0303e-05, 4.0020e-03, 1.2917e-02,
        2.6787e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.241

[Epoch: 22, batch: 172/219] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5669e-03, 1.5653e-01, 7.9495e-01, 1.0387e-05, 2.6840e-02, 4.6975e-03,
        1.4398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 22, batch: 215/219] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0019, 0.0060, 0.0118, 0.9521, 0.0076, 0.0179, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 23, batch: 43/219] total loss per batch: 0.649
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0022, 0.0011, 0.0035, 0.9667, 0.0138, 0.0088, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 23, batch: 86/219] total loss per batch: 0.631
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0506e-04, 6.8492e-01, 3.1223e-01, 1.2425e-05, 1.4265e-04, 6.7336e-07,
        2.5887e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.982

[Epoch: 23, batch: 129/219] total loss per batch: 0.607
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([8.8987e-01, 2.6476e-02, 5.3074e-03, 5.3604e-05, 2.3546e-02, 5.3239e-02,
        1.5075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.289

[Epoch: 23, batch: 172/219] total loss per batch: 0.596
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4680e-03, 7.3040e-02, 8.9611e-01, 6.2349e-06, 1.1123e-02, 4.6192e-03,
        1.1631e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.020

[Epoch: 23, batch: 215/219] total loss per batch: 0.619
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0014, 0.0032, 0.0206, 0.9579, 0.0044, 0.0104, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 24, batch: 43/219] total loss per batch: 0.643
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0015, 0.0012, 0.0045, 0.9761, 0.0086, 0.0046, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 24, batch: 86/219] total loss per batch: 0.625
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2154e-05, 7.8708e-01, 2.1220e-01, 4.8238e-06, 5.0792e-05, 7.9707e-07,
        6.0901e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.991

[Epoch: 24, batch: 129/219] total loss per batch: 0.599
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8466e-01, 3.3413e-03, 1.7476e-03, 3.2421e-05, 4.6462e-03, 5.1768e-03,
        4.0054e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.256

[Epoch: 24, batch: 172/219] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2398e-03, 8.1309e-02, 8.9798e-01, 1.6004e-06, 6.1774e-03, 5.4279e-03,
        6.8685e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 24, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0034, 0.0141, 0.9546, 0.0046, 0.0189, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 25, batch: 43/219] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0017, 0.0013, 0.0033, 0.9706, 0.0140, 0.0068, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 25, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8901e-05, 6.0405e-01, 3.9510e-01, 8.2484e-06, 2.6141e-05, 6.8634e-07,
        7.3154e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.987

[Epoch: 25, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7307e-01, 5.7759e-03, 4.0135e-03, 3.9373e-05, 8.2365e-03, 8.1557e-03,
        7.1324e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.290

[Epoch: 25, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9444e-03, 8.4944e-02, 8.9769e-01, 1.7379e-06, 4.6153e-03, 5.0617e-03,
        5.7467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 25, batch: 215/219] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0037, 0.0150, 0.9605, 0.0046, 0.0122, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 26, batch: 43/219] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0019, 0.0013, 0.0037, 0.9761, 0.0091, 0.0053, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 26, batch: 86/219] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4446e-05, 7.3881e-01, 2.6056e-01, 6.1955e-06, 2.7469e-05, 5.2853e-07,
        5.4628e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.982

[Epoch: 26, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7673e-01, 5.2853e-03, 3.0470e-03, 3.1601e-05, 6.8968e-03, 7.4953e-03,
        5.1332e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.244

[Epoch: 26, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1620e-03, 8.8803e-02, 8.9107e-01, 1.9673e-06, 7.0254e-03, 5.0765e-03,
        5.8589e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.011

[Epoch: 26, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0022, 0.0037, 0.0161, 0.9528, 0.0068, 0.0168, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 27, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0018, 0.0030, 0.9696, 0.0117, 0.0086, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 27, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9301e-05, 6.7666e-01, 3.2300e-01, 5.7563e-06, 1.6991e-05, 3.4027e-07,
        2.7606e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 27, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7894e-01, 4.4397e-03, 2.8700e-03, 3.0708e-05, 6.7375e-03, 6.3167e-03,
        6.6686e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.271

[Epoch: 27, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3281e-03, 7.0199e-02, 9.1247e-01, 1.2977e-06, 4.6592e-03, 4.0597e-03,
        6.2785e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 27, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0040, 0.0146, 0.9590, 0.0063, 0.0119, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 28, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0020, 0.0067, 0.9667, 0.0137, 0.0056, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 28, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5525e-05, 6.8832e-01, 3.1110e-01, 7.9326e-06, 2.4382e-05, 6.6967e-07,
        4.5863e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 28, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7078e-01, 6.9361e-03, 3.3501e-03, 2.4655e-05, 7.6430e-03, 1.0492e-02,
        7.7629e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.282

[Epoch: 28, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1200e-03, 8.2498e-02, 8.9935e-01, 1.4108e-06, 6.3848e-03, 4.4628e-03,
        5.1791e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 28, batch: 215/219] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0021, 0.0035, 0.0149, 0.9485, 0.0065, 0.0222, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 29, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0016, 0.0030, 0.9740, 0.0081, 0.0078, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 29, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9883e-05, 6.5355e-01, 3.4627e-01, 6.1080e-06, 1.3877e-05, 3.1523e-07,
        1.3546e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 29, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7777e-01, 4.8492e-03, 3.6847e-03, 3.7385e-05, 5.8821e-03, 7.0961e-03,
        6.8359e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.276

[Epoch: 29, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7667e-03, 6.4692e-02, 9.1579e-01, 1.1720e-06, 6.1692e-03, 4.2941e-03,
        6.2884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 29, batch: 215/219] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0021, 0.0037, 0.0131, 0.9614, 0.0070, 0.0107, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 30, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0025, 0.0055, 0.9664, 0.0126, 0.0073, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 30, batch: 86/219] total loss per batch: 0.615
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0323e-04, 7.2900e-01, 2.7055e-01, 1.2399e-05, 3.3198e-05, 8.9698e-07,
        3.0693e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 30, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7639e-01, 5.6787e-03, 2.3355e-03, 3.5953e-05, 7.7004e-03, 6.7406e-03,
        1.1157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.250

[Epoch: 30, batch: 172/219] total loss per batch: 0.587
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.6615e-03, 1.4931e-01, 8.3487e-01, 3.0095e-06, 5.8873e-03, 3.4276e-03,
        4.8355e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.003

[Epoch: 30, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0022, 0.0039, 0.0182, 0.9499, 0.0051, 0.0184, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 31, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0021, 0.0037, 0.9707, 0.0079, 0.0079, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 31, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6989e-05, 6.5248e-01, 3.4728e-01, 9.3242e-06, 1.6613e-05, 6.7495e-07,
        1.9149e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.982

[Epoch: 31, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7611e-01, 5.4487e-03, 3.5068e-03, 2.6827e-05, 5.2271e-03, 9.0700e-03,
        6.1083e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.228

[Epoch: 31, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8809e-03, 3.0907e-02, 9.5175e-01, 1.0846e-06, 5.3913e-03, 3.9168e-03,
        5.1509e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 31, batch: 215/219] total loss per batch: 0.615
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0055, 0.0155, 0.9557, 0.0078, 0.0101, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 32, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0051, 0.9604, 0.0172, 0.0074, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 32, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.7906e-05, 7.1703e-01, 2.8243e-01, 7.0826e-06, 3.0776e-05, 4.7898e-07,
        4.0298e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 32, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7759e-01, 4.4216e-03, 1.7259e-03, 3.2226e-05, 8.3736e-03, 6.8774e-03,
        9.7745e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.393

[Epoch: 32, batch: 172/219] total loss per batch: 0.589
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3401e-03, 5.3470e-02, 9.2927e-01, 2.9301e-06, 5.3386e-03, 3.5204e-03,
        6.0623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.037

[Epoch: 32, batch: 215/219] total loss per batch: 0.615
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0046, 0.0173, 0.9340, 0.0085, 0.0309, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 33, batch: 43/219] total loss per batch: 0.633
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0025, 0.0056, 0.9668, 0.0102, 0.0076, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 33, batch: 86/219] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7562e-05, 6.6820e-01, 3.3136e-01, 8.3820e-06, 1.5968e-05, 6.3692e-07,
        3.5205e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 33, batch: 129/219] total loss per batch: 0.598
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6874e-01, 5.4857e-03, 6.1934e-03, 8.8176e-05, 5.7585e-03, 1.1902e-02,
        1.8276e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.169

[Epoch: 33, batch: 172/219] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9024e-03, 1.6014e-01, 8.1868e-01, 3.2499e-06, 4.0753e-03, 9.6805e-03,
        4.5200e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.001

[Epoch: 33, batch: 215/219] total loss per batch: 0.616
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0057, 0.0207, 0.9456, 0.0086, 0.0147, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 34, batch: 43/219] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0036, 0.0070, 0.9607, 0.0117, 0.0076, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 34, batch: 86/219] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4571e-05, 6.5028e-01, 3.4906e-01, 8.5500e-06, 2.0833e-05, 5.5219e-07,
        5.4192e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 34, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7040e-01, 7.5887e-03, 3.0292e-03, 4.6367e-05, 7.4513e-03, 1.0169e-02,
        1.3121e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.282

[Epoch: 34, batch: 172/219] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9511e-03, 2.9614e-02, 9.5278e-01, 3.0567e-06, 6.1975e-03, 2.4490e-03,
        6.0082e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.031

[Epoch: 34, batch: 215/219] total loss per batch: 0.617
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0020, 0.0043, 0.0162, 0.9552, 0.0056, 0.0143, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 35, batch: 43/219] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0027, 0.0055, 0.9617, 0.0140, 0.0099, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 35, batch: 86/219] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0979e-04, 7.1470e-01, 2.8490e-01, 2.6976e-06, 9.8158e-06, 4.9830e-07,
        2.8019e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.966

[Epoch: 35, batch: 129/219] total loss per batch: 0.597
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7358e-01, 6.4872e-03, 3.2371e-03, 1.5399e-04, 7.5306e-03, 7.4904e-03,
        1.5209e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.216

[Epoch: 35, batch: 172/219] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8358e-03, 1.3646e-01, 8.4959e-01, 2.5315e-06, 3.9469e-03, 5.3784e-03,
        2.7922e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 35, batch: 215/219] total loss per batch: 0.617
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0050, 0.0197, 0.9430, 0.0103, 0.0163, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 36, batch: 43/219] total loss per batch: 0.634
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0024, 0.0031, 0.9743, 0.0068, 0.0055, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.014

[Epoch: 36, batch: 86/219] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8427e-05, 6.5611e-01, 3.4299e-01, 9.9503e-06, 3.0206e-05, 3.7182e-07,
        8.0005e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 36, batch: 129/219] total loss per batch: 0.596
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6968e-01, 4.6402e-03, 4.7695e-03, 5.0826e-05, 4.9038e-03, 1.3489e-02,
        2.4660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.333

[Epoch: 36, batch: 172/219] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2356e-03, 2.1539e-02, 9.6462e-01, 9.9983e-07, 2.5606e-03, 3.2875e-03,
        5.7515e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 36, batch: 215/219] total loss per batch: 0.616
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0019, 0.0060, 0.0150, 0.9557, 0.0061, 0.0133, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 37, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0028, 0.0048, 0.9663, 0.0132, 0.0057, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 37, batch: 86/219] total loss per batch: 0.619
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1048e-04, 7.1385e-01, 2.8562e-01, 1.4462e-05, 1.9323e-05, 2.6491e-06,
        3.8232e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 37, batch: 129/219] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7703e-01, 5.7273e-03, 3.0185e-03, 3.6646e-05, 8.4727e-03, 4.1108e-03,
        1.6028e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.193

[Epoch: 37, batch: 172/219] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2091e-03, 2.1137e-01, 7.6844e-01, 1.3405e-06, 5.4829e-03, 6.5635e-03,
        4.9364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.027

[Epoch: 37, batch: 215/219] total loss per batch: 0.615
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0050, 0.0121, 0.9512, 0.0110, 0.0163, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 38, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0014, 0.0045, 0.9736, 0.0105, 0.0044, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 38, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4739e-05, 6.8918e-01, 3.1054e-01, 1.3748e-05, 2.0081e-05, 1.7377e-06,
        1.9318e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 38, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7265e-01, 3.8130e-03, 5.4252e-03, 1.3903e-04, 4.3579e-03, 1.0771e-02,
        2.8429e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.343

[Epoch: 38, batch: 172/219] total loss per batch: 0.589
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([5.8541e-03, 1.4647e-02, 9.6053e-01, 5.2138e-07, 4.6187e-03, 3.5242e-03,
        1.0824e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.047

[Epoch: 38, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0060, 0.0147, 0.9445, 0.0125, 0.0163, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 39, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0024, 0.0037, 0.9688, 0.0120, 0.0072, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 39, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9526e-05, 7.0129e-01, 2.9845e-01, 5.9923e-06, 3.5675e-05, 8.0289e-07,
        1.2834e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.961

[Epoch: 39, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7843e-01, 5.4843e-03, 1.8770e-03, 3.2761e-05, 5.6769e-03, 7.2284e-03,
        1.2703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.220

[Epoch: 39, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2326e-03, 5.1962e-02, 9.2732e-01, 1.7740e-06, 3.9359e-03, 6.0460e-03,
        7.4984e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.072

[Epoch: 39, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0020, 0.0030, 0.0141, 0.9541, 0.0051, 0.0193, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 40, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0026, 0.0041, 0.9665, 0.0133, 0.0062, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 40, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9182e-05, 6.9534e-01, 3.0432e-01, 2.9054e-05, 1.8871e-05, 1.3899e-06,
        2.1764e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 40, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7356e-01, 4.9406e-03, 4.0368e-03, 4.8925e-05, 5.4205e-03, 9.4302e-03,
        2.5629e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.285

[Epoch: 40, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([6.0669e-03, 1.1726e-01, 8.4018e-01, 1.1778e-06, 1.7802e-02, 1.2805e-02,
        5.8855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.022

[Epoch: 40, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0062, 0.0163, 0.9514, 0.0063, 0.0130, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 41, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0028, 0.0053, 0.9698, 0.0101, 0.0057, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.019

[Epoch: 41, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0823e-05, 6.4662e-01, 3.5317e-01, 6.7807e-06, 3.1740e-05, 6.1076e-07,
        1.0629e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.978

[Epoch: 41, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7783e-01, 3.8744e-03, 2.6297e-03, 3.5947e-05, 6.6843e-03, 7.6673e-03,
        1.2745e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.321

[Epoch: 41, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1729e-03, 6.4641e-02, 9.2112e-01, 2.3347e-06, 2.5191e-03, 4.3112e-03,
        4.2307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 41, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0019, 0.0040, 0.0143, 0.9544, 0.0053, 0.0183, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 42, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0021, 0.0029, 0.0061, 0.9706, 0.0112, 0.0048, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 42, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7301e-05, 6.7663e-01, 3.2311e-01, 7.1639e-06, 3.2493e-05, 5.5538e-07,
        1.7616e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 42, batch: 129/219] total loss per batch: 0.595
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6758e-01, 7.3952e-03, 4.0913e-03, 3.4031e-05, 7.7664e-03, 1.0600e-02,
        2.5307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.104

[Epoch: 42, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8268e-03, 5.4639e-02, 9.2941e-01, 9.6413e-07, 4.0330e-03, 3.0681e-03,
        5.0228e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.017

[Epoch: 42, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0039, 0.0156, 0.9436, 0.0100, 0.0219, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 43, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0022, 0.0022, 0.0043, 0.9642, 0.0154, 0.0072, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 43, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0765e-05, 7.3980e-01, 2.5981e-01, 1.0704e-05, 2.2877e-05, 1.6812e-06,
        2.6690e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 43, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7833e-01, 2.4821e-03, 2.0982e-03, 2.2890e-05, 4.9698e-03, 1.1488e-02,
        6.0675e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.358

[Epoch: 43, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4884e-03, 1.1502e-01, 8.6735e-01, 2.2279e-06, 4.3152e-03, 4.1053e-03,
        4.7216e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.028

[Epoch: 43, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0039, 0.0121, 0.9590, 0.0054, 0.0148, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 44, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.0037, 0.9718, 0.0072, 0.0083, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 44, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.6581e-05, 6.7429e-01, 3.2526e-01, 1.3196e-05, 2.8132e-05, 4.8109e-07,
        3.4214e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.981

[Epoch: 44, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6444e-01, 5.1365e-03, 4.5664e-03, 6.8267e-05, 4.6073e-03, 1.8856e-02,
        2.3213e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.246

[Epoch: 44, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([5.6320e-03, 6.4822e-02, 9.1530e-01, 3.0551e-06, 3.6887e-03, 3.3104e-03,
        7.2458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.061

[Epoch: 44, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0089, 0.0252, 0.9342, 0.0132, 0.0128, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 45, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0023, 0.0050, 0.9728, 0.0090, 0.0040, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 45, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6374e-05, 6.8805e-01, 3.1164e-01, 4.0027e-06, 3.1867e-05, 9.1791e-07,
        1.9281e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.984

[Epoch: 45, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7979e-01, 3.7080e-03, 3.9736e-03, 3.3787e-05, 4.0905e-03, 7.3064e-03,
        1.0945e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.294

[Epoch: 45, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1288e-03, 9.3875e-02, 8.9281e-01, 1.9934e-06, 2.9036e-03, 4.8906e-03,
        2.3849e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.013

[Epoch: 45, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0030, 0.0155, 0.9465, 0.0051, 0.0248, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 46, batch: 43/219] total loss per batch: 0.631
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0049, 0.0067, 0.9518, 0.0201, 0.0086, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 46, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9564e-05, 6.6712e-01, 3.3252e-01, 7.1459e-06, 3.0491e-05, 5.8753e-07,
        2.3545e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 46, batch: 129/219] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6515e-01, 6.7103e-03, 5.7120e-03, 3.6788e-05, 5.0741e-03, 1.5725e-02,
        1.5888e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 46, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.6062e-03, 6.9116e-02, 9.1488e-01, 2.8861e-06, 4.7384e-03, 2.9522e-03,
        3.7041e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.026

[Epoch: 46, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0059, 0.0148, 0.9556, 0.0096, 0.0090, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 47, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0024, 0.0048, 0.9672, 0.0086, 0.0090, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.014

[Epoch: 47, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2091e-04, 7.0097e-01, 2.9826e-01, 9.8732e-06, 3.8433e-05, 3.2928e-06,
        5.9591e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.978

[Epoch: 47, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8102e-01, 3.4810e-03, 3.2342e-03, 1.5938e-05, 5.0775e-03, 5.8553e-03,
        1.3176e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.289

[Epoch: 47, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0083e-03, 1.0326e-01, 8.7989e-01, 1.7665e-06, 3.6158e-03, 4.9795e-03,
        4.2379e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.020

[Epoch: 47, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0029, 0.0102, 0.9578, 0.0075, 0.0167, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 48, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0044, 0.0052, 0.9667, 0.0101, 0.0062, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 48, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9883e-05, 6.8739e-01, 3.1223e-01, 5.2897e-06, 2.7147e-05, 1.4174e-06,
        2.6870e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.964

[Epoch: 48, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7607e-01, 3.6483e-03, 3.8237e-03, 3.4961e-05, 3.4459e-03, 1.1537e-02,
        1.4425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.230

[Epoch: 48, batch: 172/219] total loss per batch: 0.588
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0664e-03, 5.0532e-02, 9.3292e-01, 3.0160e-06, 3.9714e-03, 5.6403e-03,
        2.8627e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 48, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0034, 0.0165, 0.9561, 0.0063, 0.0122, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 49, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0025, 0.0029, 0.9739, 0.0091, 0.0051, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.014

[Epoch: 49, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1699e-05, 6.7854e-01, 3.2091e-01, 4.3279e-06, 3.5668e-05, 1.0460e-06,
        4.3884e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 49, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7294e-01, 4.6462e-03, 3.5774e-03, 4.4738e-05, 5.9164e-03, 9.8345e-03,
        3.0389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.297

[Epoch: 49, batch: 172/219] total loss per batch: 0.587
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1414e-03, 9.4796e-02, 8.9040e-01, 1.6683e-06, 4.9261e-03, 4.3422e-03,
        3.3970e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.012

[Epoch: 49, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0035, 0.0189, 0.9297, 0.0076, 0.0353, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 50, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0024, 0.0042, 0.9704, 0.0102, 0.0044, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 50, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0932e-05, 6.7037e-01, 3.2930e-01, 3.2731e-06, 1.5631e-05, 1.2915e-06,
        2.4175e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.982

[Epoch: 50, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7912e-01, 2.5084e-03, 3.7829e-03, 2.0406e-05, 7.4360e-03, 5.9914e-03,
        1.1420e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.270

[Epoch: 50, batch: 172/219] total loss per batch: 0.587
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0390e-03, 7.6589e-02, 9.0568e-01, 2.5608e-06, 5.7025e-03, 4.1173e-03,
        3.8696e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 50, batch: 215/219] total loss per batch: 0.613
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0020, 0.0034, 0.0104, 0.9646, 0.0074, 0.0095, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 51, batch: 43/219] total loss per batch: 0.632
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0039, 0.0049, 0.9642, 0.0114, 0.0077, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 51, batch: 86/219] total loss per batch: 0.617
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4320e-05, 7.5200e-01, 2.4772e-01, 3.6612e-06, 2.8741e-05, 4.2257e-06,
        1.5982e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.954

[Epoch: 51, batch: 129/219] total loss per batch: 0.593
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7795e-01, 3.7606e-03, 2.8470e-03, 1.9844e-05, 4.6925e-03, 9.0381e-03,
        1.6902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.210

[Epoch: 51, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5911e-03, 7.3126e-02, 9.1229e-01, 2.5724e-06, 3.8989e-03, 5.5084e-03,
        2.5815e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.009

[Epoch: 51, batch: 215/219] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0049, 0.0070, 0.0238, 0.9248, 0.0126, 0.0236, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 52, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0031, 0.0038, 0.9711, 0.0101, 0.0056, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.016

[Epoch: 52, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4881e-05, 6.9033e-01, 3.0945e-01, 1.6127e-06, 1.7595e-05, 1.7720e-06,
        1.0723e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.959

[Epoch: 52, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7364e-01, 4.5564e-03, 3.5022e-03, 2.1031e-05, 6.1349e-03, 9.8235e-03,
        2.3230e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.275

[Epoch: 52, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([6.0412e-03, 9.6305e-02, 8.8300e-01, 5.2096e-06, 4.2336e-03, 3.9515e-03,
        6.4626e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 52, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0040, 0.0154, 0.9585, 0.0063, 0.0097, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 53, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0032, 0.0059, 0.9672, 0.0111, 0.0055, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 53, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7549e-05, 6.6805e-01, 3.3183e-01, 1.4588e-06, 1.3751e-05, 1.6758e-06,
        3.7381e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.961

[Epoch: 53, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7601e-01, 4.5365e-03, 3.7456e-03, 1.6409e-05, 6.3628e-03, 7.4724e-03,
        1.8569e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.248

[Epoch: 53, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3242e-03, 5.9204e-02, 9.2753e-01, 1.3554e-06, 3.9128e-03, 3.0594e-03,
        2.9681e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 53, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.0175, 0.9506, 0.0060, 0.0161, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 54, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0021, 0.0038, 0.9720, 0.0091, 0.0072, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 54, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4721e-05, 6.9643e-01, 3.0333e-01, 6.3139e-07, 1.3846e-05, 1.0651e-06,
        1.4842e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 54, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8018e-01, 3.1584e-03, 2.8146e-03, 1.8307e-05, 4.7791e-03, 7.4642e-03,
        1.5856e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.257

[Epoch: 54, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6135e-03, 1.0402e-01, 8.8371e-01, 2.9185e-06, 2.6755e-03, 2.3875e-03,
        3.5860e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 54, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0036, 0.0137, 0.9599, 0.0058, 0.0116, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 55, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0032, 0.0046, 0.9693, 0.0102, 0.0060, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 55, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1137e-05, 6.9058e-01, 3.0928e-01, 1.0504e-06, 1.3344e-05, 8.6067e-07,
        6.1833e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 55, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8138e-01, 3.7333e-03, 2.7104e-03, 1.5705e-05, 4.1370e-03, 5.9609e-03,
        2.0594e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.257

[Epoch: 55, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2200e-03, 6.6754e-02, 9.1594e-01, 1.6695e-06, 5.3189e-03, 3.6436e-03,
        4.1178e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 55, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.0166, 0.9512, 0.0064, 0.0161, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 56, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0023, 0.0039, 0.9738, 0.0085, 0.0056, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 56, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.2005e-05, 6.8232e-01, 3.1750e-01, 4.9660e-07, 1.1096e-05, 1.3940e-06,
        9.5088e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 56, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7382e-01, 4.1554e-03, 4.2728e-03, 2.0627e-05, 4.9160e-03, 1.0571e-02,
        2.2409e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.268

[Epoch: 56, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0481e-03, 7.8924e-02, 9.0662e-01, 1.4899e-06, 4.4043e-03, 3.6288e-03,
        3.3723e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 56, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0049, 0.0163, 0.9423, 0.0101, 0.0198, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 57, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0022, 0.0033, 0.9730, 0.0108, 0.0058, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 57, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.8163e-05, 6.8670e-01, 3.1316e-01, 1.1316e-06, 1.2109e-05, 7.2581e-07,
        5.2688e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 57, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7755e-01, 3.6693e-03, 3.9980e-03, 1.7273e-05, 5.0960e-03, 7.4791e-03,
        2.1948e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.285

[Epoch: 57, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6201e-03, 7.5457e-02, 9.0991e-01, 1.6242e-06, 3.9928e-03, 3.2120e-03,
        3.8098e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.038

[Epoch: 57, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0025, 0.0033, 0.0164, 0.9551, 0.0050, 0.0153, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 58, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0026, 0.0037, 0.9734, 0.0086, 0.0055, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 58, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1374e-05, 6.8949e-01, 3.1037e-01, 9.1168e-07, 9.9682e-06, 1.2297e-06,
        7.1459e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 58, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7980e-01, 3.3424e-03, 4.1060e-03, 1.0405e-05, 3.5580e-03, 7.3416e-03,
        1.8391e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.247

[Epoch: 58, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6269e-03, 1.0842e-01, 8.7710e-01, 1.0709e-06, 3.9581e-03, 3.3266e-03,
        3.5641e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.017

[Epoch: 58, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0044, 0.0160, 0.9501, 0.0077, 0.0160, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 59, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0026, 0.0035, 0.9707, 0.0107, 0.0071, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.013

[Epoch: 59, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1851e-05, 6.7978e-01, 3.2007e-01, 1.4878e-06, 1.5267e-05, 8.9214e-07,
        3.7273e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 59, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8038e-01, 3.8906e-03, 3.0346e-03, 1.4509e-05, 4.0358e-03, 6.6876e-03,
        1.9616e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.255

[Epoch: 59, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8410e-03, 5.5499e-02, 9.2918e-01, 6.3229e-07, 3.8675e-03, 4.6008e-03,
        4.0074e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 59, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0040, 0.0034, 0.0198, 0.9411, 0.0075, 0.0207, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 60, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0030, 0.0047, 0.9673, 0.0109, 0.0063, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 60, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1096e-05, 7.0301e-01, 2.9682e-01, 1.3901e-06, 1.8156e-05, 6.8633e-07,
        9.1335e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 60, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7837e-01, 1.7788e-03, 3.9051e-03, 2.5096e-05, 4.6622e-03, 9.1117e-03,
        2.1462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.286

[Epoch: 60, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1826e-03, 5.9698e-02, 9.2679e-01, 3.1780e-06, 4.2654e-03, 2.4067e-03,
        3.6526e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.005

[Epoch: 60, batch: 215/219] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0041, 0.0136, 0.9527, 0.0074, 0.0165, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 61, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.0028, 0.9716, 0.0095, 0.0066, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 61, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.5073e-05, 6.6864e-01, 3.3120e-01, 1.1610e-06, 1.7157e-05, 9.4755e-07,
        6.3922e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 61, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8577e-01, 3.1254e-03, 2.0743e-03, 5.9699e-06, 3.0024e-03, 4.8115e-03,
        1.2103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.309

[Epoch: 61, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.9082e-03, 1.6866e-01, 8.1205e-01, 6.5770e-07, 3.0288e-03, 4.9639e-03,
        6.3882e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 61, batch: 215/219] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0066, 0.0163, 0.9494, 0.0066, 0.0156, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 62, batch: 43/219] total loss per batch: 0.628
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0021, 0.0047, 0.9711, 0.0108, 0.0056, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 62, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0115e-04, 6.9251e-01, 3.0718e-01, 3.8457e-06, 3.5013e-05, 6.8377e-07,
        1.6247e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 62, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7322e-01, 3.8583e-03, 6.2783e-03, 3.5249e-05, 6.7264e-03, 7.7334e-03,
        2.1462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.177

[Epoch: 62, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.5042e-03, 1.9675e-02, 9.6514e-01, 2.6087e-06, 7.0497e-03, 2.8054e-03,
        3.8233e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.030

[Epoch: 62, batch: 215/219] total loss per batch: 0.612
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0018, 0.0193, 0.9463, 0.0056, 0.0210, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 63, batch: 43/219] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0023, 0.0038, 0.9659, 0.0122, 0.0094, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 63, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4307e-05, 6.9236e-01, 3.0752e-01, 5.6666e-07, 9.9795e-06, 3.1707e-07,
        2.6311e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 63, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7456e-01, 4.4545e-03, 3.0454e-03, 2.4782e-05, 5.3742e-03, 9.4513e-03,
        3.0948e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 63, batch: 172/219] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0766e-03, 9.7427e-02, 8.8920e-01, 1.2295e-06, 2.5013e-03, 3.3800e-03,
        4.4166e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 63, batch: 215/219] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0061, 0.0130, 0.9589, 0.0065, 0.0104, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 64, batch: 43/219] total loss per batch: 0.629
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0020, 0.0037, 0.9680, 0.0135, 0.0074, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 64, batch: 86/219] total loss per batch: 0.614
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3401e-05, 6.8966e-01, 3.1015e-01, 2.4965e-06, 2.9246e-05, 9.1139e-07,
        7.5023e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 64, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7350e-01, 7.6890e-03, 4.0673e-03, 2.8576e-05, 4.1010e-03, 7.9790e-03,
        2.6388e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.324

[Epoch: 64, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9571e-03, 6.6679e-02, 9.1146e-01, 1.1243e-06, 6.8994e-03, 5.6714e-03,
        5.3268e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 64, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0068, 0.0182, 0.9474, 0.0094, 0.0127, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 65, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0024, 0.0043, 0.9716, 0.0102, 0.0055, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 65, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.3366e-05, 6.6060e-01, 3.3924e-01, 1.5654e-06, 1.2536e-05, 8.1877e-07,
        7.6373e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.966

[Epoch: 65, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8291e-01, 3.0613e-03, 3.2069e-03, 7.2447e-06, 4.3221e-03, 4.8497e-03,
        1.6439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.211

[Epoch: 65, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5828e-03, 8.9878e-02, 8.9464e-01, 1.4932e-06, 3.9567e-03, 3.8641e-03,
        4.0776e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 65, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0041, 0.0168, 0.9478, 0.0067, 0.0194, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 66, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0028, 0.0035, 0.9732, 0.0094, 0.0055, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.014

[Epoch: 66, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.7408e-05, 6.9264e-01, 3.0720e-01, 1.0528e-06, 1.4271e-05, 3.9799e-07,
        4.9304e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 66, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7590e-01, 4.7363e-03, 4.3635e-03, 3.1053e-05, 3.7387e-03, 9.0695e-03,
        2.1580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 66, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.7461e-03, 6.7831e-02, 9.2070e-01, 2.2008e-06, 2.9680e-03, 3.0524e-03,
        3.6979e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.036

[Epoch: 66, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0023, 0.0034, 0.0125, 0.9604, 0.0055, 0.0130, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 67, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0022, 0.0034, 0.9744, 0.0096, 0.0053, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 67, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4287e-05, 6.7619e-01, 3.2363e-01, 1.8163e-06, 1.0591e-05, 5.3395e-07,
        9.2032e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 67, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7896e-01, 3.9588e-03, 3.2652e-03, 1.1880e-05, 4.0048e-03, 7.3787e-03,
        2.4186e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.280

[Epoch: 67, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6576e-03, 9.0968e-02, 8.9238e-01, 9.1595e-07, 4.9357e-03, 4.4385e-03,
        3.6168e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 67, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0025, 0.0047, 0.0203, 0.9419, 0.0072, 0.0199, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 68, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0022, 0.0035, 0.9736, 0.0087, 0.0065, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 68, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4929e-05, 6.8817e-01, 3.1161e-01, 1.3532e-06, 1.5054e-05, 3.8242e-07,
        1.3693e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 68, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7747e-01, 3.8785e-03, 3.5003e-03, 9.6580e-06, 5.0564e-03, 7.5345e-03,
        2.5533e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.265

[Epoch: 68, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0420e-03, 7.2666e-02, 9.1520e-01, 1.5599e-06, 3.3419e-03, 3.1417e-03,
        3.6048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 68, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0034, 0.0131, 0.9568, 0.0060, 0.0155, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 69, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0044, 0.0052, 0.9657, 0.0131, 0.0055, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 69, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0525e-04, 7.0487e-01, 2.9498e-01, 1.1648e-06, 1.0281e-05, 4.6097e-07,
        3.5355e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.965

[Epoch: 69, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7649e-01, 5.0409e-03, 4.2359e-03, 1.4685e-05, 3.7214e-03, 8.1189e-03,
        2.3762e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.234

[Epoch: 69, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3695e-03, 1.0740e-01, 8.7748e-01, 5.7137e-07, 4.3037e-03, 5.2003e-03,
        3.2495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.007

[Epoch: 69, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0033, 0.0160, 0.9538, 0.0052, 0.0157, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 70, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0019, 0.0037, 0.9704, 0.0090, 0.0091, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 70, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.8418e-05, 6.7076e-01, 3.2902e-01, 1.8809e-06, 9.3528e-06, 7.6832e-07,
        1.1536e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 70, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7498e-01, 3.9372e-03, 4.7262e-03, 1.5032e-05, 6.3465e-03, 6.6216e-03,
        3.3702e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.308

[Epoch: 70, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2803e-03, 5.6183e-02, 9.2842e-01, 2.1031e-06, 4.4064e-03, 2.9563e-03,
        4.7514e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 70, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0050, 0.0176, 0.9386, 0.0073, 0.0261, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 71, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0031, 0.0031, 0.9711, 0.0138, 0.0037, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 71, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8212e-05, 6.9151e-01, 3.0830e-01, 1.1819e-06, 8.3484e-06, 1.5267e-07,
        1.0189e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.962

[Epoch: 71, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8372e-01, 3.5338e-03, 2.0246e-03, 8.5488e-06, 2.3971e-03, 6.7463e-03,
        1.5703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.237

[Epoch: 71, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2129e-03, 7.4881e-02, 9.1079e-01, 7.0950e-07, 3.5682e-03, 5.0169e-03,
        2.5263e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 71, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0029, 0.0130, 0.9640, 0.0042, 0.0096, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 72, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0047, 0.0029, 0.0045, 0.9646, 0.0094, 0.0088, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 72, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1673e-04, 6.9577e-01, 3.0405e-01, 2.8271e-06, 1.3975e-05, 1.0118e-06,
        4.6132e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 72, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8299e-01, 3.4900e-03, 3.5091e-03, 1.1425e-05, 3.1171e-03, 5.2841e-03,
        1.6002e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.245

[Epoch: 72, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8776e-03, 7.9070e-02, 9.0387e-01, 1.4692e-06, 5.3257e-03, 3.9699e-03,
        3.8849e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 72, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0049, 0.0291, 0.9284, 0.0087, 0.0225, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 73, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0036, 0.0068, 0.9648, 0.0131, 0.0038, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 73, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2587e-04, 6.7181e-01, 3.2794e-01, 2.0378e-06, 8.3138e-06, 5.1177e-07,
        1.2159e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.962

[Epoch: 73, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7236e-01, 4.9590e-03, 6.5112e-03, 1.3270e-05, 4.9536e-03, 7.4507e-03,
        3.7543e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.287

[Epoch: 73, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3210e-03, 1.0380e-01, 8.8523e-01, 1.4127e-06, 2.3418e-03, 2.5304e-03,
        3.7718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.026

[Epoch: 73, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0159, 0.9555, 0.0069, 0.0118, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 74, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0036, 0.9713, 0.0096, 0.0070, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 74, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5803e-04, 6.6120e-01, 3.3851e-01, 2.3187e-06, 2.0380e-05, 1.2212e-06,
        1.0340e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 74, batch: 129/219] total loss per batch: 0.591
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7749e-01, 4.0013e-03, 3.4051e-03, 1.4082e-05, 5.2846e-03, 7.0205e-03,
        2.7837e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.286

[Epoch: 74, batch: 172/219] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4589e-03, 7.1337e-02, 9.1232e-01, 1.5072e-06, 4.3699e-03, 4.5550e-03,
        3.9542e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.019

[Epoch: 74, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0034, 0.0157, 0.9493, 0.0063, 0.0197, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 75, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0023, 0.0038, 0.9721, 0.0114, 0.0046, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 75, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0115e-04, 6.9862e-01, 3.0120e-01, 5.6757e-07, 1.2199e-05, 4.5130e-07,
        6.5042e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.965

[Epoch: 75, batch: 129/219] total loss per batch: 0.600
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7484e-01, 3.3372e-03, 7.7929e-03, 1.7375e-06, 4.9021e-03, 7.4118e-03,
        1.7191e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.279

[Epoch: 75, batch: 172/219] total loss per batch: 0.629
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1531e-03, 4.9616e-02, 9.4352e-01, 7.3902e-06, 1.7943e-03, 1.0916e-03,
        8.1709e-04], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.130

[Epoch: 75, batch: 215/219] total loss per batch: 0.658
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0077, 0.0056, 0.9408, 0.0119, 0.0286, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 76, batch: 43/219] total loss per batch: 0.667
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0020, 0.0031, 0.0097, 0.9670, 0.0113, 0.0042, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 76, batch: 86/219] total loss per batch: 0.656
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0281e-04, 6.7419e-01, 3.2535e-01, 1.2916e-06, 2.1517e-05, 1.1764e-06,
        3.3911e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.984

[Epoch: 76, batch: 129/219] total loss per batch: 0.627
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([8.4414e-01, 6.1181e-03, 1.2063e-03, 5.9128e-05, 6.4781e-03, 1.4078e-01,
        1.2238e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.283

[Epoch: 76, batch: 172/219] total loss per batch: 0.616
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9792e-03, 8.2410e-02, 8.8672e-01, 5.8525e-06, 1.2788e-02, 8.4331e-03,
        5.6647e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 76, batch: 215/219] total loss per batch: 0.635
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0021, 0.0180, 0.9460, 0.0025, 0.0252, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 77, batch: 43/219] total loss per batch: 0.650
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0021, 0.0034, 0.0126, 0.9587, 0.0144, 0.0035, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 77, batch: 86/219] total loss per batch: 0.634
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5904e-04, 7.4912e-01, 2.4989e-01, 3.4204e-07, 1.1917e-05, 6.1693e-07,
        5.1560e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 77, batch: 129/219] total loss per batch: 0.607
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8229e-01, 2.5339e-03, 1.9381e-03, 2.7097e-05, 7.6986e-03, 3.6230e-03,
        1.8892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.301

[Epoch: 77, batch: 172/219] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8124e-03, 7.1967e-02, 9.1342e-01, 4.8732e-06, 2.9017e-03, 2.4957e-03,
        7.3997e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.010

[Epoch: 77, batch: 215/219] total loss per batch: 0.614
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0029, 0.0144, 0.9529, 0.0050, 0.0191, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 78, batch: 43/219] total loss per batch: 0.637
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0018, 0.0019, 0.0057, 0.9720, 0.0115, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 78, batch: 86/219] total loss per batch: 0.618
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1124e-04, 7.1068e-01, 2.8914e-01, 3.7860e-07, 1.3533e-05, 2.0774e-06,
        5.3217e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 78, batch: 129/219] total loss per batch: 0.592
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7968e-01, 3.8610e-03, 1.9866e-03, 5.2876e-06, 6.5158e-03, 5.2124e-03,
        2.7352e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.181

[Epoch: 78, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.7978e-03, 9.3825e-02, 8.9241e-01, 2.6333e-06, 3.0914e-03, 2.5551e-03,
        6.3175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 78, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0032, 0.0141, 0.9514, 0.0065, 0.0193, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 79, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0021, 0.0027, 0.0066, 0.9691, 0.0119, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 79, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0307e-04, 6.7935e-01, 3.2049e-01, 2.2514e-07, 9.8759e-06, 9.2899e-07,
        4.1238e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 79, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7951e-01, 3.4967e-03, 2.3108e-03, 4.9250e-06, 6.4804e-03, 5.6425e-03,
        2.5524e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.207

[Epoch: 79, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8249e-03, 7.3826e-02, 9.1281e-01, 1.8900e-06, 3.7499e-03, 2.4221e-03,
        5.3603e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 79, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0035, 0.0146, 0.9519, 0.0064, 0.0182, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 80, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0027, 0.0061, 0.9709, 0.0109, 0.0037, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 80, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0145e-05, 6.8844e-01, 3.1142e-01, 1.9261e-07, 9.4127e-06, 8.1009e-07,
        4.2084e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 80, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7961e-01, 3.4881e-03, 2.5149e-03, 4.4153e-06, 5.7929e-03, 6.1073e-03,
        2.4807e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.235

[Epoch: 80, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.7850e-03, 7.3808e-02, 9.1352e-01, 1.4188e-06, 3.7802e-03, 2.3622e-03,
        4.7439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 80, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0152, 0.9520, 0.0064, 0.0173, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 81, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0023, 0.0027, 0.0058, 0.9717, 0.0103, 0.0037, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 81, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7334e-05, 6.8454e-01, 3.1533e-01, 1.9308e-07, 9.2702e-06, 6.4795e-07,
        3.7115e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 81, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8017e-01, 3.3994e-03, 2.5521e-03, 4.0085e-06, 5.3011e-03, 6.2088e-03,
        2.3650e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.250

[Epoch: 81, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8527e-03, 8.1305e-02, 9.0589e-01, 1.0867e-06, 4.0197e-03, 2.4562e-03,
        4.4793e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 81, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0034, 0.0152, 0.9528, 0.0062, 0.0169, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 82, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0028, 0.0056, 0.9721, 0.0100, 0.0039, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 82, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6489e-05, 6.8940e-01, 3.1049e-01, 1.5733e-07, 7.5709e-06, 5.2777e-07,
        3.1867e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 82, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8018e-01, 3.3957e-03, 2.6700e-03, 3.5735e-06, 4.9551e-03, 6.4300e-03,
        2.3669e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.259

[Epoch: 82, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8900e-03, 7.6443e-02, 9.1130e-01, 8.9662e-07, 3.7464e-03, 2.3679e-03,
        4.2537e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 82, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0157, 0.9521, 0.0063, 0.0168, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 83, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0028, 0.0051, 0.9725, 0.0099, 0.0041, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 83, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0264e-05, 6.8346e-01, 3.1643e-01, 1.3668e-07, 7.0683e-06, 4.0843e-07,
        2.7842e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 83, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8022e-01, 3.4353e-03, 2.7871e-03, 3.0857e-06, 4.8005e-03, 6.4317e-03,
        2.3266e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.260

[Epoch: 83, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0033e-03, 8.3005e-02, 9.0434e-01, 7.3250e-07, 3.9397e-03, 2.5292e-03,
        4.1790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 83, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0158, 0.9521, 0.0062, 0.0169, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 84, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0024, 0.0029, 0.0051, 0.9724, 0.0099, 0.0042, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 84, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4003e-05, 6.8706e-01, 3.1285e-01, 1.1241e-07, 5.7514e-06, 3.4462e-07,
        2.3642e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 84, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8057e-01, 3.2977e-03, 2.8259e-03, 2.8411e-06, 4.4563e-03, 6.4989e-03,
        2.3510e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.264

[Epoch: 84, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0623e-03, 7.6748e-02, 9.1132e-01, 5.9189e-07, 3.5858e-03, 2.3983e-03,
        3.8831e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 84, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0036, 0.0164, 0.9514, 0.0063, 0.0167, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 85, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0028, 0.0048, 0.9722, 0.0100, 0.0044, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 85, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6269e-05, 6.8331e-01, 3.1661e-01, 9.9594e-08, 5.5521e-06, 2.6823e-07,
        2.0511e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 85, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8001e-01, 3.4181e-03, 3.0154e-03, 2.5141e-06, 4.6041e-03, 6.6161e-03,
        2.3323e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 85, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1897e-03, 8.3237e-02, 9.0396e-01, 5.1343e-07, 3.9924e-03, 2.7070e-03,
        3.9108e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 85, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0034, 0.0160, 0.9517, 0.0063, 0.0170, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 86, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0046, 0.9720, 0.0099, 0.0046, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 86, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4709e-05, 6.8804e-01, 3.1188e-01, 8.3442e-08, 4.5607e-06, 2.3074e-07,
        1.7652e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 86, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8078e-01, 3.2608e-03, 2.9619e-03, 2.2335e-06, 4.0933e-03, 6.4676e-03,
        2.4303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 86, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2659e-03, 7.8058e-02, 9.1015e-01, 4.3606e-07, 3.4324e-03, 2.4284e-03,
        3.6682e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 86, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0036, 0.0174, 0.9498, 0.0068, 0.0165, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 87, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.0047, 0.9711, 0.0104, 0.0050, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 87, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.6340e-05, 6.8003e-01, 3.1991e-01, 7.6440e-08, 4.7386e-06, 1.7874e-07,
        1.4557e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 87, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7984e-01, 3.2472e-03, 3.2563e-03, 2.3484e-06, 4.4014e-03, 6.7689e-03,
        2.4880e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 87, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5548e-03, 8.0188e-02, 9.0625e-01, 3.5715e-07, 4.1439e-03, 3.0447e-03,
        3.8159e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 87, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0036, 0.0159, 0.9509, 0.0067, 0.0173, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 88, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0043, 0.9723, 0.0094, 0.0049, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 88, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7144e-05, 6.9321e-01, 3.0673e-01, 6.5562e-08, 3.7044e-06, 1.9545e-07,
        1.5668e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 88, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8005e-01, 3.6033e-03, 3.2417e-03, 1.6176e-06, 3.9206e-03, 6.4260e-03,
        2.7582e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.245

[Epoch: 88, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3295e-03, 8.0559e-02, 9.0785e-01, 3.7074e-07, 3.2137e-03, 2.4108e-03,
        3.6377e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 88, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0172, 0.9499, 0.0067, 0.0164, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 89, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0031, 0.0039, 0.9719, 0.0097, 0.0055, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 89, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5879e-05, 6.6804e-01, 3.3190e-01, 5.9276e-08, 4.4763e-06, 1.3279e-07,
        1.2401e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 89, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8051e-01, 3.1532e-03, 3.2526e-03, 2.6643e-06, 3.9829e-03, 6.7202e-03,
        2.3785e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.292

[Epoch: 89, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9277e-03, 8.0788e-02, 9.0459e-01, 4.3489e-07, 4.6735e-03, 3.1266e-03,
        3.8907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 89, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0037, 0.0153, 0.9510, 0.0075, 0.0168, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 90, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0033, 0.0044, 0.9717, 0.0100, 0.0046, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 90, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4594e-05, 7.1352e-01, 2.8641e-01, 8.6054e-08, 3.8477e-06, 2.3648e-07,
        1.5786e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 90, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7703e-01, 4.1224e-03, 3.6862e-03, 1.4242e-06, 5.2381e-03, 7.2683e-03,
        2.6495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.247

[Epoch: 90, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9008e-03, 9.0150e-02, 8.9645e-01, 3.2173e-07, 3.7716e-03, 3.7149e-03,
        3.0084e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.041

[Epoch: 90, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0045, 0.0235, 0.9429, 0.0064, 0.0149, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 91, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0036, 0.0037, 0.9701, 0.0093, 0.0068, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 91, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7578e-05, 6.4895e-01, 3.5099e-01, 8.3284e-08, 8.6480e-06, 1.3584e-07,
        1.6918e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 91, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8133e-01, 4.2793e-03, 3.9092e-03, 2.7491e-06, 3.3218e-03, 4.8174e-03,
        2.3368e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.285

[Epoch: 91, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2913e-03, 5.5027e-02, 9.3315e-01, 6.3707e-07, 2.7977e-03, 2.8899e-03,
        3.8484e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.008

[Epoch: 91, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0042, 0.0121, 0.9522, 0.0071, 0.0184, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 92, batch: 43/219] total loss per batch: 0.627
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0044, 0.9692, 0.0125, 0.0050, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 92, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0088e-05, 7.0951e-01, 2.9039e-01, 1.1770e-07, 4.7792e-06, 1.9095e-07,
        1.7534e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 92, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7690e-01, 3.2970e-03, 4.5196e-03, 4.9635e-06, 5.0991e-03, 7.7091e-03,
        2.4655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 92, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3837e-03, 1.5573e-01, 8.2928e-01, 3.4477e-07, 4.7065e-03, 3.3863e-03,
        3.5123e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 92, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0042, 0.0196, 0.9509, 0.0071, 0.0119, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 93, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0041, 0.0032, 0.9707, 0.0083, 0.0067, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 93, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9570e-05, 7.0674e-01, 2.9318e-01, 1.1582e-07, 1.3410e-05, 1.4404e-07,
        3.1852e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 93, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8222e-01, 3.6298e-03, 3.1433e-03, 3.2294e-06, 3.7207e-03, 4.9424e-03,
        2.3451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.252

[Epoch: 93, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5777e-03, 3.3862e-02, 9.5336e-01, 3.5636e-07, 4.1680e-03, 3.4129e-03,
        2.6152e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 93, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0044, 0.0142, 0.9517, 0.0062, 0.0172, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 94, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0027, 0.0035, 0.9678, 0.0140, 0.0056, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 94, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9986e-05, 6.5360e-01, 3.4628e-01, 1.4939e-07, 8.0100e-06, 1.8478e-07,
        6.3093e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.963

[Epoch: 94, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7906e-01, 3.7086e-03, 3.5951e-03, 5.6234e-06, 4.3166e-03, 7.6948e-03,
        1.6239e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.292

[Epoch: 94, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5886e-03, 9.1463e-02, 8.9355e-01, 3.4975e-07, 3.1924e-03, 3.7651e-03,
        4.4382e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 94, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0050, 0.0155, 0.9485, 0.0064, 0.0166, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 95, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0031, 0.0034, 0.9724, 0.0074, 0.0067, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 95, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6084e-05, 7.2267e-01, 2.7723e-01, 2.0211e-07, 1.1233e-05, 1.6193e-07,
        2.9312e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 95, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8092e-01, 3.2959e-03, 3.4433e-03, 3.7745e-06, 4.3366e-03, 5.3691e-03,
        2.6314e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.204

[Epoch: 95, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4703e-03, 8.5734e-02, 8.9579e-01, 4.0860e-07, 6.8618e-03, 4.0241e-03,
        3.1202e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 95, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0038, 0.0198, 0.9420, 0.0063, 0.0205, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 96, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0025, 0.0030, 0.9724, 0.0108, 0.0047, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 96, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9313e-05, 6.6536e-01, 3.3455e-01, 1.8366e-07, 8.6810e-06, 1.5148e-07,
        4.5813e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 96, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7397e-01, 5.3101e-03, 4.5279e-03, 4.7284e-06, 4.7321e-03, 8.0257e-03,
        3.4247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.278

[Epoch: 96, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0025e-03, 8.0912e-02, 9.0687e-01, 2.3019e-07, 2.5231e-03, 4.0754e-03,
        2.6168e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 96, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0022, 0.0034, 0.0142, 0.9597, 0.0043, 0.0134, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 97, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0042, 0.9704, 0.0086, 0.0071, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 97, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.5522e-05, 6.5128e-01, 3.4863e-01, 2.5518e-07, 9.8923e-06, 2.4612e-07,
        1.2737e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 97, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7742e-01, 3.8049e-03, 4.0019e-03, 5.0094e-06, 4.1449e-03, 7.6298e-03,
        2.9899e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.277

[Epoch: 97, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9580e-03, 8.6656e-02, 8.9868e-01, 1.7941e-07, 4.2135e-03, 4.4100e-03,
        3.0809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.037

[Epoch: 97, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0041, 0.0186, 0.9387, 0.0113, 0.0193, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 98, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0023, 0.0029, 0.9755, 0.0087, 0.0049, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 98, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9768e-05, 7.3997e-01, 2.5989e-01, 2.3190e-07, 1.2249e-05, 3.1005e-07,
        7.0727e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 98, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7928e-01, 2.9609e-03, 3.5048e-03, 3.5186e-06, 3.6169e-03, 8.0146e-03,
        2.6151e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.236

[Epoch: 98, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4165e-03, 6.8465e-02, 9.1740e-01, 7.6787e-07, 3.2751e-03, 4.2349e-03,
        3.2046e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.003

[Epoch: 98, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0026, 0.0045, 0.0159, 0.9495, 0.0071, 0.0169, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 99, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0033, 0.0028, 0.9724, 0.0110, 0.0051, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 99, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7406e-05, 6.8591e-01, 3.1403e-01, 3.5607e-07, 5.5111e-06, 1.9272e-07,
        1.3477e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 99, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8335e-01, 3.0520e-03, 3.6086e-03, 3.0612e-06, 2.8794e-03, 4.8175e-03,
        2.2868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.315

[Epoch: 99, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6456e-03, 1.0418e-01, 8.7830e-01, 9.7723e-07, 5.2353e-03, 4.4301e-03,
        4.2103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 99, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0019, 0.0030, 0.0124, 0.9627, 0.0044, 0.0127, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 100, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0037, 0.9692, 0.0094, 0.0076, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 100, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7418e-05, 6.8550e-01, 3.1444e-01, 2.0597e-07, 1.0483e-05, 2.2479e-07,
        1.0823e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 100, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7360e-01, 5.3007e-03, 4.1971e-03, 3.2448e-06, 5.4454e-03, 8.0691e-03,
        3.3852e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.220

[Epoch: 100, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4661e-03, 4.7567e-02, 9.3865e-01, 3.0325e-07, 3.1098e-03, 3.6984e-03,
        3.5054e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 100, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0040, 0.0049, 0.0182, 0.9457, 0.0123, 0.0116, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 101, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.0027, 0.9690, 0.0098, 0.0082, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 101, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2559e-05, 7.0189e-01, 2.9802e-01, 5.2447e-07, 9.0189e-06, 2.4538e-07,
        3.2004e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 101, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7689e-01, 4.8345e-03, 3.6534e-03, 3.1580e-06, 4.6283e-03, 7.4316e-03,
        2.5626e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.264

[Epoch: 101, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5678e-03, 1.2138e-01, 8.6313e-01, 7.9709e-07, 3.8783e-03, 3.9005e-03,
        4.1417e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 101, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0045, 0.0049, 0.0193, 0.9331, 0.0071, 0.0266, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 102, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0038, 0.9702, 0.0090, 0.0067, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 102, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0819e-05, 6.7013e-01, 3.2980e-01, 3.2067e-07, 9.4155e-06, 2.6767e-07,
        1.9092e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 102, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8024e-01, 3.4367e-03, 3.4414e-03, 2.7915e-06, 3.3616e-03, 6.9245e-03,
        2.5939e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.288

[Epoch: 102, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7888e-03, 7.5486e-02, 9.0908e-01, 5.0535e-07, 3.7165e-03, 3.2634e-03,
        4.6609e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 102, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0035, 0.0165, 0.9545, 0.0071, 0.0109, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 103, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0033, 0.9727, 0.0092, 0.0056, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 103, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2858e-05, 7.0915e-01, 2.9080e-01, 2.9799e-07, 5.0979e-06, 1.9557e-07,
        1.8614e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 103, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7838e-01, 4.1339e-03, 3.1532e-03, 2.9485e-06, 4.1996e-03, 7.4419e-03,
        2.6893e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.253

[Epoch: 103, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0859e-03, 7.6481e-02, 9.1093e-01, 3.3106e-07, 3.3485e-03, 3.3308e-03,
        2.8234e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 103, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0158, 0.9523, 0.0059, 0.0155, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 104, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0033, 0.9720, 0.0087, 0.0065, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 104, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3661e-05, 6.5203e-01, 3.4791e-01, 2.4081e-07, 6.7165e-06, 1.8681e-07,
        1.4104e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 104, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7884e-01, 3.4907e-03, 4.3799e-03, 2.3045e-06, 3.7471e-03, 6.5778e-03,
        2.9604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.253

[Epoch: 104, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7523e-03, 8.2212e-02, 9.0506e-01, 3.0908e-07, 3.4367e-03, 3.2110e-03,
        3.3323e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 104, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0026, 0.0162, 0.9556, 0.0052, 0.0145, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 105, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0036, 0.0036, 0.9703, 0.0095, 0.0059, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 105, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.1096e-05, 7.2282e-01, 2.7714e-01, 2.4219e-07, 5.6044e-06, 1.7687e-07,
        1.1189e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 105, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8083e-01, 3.6463e-03, 3.1884e-03, 2.6286e-06, 3.3255e-03, 6.3925e-03,
        2.6140e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.275

[Epoch: 105, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7887e-03, 7.9872e-02, 9.0576e-01, 1.6442e-07, 3.5305e-03, 3.3587e-03,
        3.6943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.033

[Epoch: 105, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0030, 0.0145, 0.9545, 0.0057, 0.0156, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 106, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0039, 0.9691, 0.0090, 0.0074, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 106, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0808e-05, 6.7467e-01, 3.2529e-01, 1.4821e-07, 4.8128e-06, 1.3706e-07,
        1.2492e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 106, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7981e-01, 3.0119e-03, 3.7265e-03, 2.2550e-06, 3.7526e-03, 6.8593e-03,
        2.8341e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.263

[Epoch: 106, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2698e-03, 7.3190e-02, 9.1343e-01, 1.6656e-07, 3.0451e-03, 3.6255e-03,
        3.4357e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 106, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0038, 0.0198, 0.9436, 0.0072, 0.0196, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 107, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0026, 0.0027, 0.9749, 0.0079, 0.0060, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 107, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.1006e-05, 6.9604e-01, 3.0391e-01, 1.2104e-07, 4.1133e-06, 1.6589e-07,
        2.1199e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 107, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8197e-01, 2.9377e-03, 3.0877e-03, 1.0033e-06, 3.2988e-03, 6.0775e-03,
        2.6242e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.264

[Epoch: 107, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6944e-03, 9.6747e-02, 8.8859e-01, 1.3965e-07, 5.0152e-03, 3.7156e-03,
        3.2403e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 107, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0044, 0.0033, 0.0157, 0.9479, 0.0079, 0.0164, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 108, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0019, 0.0023, 0.0028, 0.9769, 0.0082, 0.0056, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 108, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2211e-05, 6.9026e-01, 3.0966e-01, 1.6307e-07, 6.3260e-06, 1.6471e-07,
        1.7311e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 108, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7231e-01, 4.5681e-03, 4.2397e-03, 5.7272e-06, 5.7009e-03, 9.1288e-03,
        4.0469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.258

[Epoch: 108, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4865e-03, 6.0921e-02, 9.2206e-01, 4.8033e-07, 4.7335e-03, 2.9867e-03,
        4.8139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 108, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0039, 0.0169, 0.9513, 0.0074, 0.0142, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 109, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0036, 0.9657, 0.0137, 0.0069, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 109, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5110e-05, 6.7134e-01, 3.2859e-01, 2.9155e-07, 5.8161e-06, 1.1720e-07,
        1.3468e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 109, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8290e-01, 3.6279e-03, 4.0313e-03, 1.5477e-06, 2.5904e-03, 4.5662e-03,
        2.2789e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.285

[Epoch: 109, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9649e-03, 8.7949e-02, 8.9862e-01, 3.6738e-07, 2.6645e-03, 3.1576e-03,
        3.6471e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 109, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.0146, 0.9509, 0.0059, 0.0200, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 110, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0044, 0.0050, 0.0056, 0.9641, 0.0091, 0.0074, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 110, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7979e-05, 7.1753e-01, 2.8238e-01, 1.5170e-07, 6.0517e-06, 1.1445e-07,
        3.9204e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 110, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6676e-01, 6.8178e-03, 4.6698e-03, 6.4860e-06, 7.3219e-03, 1.0575e-02,
        3.8465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.227

[Epoch: 110, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5782e-03, 8.1028e-02, 9.0401e-01, 3.0439e-07, 3.0718e-03, 3.8889e-03,
        4.4235e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.002

[Epoch: 110, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0043, 0.0047, 0.0171, 0.9494, 0.0073, 0.0134, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 111, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0042, 0.0042, 0.9635, 0.0151, 0.0066, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 111, batch: 86/219] total loss per batch: 0.612
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.3219e-05, 6.9039e-01, 3.0951e-01, 1.8468e-07, 9.0844e-06, 1.0590e-07,
        1.7346e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 111, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8505e-01, 2.0675e-03, 2.7090e-03, 1.2658e-06, 2.1464e-03, 5.7939e-03,
        2.2283e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.293

[Epoch: 111, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7391e-03, 6.8012e-02, 9.1747e-01, 6.8615e-07, 3.7493e-03, 3.4823e-03,
        3.5435e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 111, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0037, 0.0047, 0.0182, 0.9441, 0.0083, 0.0158, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 112, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0033, 0.9726, 0.0087, 0.0057, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 112, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4517e-05, 6.9084e-01, 3.0909e-01, 2.5876e-07, 5.8212e-06, 2.5637e-07,
        1.9093e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 112, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8249e-01, 3.6269e-03, 3.1469e-03, 2.2679e-06, 3.2614e-03, 4.9149e-03,
        2.5558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.261

[Epoch: 112, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8783e-03, 8.0051e-02, 9.0637e-01, 2.2225e-07, 3.2291e-03, 2.6379e-03,
        3.8355e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 112, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0172, 0.9481, 0.0049, 0.0193, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 113, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0034, 0.0029, 0.9694, 0.0105, 0.0067, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 113, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8227e-05, 6.5965e-01, 3.4025e-01, 2.2294e-07, 7.3057e-06, 2.2114e-07,
        3.1693e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 113, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7658e-01, 3.9740e-03, 4.4837e-03, 2.6263e-06, 4.0283e-03, 7.6514e-03,
        3.2793e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.239

[Epoch: 113, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0100e-03, 8.7441e-02, 8.9775e-01, 2.3111e-07, 3.3204e-03, 4.1578e-03,
        4.3164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.006

[Epoch: 113, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0137, 0.9578, 0.0070, 0.0118, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 114, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0030, 0.9684, 0.0105, 0.0073, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 114, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7322e-05, 6.9698e-01, 3.0296e-01, 1.3189e-07, 4.3373e-06, 9.1389e-08,
        1.1132e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 114, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7653e-01, 2.6392e-03, 4.9518e-03, 3.3690e-06, 3.7901e-03, 8.7284e-03,
        3.3600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.259

[Epoch: 114, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5002e-03, 8.0353e-02, 9.0595e-01, 3.2082e-07, 3.4969e-03, 2.8605e-03,
        3.8371e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 114, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0038, 0.0184, 0.9517, 0.0063, 0.0139, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 115, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0033, 0.9677, 0.0112, 0.0073, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 115, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4234e-05, 6.9205e-01, 3.0788e-01, 1.1928e-07, 6.2184e-06, 1.4037e-07,
        1.5291e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 115, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8150e-01, 2.8504e-03, 3.4374e-03, 2.5878e-06, 3.4431e-03, 6.2513e-03,
        2.5124e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.276

[Epoch: 115, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3691e-03, 7.6922e-02, 9.0807e-01, 1.6567e-07, 3.8202e-03, 3.6590e-03,
        4.1570e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 115, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0021, 0.0035, 0.0140, 0.9487, 0.0066, 0.0220, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 116, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0027, 0.0034, 0.9743, 0.0089, 0.0050, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 116, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3285e-05, 6.9229e-01, 3.0766e-01, 1.2206e-07, 4.2609e-06, 1.1162e-07,
        1.9179e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 116, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7817e-01, 3.4641e-03, 3.5192e-03, 1.9957e-06, 3.6648e-03, 7.8182e-03,
        3.3656e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 116, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5363e-03, 7.8814e-02, 9.0796e-01, 3.3111e-07, 3.4925e-03, 3.6559e-03,
        2.5389e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.044

[Epoch: 116, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0046, 0.0190, 0.9454, 0.0110, 0.0127, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 117, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0028, 0.0029, 0.9715, 0.0100, 0.0063, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 117, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2359e-05, 6.7677e-01, 3.2318e-01, 1.6236e-07, 5.1141e-06, 1.6100e-07,
        1.1640e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 117, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8061e-01, 3.0404e-03, 3.7259e-03, 2.8363e-06, 3.1699e-03, 6.2168e-03,
        3.2299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 117, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0616e-03, 8.6680e-02, 8.9881e-01, 1.4660e-07, 3.9265e-03, 3.0998e-03,
        4.4240e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 117, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0181, 0.9476, 0.0052, 0.0186, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 118, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0031, 0.9706, 0.0104, 0.0062, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 118, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8676e-05, 6.9048e-01, 3.0947e-01, 1.0651e-07, 2.7032e-06, 7.4137e-08,
        1.1678e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 118, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7272e-01, 4.3806e-03, 5.4844e-03, 3.6594e-06, 4.3978e-03, 9.2113e-03,
        3.8068e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.278

[Epoch: 118, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8420e-03, 7.3037e-02, 9.1524e-01, 1.3649e-07, 3.0420e-03, 2.7982e-03,
        3.0364e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 118, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0120, 0.9563, 0.0069, 0.0147, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 119, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.0029, 0.9716, 0.0102, 0.0058, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 119, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2760e-05, 6.9463e-01, 3.0531e-01, 1.4787e-07, 3.7447e-06, 1.0291e-07,
        1.8517e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 119, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8134e-01, 2.9618e-03, 3.8506e-03, 1.7881e-06, 3.2790e-03, 5.4752e-03,
        3.0950e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.242

[Epoch: 119, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4551e-03, 7.9686e-02, 9.0513e-01, 1.8215e-07, 3.3827e-03, 4.8863e-03,
        3.4605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 119, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0176, 0.9476, 0.0069, 0.0175, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 120, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.0042, 0.9657, 0.0114, 0.0075, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 120, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5974e-05, 6.9299e-01, 3.0696e-01, 1.0016e-07, 4.9993e-06, 8.9860e-08,
        1.0347e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 120, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8090e-01, 3.7171e-03, 2.7909e-03, 2.5797e-06, 3.1042e-03, 6.5467e-03,
        2.9422e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.299

[Epoch: 120, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0513e-03, 8.8534e-02, 8.9799e-01, 2.1912e-07, 3.6656e-03, 3.8236e-03,
        2.9399e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 120, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0025, 0.0145, 0.9561, 0.0053, 0.0162, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 121, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0041, 0.0036, 0.0027, 0.9660, 0.0124, 0.0074, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 121, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.7022e-05, 6.5848e-01, 3.4146e-01, 2.8790e-07, 7.9928e-06, 8.6407e-08,
        1.0877e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 121, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8398e-01, 2.7553e-03, 3.0010e-03, 1.4145e-06, 2.9002e-03, 4.8341e-03,
        2.5231e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.251

[Epoch: 121, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5630e-03, 6.9786e-02, 9.1830e-01, 1.4649e-07, 2.4317e-03, 2.2328e-03,
        3.6845e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 121, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0042, 0.0169, 0.9507, 0.0062, 0.0149, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 122, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0042, 0.0042, 0.0047, 0.9671, 0.0105, 0.0059, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 122, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8723e-05, 7.2257e-01, 2.7735e-01, 2.3449e-07, 6.3585e-06, 8.7852e-07,
        1.3127e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 122, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7067e-01, 5.8257e-03, 5.4915e-03, 1.4350e-06, 5.4605e-03, 8.4316e-03,
        4.1231e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 122, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3060e-03, 8.8967e-02, 8.9564e-01, 1.2904e-07, 3.4790e-03, 5.0603e-03,
        3.5497e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 122, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0041, 0.0045, 0.0226, 0.9347, 0.0101, 0.0202, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 123, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0025, 0.0023, 0.0030, 0.9745, 0.0079, 0.0064, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 123, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9162e-05, 6.7565e-01, 3.2427e-01, 1.6017e-07, 7.2434e-06, 2.0684e-07,
        3.2687e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 123, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8093e-01, 2.6638e-03, 3.2307e-03, 1.3388e-06, 3.2609e-03, 6.3335e-03,
        3.5752e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.264

[Epoch: 123, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6836e-03, 7.6335e-02, 9.0906e-01, 1.5219e-07, 4.5201e-03, 2.5179e-03,
        3.8808e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.051

[Epoch: 123, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0039, 0.0033, 0.0167, 0.9525, 0.0057, 0.0139, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 124, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0028, 0.0026, 0.9751, 0.0097, 0.0044, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 124, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.2697e-05, 6.9669e-01, 3.0321e-01, 3.6244e-07, 8.9582e-06, 5.6187e-07,
        1.2729e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 124, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8230e-01, 2.6974e-03, 3.7116e-03, 9.8887e-07, 2.5450e-03, 6.2893e-03,
        2.4552e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.261

[Epoch: 124, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5979e-03, 7.8696e-02, 9.0764e-01, 2.8716e-07, 4.1165e-03, 3.0125e-03,
        2.9408e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.004

[Epoch: 124, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0134, 0.9540, 0.0056, 0.0170, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 125, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0035, 0.9696, 0.0103, 0.0060, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 125, batch: 86/219] total loss per batch: 0.616
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8564e-05, 6.5217e-01, 3.4777e-01, 1.5744e-07, 7.8218e-06, 2.6116e-07,
        6.3567e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 125, batch: 129/219] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8058e-01, 4.2248e-03, 2.6934e-03, 7.0559e-07, 3.2588e-03, 7.2419e-03,
        1.9965e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.286

[Epoch: 125, batch: 172/219] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.7354e-03, 5.5869e-02, 9.2921e-01, 1.1830e-07, 2.8799e-03, 3.3452e-03,
        3.9636e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.022

[Epoch: 125, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0045, 0.0028, 0.0233, 0.9463, 0.0088, 0.0109, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 126, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.0050, 0.9636, 0.0130, 0.0081, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.012

[Epoch: 126, batch: 86/219] total loss per batch: 0.613
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.2313e-05, 7.2976e-01, 2.7018e-01, 4.6296e-08, 3.7047e-06, 1.7526e-07,
        1.1359e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 126, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8171e-01, 2.8445e-03, 2.2555e-03, 6.1582e-07, 3.7075e-03, 6.7372e-03,
        2.7436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.276

[Epoch: 126, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2215e-03, 1.2546e-01, 8.5814e-01, 6.5505e-08, 4.7032e-03, 4.3126e-03,
        3.1612e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.001

[Epoch: 126, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0029, 0.0155, 0.9415, 0.0049, 0.0295, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.018

[Epoch: 127, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0032, 0.0035, 0.9724, 0.0090, 0.0054, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 127, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.3343e-05, 6.4782e-01, 3.5211e-01, 1.1781e-07, 5.5574e-06, 3.1548e-07,
        7.3110e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 127, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7967e-01, 3.3448e-03, 3.5476e-03, 7.5852e-07, 3.4241e-03, 6.3553e-03,
        3.6600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.257

[Epoch: 127, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1628e-03, 5.4146e-02, 9.3120e-01, 7.6149e-08, 3.4583e-03, 3.6850e-03,
        3.3452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 127, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.0175, 0.9550, 0.0066, 0.0103, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 128, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0033, 0.0035, 0.9706, 0.0102, 0.0066, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.011

[Epoch: 128, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7528e-05, 6.9881e-01, 3.0114e-01, 1.4944e-07, 5.2265e-06, 4.7428e-07,
        8.7664e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 128, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7747e-01, 4.0698e-03, 4.2162e-03, 6.9333e-07, 3.8537e-03, 6.8422e-03,
        3.5427e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.258

[Epoch: 128, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.5035e-03, 8.8897e-02, 8.9394e-01, 7.8087e-08, 3.9863e-03, 4.8272e-03,
        3.8498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 128, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0041, 0.0182, 0.9491, 0.0083, 0.0130, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 129, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0036, 0.0037, 0.9659, 0.0120, 0.0081, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 129, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2412e-05, 7.0263e-01, 2.9733e-01, 8.0816e-08, 4.1661e-06, 2.4531e-07,
        4.5227e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 129, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7993e-01, 3.2681e-03, 3.4572e-03, 7.2421e-07, 3.2879e-03, 6.8575e-03,
        3.1940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.269

[Epoch: 129, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.5733e-03, 8.2513e-02, 9.0087e-01, 6.6355e-08, 4.6122e-03, 3.7894e-03,
        3.6427e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 129, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0159, 0.9517, 0.0066, 0.0156, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 130, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0039, 0.0034, 0.9665, 0.0117, 0.0075, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 130, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0191e-05, 6.7409e-01, 3.2586e-01, 1.2209e-07, 4.7443e-06, 2.4380e-07,
        7.0704e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 130, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8144e-01, 2.7412e-03, 3.0721e-03, 5.3444e-07, 3.2238e-03, 6.4687e-03,
        3.0569e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.279

[Epoch: 130, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5190e-03, 7.4711e-02, 9.1136e-01, 6.4746e-08, 3.3347e-03, 3.9101e-03,
        3.1614e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 130, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0145, 0.9558, 0.0062, 0.0140, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 131, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0039, 0.0041, 0.0038, 0.9651, 0.0121, 0.0074, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 131, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5670e-05, 6.9070e-01, 3.0926e-01, 1.1024e-07, 4.1039e-06, 3.2241e-07,
        5.6812e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 131, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8134e-01, 3.2259e-03, 3.1512e-03, 4.7748e-07, 3.2485e-03, 5.9177e-03,
        3.1189e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 131, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0131e-03, 7.5568e-02, 9.0987e-01, 6.0890e-08, 3.6830e-03, 3.4949e-03,
        3.3694e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.033

[Epoch: 131, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.0156, 0.9527, 0.0064, 0.0158, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 132, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0037, 0.0037, 0.9683, 0.0101, 0.0069, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 132, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0536e-05, 6.8552e-01, 3.1444e-01, 1.3045e-07, 4.5466e-06, 2.9118e-07,
        7.7543e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 132, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7853e-01, 3.9525e-03, 3.3304e-03, 6.3770e-07, 4.0223e-03, 6.5406e-03,
        3.6226e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.259

[Epoch: 132, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7798e-03, 9.2132e-02, 8.9207e-01, 8.0698e-08, 3.4119e-03, 4.1703e-03,
        4.4406e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 132, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0168, 0.9505, 0.0064, 0.0162, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 133, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0029, 0.9740, 0.0080, 0.0055, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 133, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3610e-05, 6.5981e-01, 3.4015e-01, 9.1811e-08, 3.3509e-06, 3.0221e-07,
        4.3416e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 133, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7512e-01, 3.6006e-03, 3.8903e-03, 6.4566e-07, 4.0099e-03, 9.0376e-03,
        4.3413e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 133, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2678e-03, 7.0038e-02, 9.1538e-01, 6.7759e-08, 4.6153e-03, 3.0453e-03,
        3.6565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 133, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0187, 0.9484, 0.0056, 0.0175, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 134, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.0027, 0.9718, 0.0096, 0.0061, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.010

[Epoch: 134, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0593e-05, 7.0686e-01, 2.9307e-01, 1.8236e-07, 6.4179e-06, 4.1751e-07,
        1.8849e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 134, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7906e-01, 4.6071e-03, 4.1615e-03, 1.3580e-06, 3.5971e-03, 5.3203e-03,
        3.2571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.273

[Epoch: 134, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1789e-03, 1.0438e-01, 8.8243e-01, 8.4213e-08, 3.4804e-03, 4.1671e-03,
        2.3640e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 134, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0025, 0.0150, 0.9530, 0.0067, 0.0180, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 135, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0029, 0.0025, 0.9731, 0.0093, 0.0055, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 135, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9323e-05, 6.7628e-01, 3.2367e-01, 9.4483e-08, 3.4939e-06, 1.7600e-07,
        4.8268e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 135, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7718e-01, 4.8096e-03, 3.2341e-03, 8.2505e-07, 4.0455e-03, 7.4155e-03,
        3.3161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.259

[Epoch: 135, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1916e-03, 5.3544e-02, 9.3368e-01, 1.9395e-07, 2.7453e-03, 2.7947e-03,
        4.0397e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 135, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0156, 0.9531, 0.0055, 0.0156, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 136, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.0052, 0.9657, 0.0114, 0.0067, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 136, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0958e-05, 6.9525e-01, 3.0469e-01, 1.0023e-07, 3.0962e-06, 1.6288e-07,
        1.4646e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.963

[Epoch: 136, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8207e-01, 1.8786e-03, 3.0255e-03, 5.7740e-07, 2.9580e-03, 7.9159e-03,
        2.1523e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.265

[Epoch: 136, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6457e-03, 9.6182e-02, 8.8413e-01, 2.0014e-07, 4.6510e-03, 7.1444e-03,
        4.2513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.001

[Epoch: 136, batch: 215/219] total loss per batch: 0.609
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0031, 0.0188, 0.9510, 0.0053, 0.0161, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.019

[Epoch: 137, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0057, 0.0050, 0.0037, 0.9595, 0.0137, 0.0087, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 137, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9145e-05, 6.7562e-01, 3.2430e-01, 1.6154e-07, 1.1144e-05, 5.3840e-07,
        1.7607e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 137, batch: 129/219] total loss per batch: 0.589
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7456e-01, 5.6918e-03, 4.1733e-03, 1.6423e-06, 4.8538e-03, 6.7356e-03,
        3.9882e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.267

[Epoch: 137, batch: 172/219] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1388e-03, 6.6348e-02, 9.2069e-01, 1.3592e-07, 3.8455e-03, 2.9178e-03,
        3.0582e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.025

[Epoch: 137, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0037, 0.0156, 0.9492, 0.0097, 0.0163, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 138, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0028, 0.9699, 0.0114, 0.0052, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 138, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6517e-05, 7.1721e-01, 2.8271e-01, 1.3943e-07, 5.5738e-06, 3.7283e-07,
        1.4496e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.971

[Epoch: 138, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8360e-01, 2.6904e-03, 2.9830e-03, 7.8357e-07, 2.2731e-03, 5.8504e-03,
        2.6059e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.273

[Epoch: 138, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9873e-03, 9.6093e-02, 8.9089e-01, 8.0527e-08, 3.6246e-03, 3.2254e-03,
        3.1804e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.047

[Epoch: 138, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0042, 0.0053, 0.0175, 0.9412, 0.0085, 0.0185, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.016

[Epoch: 139, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0027, 0.0036, 0.9722, 0.0084, 0.0066, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 139, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9375e-05, 6.7654e-01, 3.2340e-01, 9.1592e-08, 6.8363e-06, 1.9441e-07,
        1.0027e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 139, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7809e-01, 3.5983e-03, 2.9225e-03, 9.9778e-07, 4.0846e-03, 7.8982e-03,
        3.4015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.263

[Epoch: 139, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9135e-03, 6.0768e-02, 9.2509e-01, 2.3047e-07, 3.7870e-03, 3.4184e-03,
        4.0245e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 139, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0038, 0.0168, 0.9485, 0.0076, 0.0155, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 140, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0039, 0.0038, 0.0032, 0.9650, 0.0122, 0.0071, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 140, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0176e-05, 6.8495e-01, 3.1498e-01, 1.6154e-07, 5.6465e-06, 2.8864e-07,
        1.3997e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 140, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7600e-01, 4.0386e-03, 4.5462e-03, 1.2406e-06, 4.1582e-03, 7.2469e-03,
        4.0101e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.255

[Epoch: 140, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4748e-03, 9.1993e-02, 8.9271e-01, 7.5505e-08, 5.5224e-03, 3.0796e-03,
        3.2177e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.001

[Epoch: 140, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0131, 0.9569, 0.0053, 0.0154, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 141, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0037, 0.9709, 0.0092, 0.0059, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 141, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.0599e-05, 6.8631e-01, 3.1364e-01, 1.1073e-07, 4.0319e-06, 1.4798e-07,
        9.3913e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 141, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7922e-01, 3.2286e-03, 3.5828e-03, 1.6323e-06, 4.0406e-03, 6.8296e-03,
        3.0983e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.267

[Epoch: 141, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0647e-03, 8.2157e-02, 9.0225e-01, 1.4871e-07, 3.9909e-03, 4.0867e-03,
        3.4494e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 141, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0184, 0.9473, 0.0059, 0.0184, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 142, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0041, 0.0039, 0.0039, 0.9654, 0.0102, 0.0080, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 142, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3838e-05, 6.8306e-01, 3.1689e-01, 1.2821e-07, 5.2719e-06, 9.8048e-08,
        1.0099e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 142, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8131e-01, 3.0229e-03, 3.0044e-03, 8.1384e-07, 3.1750e-03, 6.3062e-03,
        3.1834e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.268

[Epoch: 142, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0993e-03, 7.6675e-02, 9.0701e-01, 2.0871e-07, 4.1344e-03, 4.1419e-03,
        3.9350e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.010

[Epoch: 142, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0045, 0.0171, 0.9467, 0.0097, 0.0150, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 143, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.0031, 0.9688, 0.0108, 0.0066, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 143, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5983e-05, 6.9452e-01, 3.0541e-01, 1.1025e-07, 6.6651e-06, 2.8152e-07,
        1.5541e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 143, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7849e-01, 3.8072e-03, 4.0705e-03, 7.4488e-07, 3.8560e-03, 5.9379e-03,
        3.8375e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.258

[Epoch: 143, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1419e-03, 7.9090e-02, 9.0869e-01, 1.0996e-07, 3.3984e-03, 2.6262e-03,
        3.0563e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.029

[Epoch: 143, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0154, 0.9516, 0.0060, 0.0166, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.015

[Epoch: 144, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0024, 0.0032, 0.9754, 0.0069, 0.0065, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 144, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5886e-05, 6.9755e-01, 3.0240e-01, 5.7368e-08, 2.3300e-06, 1.1230e-07,
        1.3803e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 144, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7442e-01, 3.8211e-03, 4.6927e-03, 1.4902e-06, 3.9626e-03, 8.2636e-03,
        4.8410e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.284

[Epoch: 144, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5089e-03, 8.6632e-02, 8.9796e-01, 1.7624e-07, 4.3722e-03, 3.8829e-03,
        3.6460e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 144, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0041, 0.0045, 0.0191, 0.9421, 0.0080, 0.0179, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 145, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.0025, 0.9723, 0.0115, 0.0057, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 145, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8782e-05, 6.6839e-01, 3.3154e-01, 1.0991e-07, 3.2817e-06, 1.3724e-07,
        6.0506e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 145, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8028e-01, 2.8482e-03, 3.2424e-03, 5.8792e-07, 3.5430e-03, 7.0724e-03,
        3.0125e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.256

[Epoch: 145, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6238e-03, 7.1437e-02, 9.1681e-01, 1.3434e-07, 3.0561e-03, 3.1609e-03,
        2.9106e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 145, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0023, 0.0107, 0.9595, 0.0037, 0.0169, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 146, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0033, 0.0042, 0.9674, 0.0107, 0.0061, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 146, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5110e-05, 6.9364e-01, 3.0630e-01, 1.6879e-07, 4.5263e-06, 1.6612e-07,
        1.8319e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 146, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8166e-01, 3.5848e-03, 3.2089e-03, 1.1026e-06, 2.8477e-03, 5.9319e-03,
        2.7661e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.260

[Epoch: 146, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7644e-03, 9.6701e-02, 8.8941e-01, 2.7134e-07, 3.7685e-03, 3.0083e-03,
        3.3450e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 146, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0024, 0.0035, 0.0182, 0.9546, 0.0048, 0.0140, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 147, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0030, 0.9697, 0.0100, 0.0076, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 147, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6677e-05, 6.7310e-01, 3.2680e-01, 1.7568e-07, 7.5464e-06, 1.7188e-07,
        8.5497e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 147, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8354e-01, 3.0454e-03, 3.0942e-03, 8.3275e-07, 3.1556e-03, 5.1413e-03,
        2.0267e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.285

[Epoch: 147, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3452e-03, 4.1193e-02, 9.4690e-01, 1.1955e-07, 3.5416e-03, 3.1363e-03,
        2.8870e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.028

[Epoch: 147, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0049, 0.0033, 0.0209, 0.9422, 0.0075, 0.0181, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 148, batch: 43/219] total loss per batch: 0.626
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.0032, 0.9699, 0.0096, 0.0082, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 148, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5330e-05, 6.9887e-01, 3.0105e-01, 2.0190e-07, 6.6332e-06, 2.9171e-07,
        1.8613e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 148, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7425e-01, 3.9887e-03, 2.8676e-03, 9.7478e-07, 4.9387e-03, 9.3569e-03,
        4.6018e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.242

[Epoch: 148, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1568e-03, 1.4193e-01, 8.3997e-01, 1.3400e-07, 4.2692e-03, 4.2966e-03,
        5.3793e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.033

[Epoch: 148, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0159, 0.9538, 0.0058, 0.0149, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 149, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0026, 0.0022, 0.9748, 0.0086, 0.0051, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.008

[Epoch: 149, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.9176e-05, 6.4036e-01, 3.5957e-01, 9.5993e-08, 6.7301e-06, 1.1406e-07,
        1.1403e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 149, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7578e-01, 4.2464e-03, 5.9773e-03, 9.5450e-07, 4.0316e-03, 5.7071e-03,
        4.2605e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.251

[Epoch: 149, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7249e-03, 5.2295e-02, 9.3296e-01, 1.0296e-07, 3.7995e-03, 3.6082e-03,
        4.6145e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 149, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0031, 0.0157, 0.9504, 0.0074, 0.0164, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 150, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0032, 0.0038, 0.9726, 0.0088, 0.0055, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 150, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0020e-05, 7.4091e-01, 2.5902e-01, 1.9507e-07, 4.1808e-06, 8.6388e-08,
        1.5197e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 150, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8330e-01, 2.1147e-03, 2.7578e-03, 5.4813e-07, 2.5435e-03, 7.0463e-03,
        2.2326e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.282

[Epoch: 150, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0452e-03, 8.9817e-02, 8.9211e-01, 1.8644e-07, 5.2786e-03, 4.7736e-03,
        3.9712e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 150, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0025, 0.0197, 0.9486, 0.0063, 0.0172, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.017

[Epoch: 151, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0043, 0.0037, 0.0038, 0.9657, 0.0116, 0.0074, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 151, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.8228e-05, 6.7044e-01, 3.2949e-01, 2.5966e-07, 7.5095e-06, 1.0834e-07,
        1.7965e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 151, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8321e-01, 3.3461e-03, 2.8703e-03, 1.0603e-06, 2.7806e-03, 5.6899e-03,
        2.1048e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.259

[Epoch: 151, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0850e-03, 6.8218e-02, 9.1776e-01, 1.9016e-07, 2.6636e-03, 3.9469e-03,
        4.3259e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.032

[Epoch: 151, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0128, 0.9533, 0.0080, 0.0158, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 152, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0038, 0.0035, 0.9677, 0.0110, 0.0065, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 152, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.2988e-05, 6.8120e-01, 3.1874e-01, 2.2971e-07, 6.5858e-06, 1.1634e-07,
        1.5614e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 152, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7980e-01, 3.1097e-03, 3.3461e-03, 5.8799e-07, 3.4267e-03, 7.2743e-03,
        3.0378e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 152, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3943e-03, 8.1034e-02, 9.0421e-01, 1.9431e-07, 3.5138e-03, 4.0874e-03,
        3.7584e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 152, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0156, 0.9516, 0.0062, 0.0165, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 153, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0039, 0.0034, 0.9680, 0.0103, 0.0071, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 153, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2302e-05, 6.7939e-01, 3.2056e-01, 1.7721e-07, 6.0145e-06, 9.2607e-08,
        1.5986e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 153, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7894e-01, 3.5151e-03, 3.5032e-03, 7.3611e-07, 3.3747e-03, 7.0350e-03,
        3.6318e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.261

[Epoch: 153, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4232e-03, 8.3763e-02, 9.0025e-01, 1.4982e-07, 3.8950e-03, 4.4037e-03,
        4.2691e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 153, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.0177, 0.9478, 0.0066, 0.0175, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 154, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.0031, 0.9698, 0.0100, 0.0067, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 154, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9982e-05, 6.9527e-01, 3.0468e-01, 1.2760e-07, 5.0010e-06, 8.3341e-08,
        8.9716e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 154, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7923e-01, 3.4804e-03, 3.5153e-03, 5.8336e-07, 3.5098e-03, 7.0304e-03,
        3.2375e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.267

[Epoch: 154, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5194e-03, 8.2488e-02, 9.0293e-01, 1.5899e-07, 3.3937e-03, 3.7729e-03,
        3.8983e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 154, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0165, 0.9515, 0.0064, 0.0164, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 155, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0037, 0.0034, 0.9688, 0.0100, 0.0071, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 155, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4713e-05, 6.9074e-01, 3.0923e-01, 1.0322e-07, 3.8031e-06, 4.2747e-08,
        8.9489e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 155, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8111e-01, 3.1366e-03, 3.2786e-03, 4.6273e-07, 3.1873e-03, 6.4363e-03,
        2.8517e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.263

[Epoch: 155, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1080e-03, 7.9578e-02, 9.0704e-01, 1.3669e-07, 3.3391e-03, 3.6386e-03,
        3.3006e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 155, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0035, 0.0157, 0.9510, 0.0069, 0.0164, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 156, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0035, 0.9698, 0.0103, 0.0066, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 156, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6453e-05, 6.8283e-01, 3.1714e-01, 8.4974e-08, 3.2284e-06, 3.9683e-08,
        6.8487e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 156, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7983e-01, 3.2846e-03, 3.3450e-03, 4.7389e-07, 3.3999e-03, 6.8524e-03,
        3.2884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.265

[Epoch: 156, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1916e-03, 7.3214e-02, 9.1307e-01, 1.3525e-07, 3.3021e-03, 3.8842e-03,
        3.3361e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.016

[Epoch: 156, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0175, 0.9499, 0.0070, 0.0157, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 157, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0034, 0.0033, 0.9704, 0.0091, 0.0068, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 157, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3024e-05, 6.8415e-01, 3.1582e-01, 7.7041e-08, 3.4737e-06, 5.0548e-08,
        5.5835e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 157, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7916e-01, 3.4432e-03, 3.9306e-03, 4.2811e-07, 3.5730e-03, 6.4080e-03,
        3.4883e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.265

[Epoch: 157, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7112e-03, 8.1532e-02, 9.0633e-01, 8.6917e-08, 3.1980e-03, 2.9740e-03,
        3.2571e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 157, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0153, 0.9516, 0.0055, 0.0177, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 158, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0032, 0.9719, 0.0096, 0.0062, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 158, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1706e-05, 6.8214e-01, 3.1782e-01, 7.7674e-08, 3.1078e-06, 5.9375e-08,
        7.5099e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 158, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7823e-01, 3.2987e-03, 3.2377e-03, 4.7375e-07, 4.2955e-03, 7.5543e-03,
        3.3803e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.272

[Epoch: 158, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2185e-03, 8.0714e-02, 9.0538e-01, 1.1387e-07, 3.2832e-03, 3.6665e-03,
        3.7418e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 158, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.0196, 0.9446, 0.0070, 0.0180, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 159, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.0027, 0.9723, 0.0094, 0.0062, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 159, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.8570e-05, 7.0011e-01, 2.9985e-01, 5.4538e-08, 2.4333e-06, 2.1609e-08,
        7.1890e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 159, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7851e-01, 4.3755e-03, 4.0871e-03, 2.9643e-07, 3.2840e-03, 6.5774e-03,
        3.1649e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.264

[Epoch: 159, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0228e-03, 8.3916e-02, 9.0288e-01, 9.5910e-08, 3.4575e-03, 3.2455e-03,
        3.4779e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 159, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0027, 0.0133, 0.9584, 0.0067, 0.0134, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 160, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0034, 0.0036, 0.0038, 0.9685, 0.0113, 0.0060, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 160, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9513e-05, 6.7082e-01, 3.2915e-01, 7.0272e-08, 4.7979e-06, 6.0998e-08,
        9.7151e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 160, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7768e-01, 3.2453e-03, 3.5157e-03, 1.0364e-06, 3.9257e-03, 7.3469e-03,
        4.2865e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 160, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1804e-03, 6.7905e-02, 9.2089e-01, 1.0565e-07, 2.7946e-03, 3.2636e-03,
        2.9634e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.018

[Epoch: 160, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.0173, 0.9524, 0.0054, 0.0152, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 161, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0025, 0.0026, 0.9742, 0.0086, 0.0056, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 161, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4898e-05, 6.7761e-01, 3.2232e-01, 1.4613e-07, 6.5548e-06, 6.8879e-08,
        2.0665e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 161, batch: 129/219] total loss per batch: 0.588
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8160e-01, 3.8206e-03, 3.3677e-03, 9.1095e-07, 2.6683e-03, 6.0096e-03,
        2.5325e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.255

[Epoch: 161, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1434e-03, 8.9832e-02, 8.9277e-01, 2.2569e-07, 4.1202e-03, 4.3070e-03,
        4.8236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 161, batch: 215/219] total loss per batch: 0.608
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0037, 0.0214, 0.9344, 0.0092, 0.0245, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.014

[Epoch: 162, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0034, 0.9697, 0.0091, 0.0084, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 162, batch: 86/219] total loss per batch: 0.611
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9307e-05, 7.0967e-01, 2.9028e-01, 7.9764e-08, 3.2554e-06, 2.2525e-08,
        1.3758e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 162, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8029e-01, 2.3151e-03, 2.8574e-03, 6.3674e-07, 4.3948e-03, 6.8934e-03,
        3.2447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.280

[Epoch: 162, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3864e-03, 7.7901e-02, 9.0841e-01, 1.3680e-07, 3.0381e-03, 3.4473e-03,
        3.8156e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 162, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0047, 0.0043, 0.0144, 0.9515, 0.0088, 0.0118, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 163, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0027, 0.9671, 0.0136, 0.0072, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 163, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7194e-05, 6.7916e-01, 3.2078e-01, 1.2835e-07, 3.8434e-06, 5.7378e-08,
        1.8587e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 163, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8249e-01, 3.0216e-03, 3.3660e-03, 7.3355e-07, 2.8071e-03, 5.8704e-03,
        2.4473e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.250

[Epoch: 163, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4030e-03, 7.5957e-02, 9.1022e-01, 1.1442e-07, 4.0417e-03, 3.3563e-03,
        3.0176e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.004

[Epoch: 163, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.0178, 0.9482, 0.0058, 0.0178, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 164, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0041, 0.0040, 0.0038, 0.9660, 0.0103, 0.0077, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 164, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9763e-05, 6.7604e-01, 3.2392e-01, 1.0577e-07, 3.6893e-06, 2.4379e-08,
        1.0210e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 164, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7900e-01, 3.2240e-03, 3.3879e-03, 8.1529e-07, 3.9343e-03, 7.3102e-03,
        3.1436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.271

[Epoch: 164, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2089e-03, 8.3416e-02, 9.0226e-01, 1.0046e-07, 3.5680e-03, 4.0510e-03,
        3.4969e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.009

[Epoch: 164, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0038, 0.0149, 0.9507, 0.0071, 0.0160, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 165, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.0038, 0.9675, 0.0115, 0.0065, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 165, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7014e-05, 6.8904e-01, 3.1091e-01, 1.1701e-07, 3.0799e-06, 3.4285e-08,
        1.5980e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 165, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8133e-01, 2.9047e-03, 3.0403e-03, 7.3364e-07, 3.2447e-03, 6.5654e-03,
        2.9175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.256

[Epoch: 165, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4488e-03, 7.7712e-02, 9.0743e-01, 9.3110e-08, 3.7107e-03, 4.1475e-03,
        3.5539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 165, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0043, 0.0175, 0.9475, 0.0064, 0.0171, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 166, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0034, 0.0034, 0.9694, 0.0094, 0.0070, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 166, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.8869e-05, 6.8676e-01, 3.1320e-01, 1.1914e-07, 3.4691e-06, 1.8647e-08,
        5.2309e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 166, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7799e-01, 3.8517e-03, 3.7961e-03, 6.7303e-07, 3.5902e-03, 7.1062e-03,
        3.6700e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.267

[Epoch: 166, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.9809e-03, 9.1578e-02, 8.9098e-01, 8.1509e-08, 4.6132e-03, 3.6521e-03,
        4.1910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.019

[Epoch: 166, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0041, 0.0178, 0.9470, 0.0066, 0.0167, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 167, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0031, 0.9720, 0.0090, 0.0066, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 167, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2756e-05, 6.8881e-01, 3.1115e-01, 8.6139e-08, 2.8261e-06, 3.9488e-08,
        1.3838e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 167, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8000e-01, 3.3918e-03, 3.7259e-03, 7.9641e-07, 3.3712e-03, 6.1312e-03,
        3.3829e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.262

[Epoch: 167, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2994e-03, 7.2872e-02, 9.1321e-01, 6.1906e-08, 3.5068e-03, 3.6551e-03,
        3.4604e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 167, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.0161, 0.9507, 0.0062, 0.0168, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 168, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0034, 0.0031, 0.9721, 0.0092, 0.0062, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 168, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9958e-05, 6.9338e-01, 3.0659e-01, 7.4518e-08, 2.2124e-06, 1.6497e-08,
        4.9376e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 168, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7694e-01, 3.5916e-03, 3.2469e-03, 4.9188e-07, 3.6888e-03, 8.8929e-03,
        3.6383e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.280

[Epoch: 168, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0417e-03, 8.4858e-02, 9.0148e-01, 8.7875e-08, 3.3854e-03, 3.9611e-03,
        3.2746e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.015

[Epoch: 168, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0130, 0.9550, 0.0049, 0.0182, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 169, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.0034, 0.9699, 0.0106, 0.0063, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 169, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7609e-05, 6.8325e-01, 3.1672e-01, 4.1582e-08, 1.8796e-06, 1.8673e-08,
        4.8934e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 169, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8256e-01, 2.7416e-03, 3.5506e-03, 6.3560e-07, 2.8393e-03, 5.7839e-03,
        2.5197e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.257

[Epoch: 169, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.6908e-03, 7.9456e-02, 9.0482e-01, 7.2070e-08, 4.1116e-03, 3.5144e-03,
        3.4049e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.014

[Epoch: 169, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0196, 0.9491, 0.0072, 0.0144, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 170, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0036, 0.0032, 0.9674, 0.0106, 0.0080, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 170, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7673e-05, 6.8697e-01, 3.1300e-01, 7.3685e-08, 1.7928e-06, 1.3683e-08,
        4.1863e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.978

[Epoch: 170, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8401e-01, 2.5227e-03, 2.8232e-03, 5.6972e-07, 2.8195e-03, 5.3343e-03,
        2.4915e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.258

[Epoch: 170, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4387e-03, 6.7850e-02, 9.2166e-01, 2.0036e-07, 2.5253e-03, 2.4268e-03,
        3.0957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.055

[Epoch: 170, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0025, 0.0029, 0.0136, 0.9574, 0.0050, 0.0157, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 171, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0037, 0.0043, 0.9683, 0.0107, 0.0064, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 171, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4325e-05, 6.8885e-01, 3.1111e-01, 9.4908e-08, 2.7546e-06, 2.6650e-08,
        1.1642e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 171, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8149e-01, 4.0103e-03, 2.8174e-03, 4.8725e-07, 3.5600e-03, 5.3040e-03,
        2.8128e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.293

[Epoch: 171, batch: 172/219] total loss per batch: 0.582
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8625e-03, 9.4548e-02, 8.9114e-01, 7.5921e-08, 2.8391e-03, 3.5893e-03,
        4.0195e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.003

[Epoch: 171, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0030, 0.0179, 0.9495, 0.0068, 0.0166, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.006

[Epoch: 172, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0027, 0.0028, 0.9739, 0.0083, 0.0064, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.001

[Epoch: 172, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5302e-05, 6.8402e-01, 3.1594e-01, 5.9835e-08, 2.5125e-06, 4.1540e-08,
        8.3626e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 172, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.6999e-01, 4.2287e-03, 5.0709e-03, 1.1553e-06, 4.9726e-03, 1.1029e-02,
        4.7084e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.267

[Epoch: 172, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8051e-03, 7.0619e-02, 9.1382e-01, 5.2252e-08, 5.4674e-03, 3.8350e-03,
        3.4539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.003

[Epoch: 172, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0040, 0.0039, 0.0184, 0.9451, 0.0080, 0.0169, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 173, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0029, 0.0037, 0.9714, 0.0098, 0.0057, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 173, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4675e-05, 6.5703e-01, 3.4293e-01, 5.7488e-08, 2.4555e-06, 2.8623e-08,
        6.1793e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.968

[Epoch: 173, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7723e-01, 3.5538e-03, 3.9641e-03, 7.1292e-07, 4.3511e-03, 6.3821e-03,
        4.5166e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.239

[Epoch: 173, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2103e-03, 8.2061e-02, 9.0524e-01, 1.1660e-07, 2.9115e-03, 2.8719e-03,
        3.7075e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 0.001

[Epoch: 173, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0030, 0.0180, 0.9479, 0.0072, 0.0177, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 174, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0036, 0.0031, 0.9682, 0.0101, 0.0081, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 174, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9062e-05, 7.3191e-01, 2.6806e-01, 5.9081e-08, 3.3104e-06, 5.4556e-08,
        4.8388e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 174, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7999e-01, 4.2505e-03, 3.2379e-03, 4.4195e-07, 3.1571e-03, 6.4807e-03,
        2.8847e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.269

[Epoch: 174, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2361e-03, 8.4101e-02, 9.0199e-01, 7.0698e-08, 3.9589e-03, 3.5757e-03,
        3.1408e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.007

[Epoch: 174, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0141, 0.9525, 0.0074, 0.0165, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 175, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0037, 0.0052, 0.9633, 0.0130, 0.0070, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 175, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4066e-05, 6.8458e-01, 3.1536e-01, 9.6781e-08, 3.2195e-06, 2.3576e-08,
        1.8105e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.972

[Epoch: 175, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8452e-01, 2.5591e-03, 3.0047e-03, 5.2710e-07, 2.3918e-03, 5.5020e-03,
        2.0250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.269

[Epoch: 175, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7013e-03, 6.7495e-02, 9.2062e-01, 1.4563e-07, 2.6934e-03, 2.9283e-03,
        3.5635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 175, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0184, 0.9450, 0.0089, 0.0168, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 176, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0045, 0.0041, 0.0039, 0.9672, 0.0105, 0.0066, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 176, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4683e-05, 6.8100e-01, 3.1895e-01, 1.3882e-07, 4.4861e-06, 4.8220e-08,
        1.0132e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 176, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8098e-01, 3.4829e-03, 3.6422e-03, 6.2307e-07, 3.2047e-03, 5.3973e-03,
        3.2940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.260

[Epoch: 176, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4701e-03, 8.3283e-02, 9.0421e-01, 5.5337e-08, 3.4174e-03, 3.2810e-03,
        3.3415e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.023

[Epoch: 176, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0041, 0.0165, 0.9485, 0.0057, 0.0177, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 177, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0036, 0.9705, 0.0103, 0.0059, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 177, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.8458e-05, 6.8119e-01, 3.1877e-01, 9.5838e-08, 3.7413e-06, 4.4791e-08,
        9.1209e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 177, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7866e-01, 3.7320e-03, 3.5565e-03, 4.4398e-07, 3.7889e-03, 6.6632e-03,
        3.6008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.282

[Epoch: 177, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1883e-03, 8.9289e-02, 8.9522e-01, 5.0307e-08, 4.1960e-03, 4.3062e-03,
        3.7961e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 177, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.0145, 0.9528, 0.0067, 0.0151, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 178, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0034, 0.0034, 0.9696, 0.0100, 0.0069, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 178, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2008e-05, 6.8669e-01, 3.1328e-01, 5.1148e-08, 2.7777e-06, 2.3690e-08,
        4.6852e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 178, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7953e-01, 3.6420e-03, 3.5292e-03, 6.8113e-07, 3.8429e-03, 6.5244e-03,
        2.9301e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.254

[Epoch: 178, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6539e-03, 8.0815e-02, 9.0488e-01, 4.9126e-08, 3.5977e-03, 3.4886e-03,
        3.5675e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 178, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0182, 0.9498, 0.0052, 0.0174, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 179, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0030, 0.0028, 0.9719, 0.0087, 0.0062, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 179, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3573e-05, 7.0394e-01, 2.9603e-01, 4.0556e-08, 2.5383e-06, 1.3412e-08,
        4.2037e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 179, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7868e-01, 4.1568e-03, 3.7697e-03, 4.0486e-07, 3.5380e-03, 6.3295e-03,
        3.5212e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 179, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5656e-03, 8.2474e-02, 9.0332e-01, 7.9263e-08, 3.4452e-03, 3.4021e-03,
        3.7907e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 179, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0041, 0.0173, 0.9482, 0.0080, 0.0161, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 180, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0033, 0.0040, 0.9716, 0.0086, 0.0067, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 180, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9648e-05, 6.7628e-01, 3.2369e-01, 4.4155e-08, 3.6922e-06, 2.2301e-08,
        7.0808e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 180, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8030e-01, 2.8727e-03, 3.1470e-03, 3.9989e-07, 4.0070e-03, 6.8430e-03,
        2.8293e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.271

[Epoch: 180, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3477e-03, 8.2285e-02, 9.0294e-01, 4.2545e-08, 3.6760e-03, 4.7461e-03,
        3.0013e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 180, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0045, 0.0048, 0.0185, 0.9371, 0.0097, 0.0204, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 181, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0028, 0.0035, 0.9733, 0.0096, 0.0054, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.007

[Epoch: 181, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0434e-05, 6.9625e-01, 3.0372e-01, 2.7882e-08, 1.9526e-06, 1.9033e-08,
        5.6353e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.980

[Epoch: 181, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7835e-01, 3.2160e-03, 4.3913e-03, 7.4562e-07, 3.1946e-03, 7.2677e-03,
        3.5810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.268

[Epoch: 181, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3326e-03, 7.0583e-02, 9.1607e-01, 6.1573e-08, 3.9977e-03, 3.0694e-03,
        2.9487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.033

[Epoch: 181, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0028, 0.0123, 0.9613, 0.0045, 0.0134, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.013

[Epoch: 182, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0028, 0.9688, 0.0100, 0.0079, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 182, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0364e-05, 6.5989e-01, 3.4008e-01, 6.8438e-08, 2.8445e-06, 4.5716e-08,
        4.7828e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.977

[Epoch: 182, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7874e-01, 3.7704e-03, 3.6283e-03, 6.1187e-07, 3.6851e-03, 6.5019e-03,
        3.6721e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.269

[Epoch: 182, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6815e-03, 8.4950e-02, 9.0228e-01, 4.8089e-08, 2.8895e-03, 3.1310e-03,
        3.0632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 182, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0027, 0.0031, 0.0147, 0.9543, 0.0051, 0.0172, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 183, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0036, 0.0041, 0.0033, 0.9699, 0.0101, 0.0058, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 183, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0749e-05, 7.1196e-01, 2.8799e-01, 5.3139e-08, 2.8467e-06, 3.3254e-08,
        1.8108e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.967

[Epoch: 183, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8129e-01, 2.9561e-03, 3.0685e-03, 7.5079e-07, 3.4748e-03, 6.5443e-03,
        2.6703e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.274

[Epoch: 183, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2928e-03, 8.6937e-02, 8.9918e-01, 5.4997e-08, 3.2181e-03, 3.6806e-03,
        3.6893e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.002

[Epoch: 183, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0030, 0.0027, 0.0199, 0.9509, 0.0055, 0.0150, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 184, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0038, 0.0038, 0.0036, 0.9661, 0.0117, 0.0069, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 184, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4872e-05, 6.8908e-01, 3.1088e-01, 7.5450e-08, 3.3840e-06, 5.7086e-08,
        7.3895e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 184, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7628e-01, 4.5642e-03, 3.9497e-03, 7.8340e-07, 3.5383e-03, 7.5863e-03,
        4.0826e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.250

[Epoch: 184, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1338e-03, 7.4031e-02, 9.1128e-01, 5.5174e-08, 3.4347e-03, 3.1941e-03,
        3.9252e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.024

[Epoch: 184, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0028, 0.0035, 0.0177, 0.9470, 0.0084, 0.0173, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 185, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0027, 0.0028, 0.0032, 0.9735, 0.0079, 0.0068, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 185, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2951e-05, 6.7482e-01, 3.2514e-01, 8.8547e-08, 3.9012e-06, 1.1785e-07,
        1.0816e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.970

[Epoch: 185, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7972e-01, 2.9954e-03, 3.0513e-03, 7.5518e-07, 4.1131e-03, 6.3988e-03,
        3.7183e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.263

[Epoch: 185, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1429e-03, 8.2060e-02, 9.0453e-01, 4.9620e-08, 3.2421e-03, 3.8361e-03,
        3.1855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 185, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0157, 0.9527, 0.0061, 0.0152, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 186, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0038, 0.9703, 0.0106, 0.0060, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 186, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1830e-05, 6.9464e-01, 3.0533e-01, 9.4276e-08, 4.0334e-06, 7.5611e-08,
        7.3435e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 186, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8234e-01, 2.7855e-03, 3.0868e-03, 3.8104e-07, 3.1803e-03, 6.2002e-03,
        2.4044e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.275

[Epoch: 186, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0130e-03, 8.1279e-02, 9.0622e-01, 6.2354e-08, 3.1791e-03, 3.3975e-03,
        2.9163e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.011

[Epoch: 186, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0029, 0.0176, 0.9451, 0.0061, 0.0213, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 187, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0037, 0.0038, 0.0033, 0.9673, 0.0115, 0.0067, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.003

[Epoch: 187, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5822e-05, 6.9309e-01, 3.0687e-01, 5.7052e-08, 2.1851e-06, 2.7127e-08,
        8.9124e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 187, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7930e-01, 3.8719e-03, 3.6482e-03, 9.7141e-07, 2.7741e-03, 7.3785e-03,
        3.0288e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.257

[Epoch: 187, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1701e-03, 7.1779e-02, 9.1578e-01, 6.4315e-08, 3.2200e-03, 2.8745e-03,
        3.1793e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 187, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0133, 0.9581, 0.0063, 0.0124, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 188, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0039, 0.0039, 0.0041, 0.9663, 0.0108, 0.0072, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 188, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7302e-05, 6.6092e-01, 3.3904e-01, 1.0268e-07, 3.3386e-06, 6.1583e-08,
        6.7770e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 188, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8313e-01, 2.8651e-03, 3.0148e-03, 6.2157e-07, 2.9328e-03, 5.2434e-03,
        2.8107e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.258

[Epoch: 188, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6721e-03, 8.9068e-02, 8.9595e-01, 5.4978e-08, 3.7252e-03, 3.9263e-03,
        3.6617e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 188, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0187, 0.9450, 0.0063, 0.0193, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 189, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.0039, 0.9711, 0.0099, 0.0060, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 189, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2925e-05, 7.0391e-01, 2.9606e-01, 6.5768e-08, 3.0281e-06, 7.1594e-08,
        6.7040e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.969

[Epoch: 189, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7484e-01, 4.0853e-03, 3.6039e-03, 7.0925e-07, 5.4585e-03, 7.5248e-03,
        4.4872e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.275

[Epoch: 189, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7495e-03, 7.4231e-02, 9.1289e-01, 5.1099e-08, 2.9866e-03, 4.1002e-03,
        3.0436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.027

[Epoch: 189, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0035, 0.0037, 0.0174, 0.9479, 0.0088, 0.0149, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 190, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.0027, 0.9713, 0.0091, 0.0061, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 190, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0379e-05, 6.7908e-01, 3.2089e-01, 3.6459e-08, 1.8969e-06, 4.8198e-08,
        4.4251e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 190, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7653e-01, 3.8201e-03, 4.3083e-03, 6.3457e-07, 3.5406e-03, 7.7250e-03,
        4.0718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.278

[Epoch: 190, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.7616e-03, 9.2469e-02, 8.8890e-01, 3.4571e-08, 5.6629e-03, 3.6459e-03,
        4.5592e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.006

[Epoch: 190, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0039, 0.0037, 0.0174, 0.9440, 0.0076, 0.0190, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 191, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.0037, 0.9669, 0.0115, 0.0075, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.009

[Epoch: 191, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1185e-05, 7.0024e-01, 2.9973e-01, 5.5361e-08, 3.6571e-06, 5.5488e-08,
        4.6859e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.975

[Epoch: 191, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8092e-01, 3.0894e-03, 3.0392e-03, 3.7964e-07, 3.4450e-03, 6.7030e-03,
        2.7994e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.253

[Epoch: 191, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1534e-03, 7.6767e-02, 9.0992e-01, 1.0081e-07, 3.4287e-03, 3.6276e-03,
        3.1031e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.021

[Epoch: 191, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0032, 0.0033, 0.0177, 0.9512, 0.0060, 0.0152, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 192, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0034, 0.0035, 0.9699, 0.0101, 0.0066, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.005

[Epoch: 192, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9996e-05, 6.8641e-01, 3.1356e-01, 6.1310e-08, 2.0889e-06, 3.4699e-08,
        5.7132e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.978

[Epoch: 192, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8394e-01, 2.8176e-03, 2.3890e-03, 4.5688e-07, 2.5093e-03, 6.2379e-03,
        2.1085e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.277

[Epoch: 192, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3924e-03, 7.3123e-02, 9.1386e-01, 6.0746e-08, 3.1755e-03, 3.5102e-03,
        2.9381e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.005

[Epoch: 192, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0033, 0.0141, 0.9560, 0.0057, 0.0153, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.010

[Epoch: 193, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0040, 0.0040, 0.0041, 0.9667, 0.0105, 0.0065, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.001

[Epoch: 193, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0348e-05, 6.8174e-01, 3.1823e-01, 5.8050e-08, 2.1278e-06, 2.0001e-08,
        4.9542e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.976

[Epoch: 193, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7750e-01, 4.1551e-03, 3.6206e-03, 1.0838e-06, 3.5146e-03, 7.3644e-03,
        3.8482e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 193, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1298e-03, 8.4410e-02, 9.0342e-01, 8.5576e-08, 2.8537e-03, 2.7657e-03,
        3.4188e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.020

[Epoch: 193, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0038, 0.0194, 0.9466, 0.0066, 0.0167, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 194, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0031, 0.0033, 0.9736, 0.0080, 0.0059, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 194, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.0100e-05, 6.7782e-01, 3.2213e-01, 1.1037e-07, 3.0891e-06, 1.2167e-07,
        7.4384e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 194, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7759e-01, 4.0611e-03, 3.7876e-03, 5.7427e-07, 4.0522e-03, 6.3908e-03,
        4.1152e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.273

[Epoch: 194, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5544e-03, 7.9518e-02, 9.0843e-01, 4.5024e-08, 3.1842e-03, 3.0124e-03,
        3.3008e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.029

[Epoch: 194, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0029, 0.0133, 0.9562, 0.0055, 0.0154, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.007

[Epoch: 195, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0028, 0.0030, 0.0025, 0.9723, 0.0092, 0.0064, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 195, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2741e-05, 6.9316e-01, 3.0680e-01, 4.1920e-08, 2.7919e-06, 3.3855e-08,
        6.9583e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 195, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7928e-01, 3.2597e-03, 3.0521e-03, 4.8972e-07, 3.1181e-03, 7.7778e-03,
        3.5105e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.263

[Epoch: 195, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3395e-03, 8.3853e-02, 9.0254e-01, 5.3565e-08, 3.2383e-03, 3.8515e-03,
        3.1743e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.017

[Epoch: 195, batch: 215/219] total loss per batch: 0.610
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0025, 0.0032, 0.0246, 0.9325, 0.0053, 0.0291, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.011

[Epoch: 196, batch: 43/219] total loss per batch: 0.630
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0029, 0.0024, 0.0033, 0.9695, 0.0098, 0.0097, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.006

[Epoch: 196, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9048e-05, 6.8846e-01, 3.1150e-01, 4.8030e-07, 2.7091e-06, 1.2909e-07,
        9.6755e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.979

[Epoch: 196, batch: 129/219] total loss per batch: 0.587
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8164e-01, 3.0244e-03, 3.4022e-03, 4.8528e-07, 3.0568e-03, 6.1217e-03,
        2.7587e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.251

[Epoch: 196, batch: 172/219] total loss per batch: 0.581
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5950e-03, 8.2466e-02, 9.0344e-01, 1.5239e-07, 4.0377e-03, 2.6881e-03,
        3.7679e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.008

[Epoch: 196, batch: 215/219] total loss per batch: 0.611
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.0125, 0.9651, 0.0050, 0.0080, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.012

[Epoch: 197, batch: 43/219] total loss per batch: 0.625
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0033, 0.0029, 0.0034, 0.9725, 0.0103, 0.0050, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 197, batch: 86/219] total loss per batch: 0.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6183e-05, 6.9212e-01, 3.0785e-01, 3.4335e-07, 3.3765e-06, 1.3066e-07,
        2.9035e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 197, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7979e-01, 3.3873e-03, 3.2239e-03, 4.5035e-07, 3.5801e-03, 6.9613e-03,
        3.0598e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.266

[Epoch: 197, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6359e-03, 7.5634e-02, 9.1105e-01, 1.5253e-07, 3.9501e-03, 3.0615e-03,
        3.6660e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.002

[Epoch: 197, batch: 215/219] total loss per batch: 0.607
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0036, 0.0038, 0.0134, 0.9551, 0.0074, 0.0124, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.009

[Epoch: 198, batch: 43/219] total loss per batch: 0.624
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0026, 0.9732, 0.0091, 0.0060, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.006

[Epoch: 198, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4851e-05, 6.7787e-01, 3.2210e-01, 2.8528e-07, 3.0462e-06, 1.2624e-07,
        3.0673e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 198, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.7916e-01, 3.4421e-03, 3.2982e-03, 4.3465e-07, 3.9614e-03, 6.8602e-03,
        3.2805e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.265

[Epoch: 198, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0251e-03, 7.6525e-02, 9.0975e-01, 1.1905e-07, 3.9816e-03, 3.0539e-03,
        3.6629e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 198, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0145, 0.9513, 0.0073, 0.0155, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 199, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0031, 0.0031, 0.0027, 0.9717, 0.0101, 0.0061, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 199, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2851e-05, 6.8900e-01, 3.1097e-01, 2.5749e-07, 2.5162e-06, 1.3936e-07,
        3.1673e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.973

[Epoch: 199, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8017e-01, 3.4210e-03, 3.1322e-03, 3.9383e-07, 3.6772e-03, 6.4811e-03,
        3.1176e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.270

[Epoch: 199, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2271e-03, 8.2209e-02, 9.0381e-01, 1.0882e-07, 3.9551e-03, 3.1364e-03,
        3.6674e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.013

[Epoch: 199, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0036, 0.0149, 0.9517, 0.0066, 0.0161, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

[Epoch: 200, batch: 43/219] total loss per batch: 0.623
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.9700, 0.0100, 0.0067, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0028, 0.9704, 0.0103, 0.0065, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 200, batch: 86/219] total loss per batch: 0.609
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.6867, 0.3133, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0722e-05, 6.8356e-01, 3.1641e-01, 1.7639e-07, 1.9855e-06, 9.8921e-08,
        2.7580e-06], grad_fn=<SelectBackward>)Value (actual, predicted): 0.975 0.974

[Epoch: 200, batch: 129/219] total loss per batch: 0.586
Policy (actual, predicted): 0 0
Policy data: tensor([0.9800, 0.0033, 0.0033, 0.0000, 0.0033, 0.0067, 0.0033])
Policy pred: tensor([9.8022e-01, 3.4351e-03, 3.1526e-03, 3.8346e-07, 3.6094e-03, 6.4085e-03,
        3.1698e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.267 -0.268

[Epoch: 200, batch: 172/219] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0800, 0.9067, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2959e-03, 8.2075e-02, 9.0387e-01, 8.6094e-08, 3.8479e-03, 3.3042e-03,
        3.6035e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.016 -0.012

[Epoch: 200, batch: 215/219] total loss per batch: 0.606
Policy (actual, predicted): 3 3
Policy data: tensor([0.0033, 0.0033, 0.0167, 0.9500, 0.0067, 0.0167, 0.0033])
Policy pred: tensor([0.0033, 0.0037, 0.0160, 0.9497, 0.0069, 0.0167, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.009 -0.008

