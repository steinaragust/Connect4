Training set samples: 6654
Batch size: 32
[Epoch: 1, batch: 41/208] total loss per batch: 0.998
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([1.2130e-02, 2.1388e-01, 5.7392e-02, 2.2925e-09, 3.2949e-01, 7.9644e-02,
        3.0746e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 1, batch: 82/208] total loss per batch: 1.073
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0149, 0.0979, 0.1535, 0.4171, 0.0140, 0.1973, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 1, batch: 123/208] total loss per batch: 1.011
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([4.3085e-01, 3.2490e-05, 1.4959e-01, 9.7416e-08, 2.0435e-06, 3.4354e-01,
        7.5981e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.058

[Epoch: 1, batch: 164/208] total loss per batch: 0.961
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.3569e-03, 3.9670e-02, 9.1985e-01, 5.1176e-06, 8.0491e-03, 8.0210e-03,
        2.0048e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 1, batch: 205/208] total loss per batch: 0.945
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([7.1010e-02, 4.4117e-02, 7.1086e-02, 1.4500e-07, 2.8472e-01, 4.9871e-01,
        3.0355e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.076

[Epoch: 2, batch: 41/208] total loss per batch: 0.760
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([9.2103e-03, 1.6765e-01, 6.2657e-02, 5.4707e-09, 3.8375e-01, 8.8503e-02,
        2.8823e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 2, batch: 82/208] total loss per batch: 0.803
Policy (actual, predicted): 2 5
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0120, 0.1266, 0.2334, 0.1376, 0.0161, 0.3854, 0.0890],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 2, batch: 123/208] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.0941e-01, 4.9429e-05, 1.3759e-01, 1.1296e-06, 1.1120e-06, 2.9557e-01,
        5.7382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.245

[Epoch: 2, batch: 164/208] total loss per batch: 0.744
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6263e-03, 4.2057e-02, 9.2190e-01, 1.0728e-06, 6.5422e-03, 7.4005e-03,
        1.9477e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 2, batch: 205/208] total loss per batch: 0.719
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([1.2209e-01, 3.6374e-02, 9.9661e-02, 4.7296e-08, 2.5707e-01, 4.5701e-01,
        2.7797e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.083

[Epoch: 3, batch: 41/208] total loss per batch: 0.647
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([9.2434e-03, 1.0823e-01, 6.5568e-02, 2.8053e-09, 3.6688e-01, 2.2832e-01,
        2.2176e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 3, batch: 82/208] total loss per batch: 0.668
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0108, 0.1669, 0.3135, 0.3349, 0.0153, 0.0957, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.015

[Epoch: 3, batch: 123/208] total loss per batch: 0.645
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7878e-01, 1.4982e-05, 7.6939e-02, 6.6083e-07, 2.8569e-07, 1.7921e-01,
        6.5052e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.029

[Epoch: 3, batch: 164/208] total loss per batch: 0.641
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9182e-03, 2.7383e-02, 9.4568e-01, 9.1889e-07, 4.6758e-03, 5.3723e-03,
        1.4967e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.047

[Epoch: 3, batch: 205/208] total loss per batch: 0.621
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([2.1343e-01, 3.2138e-02, 8.4820e-02, 2.0359e-08, 2.0990e-01, 4.3703e-01,
        2.2672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.105

[Epoch: 4, batch: 41/208] total loss per batch: 0.609
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.1369e-03, 4.9437e-02, 7.6608e-02, 2.4368e-09, 3.7237e-01, 1.8054e-01,
        3.1491e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 4, batch: 82/208] total loss per batch: 0.623
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0128, 0.2297, 0.3357, 0.1888, 0.0187, 0.1557, 0.0586],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.016

[Epoch: 4, batch: 123/208] total loss per batch: 0.609
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([8.3926e-01, 1.9314e-05, 3.0717e-02, 1.6627e-06, 1.5233e-07, 7.8069e-02,
        5.1931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.161

[Epoch: 4, batch: 164/208] total loss per batch: 0.609
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([7.8321e-04, 8.0233e-03, 9.7980e-01, 1.0669e-06, 1.6553e-03, 2.6150e-03,
        7.1226e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 4, batch: 205/208] total loss per batch: 0.598
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([2.8346e-01, 1.7186e-02, 6.0366e-02, 5.5662e-09, 1.6245e-01, 4.6435e-01,
        1.2187e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.085

[Epoch: 5, batch: 41/208] total loss per batch: 0.591
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.6668e-03, 3.8425e-02, 8.4901e-02, 1.9778e-09, 5.1355e-01, 2.9202e-01,
        6.3438e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 5, batch: 82/208] total loss per batch: 0.603
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0094, 0.1937, 0.3607, 0.2154, 0.0136, 0.1542, 0.0530],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 5, batch: 123/208] total loss per batch: 0.590
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.0146e-01, 1.3093e-05, 3.5400e-02, 9.4340e-07, 1.9118e-07, 1.1043e-01,
        2.5269e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.038

[Epoch: 5, batch: 164/208] total loss per batch: 0.592
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.3194e-03, 1.5303e-02, 9.6614e-01, 1.1757e-06, 5.6182e-03, 3.4688e-03,
        8.1541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.025

[Epoch: 5, batch: 205/208] total loss per batch: 0.580
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.2901e-01, 2.3189e-02, 7.2742e-02, 2.8189e-08, 1.6709e-01, 3.9093e-01,
        1.7042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.077

[Epoch: 6, batch: 41/208] total loss per batch: 0.578
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.3169e-03, 5.6361e-02, 6.3827e-02, 4.3868e-09, 4.5197e-01, 2.3496e-01,
        1.8556e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 6, batch: 82/208] total loss per batch: 0.591
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0093, 0.2630, 0.2806, 0.2584, 0.0123, 0.1353, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 6, batch: 123/208] total loss per batch: 0.577
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([8.4287e-01, 6.3410e-06, 1.0376e-02, 1.7137e-06, 1.0902e-07, 2.9803e-02,
        1.1695e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.352

[Epoch: 6, batch: 164/208] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([6.2484e-04, 6.1896e-03, 9.7989e-01, 1.2671e-06, 3.5049e-03, 2.8574e-03,
        6.9355e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 6, batch: 205/208] total loss per batch: 0.567
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0604e-01, 1.7807e-02, 6.2368e-02, 2.2618e-08, 1.7763e-01, 3.2608e-01,
        1.0085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.062

[Epoch: 7, batch: 41/208] total loss per batch: 0.572
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.8107e-03, 4.5941e-02, 6.8728e-02, 8.2011e-09, 4.0730e-01, 3.5193e-01,
        1.1829e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 7, batch: 82/208] total loss per batch: 0.584
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0092, 0.2151, 0.3143, 0.2850, 0.0155, 0.1017, 0.0593],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.018

[Epoch: 7, batch: 123/208] total loss per batch: 0.567
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.3128e-01, 3.0278e-05, 2.4584e-02, 1.7933e-06, 1.7726e-07, 9.9682e-02,
        3.4442e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.104

[Epoch: 7, batch: 164/208] total loss per batch: 0.573
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([8.6338e-04, 9.4652e-03, 9.7572e-01, 2.4753e-06, 3.8510e-03, 3.3516e-03,
        6.7458e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 7, batch: 205/208] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.8189e-01, 1.8011e-02, 6.2586e-02, 5.8288e-08, 1.5602e-01, 3.6664e-01,
        1.4853e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 8, batch: 41/208] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.1664e-03, 4.7219e-02, 4.5749e-02, 5.7078e-09, 4.8582e-01, 2.9362e-01,
        1.2043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 8, batch: 82/208] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0070, 0.2196, 0.3844, 0.2140, 0.0107, 0.1309, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.018

[Epoch: 8, batch: 123/208] total loss per batch: 0.563
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7198e-01, 2.5519e-05, 2.0216e-02, 1.4623e-06, 2.0090e-07, 2.8985e-02,
        2.7879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.022

[Epoch: 8, batch: 164/208] total loss per batch: 0.568
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.2987e-03, 1.1466e-02, 9.6470e-01, 3.0418e-06, 7.9626e-03, 4.4519e-03,
        1.0122e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 8, batch: 205/208] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1156e-01, 1.4221e-02, 6.1403e-02, 2.6489e-08, 2.1254e-01, 2.8894e-01,
        1.1329e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.058

[Epoch: 9, batch: 41/208] total loss per batch: 0.558
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([9.6629e-03, 5.0117e-02, 8.4631e-02, 1.1394e-08, 4.0680e-01, 2.4107e-01,
        2.0772e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 9, batch: 82/208] total loss per batch: 0.577
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0087, 0.2619, 0.2950, 0.2773, 0.0119, 0.1053, 0.0399],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.017

[Epoch: 9, batch: 123/208] total loss per batch: 0.559
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9665e-01, 1.7764e-05, 1.6292e-02, 1.3038e-06, 1.2246e-07, 2.0717e-02,
        2.6632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.079

[Epoch: 9, batch: 164/208] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.2450e-03, 6.7472e-03, 9.7569e-01, 3.0983e-06, 5.0702e-03, 3.0951e-03,
        8.1488e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 9, batch: 205/208] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2597e-01, 1.1667e-02, 5.7742e-02, 3.9501e-08, 1.6155e-01, 3.3014e-01,
        1.2943e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.080

[Epoch: 10, batch: 41/208] total loss per batch: 0.555
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.7266e-03, 3.2667e-02, 4.0964e-02, 5.1337e-09, 4.6670e-01, 3.9850e-01,
        5.5451e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 10, batch: 82/208] total loss per batch: 0.573
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0064, 0.2642, 0.3485, 0.2104, 0.0088, 0.1266, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.017

[Epoch: 10, batch: 123/208] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3287e-01, 1.7964e-05, 1.1523e-02, 1.7790e-06, 1.2069e-07, 1.5056e-02,
        2.4053e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.073

[Epoch: 10, batch: 164/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.0403e-03, 6.2082e-03, 9.7750e-01, 5.3331e-06, 3.9392e-03, 3.6048e-03,
        7.7058e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.011

[Epoch: 10, batch: 205/208] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4584e-01, 1.1045e-02, 6.8991e-02, 6.6256e-08, 1.9422e-01, 2.6630e-01,
        1.3602e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.067

[Epoch: 11, batch: 41/208] total loss per batch: 0.554
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.8842e-03, 3.9642e-02, 4.5773e-02, 4.9553e-08, 4.8494e-01, 2.4456e-01,
        1.7721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 11, batch: 82/208] total loss per batch: 0.570
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0079, 0.2322, 0.3002, 0.2768, 0.0127, 0.1208, 0.0494],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 11, batch: 123/208] total loss per batch: 0.557
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6090e-01, 2.8409e-05, 1.6103e-02, 2.6477e-06, 1.6577e-07, 2.3996e-02,
        2.9897e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.071

[Epoch: 11, batch: 164/208] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.1778e-03, 4.8311e-03, 9.8214e-01, 5.3975e-06, 4.4336e-03, 2.7959e-03,
        4.6157e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.015

[Epoch: 11, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9492e-01, 1.0501e-02, 7.0611e-02, 5.1630e-08, 1.5151e-01, 3.5982e-01,
        1.2638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.078

[Epoch: 12, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.9345e-03, 7.1370e-02, 6.8960e-02, 8.5766e-09, 3.4548e-01, 3.9581e-01,
        1.1244e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.014

[Epoch: 12, batch: 82/208] total loss per batch: 0.570
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0061, 0.2886, 0.3485, 0.2158, 0.0094, 0.0936, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.014

[Epoch: 12, batch: 123/208] total loss per batch: 0.556
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([8.0372e-01, 2.6329e-05, 8.3989e-03, 3.6713e-06, 2.1905e-07, 1.6482e-02,
        1.7137e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.143

[Epoch: 12, batch: 164/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.0430e-03, 3.2679e-03, 9.8051e-01, 3.9954e-06, 3.3117e-03, 2.7684e-03,
        9.0972e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.004

[Epoch: 12, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2154e-01, 1.0290e-02, 7.3262e-02, 7.5928e-08, 2.2220e-01, 2.6009e-01,
        1.2623e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.062

[Epoch: 13, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.6184e-03, 2.1188e-02, 5.2048e-02, 3.3830e-08, 7.2436e-01, 1.3718e-01,
        6.0605e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 13, batch: 82/208] total loss per batch: 0.570
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0073, 0.3026, 0.2996, 0.2204, 0.0122, 0.1169, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 13, batch: 123/208] total loss per batch: 0.557
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6561e-01, 2.5583e-05, 2.0256e-02, 3.4836e-06, 1.9811e-07, 2.8821e-02,
        2.8528e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.017

[Epoch: 13, batch: 164/208] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.7130e-03, 1.0861e-02, 9.7070e-01, 7.7999e-06, 6.4306e-03, 3.7559e-03,
        6.5338e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 13, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9923e-01, 9.6401e-03, 5.7931e-02, 8.4187e-08, 1.8966e-01, 3.3437e-01,
        9.1710e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.064

[Epoch: 14, batch: 41/208] total loss per batch: 0.555
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([9.6388e-03, 6.8932e-02, 5.7120e-02, 2.7850e-08, 1.2661e-01, 5.5645e-01,
        1.8125e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 14, batch: 82/208] total loss per batch: 0.571
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0078, 0.1940, 0.3029, 0.3866, 0.0088, 0.0685, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 14, batch: 123/208] total loss per batch: 0.557
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4492e-01, 2.8950e-05, 1.2632e-02, 3.8038e-06, 4.8596e-07, 1.7004e-02,
        3.2541e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.111

[Epoch: 14, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.0663e-03, 2.3035e-03, 9.8426e-01, 4.0448e-06, 3.4084e-03, 3.1290e-03,
        5.8331e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.021

[Epoch: 14, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.8184e-01, 6.5526e-03, 4.6082e-02, 3.6152e-08, 2.2914e-01, 3.2446e-01,
        1.1923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.056

[Epoch: 15, batch: 41/208] total loss per batch: 0.555
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.7794e-03, 5.0913e-02, 4.1757e-02, 2.7273e-08, 5.3725e-01, 2.2366e-01,
        1.3964e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 15, batch: 82/208] total loss per batch: 0.572
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0070, 0.2763, 0.4480, 0.0804, 0.0130, 0.1389, 0.0364],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 15, batch: 123/208] total loss per batch: 0.557
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5702e-01, 2.7453e-05, 1.2794e-02, 9.5737e-07, 2.6948e-07, 2.3852e-02,
        2.0631e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.124

[Epoch: 15, batch: 164/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.2780e-03, 5.9659e-03, 9.7870e-01, 6.4521e-06, 5.5213e-03, 2.4624e-03,
        6.0648e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.029

[Epoch: 15, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5074e-01, 1.1278e-02, 5.5609e-02, 9.7306e-08, 1.4815e-01, 3.1897e-01,
        1.5252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.081

[Epoch: 16, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([7.5130e-03, 4.0336e-02, 7.7299e-02, 3.1004e-08, 4.1372e-01, 3.0632e-01,
        1.5481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 16, batch: 82/208] total loss per batch: 0.570
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0080, 0.2586, 0.2210, 0.3765, 0.0095, 0.0879, 0.0384],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 16, batch: 123/208] total loss per batch: 0.556
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5910e-01, 3.2799e-05, 5.8583e-03, 4.9223e-06, 3.6335e-07, 1.1257e-02,
        2.2374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.222

[Epoch: 16, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.3504e-03, 5.1351e-03, 9.7619e-01, 8.6836e-06, 5.7040e-03, 3.6418e-03,
        7.9681e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 16, batch: 205/208] total loss per batch: 0.550
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.6113e-01, 6.3354e-03, 5.7266e-02, 6.6905e-08, 1.9770e-01, 3.6652e-01,
        1.1054e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.071

[Epoch: 17, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.1087e-03, 8.0167e-02, 4.3425e-02, 2.6816e-08, 4.6577e-01, 2.8705e-01,
        1.1747e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 17, batch: 82/208] total loss per batch: 0.571
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0093, 0.2345, 0.4700, 0.0791, 0.0191, 0.1534, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.019

[Epoch: 17, batch: 123/208] total loss per batch: 0.555
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.5737e-01, 5.0849e-05, 1.9881e-02, 1.8065e-06, 4.1892e-07, 2.9621e-02,
        2.9307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.012

[Epoch: 17, batch: 164/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.0339e-03, 2.8533e-03, 9.8639e-01, 5.1120e-06, 4.1779e-03, 1.9620e-03,
        3.5814e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 17, batch: 205/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2857e-01, 4.6224e-03, 5.4076e-02, 1.4622e-07, 1.8112e-01, 3.1846e-01,
        1.3152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.069

[Epoch: 18, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4180e-03, 3.1499e-02, 6.4128e-02, 2.7808e-08, 4.1507e-01, 3.2204e-01,
        1.6385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 18, batch: 82/208] total loss per batch: 0.571
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0061, 0.2633, 0.1927, 0.4142, 0.0081, 0.0894, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.017

[Epoch: 18, batch: 123/208] total loss per batch: 0.554
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9146e-01, 1.8020e-05, 6.6941e-03, 1.5019e-06, 2.5268e-07, 2.7761e-02,
        2.7406e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.188

[Epoch: 18, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8146e-03, 3.0248e-03, 9.8098e-01, 4.7231e-06, 6.3629e-03, 3.1915e-03,
        4.6222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.023

[Epoch: 18, batch: 205/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9122e-01, 7.3527e-03, 7.2278e-02, 1.7230e-07, 1.9378e-01, 3.2292e-01,
        1.2452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.072

[Epoch: 19, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0973e-03, 3.5563e-02, 2.2496e-02, 7.3428e-09, 5.5256e-01, 2.7310e-01,
        1.1319e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 19, batch: 82/208] total loss per batch: 0.573
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0099, 0.2182, 0.4641, 0.1570, 0.0129, 0.1045, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 19, batch: 123/208] total loss per batch: 0.555
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4782e-01, 7.6272e-05, 6.4453e-03, 7.0983e-06, 9.4586e-07, 1.9077e-02,
        3.2657e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.038

[Epoch: 19, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([9.3078e-04, 3.9328e-03, 9.8290e-01, 1.1828e-05, 5.1361e-03, 2.6720e-03,
        4.4202e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.013

[Epoch: 19, batch: 205/208] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3120e-01, 8.1020e-03, 6.1007e-02, 2.2762e-07, 2.2657e-01, 2.6413e-01,
        8.9838e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.078

[Epoch: 20, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.2154e-03, 5.7567e-02, 9.5299e-02, 2.4391e-07, 3.0761e-01, 3.7660e-01,
        1.5671e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 20, batch: 82/208] total loss per batch: 0.572
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0079, 0.2806, 0.2519, 0.3025, 0.0110, 0.1144, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 20, batch: 123/208] total loss per batch: 0.556
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.9722e-01, 2.5215e-05, 1.1652e-02, 2.4890e-06, 4.0142e-07, 1.7485e-02,
        1.7361e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.187

[Epoch: 20, batch: 164/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6857e-03, 8.3757e-03, 9.6849e-01, 9.9706e-06, 8.3506e-03, 3.8175e-03,
        8.2676e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.010

[Epoch: 20, batch: 205/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0702e-01, 8.8392e-03, 6.7179e-02, 5.3304e-07, 1.5441e-01, 3.5051e-01,
        1.2047e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.089

[Epoch: 21, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.5389e-03, 5.5642e-02, 3.9374e-02, 1.3118e-07, 5.7241e-01, 2.1025e-01,
        1.1779e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.018

[Epoch: 21, batch: 82/208] total loss per batch: 0.572
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0097, 0.2756, 0.3785, 0.1782, 0.0078, 0.1208, 0.0293],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.004

[Epoch: 21, batch: 123/208] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.6688e-01, 9.4438e-05, 2.6538e-02, 6.3205e-06, 9.1997e-07, 3.1661e-02,
        3.7482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.029

[Epoch: 21, batch: 164/208] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0196e-03, 3.4749e-03, 9.8452e-01, 4.5323e-05, 3.7791e-03, 1.5065e-03,
        4.6535e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.037

[Epoch: 21, batch: 205/208] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1123e-01, 7.1103e-03, 5.3749e-02, 1.2145e-07, 1.4494e-01, 3.6663e-01,
        1.6332e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.061

[Epoch: 22, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.2561e-03, 4.2107e-02, 4.5015e-02, 2.1860e-08, 3.7739e-01, 3.7580e-01,
        1.5343e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 22, batch: 82/208] total loss per batch: 0.571
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0063, 0.2839, 0.2525, 0.3314, 0.0087, 0.0820, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 22, batch: 123/208] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.6369e-01, 4.1699e-05, 8.2266e-03, 1.0540e-06, 5.9979e-07, 1.5934e-02,
        2.1211e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.072

[Epoch: 22, batch: 164/208] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.6115e-03, 2.9989e-03, 9.8459e-01, 1.2342e-05, 2.6222e-03, 2.8292e-03,
        5.3404e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.011

[Epoch: 22, batch: 205/208] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4514e-01, 7.7784e-03, 6.6189e-02, 1.7392e-07, 1.6626e-01, 3.0084e-01,
        1.3796e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.072

[Epoch: 23, batch: 41/208] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.4341e-03, 4.7645e-02, 6.2744e-02, 4.6038e-08, 4.6920e-01, 2.9451e-01,
        1.1946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 23, batch: 82/208] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0097, 0.1804, 0.4264, 0.2647, 0.0151, 0.0793, 0.0243],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 23, batch: 123/208] total loss per batch: 0.619
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.3052e-01, 1.8401e-04, 9.8529e-03, 1.3335e-05, 2.1703e-06, 1.0897e-02,
        3.4853e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.174

[Epoch: 23, batch: 164/208] total loss per batch: 0.604
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.0580e-03, 5.7858e-04, 9.9383e-01, 4.0288e-05, 1.1912e-03, 4.5381e-04,
        2.8482e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.026

[Epoch: 23, batch: 205/208] total loss per batch: 0.593
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([2.2842e-01, 4.0868e-03, 6.2626e-02, 5.2699e-07, 2.8327e-01, 4.0632e-01,
        1.5268e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.026

[Epoch: 24, batch: 41/208] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([9.0459e-03, 6.3249e-02, 8.6297e-02, 1.2027e-06, 4.5413e-01, 2.5188e-01,
        1.3540e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.016

[Epoch: 24, batch: 82/208] total loss per batch: 0.624
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0056, 0.2424, 0.2994, 0.2055, 0.0123, 0.1892, 0.0457],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.023

[Epoch: 24, batch: 123/208] total loss per batch: 0.603
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.5634e-01, 1.4265e-04, 1.4589e-02, 9.6824e-06, 9.7920e-07, 2.3822e-01,
        9.0698e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.159

[Epoch: 24, batch: 164/208] total loss per batch: 0.607
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.8843e-03, 7.2859e-02, 8.7343e-01, 7.9918e-04, 2.9736e-02, 4.0256e-03,
        1.4271e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 24, batch: 205/208] total loss per batch: 0.584
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.3348e-01, 3.8618e-03, 6.3506e-02, 8.4859e-07, 1.5175e-01, 4.3385e-01,
        1.3547e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 25, batch: 41/208] total loss per batch: 0.579
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.5746e-03, 2.8432e-02, 1.2305e-01, 2.4989e-07, 2.9732e-01, 3.3737e-01,
        2.0825e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.034

[Epoch: 25, batch: 82/208] total loss per batch: 0.602
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0072, 0.1899, 0.3218, 0.3809, 0.0050, 0.0640, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.033

[Epoch: 25, batch: 123/208] total loss per batch: 0.578
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6861e-01, 3.5067e-04, 2.4570e-02, 3.7400e-06, 7.1977e-06, 8.6020e-02,
        2.2043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.070

[Epoch: 25, batch: 164/208] total loss per batch: 0.583
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0938e-03, 3.7413e-03, 9.8471e-01, 1.1497e-06, 2.3119e-03, 1.5335e-03,
        5.6132e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 25, batch: 205/208] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.7495e-01, 3.1942e-03, 4.7700e-02, 3.1042e-07, 2.4897e-01, 3.1231e-01,
        1.2878e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 26, batch: 41/208] total loss per batch: 0.559
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9134e-03, 3.4875e-02, 3.2641e-02, 4.6854e-07, 6.9361e-01, 1.9356e-01,
        4.1399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.026

[Epoch: 26, batch: 82/208] total loss per batch: 0.574
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0080, 0.3089, 0.2918, 0.2062, 0.0109, 0.1315, 0.0427],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.030

[Epoch: 26, batch: 123/208] total loss per batch: 0.558
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.7916e-01, 8.3039e-05, 2.3026e-02, 6.3188e-06, 4.5184e-06, 5.7688e-02,
        3.4004e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.051

[Epoch: 26, batch: 164/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1041e-03, 2.8045e-03, 9.8024e-01, 4.7146e-06, 4.2517e-03, 3.0850e-03,
        7.5120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.009

[Epoch: 26, batch: 205/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0158e-01, 4.4240e-03, 5.4296e-02, 2.8339e-07, 1.7608e-01, 3.4666e-01,
        1.6956e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.123

[Epoch: 27, batch: 41/208] total loss per batch: 0.549
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([8.4561e-03, 5.9513e-02, 5.8574e-02, 3.7762e-07, 2.5224e-01, 4.9626e-01,
        1.2496e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.018

[Epoch: 27, batch: 82/208] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0083, 0.2897, 0.3253, 0.2293, 0.0100, 0.1007, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.019

[Epoch: 27, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.7661e-01, 7.8568e-05, 1.1509e-02, 5.9477e-06, 3.2399e-06, 2.5902e-02,
        1.8589e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.049

[Epoch: 27, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4591e-03, 3.6187e-03, 9.8128e-01, 2.0410e-06, 5.3306e-03, 2.9680e-03,
        4.3441e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.005

[Epoch: 27, batch: 205/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3038e-01, 3.9775e-03, 5.2787e-02, 2.2806e-07, 1.9546e-01, 3.0439e-01,
        1.3011e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 28, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.2760e-03, 4.8633e-02, 6.2853e-02, 1.9156e-07, 4.8571e-01, 3.0155e-01,
        9.5980e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.016

[Epoch: 28, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0074, 0.2299, 0.3651, 0.2426, 0.0110, 0.1101, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.014

[Epoch: 28, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1201e-01, 7.7695e-05, 1.1744e-02, 4.7401e-06, 2.1741e-06, 2.5648e-02,
        2.5051e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.074

[Epoch: 28, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2925e-03, 3.6364e-03, 9.8214e-01, 1.6604e-06, 4.6653e-03, 2.8786e-03,
        4.3892e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 28, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3759e-01, 3.8443e-03, 5.8858e-02, 1.6334e-07, 1.8255e-01, 3.0618e-01,
        1.0981e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 29, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.3795e-03, 5.4924e-02, 6.5294e-02, 1.7607e-07, 4.3963e-01, 2.9646e-01,
        1.3831e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.011

[Epoch: 29, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0063, 0.2798, 0.3303, 0.2272, 0.0095, 0.1126, 0.0343],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 29, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2581e-01, 5.6889e-05, 1.1404e-02, 3.2404e-06, 1.6083e-06, 1.8747e-02,
        2.4398e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.084

[Epoch: 29, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0051e-03, 3.9813e-03, 9.8198e-01, 1.3732e-06, 4.7857e-03, 2.8105e-03,
        4.4326e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 29, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3857e-01, 4.2642e-03, 5.8952e-02, 1.5542e-07, 1.7878e-01, 3.0802e-01,
        1.1427e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 30, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.3441e-03, 4.4501e-02, 5.1401e-02, 1.4722e-07, 4.6009e-01, 3.1975e-01,
        1.1891e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.012

[Epoch: 30, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0063, 0.2432, 0.3454, 0.2620, 0.0090, 0.0990, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.012

[Epoch: 30, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2253e-01, 5.3020e-05, 9.8818e-03, 2.9771e-06, 1.4656e-06, 1.8421e-02,
        2.4911e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.103

[Epoch: 30, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0799e-03, 3.7661e-03, 9.8171e-01, 1.0539e-06, 5.0786e-03, 3.0960e-03,
        4.2643e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 30, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4102e-01, 4.0091e-03, 6.1479e-02, 1.0406e-07, 1.8236e-01, 3.0109e-01,
        1.0042e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 31, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.9264e-03, 5.4443e-02, 5.3121e-02, 9.0550e-08, 4.5283e-01, 3.0050e-01,
        1.3419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 31, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.3028, 0.3259, 0.2239, 0.0085, 0.1017, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.009

[Epoch: 31, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0594e-01, 4.4363e-05, 1.1170e-02, 1.7309e-06, 1.0660e-06, 1.6615e-02,
        2.6623e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.100

[Epoch: 31, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1463e-03, 3.9507e-03, 9.8181e-01, 1.3031e-06, 4.5283e-03, 3.3926e-03,
        4.1739e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 31, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4086e-01, 3.4711e-03, 5.9406e-02, 1.0672e-07, 1.7129e-01, 3.1530e-01,
        9.6688e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 32, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.1272e-03, 4.3818e-02, 5.3618e-02, 1.0553e-07, 4.5886e-01, 3.1173e-01,
        1.2684e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.010

[Epoch: 32, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.2340, 0.3663, 0.2517, 0.0081, 0.1020, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.010

[Epoch: 32, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1699e-01, 4.2276e-05, 9.8291e-03, 1.8112e-06, 1.4842e-06, 1.7181e-02,
        2.5596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.118

[Epoch: 32, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1715e-03, 3.5638e-03, 9.8156e-01, 9.2379e-07, 5.5090e-03, 3.0412e-03,
        4.1580e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 32, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2061e-01, 3.8054e-03, 5.9769e-02, 7.7728e-08, 1.8776e-01, 3.1798e-01,
        1.0071e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.089

[Epoch: 33, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2835e-03, 4.8272e-02, 5.9885e-02, 7.1573e-08, 4.2761e-01, 3.1469e-01,
        1.4526e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.005

[Epoch: 33, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0055, 0.2756, 0.3116, 0.2451, 0.0105, 0.1160, 0.0357],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.009

[Epoch: 33, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2034e-01, 3.3551e-05, 1.1980e-02, 8.7204e-07, 9.9484e-07, 1.5440e-02,
        2.5221e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.114

[Epoch: 33, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9358e-03, 3.7658e-03, 9.8428e-01, 9.3850e-07, 3.9008e-03, 2.3287e-03,
        3.7835e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.008

[Epoch: 33, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3152e-01, 4.2007e-03, 5.7954e-02, 1.7016e-07, 1.7498e-01, 3.2189e-01,
        9.4611e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.083

[Epoch: 34, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3741e-03, 4.5877e-02, 3.8465e-02, 9.1641e-08, 5.0761e-01, 2.9179e-01,
        1.1189e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 34, batch: 82/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0055, 0.2320, 0.3501, 0.2719, 0.0065, 0.1017, 0.0323],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.014

[Epoch: 34, batch: 123/208] total loss per batch: 0.548
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1720e-01, 2.6246e-05, 1.0036e-02, 2.2497e-06, 1.7594e-06, 1.7240e-02,
        2.5549e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.126

[Epoch: 34, batch: 164/208] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4756e-03, 3.5238e-03, 9.8269e-01, 1.2744e-06, 4.5294e-03, 2.9955e-03,
        3.7806e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 34, batch: 205/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1964e-01, 5.7236e-03, 6.6647e-02, 6.5034e-08, 1.7681e-01, 3.1924e-01,
        1.1936e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 35, batch: 41/208] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.7896e-03, 4.3684e-02, 6.7329e-02, 6.7072e-08, 3.7361e-01, 3.3225e-01,
        1.7834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 35, batch: 82/208] total loss per batch: 0.562
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0056, 0.3350, 0.3421, 0.1686, 0.0088, 0.1011, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.018

[Epoch: 35, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1424e-01, 4.1284e-05, 1.0664e-02, 1.6268e-06, 8.4986e-07, 1.3758e-02,
        2.6130e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.123

[Epoch: 35, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1284e-03, 5.9723e-03, 9.7608e-01, 1.3314e-06, 5.8625e-03, 3.3299e-03,
        5.6271e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.004

[Epoch: 35, batch: 205/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3578e-01, 4.2741e-03, 5.6744e-02, 1.4328e-07, 2.0044e-01, 2.9518e-01,
        7.5917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.068

[Epoch: 36, batch: 41/208] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9463e-03, 6.8744e-02, 5.5603e-02, 3.7507e-07, 4.5855e-01, 3.0156e-01,
        1.1159e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 36, batch: 82/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0058, 0.2061, 0.3488, 0.3074, 0.0076, 0.0955, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.014

[Epoch: 36, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0851e-01, 3.9209e-05, 1.3205e-02, 1.9373e-06, 1.3500e-06, 2.4095e-02,
        2.5415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.070

[Epoch: 36, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1128e-03, 4.0288e-03, 9.7833e-01, 6.3307e-07, 9.2004e-03, 2.9970e-03,
        3.3290e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 36, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9578e-01, 4.9176e-03, 7.9969e-02, 9.9544e-08, 1.6692e-01, 3.4045e-01,
        1.1972e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.085

[Epoch: 37, batch: 41/208] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3406e-03, 2.4415e-02, 4.0109e-02, 2.3316e-08, 5.1415e-01, 2.9598e-01,
        1.2100e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 37, batch: 82/208] total loss per batch: 0.564
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0065, 0.3112, 0.2952, 0.2447, 0.0072, 0.0977, 0.0376],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 37, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3379e-01, 3.9779e-05, 8.5133e-03, 1.5225e-06, 1.5027e-06, 1.1729e-02,
        2.4593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.188

[Epoch: 37, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8228e-03, 4.1669e-03, 9.8155e-01, 1.2455e-06, 3.8868e-03, 3.5931e-03,
        4.9762e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 37, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3535e-01, 4.3021e-03, 5.5919e-02, 1.5222e-07, 2.2527e-01, 2.6557e-01,
        1.3592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 38, batch: 41/208] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.4580e-03, 9.3320e-02, 7.7294e-02, 6.5111e-07, 3.8034e-01, 3.3097e-01,
        1.1162e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 38, batch: 82/208] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0073, 0.2343, 0.4058, 0.1867, 0.0107, 0.1212, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 38, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4457e-01, 2.4762e-05, 9.8212e-03, 1.0201e-06, 1.8212e-06, 1.3126e-02,
        2.3245e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.034

[Epoch: 38, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2564e-03, 4.0300e-03, 9.8235e-01, 1.2195e-06, 5.0608e-03, 2.2915e-03,
        4.0129e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.020

[Epoch: 38, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0431e-01, 5.3388e-03, 6.7101e-02, 1.7751e-07, 2.0747e-01, 3.0426e-01,
        1.1515e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.082

[Epoch: 39, batch: 41/208] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8984e-03, 4.2884e-02, 6.4768e-02, 8.9241e-08, 4.5046e-01, 2.6668e-01,
        1.7130e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.014

[Epoch: 39, batch: 82/208] total loss per batch: 0.564
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0076, 0.2706, 0.2379, 0.3323, 0.0082, 0.1092, 0.0342],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 39, batch: 123/208] total loss per batch: 0.549
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4357e-01, 2.6539e-05, 1.2100e-02, 1.9297e-06, 6.2052e-07, 2.4239e-02,
        3.2006e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.116

[Epoch: 39, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0762e-03, 3.7446e-03, 9.8162e-01, 2.9765e-06, 6.1160e-03, 3.1979e-03,
        3.2463e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 39, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0656e-01, 3.2206e-03, 6.7650e-02, 2.6186e-07, 2.1140e-01, 3.0197e-01,
        9.2012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 40, batch: 41/208] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.1878e-03, 4.6132e-02, 2.3469e-02, 2.4123e-07, 4.9531e-01, 2.9738e-01,
        1.3452e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 40, batch: 82/208] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0072, 0.2167, 0.3876, 0.2437, 0.0097, 0.0961, 0.0390],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.004

[Epoch: 40, batch: 123/208] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9833e-01, 3.2358e-05, 9.0954e-03, 2.6615e-06, 2.6977e-06, 1.8011e-02,
        2.7453e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.157

[Epoch: 40, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0840e-03, 3.7169e-03, 9.8474e-01, 1.6444e-06, 4.9416e-03, 2.0563e-03,
        2.4581e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.007

[Epoch: 40, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9962e-01, 4.3041e-03, 5.1065e-02, 1.5280e-07, 1.7509e-01, 3.5316e-01,
        1.6757e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.087

[Epoch: 41, batch: 41/208] total loss per batch: 0.547
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.9061e-03, 5.7738e-02, 8.5775e-02, 4.0590e-07, 3.7696e-01, 3.0749e-01,
        1.6713e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 41, batch: 82/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0068, 0.3094, 0.3696, 0.1500, 0.0102, 0.1173, 0.0367],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.003

[Epoch: 41, batch: 123/208] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.6214e-01, 8.1354e-05, 9.1652e-03, 7.9803e-06, 1.1861e-06, 1.6151e-02,
        2.1246e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.040

[Epoch: 41, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2382e-03, 4.8272e-03, 9.8174e-01, 4.2074e-06, 3.7404e-03, 3.6161e-03,
        3.8313e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.018

[Epoch: 41, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5893e-01, 6.8857e-03, 6.6786e-02, 2.5917e-07, 1.2918e-01, 3.2775e-01,
        1.0475e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 42, batch: 41/208] total loss per batch: 0.553
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.4295e-03, 3.7558e-02, 2.2220e-02, 1.3282e-07, 5.2269e-01, 3.2132e-01,
        9.1790e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.016

[Epoch: 42, batch: 82/208] total loss per batch: 0.567
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.1768, 0.2921, 0.4002, 0.0078, 0.0903, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 42, batch: 123/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6652e-01, 2.4738e-05, 1.5246e-02, 4.1306e-06, 1.1535e-06, 1.8875e-02,
        2.9933e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.118

[Epoch: 42, batch: 164/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4294e-03, 2.1977e-03, 9.8252e-01, 1.4544e-06, 6.7437e-03, 2.5713e-03,
        3.5393e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.013

[Epoch: 42, batch: 205/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1453e-01, 5.6783e-03, 7.6928e-02, 2.1656e-07, 1.3984e-01, 3.5426e-01,
        8.7595e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.105

[Epoch: 43, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2598e-03, 7.5917e-02, 6.3652e-02, 3.9175e-08, 4.3120e-01, 2.4848e-01,
        1.7649e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 43, batch: 82/208] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0067, 0.2850, 0.3320, 0.2203, 0.0066, 0.1051, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 43, batch: 123/208] total loss per batch: 0.551
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1280e-01, 8.3719e-05, 1.1242e-02, 3.9473e-06, 1.4461e-06, 1.9950e-02,
        2.5592e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.161

[Epoch: 43, batch: 164/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2527e-03, 3.9081e-03, 9.7876e-01, 5.9611e-06, 6.2863e-03, 3.6490e-03,
        5.1383e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 43, batch: 205/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.7310e-01, 6.3659e-03, 5.8185e-02, 5.6467e-07, 1.6389e-01, 2.8492e-01,
        1.3537e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 44, batch: 41/208] total loss per batch: 0.552
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.3748e-03, 2.4930e-02, 5.2102e-02, 7.7211e-07, 3.1827e-01, 4.9909e-01,
        1.0024e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 44, batch: 82/208] total loss per batch: 0.571
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.3251, 0.2853, 0.1961, 0.0084, 0.1413, 0.0381],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 44, batch: 123/208] total loss per batch: 0.554
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0192e-01, 2.8450e-05, 1.0142e-02, 1.3871e-06, 1.7497e-06, 2.7750e-02,
        2.6015e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.056

[Epoch: 44, batch: 164/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.8416e-03, 2.3866e-03, 9.8530e-01, 5.5409e-06, 3.9263e-03, 3.2667e-03,
        3.2777e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.021

[Epoch: 44, batch: 205/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1072e-01, 4.6089e-03, 4.1867e-02, 5.7108e-07, 2.1309e-01, 3.1670e-01,
        1.3020e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.079

[Epoch: 45, batch: 41/208] total loss per batch: 0.549
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3628e-03, 4.5335e-02, 6.1483e-02, 4.6799e-07, 5.1903e-01, 1.7552e-01,
        1.9427e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 45, batch: 82/208] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0065, 0.2111, 0.3818, 0.2762, 0.0086, 0.0836, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 45, batch: 123/208] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.8892e-01, 1.9815e-05, 1.2578e-02, 1.8438e-06, 1.4064e-06, 1.7548e-02,
        1.8093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.162

[Epoch: 45, batch: 164/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.7760e-03, 1.8303e-03, 9.7921e-01, 3.4172e-07, 5.5739e-03, 4.8334e-03,
        6.7724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 45, batch: 205/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0447e-01, 7.6707e-03, 4.9370e-02, 2.1172e-06, 2.1246e-01, 3.1882e-01,
        7.2128e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.062

[Epoch: 46, batch: 41/208] total loss per batch: 0.554
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([8.1993e-03, 6.1059e-02, 7.7140e-02, 3.0734e-07, 4.9930e-01, 2.3728e-01,
        1.1702e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 46, batch: 82/208] total loss per batch: 0.567
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0050, 0.2521, 0.3293, 0.2314, 0.0057, 0.1355, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.018

[Epoch: 46, batch: 123/208] total loss per batch: 0.556
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4871e-01, 4.9646e-05, 2.8130e-02, 1.0634e-05, 3.9763e-06, 3.9277e-02,
        2.8382e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.047

[Epoch: 46, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2931e-03, 1.0227e-02, 9.6948e-01, 1.8238e-06, 6.1270e-03, 5.7203e-03,
        6.1493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.034

[Epoch: 46, batch: 205/208] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0111e-01, 6.4923e-03, 7.0419e-02, 1.8251e-06, 2.1730e-01, 2.9121e-01,
        1.3470e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.103

[Epoch: 47, batch: 41/208] total loss per batch: 0.571
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7672e-02, 5.3562e-02, 2.6742e-02, 6.7317e-08, 4.2095e-01, 3.0203e-01,
        1.6904e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 47, batch: 82/208] total loss per batch: 0.586
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0063, 0.2397, 0.3377, 0.2576, 0.0053, 0.1165, 0.0369],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 47, batch: 123/208] total loss per batch: 0.570
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.7186e-01, 3.9403e-04, 9.9757e-03, 1.2715e-04, 2.4434e-05, 7.0680e-02,
        1.4694e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.076

[Epoch: 47, batch: 164/208] total loss per batch: 0.580
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5856e-03, 1.1509e-02, 9.6868e-01, 1.9186e-06, 8.6806e-03, 3.9837e-03,
        3.5579e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 47, batch: 205/208] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3467e-01, 4.2187e-03, 4.4842e-02, 2.6001e-07, 2.1708e-01, 2.8231e-01,
        1.6887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 48, batch: 41/208] total loss per batch: 0.574
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.3590e-03, 4.1095e-02, 1.3853e-01, 1.7156e-07, 4.5684e-01, 2.5686e-01,
        1.0432e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 48, batch: 82/208] total loss per batch: 0.610
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0133, 0.2756, 0.3060, 0.1990, 0.0134, 0.1389, 0.0538],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 48, batch: 123/208] total loss per batch: 0.594
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3207e-01, 6.7960e-06, 2.7983e-03, 4.0184e-07, 5.0858e-06, 1.6110e-01,
        1.0402e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.231

[Epoch: 48, batch: 164/208] total loss per batch: 0.593
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4003e-03, 4.2279e-03, 9.8365e-01, 2.9864e-07, 4.0542e-03, 2.5831e-03,
        3.0857e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 48, batch: 205/208] total loss per batch: 0.570
Policy (actual, predicted): 0 5
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.6652e-01, 8.3000e-03, 5.2260e-02, 3.2005e-06, 1.4096e-01, 4.0654e-01,
        2.5410e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.092

[Epoch: 49, batch: 41/208] total loss per batch: 0.561
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([1.3072e-03, 8.5158e-02, 3.1302e-02, 8.8029e-08, 3.2469e-01, 4.0119e-01,
        1.5636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.015

[Epoch: 49, batch: 82/208] total loss per batch: 0.582
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0064, 0.2216, 0.3167, 0.3241, 0.0069, 0.0935, 0.0308],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.009

[Epoch: 49, batch: 123/208] total loss per batch: 0.565
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4534e-01, 3.7949e-05, 8.8228e-03, 1.2529e-05, 9.3511e-06, 1.3964e-02,
        3.3182e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.079

[Epoch: 49, batch: 164/208] total loss per batch: 0.566
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6215e-03, 4.8387e-03, 9.7743e-01, 4.4513e-07, 7.6281e-03, 4.1251e-03,
        3.3550e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.020

[Epoch: 49, batch: 205/208] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0066e-01, 5.9193e-03, 6.3226e-02, 1.4216e-06, 1.8776e-01, 3.2685e-01,
        1.5573e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.063

[Epoch: 50, batch: 41/208] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.1946e-03, 5.4952e-02, 5.1444e-02, 7.8998e-08, 5.1677e-01, 2.8458e-01,
        9.0060e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.014

[Epoch: 50, batch: 82/208] total loss per batch: 0.574
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0056, 0.2479, 0.3129, 0.3157, 0.0054, 0.0777, 0.0347],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.014

[Epoch: 50, batch: 123/208] total loss per batch: 0.550
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3603e-01, 2.1275e-05, 6.9640e-03, 1.2814e-06, 3.7621e-06, 1.4801e-02,
        2.4218e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.112

[Epoch: 50, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.3280e-03, 2.7370e-03, 9.8483e-01, 4.8139e-07, 4.5694e-03, 3.3720e-03,
        3.1633e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 50, batch: 205/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.9196e-01, 5.1009e-03, 5.2979e-02, 6.6605e-07, 1.3966e-01, 2.9565e-01,
        1.4644e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.060

[Epoch: 51, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7868e-03, 5.6067e-02, 6.6749e-02, 1.5104e-07, 4.0228e-01, 3.6959e-01,
        1.0252e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.010

[Epoch: 51, batch: 82/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0059, 0.2383, 0.3699, 0.2517, 0.0067, 0.0921, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 51, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0933e-01, 2.3317e-05, 1.0858e-02, 1.7905e-06, 2.8169e-06, 1.2372e-02,
        2.6741e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.114

[Epoch: 51, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9805e-03, 4.2484e-03, 9.8193e-01, 4.5440e-07, 3.8878e-03, 4.3532e-03,
        3.6011e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 51, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1886e-01, 4.4878e-03, 5.4451e-02, 4.5279e-07, 1.7706e-01, 3.3104e-01,
        1.4101e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.063

[Epoch: 52, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7783e-03, 4.5258e-02, 4.9721e-02, 7.8138e-08, 4.7397e-01, 3.0044e-01,
        1.2784e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 52, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.2779, 0.3459, 0.2269, 0.0058, 0.1009, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.005

[Epoch: 52, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0325e-01, 1.8963e-05, 1.0074e-02, 1.2098e-06, 2.2000e-06, 1.2563e-02,
        2.7409e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.092

[Epoch: 52, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0271e-03, 4.5427e-03, 9.8071e-01, 3.7185e-07, 4.2509e-03, 4.5786e-03,
        3.8886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 52, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4755e-01, 3.9607e-03, 5.5095e-02, 3.3049e-07, 1.6219e-01, 3.1825e-01,
        1.2961e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.072

[Epoch: 53, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8100e-03, 4.6679e-02, 5.5347e-02, 6.2646e-08, 4.3519e-01, 3.2460e-01,
        1.3537e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 53, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0055, 0.2646, 0.3336, 0.2520, 0.0055, 0.1007, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 53, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1903e-01, 1.6417e-05, 9.2663e-03, 9.3186e-07, 1.8195e-06, 1.1836e-02,
        2.5985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.138

[Epoch: 53, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3258e-03, 4.7361e-03, 9.7978e-01, 3.5214e-07, 4.3366e-03, 4.7472e-03,
        4.0780e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 53, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3466e-01, 3.8360e-03, 5.5947e-02, 2.5229e-07, 1.7571e-01, 3.1745e-01,
        1.2395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.070

[Epoch: 54, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7575e-03, 4.3955e-02, 5.6715e-02, 4.4132e-08, 4.6380e-01, 3.0349e-01,
        1.2928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.009

[Epoch: 54, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2579, 0.3504, 0.2404, 0.0054, 0.1037, 0.0370],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 54, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0256e-01, 1.4324e-05, 1.0131e-02, 7.1657e-07, 1.5481e-06, 1.3002e-02,
        2.7429e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.095

[Epoch: 54, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3589e-03, 4.5799e-03, 9.8006e-01, 2.7303e-07, 4.2370e-03, 4.7796e-03,
        3.9868e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 54, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3226e-01, 3.8553e-03, 5.6872e-02, 2.0091e-07, 1.8166e-01, 3.1350e-01,
        1.1849e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.073

[Epoch: 55, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9507e-03, 4.5262e-02, 5.4771e-02, 3.3789e-08, 4.4528e-01, 3.1808e-01,
        1.3366e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 55, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0050, 0.2659, 0.3339, 0.2480, 0.0052, 0.1045, 0.0374],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 55, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2110e-01, 1.2466e-05, 9.7940e-03, 6.3437e-07, 1.3358e-06, 1.3328e-02,
        2.5576e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.136

[Epoch: 55, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3722e-03, 4.4999e-03, 9.8037e-01, 2.6628e-07, 4.1318e-03, 4.6554e-03,
        3.9745e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 55, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3451e-01, 3.6587e-03, 5.5676e-02, 1.4792e-07, 1.8111e-01, 3.1375e-01,
        1.1288e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.076

[Epoch: 56, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8750e-03, 4.4944e-02, 5.4946e-02, 2.4903e-08, 4.6212e-01, 3.0668e-01,
        1.2844e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.008

[Epoch: 56, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.2617, 0.3486, 0.2369, 0.0055, 0.1056, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 56, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9674e-01, 1.0849e-05, 1.0535e-02, 5.0450e-07, 1.2016e-06, 1.4001e-02,
        2.7872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.105

[Epoch: 56, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4441e-03, 4.4458e-03, 9.8084e-01, 2.0591e-07, 4.1050e-03, 4.4694e-03,
        3.6998e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 56, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3507e-01, 3.7657e-03, 5.7182e-02, 1.2191e-07, 1.7981e-01, 3.1308e-01,
        1.1086e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.080

[Epoch: 57, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.1387e-03, 4.5755e-02, 5.3131e-02, 1.9018e-08, 4.4818e-01, 3.1287e-01,
        1.3692e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 57, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0047, 0.2614, 0.3292, 0.2560, 0.0051, 0.1057, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 57, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2284e-01, 9.6152e-06, 1.0605e-02, 4.0554e-07, 9.9557e-07, 1.4595e-02,
        2.5195e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.122

[Epoch: 57, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4397e-03, 4.4127e-03, 9.8075e-01, 2.0539e-07, 4.0397e-03, 4.3902e-03,
        3.9640e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 57, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3129e-01, 3.6111e-03, 5.7080e-02, 1.0034e-07, 1.8143e-01, 3.1601e-01,
        1.0572e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.080

[Epoch: 58, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9298e-03, 4.6652e-02, 5.5474e-02, 1.5284e-08, 4.5885e-01, 3.1169e-01,
        1.2440e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.007

[Epoch: 58, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0047, 0.2645, 0.3624, 0.2224, 0.0059, 0.1034, 0.0368],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 58, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9231e-01, 8.1138e-06, 1.0234e-02, 3.5979e-07, 1.0263e-06, 1.4930e-02,
        2.8252e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.124

[Epoch: 58, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5689e-03, 4.4595e-03, 9.8135e-01, 1.6718e-07, 4.1775e-03, 3.9696e-03,
        3.4728e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 58, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3942e-01, 3.5858e-03, 5.5735e-02, 5.9083e-08, 1.7797e-01, 3.1271e-01,
        1.0580e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 59, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2936e-03, 4.4177e-02, 5.2584e-02, 1.1514e-08, 4.4810e-01, 3.0976e-01,
        1.4209e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 59, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0046, 0.2558, 0.2946, 0.2926, 0.0053, 0.1078, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.005

[Epoch: 59, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2958e-01, 9.0115e-06, 1.1548e-02, 3.2586e-07, 7.3220e-07, 1.5768e-02,
        2.4310e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.093

[Epoch: 59, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6997e-03, 4.1362e-03, 9.8119e-01, 1.6469e-07, 3.9261e-03, 4.2218e-03,
        3.8287e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 59, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1197e-01, 3.5813e-03, 6.2764e-02, 9.3871e-08, 1.8748e-01, 3.2408e-01,
        1.0128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 60, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8637e-03, 4.8582e-02, 5.2149e-02, 9.4217e-09, 4.6318e-01, 3.1046e-01,
        1.2277e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 60, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2585, 0.4154, 0.1768, 0.0065, 0.1046, 0.0340],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 60, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6842e-01, 6.7866e-06, 9.7315e-03, 2.9622e-07, 1.0387e-06, 1.4476e-02,
        3.0736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.162

[Epoch: 60, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4770e-03, 5.2591e-03, 9.8107e-01, 2.2270e-07, 4.4950e-03, 3.2501e-03,
        3.4467e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 60, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4214e-01, 3.9310e-03, 5.1407e-02, 3.4022e-08, 1.8256e-01, 3.0999e-01,
        9.9782e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.077

[Epoch: 61, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4687e-03, 4.8688e-02, 5.5847e-02, 1.6854e-08, 4.0533e-01, 3.3513e-01,
        1.5154e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 61, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2648, 0.2451, 0.3341, 0.0059, 0.1062, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.009

[Epoch: 61, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.7366e-01, 1.0953e-05, 1.0752e-02, 2.8938e-07, 8.6588e-07, 1.5914e-02,
        1.9967e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.078

[Epoch: 61, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9509e-03, 3.5041e-03, 9.8212e-01, 1.4742e-07, 3.6555e-03, 4.0504e-03,
        3.7234e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.013

[Epoch: 61, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2473e-01, 4.2484e-03, 6.1478e-02, 2.1693e-07, 1.8054e-01, 3.1688e-01,
        1.2117e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 62, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6469e-03, 4.3342e-02, 4.6736e-02, 7.8900e-09, 5.5916e-01, 2.4381e-01,
        1.0330e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 62, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0046, 0.2411, 0.4294, 0.1724, 0.0077, 0.1112, 0.0336],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 62, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.6583e-01, 7.5104e-06, 1.8266e-02, 3.2017e-07, 1.1597e-06, 1.9010e-02,
        3.9688e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.080

[Epoch: 62, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3188e-03, 4.5724e-03, 9.8238e-01, 2.6979e-07, 4.0824e-03, 3.0411e-03,
        3.6033e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.015

[Epoch: 62, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9342e-01, 3.4287e-03, 5.4278e-02, 6.1875e-08, 1.9853e-01, 3.4256e-01,
        7.7765e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.064

[Epoch: 63, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6626e-03, 5.1123e-02, 6.3419e-02, 6.2759e-08, 3.3055e-01, 3.8998e-01,
        1.6127e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 63, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0055, 0.2726, 0.3220, 0.2512, 0.0055, 0.1038, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 63, batch: 123/208] total loss per batch: 0.547
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([8.2858e-01, 1.5544e-05, 8.0748e-03, 3.2227e-07, 2.1903e-06, 1.5152e-02,
        1.4818e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.173

[Epoch: 63, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9077e-03, 3.9657e-03, 9.8043e-01, 6.2778e-07, 4.1939e-03, 4.7090e-03,
        3.7923e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.005

[Epoch: 63, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3948e-01, 5.1139e-03, 6.3167e-02, 1.5418e-07, 1.6736e-01, 3.1174e-01,
        1.3144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 64, batch: 41/208] total loss per batch: 0.546
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3147e-03, 5.0190e-02, 4.2778e-02, 1.5250e-08, 5.6483e-01, 2.3840e-01,
        1.0048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.006

[Epoch: 64, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.3042, 0.2884, 0.2599, 0.0068, 0.0928, 0.0428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 64, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([5.9703e-01, 1.0143e-05, 2.1073e-02, 2.5101e-07, 1.0314e-06, 2.6917e-02,
        3.5497e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.092

[Epoch: 64, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8255e-03, 3.5105e-03, 9.8411e-01, 3.7643e-07, 3.5941e-03, 2.7974e-03,
        3.1643e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 64, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2658e-01, 4.8587e-03, 5.0574e-02, 1.7215e-07, 2.1389e-01, 2.9225e-01,
        1.1848e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 65, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.2290e-03, 4.1435e-02, 7.6450e-02, 5.0088e-08, 3.9700e-01, 2.9522e-01,
        1.8466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 65, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2031, 0.4137, 0.2339, 0.0066, 0.1122, 0.0273],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 65, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0160e-01, 1.2184e-05, 8.5821e-03, 2.0874e-07, 1.7837e-06, 2.0190e-02,
        2.6961e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.150

[Epoch: 65, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4528e-03, 4.2015e-03, 9.8347e-01, 4.2560e-07, 3.5727e-03, 3.3014e-03,
        3.0050e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 65, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4579e-01, 3.9926e-03, 7.5689e-02, 2.7787e-07, 1.3812e-01, 3.2606e-01,
        1.0349e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.087

[Epoch: 66, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7232e-03, 6.1329e-02, 3.8093e-02, 2.5941e-08, 4.6653e-01, 3.3145e-01,
        9.9873e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 66, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0079, 0.2902, 0.2790, 0.2747, 0.0085, 0.0938, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 66, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3935e-01, 2.7815e-05, 1.1358e-02, 3.5668e-07, 1.3088e-06, 1.5485e-02,
        2.3378e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.094

[Epoch: 66, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1052e-03, 4.5922e-03, 9.8084e-01, 3.6422e-07, 4.1179e-03, 3.5302e-03,
        3.8104e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 66, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4997e-01, 5.9260e-03, 5.9904e-02, 1.2556e-07, 1.3285e-01, 3.3929e-01,
        1.2059e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 67, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3936e-03, 4.0693e-02, 8.2423e-02, 1.6151e-08, 4.3836e-01, 2.8106e-01,
        1.5307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 67, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2542, 0.3566, 0.2449, 0.0055, 0.1011, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 67, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7556e-01, 8.8733e-06, 1.1909e-02, 6.1963e-08, 1.4704e-06, 1.8217e-02,
        2.9431e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.105

[Epoch: 67, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7894e-03, 4.2305e-03, 9.8162e-01, 2.1647e-07, 4.6012e-03, 3.5168e-03,
        3.2425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 67, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1033e-01, 4.0872e-03, 6.4901e-02, 3.2950e-07, 2.1972e-01, 2.9087e-01,
        1.0101e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 68, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2561e-03, 5.2886e-02, 3.0106e-02, 4.9259e-08, 4.8630e-01, 3.0643e-01,
        1.2102e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 68, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2725, 0.3474, 0.2147, 0.0083, 0.1126, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 68, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2074e-01, 9.0942e-06, 9.0401e-03, 1.4075e-07, 1.6966e-06, 1.5270e-02,
        2.5494e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.175

[Epoch: 68, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8505e-03, 3.6257e-03, 9.8308e-01, 4.5975e-07, 3.5965e-03, 2.9254e-03,
        3.9175e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 68, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3443e-01, 3.9576e-03, 5.2605e-02, 2.5845e-07, 1.8353e-01, 3.1424e-01,
        1.1242e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.059

[Epoch: 69, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8507e-03, 4.4304e-02, 5.9362e-02, 1.9027e-08, 4.2307e-01, 3.4187e-01,
        1.2854e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 69, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2407, 0.3055, 0.2862, 0.0052, 0.1238, 0.0334],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 69, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1196e-01, 1.7284e-05, 1.1610e-02, 1.7439e-07, 2.5175e-06, 1.3265e-02,
        2.6314e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.001

[Epoch: 69, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8349e-03, 5.5431e-03, 9.7613e-01, 7.5740e-07, 5.6787e-03, 5.0486e-03,
        3.7635e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.015

[Epoch: 69, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3408e-01, 4.6179e-03, 6.5070e-02, 3.0356e-07, 1.6566e-01, 3.2111e-01,
        9.4621e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.094

[Epoch: 70, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5426e-03, 5.0677e-02, 4.9595e-02, 1.8622e-08, 4.5784e-01, 2.8816e-01,
        1.5019e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 70, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.3051, 0.3609, 0.2074, 0.0058, 0.0870, 0.0294],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.004

[Epoch: 70, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3837e-01, 1.3999e-05, 9.4679e-03, 1.6868e-07, 1.7305e-06, 2.0724e-02,
        2.3143e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.201

[Epoch: 70, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2355e-03, 2.4114e-03, 9.8588e-01, 4.1317e-07, 3.2452e-03, 3.0386e-03,
        3.1883e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 70, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1866e-01, 5.4780e-03, 7.0104e-02, 4.0539e-07, 2.0611e-01, 2.9012e-01,
        9.5272e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 71, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0796e-03, 4.8757e-02, 5.4968e-02, 9.6272e-08, 4.8445e-01, 2.9141e-01,
        1.1734e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 71, batch: 82/208] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2523, 0.3430, 0.2599, 0.0071, 0.1011, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.005

[Epoch: 71, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.3015e-01, 2.9634e-05, 1.6139e-02, 6.5021e-07, 3.3107e-06, 1.8994e-02,
        3.3468e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.102

[Epoch: 71, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0082e-03, 5.2010e-03, 9.8056e-01, 5.5118e-07, 3.5730e-03, 2.6294e-03,
        4.0324e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 71, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2293e-01, 2.5873e-03, 5.5344e-02, 1.4202e-07, 1.9460e-01, 3.1562e-01,
        8.9222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.064

[Epoch: 72, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3340e-03, 5.5954e-02, 3.7805e-02, 1.7429e-08, 4.4755e-01, 3.0262e-01,
        1.5174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 72, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0055, 0.2135, 0.3541, 0.2407, 0.0076, 0.1255, 0.0531],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 72, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.9349e-01, 1.4108e-05, 7.0176e-03, 2.6652e-07, 1.2749e-06, 1.1946e-02,
        1.8753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.177

[Epoch: 72, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7890e-03, 3.8569e-03, 9.8095e-01, 5.1246e-06, 3.8197e-03, 3.7089e-03,
        3.8743e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 72, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2904e-01, 4.8231e-03, 5.8367e-02, 2.6679e-07, 1.7643e-01, 3.1905e-01,
        1.2292e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.087

[Epoch: 73, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2426e-03, 4.4630e-02, 9.3872e-02, 2.5633e-08, 3.8928e-01, 3.4436e-01,
        1.2461e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 73, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.3624, 0.2773, 0.2326, 0.0064, 0.0930, 0.0236],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 73, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.5995e-01, 1.3661e-05, 1.4415e-02, 8.0348e-08, 1.2852e-06, 1.8662e-02,
        3.0696e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.088

[Epoch: 73, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0101e-03, 2.6577e-03, 9.8436e-01, 1.1586e-06, 3.6986e-03, 3.4734e-03,
        3.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 73, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4343e-01, 7.4116e-03, 7.9336e-02, 1.7073e-06, 1.7607e-01, 2.8135e-01,
        1.2399e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.106

[Epoch: 74, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8991e-03, 5.7731e-02, 2.1518e-02, 4.7886e-08, 5.0460e-01, 2.7910e-01,
        1.3415e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 74, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2091, 0.4272, 0.2140, 0.0068, 0.1107, 0.0286],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 74, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6048e-01, 2.0633e-05, 1.0472e-02, 2.7845e-07, 2.1312e-06, 2.3169e-02,
        3.0585e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.133

[Epoch: 74, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2460e-03, 3.2711e-03, 9.8315e-01, 1.2455e-06, 3.7241e-03, 2.7354e-03,
        3.8692e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.021

[Epoch: 74, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5743e-01, 6.9853e-03, 5.6522e-02, 4.2304e-07, 1.7630e-01, 2.9256e-01,
        1.0199e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.110

[Epoch: 75, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8281e-03, 5.9988e-02, 6.5025e-02, 3.3787e-08, 4.2554e-01, 3.1459e-01,
        1.3103e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 75, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0046, 0.2745, 0.2810, 0.2815, 0.0064, 0.1119, 0.0401],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.010

[Epoch: 75, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.6550e-01, 1.3175e-05, 8.2163e-03, 2.2481e-07, 1.2459e-06, 1.1637e-02,
        2.1463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.140

[Epoch: 75, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2061e-03, 4.3433e-03, 9.7610e-01, 1.7212e-06, 5.1405e-03, 5.5370e-03,
        4.6747e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 75, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4254e-01, 3.2496e-03, 5.3812e-02, 4.1097e-07, 1.8040e-01, 3.0784e-01,
        1.2163e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.070

[Epoch: 76, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7399e-03, 4.3140e-02, 4.9234e-02, 6.0868e-08, 4.6635e-01, 3.1036e-01,
        1.2717e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 76, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2648, 0.3475, 0.2451, 0.0081, 0.0923, 0.0371],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 76, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0126e-01, 3.3817e-05, 1.1747e-02, 5.2490e-07, 2.0472e-06, 1.3919e-02,
        2.7304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.081

[Epoch: 76, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9555e-03, 3.6577e-03, 9.8302e-01, 1.0865e-06, 3.2940e-03, 3.4020e-03,
        3.6744e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.018

[Epoch: 76, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0440e-01, 4.3198e-03, 4.6798e-02, 4.2209e-07, 1.8604e-01, 3.4658e-01,
        1.1860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.078

[Epoch: 77, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0251e-03, 4.2593e-02, 6.2578e-02, 5.2174e-08, 4.3277e-01, 3.1584e-01,
        1.4320e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 77, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0043, 0.2505, 0.3446, 0.2239, 0.0090, 0.1293, 0.0384],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.002

[Epoch: 77, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0930e-01, 1.4123e-05, 9.5910e-03, 2.0401e-07, 2.1468e-06, 1.8130e-02,
        2.6296e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.140

[Epoch: 77, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([1.9028e-03, 2.5807e-03, 9.8726e-01, 1.0072e-06, 3.1704e-03, 2.1626e-03,
        2.9247e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 77, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2767e-01, 5.7529e-03, 6.0157e-02, 5.0433e-07, 1.8204e-01, 3.1533e-01,
        9.0491e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 78, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2507e-03, 3.6580e-02, 3.2463e-02, 4.5078e-08, 4.9686e-01, 3.0800e-01,
        1.2285e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 78, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2242, 0.3614, 0.2673, 0.0048, 0.1053, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.003

[Epoch: 78, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0554e-01, 2.1069e-05, 1.1877e-02, 3.1911e-07, 2.0469e-06, 1.8502e-02,
        2.6405e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.089

[Epoch: 78, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9786e-03, 5.3500e-03, 9.7906e-01, 2.9011e-06, 4.3625e-03, 2.6219e-03,
        4.6223e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 78, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1891e-01, 4.0239e-03, 6.0646e-02, 3.4560e-07, 1.6640e-01, 3.3548e-01,
        1.4531e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 79, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6248e-03, 4.9500e-02, 9.1866e-02, 6.6299e-08, 4.0880e-01, 2.7775e-01,
        1.6846e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 79, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.3602, 0.2759, 0.2258, 0.0059, 0.0955, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 79, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5492e-01, 1.2601e-05, 6.2494e-03, 9.3198e-08, 8.6598e-07, 1.2614e-02,
        2.2620e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.162

[Epoch: 79, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1425e-03, 2.8425e-03, 9.8392e-01, 7.9708e-07, 3.7715e-03, 2.9080e-03,
        3.4173e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 79, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3944e-01, 5.1706e-03, 6.0176e-02, 4.1218e-07, 1.8276e-01, 3.0249e-01,
        9.9632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.082

[Epoch: 80, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7444e-03, 6.7095e-02, 2.9773e-02, 6.8840e-08, 4.5846e-01, 3.2319e-01,
        1.1774e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 80, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0060, 0.1971, 0.3951, 0.2403, 0.0087, 0.1045, 0.0484],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.002

[Epoch: 80, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.1685e-01, 2.9480e-05, 1.6765e-02, 4.1740e-07, 1.9362e-06, 1.9134e-02,
        3.4721e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.068

[Epoch: 80, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1799e-03, 3.8434e-03, 9.8459e-01, 3.6990e-07, 3.2852e-03, 2.5251e-03,
        3.5749e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 80, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5582e-01, 5.2654e-03, 5.9184e-02, 4.5238e-07, 1.7732e-01, 2.9309e-01,
        9.3241e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.089

[Epoch: 81, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7398e-03, 3.0269e-02, 7.1169e-02, 3.9568e-08, 4.4331e-01, 3.2794e-01,
        1.2357e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 81, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2358, 0.3340, 0.2726, 0.0077, 0.1166, 0.0288],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 81, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3749e-01, 2.1899e-05, 1.2623e-02, 2.0336e-07, 1.7467e-06, 1.7129e-02,
        2.3274e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.127

[Epoch: 81, batch: 164/208] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3736e-03, 4.7595e-03, 9.8172e-01, 2.6250e-06, 3.2891e-03, 3.7067e-03,
        4.1466e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.007

[Epoch: 81, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.8874e-01, 3.6974e-03, 5.8583e-02, 5.2669e-07, 2.4479e-01, 2.9111e-01,
        1.3079e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.063

[Epoch: 82, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7340e-03, 5.6184e-02, 3.4638e-02, 6.2898e-08, 5.2720e-01, 2.6229e-01,
        1.1696e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 82, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0045, 0.3162, 0.3363, 0.2074, 0.0046, 0.1037, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 82, batch: 123/208] total loss per batch: 0.546
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2206e-01, 3.7018e-05, 8.1223e-03, 4.6397e-07, 1.6967e-06, 1.0685e-02,
        2.5909e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.098

[Epoch: 82, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5916e-03, 4.8203e-03, 9.7594e-01, 1.1792e-06, 7.0173e-03, 3.5384e-03,
        5.0871e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 82, batch: 205/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3855e-01, 4.8980e-03, 6.5873e-02, 1.9538e-07, 1.5676e-01, 3.2036e-01,
        1.3563e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.115

[Epoch: 83, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4600e-03, 3.3630e-02, 6.6273e-02, 4.6663e-08, 3.9526e-01, 3.4146e-01,
        1.5992e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 83, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.2182, 0.3713, 0.2663, 0.0091, 0.0907, 0.0386],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 83, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8761e-01, 1.1509e-05, 1.1497e-02, 6.9475e-07, 1.0964e-06, 1.7138e-02,
        2.8374e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.134

[Epoch: 83, batch: 164/208] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.2155e-03, 1.6217e-03, 9.8775e-01, 9.5342e-07, 2.7560e-03, 2.1995e-03,
        3.4606e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 83, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4923e-01, 4.4798e-03, 4.5686e-02, 3.4325e-07, 1.8810e-01, 3.0221e-01,
        1.0290e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.104

[Epoch: 84, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0927e-03, 4.7640e-02, 4.6217e-02, 2.8020e-08, 4.3817e-01, 3.1968e-01,
        1.4519e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 84, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.2675, 0.3178, 0.2321, 0.0076, 0.1264, 0.0437],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.002

[Epoch: 84, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0130e-01, 2.2973e-05, 1.2993e-02, 3.1171e-07, 1.2848e-06, 2.3466e-02,
        2.6222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.113

[Epoch: 84, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6831e-03, 4.4417e-03, 9.7980e-01, 1.5989e-06, 4.7173e-03, 3.0838e-03,
        4.2724e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.004

[Epoch: 84, batch: 205/208] total loss per batch: 0.540
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2948e-01, 4.3059e-03, 6.6459e-02, 4.2541e-07, 1.6238e-01, 3.2732e-01,
        1.0051e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 85, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6170e-03, 7.4202e-02, 5.5579e-02, 1.9758e-08, 4.6581e-01, 2.8600e-01,
        1.1479e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 85, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2667, 0.3457, 0.2405, 0.0079, 0.1062, 0.0287],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 85, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1511e-01, 1.8820e-05, 1.0278e-02, 5.2744e-07, 1.4838e-06, 1.6478e-02,
        2.5811e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.102

[Epoch: 85, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1480e-03, 4.9635e-03, 9.7375e-01, 1.0868e-06, 6.1589e-03, 4.4155e-03,
        6.5680e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 85, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2810e-01, 4.7517e-03, 6.2808e-02, 7.2138e-07, 1.8441e-01, 3.0900e-01,
        1.0923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 86, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5838e-03, 3.0993e-02, 5.8722e-02, 3.1077e-08, 4.4090e-01, 3.3068e-01,
        1.3512e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 86, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0051, 0.2729, 0.3492, 0.2185, 0.0063, 0.1098, 0.0382],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.000

[Epoch: 86, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2091e-01, 1.4608e-05, 8.0563e-03, 3.2587e-07, 8.9687e-07, 1.2199e-02,
        2.5882e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.170

[Epoch: 86, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2339e-03, 4.4262e-03, 9.7685e-01, 1.1142e-06, 6.0362e-03, 4.0211e-03,
        4.4300e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.007

[Epoch: 86, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5175e-01, 4.1441e-03, 6.6583e-02, 4.0060e-07, 1.8001e-01, 2.8792e-01,
        9.5999e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.078

[Epoch: 87, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.8911e-03, 5.0393e-02, 5.7145e-02, 4.6238e-08, 4.4600e-01, 2.9230e-01,
        1.4928e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 87, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0052, 0.2449, 0.3309, 0.2716, 0.0077, 0.0957, 0.0440],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 87, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7680e-01, 4.4259e-05, 1.6611e-02, 1.9495e-07, 1.1154e-06, 2.3557e-02,
        2.8299e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.103

[Epoch: 87, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3319e-03, 2.6580e-03, 9.8623e-01, 1.1478e-06, 3.3570e-03, 1.9276e-03,
        3.4977e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.017

[Epoch: 87, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([3.9632e-01, 3.4102e-03, 5.1213e-02, 2.5493e-07, 2.1610e-01, 3.2294e-01,
        1.0011e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 88, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5787e-03, 4.3648e-02, 4.6224e-02, 2.4800e-08, 4.6911e-01, 3.1564e-01,
        1.2180e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 88, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0053, 0.2608, 0.3501, 0.2423, 0.0081, 0.1065, 0.0270],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 88, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5845e-01, 1.3737e-05, 8.1375e-03, 3.5040e-07, 8.0495e-07, 1.1813e-02,
        2.2158e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.079

[Epoch: 88, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1086e-03, 4.0372e-03, 9.8224e-01, 2.3838e-06, 3.9057e-03, 3.2654e-03,
        3.4420e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 88, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1797e-01, 3.8108e-03, 5.6654e-02, 3.5482e-07, 1.6878e-01, 3.4075e-01,
        1.2040e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 89, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8762e-03, 4.1509e-02, 4.7933e-02, 1.9516e-08, 4.5610e-01, 3.2595e-01,
        1.2463e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 89, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0043, 0.2471, 0.3569, 0.2267, 0.0076, 0.1248, 0.0327],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.012

[Epoch: 89, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9285e-01, 1.1901e-05, 7.5646e-03, 3.0809e-07, 1.1078e-06, 1.0495e-02,
        2.8908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.144

[Epoch: 89, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9775e-03, 3.9613e-03, 9.7876e-01, 6.8179e-07, 4.1591e-03, 4.7995e-03,
        4.3426e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.005

[Epoch: 89, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.6121e-01, 5.0196e-03, 7.1861e-02, 8.9188e-07, 1.6547e-01, 2.8393e-01,
        1.2507e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.106

[Epoch: 90, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7495e-03, 4.6609e-02, 5.0611e-02, 4.3645e-08, 4.4640e-01, 2.9935e-01,
        1.5429e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 90, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0051, 0.2801, 0.2936, 0.2829, 0.0065, 0.0863, 0.0454],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.003

[Epoch: 90, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0623e-01, 1.5968e-05, 1.4086e-02, 8.8643e-08, 1.1179e-06, 2.3510e-02,
        2.5616e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.112

[Epoch: 90, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9533e-03, 3.4262e-03, 9.8134e-01, 1.4237e-06, 4.0060e-03, 4.0113e-03,
        4.2644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.007

[Epoch: 90, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5678e-01, 4.8144e-03, 5.8769e-02, 4.6823e-07, 1.6195e-01, 3.0917e-01,
        8.5203e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 91, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.7563e-03, 6.1276e-02, 6.8555e-02, 2.4117e-08, 4.4293e-01, 2.9934e-01,
        1.2214e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 91, batch: 82/208] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2643, 0.4406, 0.1310, 0.0075, 0.1186, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.011

[Epoch: 91, batch: 123/208] total loss per batch: 0.552
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7796e-01, 1.8503e-05, 1.1077e-02, 1.4254e-07, 8.8620e-07, 2.6766e-02,
        2.8418e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.162

[Epoch: 91, batch: 164/208] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4786e-03, 2.7841e-03, 9.8082e-01, 8.7188e-07, 5.7466e-03, 3.4651e-03,
        3.7020e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 91, batch: 205/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0822e-01, 5.4124e-03, 5.7362e-02, 5.8728e-07, 1.7382e-01, 3.4502e-01,
        1.0154e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.103

[Epoch: 92, batch: 41/208] total loss per batch: 0.551
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([1.5943e-03, 3.5174e-02, 7.6371e-02, 4.4469e-09, 4.2511e-01, 3.0772e-01,
        1.5403e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.004

[Epoch: 92, batch: 82/208] total loss per batch: 0.570
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0060, 0.2776, 0.2692, 0.2583, 0.0065, 0.1358, 0.0467],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 92, batch: 123/208] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0824e-01, 1.9652e-05, 7.1374e-03, 1.0911e-07, 4.6810e-07, 2.5010e-02,
        2.5959e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.055

[Epoch: 92, batch: 164/208] total loss per batch: 0.564
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2413e-03, 2.8050e-03, 9.8296e-01, 4.8475e-07, 4.4548e-03, 3.7150e-03,
        2.8194e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 92, batch: 205/208] total loss per batch: 0.554
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5213e-01, 7.3061e-03, 5.1869e-02, 2.2061e-07, 1.8877e-01, 2.8919e-01,
        1.0738e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.114

[Epoch: 93, batch: 41/208] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.0020e-03, 4.1405e-02, 2.1377e-02, 1.5890e-08, 5.7153e-01, 2.9410e-01,
        6.7588e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.013

[Epoch: 93, batch: 82/208] total loss per batch: 0.569
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0056, 0.2702, 0.3497, 0.2398, 0.0042, 0.0972, 0.0333],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.017

[Epoch: 93, batch: 123/208] total loss per batch: 0.553
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4418e-01, 5.9768e-05, 6.0592e-03, 2.2428e-07, 4.1695e-07, 1.7412e-02,
        2.3229e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.060

[Epoch: 93, batch: 164/208] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9504e-03, 3.2959e-03, 9.8270e-01, 1.9570e-07, 4.6760e-03, 2.5352e-03,
        3.8440e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.022

[Epoch: 93, batch: 205/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5909e-01, 4.4686e-03, 4.4390e-02, 6.4492e-07, 1.7340e-01, 3.0887e-01,
        9.7877e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.113

[Epoch: 94, batch: 41/208] total loss per batch: 0.548
Policy (actual, predicted): 4 5
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.1683e-03, 6.9866e-02, 5.2299e-02, 6.9781e-08, 3.2713e-01, 3.8171e-01,
        1.6282e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 94, batch: 82/208] total loss per batch: 0.561
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0058, 0.2359, 0.2543, 0.3415, 0.0054, 0.1061, 0.0511],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.016

[Epoch: 94, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7797e-01, 3.8723e-05, 8.1613e-03, 4.4779e-07, 4.2050e-07, 1.3638e-02,
        3.0019e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 94, batch: 164/208] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2232e-03, 3.1180e-03, 9.8068e-01, 3.2682e-07, 4.7468e-03, 4.6656e-03,
        3.5699e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.010

[Epoch: 94, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5052e-01, 3.9822e-03, 5.5725e-02, 4.8948e-07, 1.7562e-01, 3.0480e-01,
        9.3641e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.105

[Epoch: 95, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.5257e-03, 5.6626e-02, 6.1184e-02, 2.5622e-08, 3.8973e-01, 3.0753e-01,
        1.7840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 95, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.2556, 0.3529, 0.2356, 0.0072, 0.1031, 0.0408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.015

[Epoch: 95, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1882e-01, 3.6271e-05, 8.7130e-03, 2.9248e-07, 4.2079e-07, 1.6299e-02,
        2.5613e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.151

[Epoch: 95, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1641e-03, 3.2018e-03, 9.8063e-01, 2.2716e-07, 5.4095e-03, 3.4566e-03,
        4.1366e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.005

[Epoch: 95, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3636e-01, 3.8837e-03, 5.9120e-02, 5.4118e-07, 1.8584e-01, 3.0582e-01,
        8.9733e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 96, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([6.2684e-03, 5.4741e-02, 5.4678e-02, 2.8525e-08, 4.3594e-01, 3.0660e-01,
        1.4178e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 96, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0045, 0.2779, 0.3334, 0.2323, 0.0067, 0.1079, 0.0372],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 96, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0670e-01, 4.2430e-05, 1.0748e-02, 2.9551e-07, 3.7341e-07, 1.7313e-02,
        2.6520e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.097

[Epoch: 96, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1496e-03, 3.4036e-03, 9.8105e-01, 2.1406e-07, 4.9166e-03, 3.5050e-03,
        3.9722e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.005

[Epoch: 96, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4044e-01, 3.9320e-03, 5.9489e-02, 3.6203e-07, 1.7848e-01, 3.0800e-01,
        9.6565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 97, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.4047e-03, 5.3964e-02, 5.4542e-02, 2.0771e-08, 4.4571e-01, 3.0549e-01,
        1.3490e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 97, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2556, 0.3481, 0.2363, 0.0070, 0.1085, 0.0400],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.013

[Epoch: 97, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1333e-01, 3.0560e-05, 8.8734e-03, 2.1356e-07, 2.8454e-07, 1.6149e-02,
        2.6161e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.137

[Epoch: 97, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0898e-03, 3.4495e-03, 9.8121e-01, 1.8284e-07, 4.8543e-03, 3.4169e-03,
        3.9752e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 97, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3354e-01, 3.9733e-03, 6.0634e-02, 3.0965e-07, 1.7437e-01, 3.1754e-01,
        9.9371e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 98, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([5.1428e-03, 4.9110e-02, 5.4329e-02, 1.6242e-08, 4.5163e-01, 3.1345e-01,
        1.2633e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 98, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2624, 0.3424, 0.2459, 0.0068, 0.1033, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.011

[Epoch: 98, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1231e-01, 3.1689e-05, 9.9810e-03, 1.9704e-07, 2.6800e-07, 1.6701e-02,
        2.6097e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.102

[Epoch: 98, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9039e-03, 3.2558e-03, 9.8250e-01, 1.4420e-07, 4.4587e-03, 3.1028e-03,
        3.7809e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 98, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3603e-01, 4.0422e-03, 6.1337e-02, 2.4670e-07, 1.7594e-01, 3.1285e-01,
        9.8037e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 99, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.8314e-03, 4.8472e-02, 5.4845e-02, 1.2907e-08, 4.5151e-01, 3.0987e-01,
        1.3048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 99, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2671, 0.3425, 0.2402, 0.0066, 0.1040, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.010

[Epoch: 99, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0628e-01, 2.4554e-05, 1.0227e-02, 1.3688e-07, 1.9836e-07, 1.6471e-02,
        2.6700e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.126

[Epoch: 99, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7262e-03, 3.2010e-03, 9.8301e-01, 1.2257e-07, 4.1778e-03, 3.1831e-03,
        3.7039e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 99, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3577e-01, 3.8632e-03, 6.2378e-02, 1.9719e-07, 1.7943e-01, 3.0864e-01,
        9.9236e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 100, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2129e-03, 4.5961e-02, 5.1292e-02, 1.0573e-08, 4.5262e-01, 3.0838e-01,
        1.3754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 100, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2547, 0.3426, 0.2476, 0.0071, 0.1077, 0.0362],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 100, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0701e-01, 2.8634e-05, 1.0570e-02, 1.4758e-07, 2.3343e-07, 1.7182e-02,
        2.6521e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.110

[Epoch: 100, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9884e-03, 3.3322e-03, 9.8234e-01, 1.2358e-07, 4.2272e-03, 3.2529e-03,
        3.8601e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 100, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4442e-01, 4.0863e-03, 5.8475e-02, 1.8448e-07, 1.7681e-01, 3.0642e-01,
        9.7862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 101, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3282e-03, 4.7697e-02, 5.4482e-02, 9.1964e-09, 4.5276e-01, 3.1171e-01,
        1.2902e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 101, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2719, 0.3361, 0.2380, 0.0067, 0.1063, 0.0369],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.008

[Epoch: 101, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2349e-01, 1.9684e-05, 9.5730e-03, 9.7831e-08, 1.7229e-07, 1.5019e-02,
        2.5190e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.129

[Epoch: 101, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8473e-03, 3.3454e-03, 9.8333e-01, 9.0481e-08, 4.0498e-03, 2.9497e-03,
        3.4747e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 101, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2544e-01, 3.6645e-03, 6.1105e-02, 1.5899e-07, 1.8531e-01, 3.1408e-01,
        1.0398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 102, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.6832e-03, 4.5663e-02, 5.1377e-02, 7.7854e-09, 4.5357e-01, 3.1102e-01,
        1.3369e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 102, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2546, 0.3380, 0.2470, 0.0071, 0.1113, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 102, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9955e-01, 2.2098e-05, 1.0477e-02, 1.1231e-07, 1.9466e-07, 1.7583e-02,
        2.7237e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.110

[Epoch: 102, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9880e-03, 3.0833e-03, 9.8290e-01, 1.1522e-07, 4.1367e-03, 3.4653e-03,
        3.4309e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 102, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3892e-01, 3.5688e-03, 5.8318e-02, 1.3929e-07, 1.7974e-01, 3.0997e-01,
        9.4886e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 103, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2676e-03, 4.7422e-02, 5.5620e-02, 7.1944e-09, 4.5663e-01, 3.0410e-01,
        1.3196e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 103, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2715, 0.3399, 0.2414, 0.0066, 0.1012, 0.0356],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 103, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0360e-01, 1.5589e-05, 9.1718e-03, 7.5365e-08, 1.3655e-07, 1.5310e-02,
        2.7190e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.126

[Epoch: 103, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1569e-03, 2.9656e-03, 9.8325e-01, 8.4617e-08, 4.0602e-03, 3.1464e-03,
        3.4252e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 103, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3667e-01, 3.6212e-03, 5.7969e-02, 1.1124e-07, 1.8061e-01, 3.1114e-01,
        9.9903e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 104, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.0388e-03, 4.5797e-02, 5.0655e-02, 5.8195e-09, 4.4946e-01, 3.1733e-01,
        1.3273e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 104, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2551, 0.3444, 0.2434, 0.0071, 0.1105, 0.0357],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 104, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0936e-01, 1.9224e-05, 1.0412e-02, 8.6884e-08, 1.7814e-07, 1.9324e-02,
        2.6089e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.115

[Epoch: 104, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8355e-03, 2.8281e-03, 9.8509e-01, 8.1892e-08, 3.4785e-03, 2.5147e-03,
        3.2574e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 104, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3570e-01, 3.8939e-03, 6.2703e-02, 1.3909e-07, 1.8759e-01, 3.0066e-01,
        9.4586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 105, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9783e-03, 5.0281e-02, 5.2997e-02, 6.1765e-09, 4.5680e-01, 2.9958e-01,
        1.3636e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 105, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2825, 0.3307, 0.2387, 0.0059, 0.1028, 0.0359],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.006

[Epoch: 105, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9565e-01, 1.9354e-05, 9.2855e-03, 8.8709e-08, 1.2947e-07, 1.5152e-02,
        2.7989e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.129

[Epoch: 105, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6724e-03, 2.9787e-03, 9.8451e-01, 5.9246e-08, 3.4909e-03, 2.8839e-03,
        3.4613e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 105, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1354e-01, 3.4888e-03, 5.6917e-02, 9.4958e-08, 1.7924e-01, 3.3598e-01,
        1.0830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.092

[Epoch: 106, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6790e-03, 4.6357e-02, 4.8523e-02, 6.7598e-09, 4.4937e-01, 3.2557e-01,
        1.2650e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 106, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2309, 0.3592, 0.2468, 0.0062, 0.1157, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.007

[Epoch: 106, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5756e-01, 2.3002e-05, 1.1047e-02, 1.0042e-07, 2.1921e-07, 1.9789e-02,
        2.1158e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.103

[Epoch: 106, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7309e-03, 3.4629e-03, 9.8217e-01, 1.1163e-07, 4.3365e-03, 3.2166e-03,
        4.0830e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.011

[Epoch: 106, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4856e-01, 3.5066e-03, 5.1804e-02, 1.2405e-07, 1.7628e-01, 3.1020e-01,
        9.6465e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 107, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.4807e-03, 4.1222e-02, 5.6678e-02, 4.1756e-09, 4.5381e-01, 2.9037e-01,
        1.5343e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 107, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2972, 0.3141, 0.2544, 0.0080, 0.0903, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.003

[Epoch: 107, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8711e-01, 1.3353e-05, 9.2360e-03, 5.0013e-08, 9.0232e-08, 9.6375e-03,
        2.9401e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.141

[Epoch: 107, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0028e-03, 3.1050e-03, 9.8347e-01, 1.3076e-07, 3.3247e-03, 4.2481e-03,
        2.8513e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 107, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3282e-01, 5.2037e-03, 6.8113e-02, 1.3633e-07, 1.7052e-01, 3.1249e-01,
        1.0849e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 108, batch: 41/208] total loss per batch: 0.544
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.1169e-03, 6.3699e-02, 5.1567e-02, 2.8814e-08, 4.3778e-01, 3.1596e-01,
        1.2688e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 108, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2416, 0.3627, 0.2321, 0.0049, 0.1157, 0.0397],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 108, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9428e-01, 5.1097e-05, 1.6355e-02, 1.1713e-07, 3.4633e-07, 2.4279e-02,
        2.6503e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.070

[Epoch: 108, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9234e-03, 2.8389e-03, 9.8363e-01, 1.7916e-07, 3.2878e-03, 3.0979e-03,
        3.2252e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 108, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4384e-01, 5.0420e-03, 6.0774e-02, 2.5230e-07, 1.9159e-01, 2.8680e-01,
        1.1946e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.103

[Epoch: 109, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2747e-03, 4.3205e-02, 4.6618e-02, 5.3169e-08, 4.7613e-01, 3.1565e-01,
        1.1412e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 109, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0050, 0.2523, 0.2897, 0.2862, 0.0074, 0.1214, 0.0380],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 109, batch: 123/208] total loss per batch: 0.545
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3459e-01, 2.6109e-05, 7.4614e-03, 2.5677e-07, 6.2663e-07, 1.6617e-02,
        2.4131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.147

[Epoch: 109, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5706e-03, 4.1772e-03, 9.8130e-01, 1.8002e-07, 3.4336e-03, 2.8527e-03,
        4.6652e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 109, batch: 205/208] total loss per batch: 0.539
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1419e-01, 3.3623e-03, 5.1593e-02, 2.9992e-07, 1.9076e-01, 3.3214e-01,
        7.9565e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.071

[Epoch: 110, batch: 41/208] total loss per batch: 0.545
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6419e-03, 3.8851e-02, 5.4348e-02, 8.8194e-09, 4.3205e-01, 3.1105e-01,
        1.6006e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 110, batch: 82/208] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0040, 0.2726, 0.3807, 0.2131, 0.0069, 0.0913, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.005

[Epoch: 110, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1549e-01, 3.6440e-05, 1.1482e-02, 9.7785e-08, 3.5943e-07, 1.1561e-02,
        2.6143e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.142

[Epoch: 110, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8981e-03, 3.5979e-03, 9.8275e-01, 3.2265e-07, 3.9068e-03, 2.6085e-03,
        4.2397e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 110, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2912e-01, 4.5212e-03, 6.1010e-02, 4.4457e-07, 1.8289e-01, 3.1034e-01,
        1.2120e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.105

[Epoch: 111, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8690e-03, 5.3676e-02, 5.0086e-02, 3.0591e-08, 4.9360e-01, 2.8532e-01,
        1.1345e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 111, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2530, 0.3306, 0.2727, 0.0065, 0.1011, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.005

[Epoch: 111, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9344e-01, 2.7530e-05, 1.0106e-02, 1.2283e-07, 5.6926e-07, 2.0074e-02,
        2.7635e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.094

[Epoch: 111, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9307e-03, 3.2375e-03, 9.8191e-01, 2.8806e-07, 3.9058e-03, 3.3053e-03,
        3.7138e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 111, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3581e-01, 4.4378e-03, 6.3252e-02, 5.1178e-07, 1.8372e-01, 3.0214e-01,
        1.0638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.094

[Epoch: 112, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6359e-03, 4.5118e-02, 6.3578e-02, 2.8398e-08, 4.3259e-01, 3.0891e-01,
        1.4617e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 112, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2686, 0.3509, 0.2176, 0.0075, 0.1097, 0.0415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 112, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2549e-01, 3.1806e-05, 7.6093e-03, 1.5413e-07, 3.0431e-07, 1.2794e-02,
        2.5408e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.146

[Epoch: 112, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8151e-03, 3.3097e-03, 9.8525e-01, 1.7319e-07, 2.8755e-03, 2.5243e-03,
        3.2255e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.013

[Epoch: 112, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4425e-01, 3.9577e-03, 6.1360e-02, 4.0396e-07, 1.7789e-01, 3.0387e-01,
        8.6754e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.081

[Epoch: 113, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9643e-03, 4.3001e-02, 4.3208e-02, 1.5847e-08, 4.5939e-01, 3.2272e-01,
        1.2872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 113, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2786, 0.3309, 0.2468, 0.0057, 0.1001, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.002

[Epoch: 113, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9958e-01, 3.1645e-05, 1.2321e-02, 1.2620e-07, 3.7268e-07, 1.9087e-02,
        2.6898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.093

[Epoch: 113, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4783e-03, 4.8330e-03, 9.7953e-01, 6.4113e-07, 4.7584e-03, 3.5947e-03,
        3.8014e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 113, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1488e-01, 3.5424e-03, 5.8144e-02, 2.4636e-07, 1.7831e-01, 3.3545e-01,
        9.6782e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.083

[Epoch: 114, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.4341e-03, 5.6365e-02, 6.3143e-02, 2.3582e-08, 4.5670e-01, 2.8430e-01,
        1.3506e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 114, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2476, 0.3443, 0.2546, 0.0070, 0.1045, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.000

[Epoch: 114, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1950e-01, 2.9762e-05, 7.7197e-03, 9.5063e-08, 2.9978e-07, 1.5530e-02,
        2.5722e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.126

[Epoch: 114, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4425e-03, 3.4247e-03, 9.8009e-01, 2.3569e-07, 4.0514e-03, 3.7177e-03,
        4.2770e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 114, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3788e-01, 4.3832e-03, 5.5597e-02, 5.0734e-07, 1.8812e-01, 3.0163e-01,
        1.2394e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 115, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9392e-03, 3.5530e-02, 4.3188e-02, 2.1754e-08, 4.3235e-01, 3.5705e-01,
        1.2895e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 115, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2700, 0.3291, 0.2425, 0.0072, 0.1131, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.002

[Epoch: 115, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1675e-01, 2.0048e-05, 9.1386e-03, 1.0407e-07, 3.8090e-07, 1.3612e-02,
        2.6048e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.165

[Epoch: 115, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8992e-03, 2.7692e-03, 9.8598e-01, 1.7038e-07, 3.2583e-03, 1.8389e-03,
        3.2525e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 115, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3854e-01, 3.9728e-03, 6.0495e-02, 2.7984e-07, 1.6844e-01, 3.1921e-01,
        9.3495e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 116, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.2229e-03, 6.2410e-02, 6.4773e-02, 2.9737e-08, 4.5640e-01, 2.6449e-01,
        1.4770e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 116, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2511, 0.3551, 0.2444, 0.0058, 0.1021, 0.0375],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 116, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1090e-01, 3.1836e-05, 1.6305e-02, 8.8662e-08, 2.5447e-07, 2.1788e-02,
        2.5098e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.093

[Epoch: 116, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9478e-03, 4.0669e-03, 9.8341e-01, 2.9831e-07, 3.0889e-03, 3.3828e-03,
        3.0996e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 116, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2759e-01, 2.8540e-03, 6.6197e-02, 2.2390e-07, 1.7409e-01, 3.2141e-01,
        7.8612e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.094

[Epoch: 117, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.0218e-03, 4.0476e-02, 4.2759e-02, 6.9090e-09, 4.4530e-01, 3.3325e-01,
        1.3419e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.002

[Epoch: 117, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2726, 0.3325, 0.2474, 0.0069, 0.1005, 0.0361],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 117, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4944e-01, 2.9406e-05, 8.2967e-03, 9.3886e-08, 4.3973e-07, 1.7939e-02,
        3.2430e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.130

[Epoch: 117, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9914e-03, 3.8845e-03, 9.8229e-01, 5.7646e-07, 3.4951e-03, 3.4351e-03,
        3.9068e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 117, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3093e-01, 4.5645e-03, 5.4828e-02, 4.7367e-07, 1.9585e-01, 3.0082e-01,
        1.3008e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.092

[Epoch: 118, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.4412e-03, 3.4545e-02, 5.7054e-02, 9.2963e-09, 4.8787e-01, 2.9569e-01,
        1.2240e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 118, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2587, 0.3363, 0.2416, 0.0072, 0.1187, 0.0337],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 118, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.9262e-01, 3.2254e-05, 6.4199e-03, 7.1103e-08, 4.5703e-07, 1.5201e-02,
        1.8573e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.087

[Epoch: 118, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5043e-03, 2.6486e-03, 9.8573e-01, 4.4795e-07, 2.9910e-03, 2.8598e-03,
        3.2667e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 118, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3646e-01, 4.2711e-03, 6.6285e-02, 8.0533e-07, 1.8473e-01, 2.9666e-01,
        1.1595e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.121

[Epoch: 119, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4606e-03, 5.6424e-02, 5.5089e-02, 7.5950e-08, 4.1037e-01, 3.3470e-01,
        1.3996e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 119, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2863, 0.3417, 0.2308, 0.0063, 0.0862, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.004

[Epoch: 119, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8697e-01, 2.4664e-05, 1.3426e-02, 5.9906e-08, 1.9131e-07, 1.5104e-02,
        2.8447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.131

[Epoch: 119, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1851e-03, 3.1347e-03, 9.8036e-01, 2.5480e-07, 4.3872e-03, 3.7350e-03,
        4.1980e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 119, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2785e-01, 3.9917e-03, 5.7223e-02, 4.3563e-07, 1.6931e-01, 3.3311e-01,
        8.5137e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.092

[Epoch: 120, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3763e-03, 5.8165e-02, 4.7939e-02, 3.2301e-08, 4.7447e-01, 2.7912e-01,
        1.3593e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 120, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2450, 0.3479, 0.2478, 0.0074, 0.1156, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.009

[Epoch: 120, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.5104e-01, 6.4937e-05, 1.5537e-02, 2.2519e-07, 6.4511e-07, 2.7558e-02,
        3.0580e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.069

[Epoch: 120, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8829e-03, 3.9833e-03, 9.8466e-01, 4.8029e-07, 2.8908e-03, 2.7242e-03,
        2.8564e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 120, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2497e-01, 4.1239e-03, 5.8714e-02, 4.4236e-07, 1.7441e-01, 3.2855e-01,
        9.2369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.077

[Epoch: 121, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.1070e-03, 4.3770e-02, 6.1889e-02, 4.0433e-08, 4.3709e-01, 3.2578e-01,
        1.2736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 121, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2470, 0.3341, 0.2622, 0.0060, 0.1158, 0.0307],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.016

[Epoch: 121, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([8.0183e-01, 1.2400e-05, 2.9462e-03, 5.0292e-08, 2.3675e-07, 7.1259e-03,
        1.8808e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.269

[Epoch: 121, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1271e-03, 3.3863e-03, 9.8549e-01, 4.8278e-07, 2.4035e-03, 2.6830e-03,
        2.9120e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.022

[Epoch: 121, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5914e-01, 3.6541e-03, 5.1655e-02, 3.8769e-07, 1.8589e-01, 2.8781e-01,
        1.1850e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 122, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.4334e-03, 3.9293e-02, 4.1529e-02, 1.6901e-08, 4.5656e-01, 3.3068e-01,
        1.2950e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 122, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2838, 0.3212, 0.2354, 0.0067, 0.1057, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 122, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7593e-01, 4.3086e-05, 1.6616e-02, 6.0401e-08, 4.8948e-07, 2.8556e-02,
        2.7885e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 0.003

[Epoch: 122, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7027e-03, 2.4342e-03, 9.8469e-01, 5.2109e-07, 3.2404e-03, 2.9005e-03,
        4.0359e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 122, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4435e-01, 3.9795e-03, 6.7067e-02, 3.0906e-07, 1.7097e-01, 3.0510e-01,
        8.5272e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 123, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3400e-03, 4.9472e-02, 5.8646e-02, 1.9623e-08, 4.4711e-01, 2.8861e-01,
        1.5281e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 123, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2642, 0.3481, 0.2484, 0.0059, 0.0977, 0.0321],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 123, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7744e-01, 4.2503e-05, 1.0509e-02, 1.5892e-07, 4.1470e-07, 1.3030e-02,
        2.9897e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.131

[Epoch: 123, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4298e-03, 4.5973e-03, 9.8113e-01, 5.6777e-07, 3.8735e-03, 2.5924e-03,
        3.3729e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 123, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1566e-01, 4.8093e-03, 5.6938e-02, 6.1808e-07, 1.7963e-01, 3.3375e-01,
        9.2062e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 124, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3746e-03, 4.9273e-02, 5.0486e-02, 2.2314e-08, 4.5725e-01, 3.1090e-01,
        1.2872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 124, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2500, 0.3594, 0.2328, 0.0060, 0.1097, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.012

[Epoch: 124, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2522e-01, 4.3791e-05, 9.3931e-03, 1.3037e-07, 4.3479e-07, 1.5979e-02,
        2.4937e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.112

[Epoch: 124, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2450e-03, 4.1259e-03, 9.7917e-01, 4.8988e-07, 4.9330e-03, 3.6004e-03,
        3.9275e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 124, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4353e-01, 3.1406e-03, 6.4057e-02, 3.1651e-07, 1.8503e-01, 2.9278e-01,
        1.1459e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 125, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9324e-03, 4.3946e-02, 6.1194e-02, 2.0356e-08, 4.4213e-01, 3.2148e-01,
        1.2832e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 125, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2599, 0.3057, 0.2758, 0.0087, 0.1052, 0.0402],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 125, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9912e-01, 3.4415e-05, 1.2513e-02, 7.1822e-08, 3.7502e-07, 1.9519e-02,
        2.6881e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.106

[Epoch: 125, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7718e-03, 2.8321e-03, 9.8504e-01, 4.0647e-07, 3.0949e-03, 3.0280e-03,
        3.2296e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 125, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5930e-01, 4.3801e-03, 4.9743e-02, 4.8759e-07, 1.7101e-01, 3.0498e-01,
        1.0589e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.074

[Epoch: 126, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9924e-03, 4.1514e-02, 3.6403e-02, 1.4465e-08, 4.9668e-01, 2.8735e-01,
        1.3506e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 126, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2929, 0.3397, 0.2188, 0.0053, 0.1084, 0.0315],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 126, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2945e-01, 5.9971e-05, 1.0209e-02, 2.1759e-07, 6.6727e-07, 1.5416e-02,
        2.4487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.145

[Epoch: 126, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6913e-03, 4.7049e-03, 9.8068e-01, 6.5415e-07, 4.0598e-03, 2.9216e-03,
        3.9376e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 126, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2709e-01, 4.0567e-03, 6.5167e-02, 3.8830e-07, 1.8117e-01, 3.1281e-01,
        9.7066e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.114

[Epoch: 127, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8619e-03, 5.2175e-02, 7.0531e-02, 1.5165e-08, 4.1694e-01, 3.2790e-01,
        1.2859e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 127, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2330, 0.3812, 0.2297, 0.0058, 0.1056, 0.0412],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.000

[Epoch: 127, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1184e-01, 2.6598e-05, 5.9002e-03, 5.5072e-08, 2.5755e-07, 1.1905e-02,
        2.7033e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.114

[Epoch: 127, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0964e-03, 3.8377e-03, 9.8070e-01, 3.9941e-07, 3.5956e-03, 3.8446e-03,
        3.9209e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 127, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1589e-01, 2.6836e-03, 5.9631e-02, 4.4161e-07, 1.8884e-01, 3.2449e-01,
        8.4618e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.087

[Epoch: 128, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5368e-03, 5.0613e-02, 5.3072e-02, 2.0884e-08, 4.4664e-01, 2.9487e-01,
        1.5127e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 128, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0057, 0.2645, 0.3043, 0.2768, 0.0091, 0.1040, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 128, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8414e-01, 2.0820e-05, 1.4020e-02, 3.2142e-08, 2.6569e-07, 1.8780e-02,
        2.8304e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.117

[Epoch: 128, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5731e-03, 2.6603e-03, 9.8726e-01, 3.1903e-07, 2.5235e-03, 2.3874e-03,
        2.5969e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 128, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3695e-01, 3.8847e-03, 5.5291e-02, 2.9283e-07, 1.7723e-01, 3.1340e-01,
        1.3238e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 129, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.5304e-03, 4.1153e-02, 4.6762e-02, 1.2940e-08, 4.6734e-01, 3.2638e-01,
        1.1584e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 129, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0040, 0.2804, 0.3419, 0.2209, 0.0066, 0.1158, 0.0304],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.001

[Epoch: 129, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1434e-01, 3.4950e-05, 1.0148e-02, 4.8728e-08, 4.9113e-07, 2.0438e-02,
        2.5504e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.109

[Epoch: 129, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7186e-03, 3.0514e-03, 9.8466e-01, 4.0180e-07, 3.5617e-03, 2.6950e-03,
        3.3086e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.017

[Epoch: 129, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2199e-01, 5.7624e-03, 6.4540e-02, 2.8069e-07, 1.8837e-01, 3.0811e-01,
        1.1223e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.107

[Epoch: 130, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7031e-03, 4.4531e-02, 5.2783e-02, 7.0597e-08, 4.4130e-01, 3.1461e-01,
        1.4307e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 130, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2483, 0.3551, 0.2487, 0.0068, 0.0991, 0.0383],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 130, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4403e-01, 2.9560e-05, 7.9129e-03, 7.2498e-08, 3.4123e-07, 1.3189e-02,
        2.3484e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 130, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8769e-03, 4.4882e-03, 9.7652e-01, 8.8001e-07, 4.3698e-03, 5.9692e-03,
        4.7727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 130, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1256e-01, 4.0881e-03, 5.9956e-02, 3.1988e-07, 2.0796e-01, 3.0633e-01,
        9.1090e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.086

[Epoch: 131, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9891e-03, 5.1669e-02, 5.9099e-02, 3.0481e-08, 4.4930e-01, 2.8572e-01,
        1.5023e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 131, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 1
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2966, 0.2910, 0.2531, 0.0063, 0.1112, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 131, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.5033e-01, 3.4249e-05, 9.5809e-03, 1.8905e-07, 4.6094e-07, 1.6076e-02,
        3.2397e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.117

[Epoch: 131, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8242e-03, 3.3701e-03, 9.8245e-01, 3.1541e-07, 4.0184e-03, 2.3192e-03,
        4.0150e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.001

[Epoch: 131, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.6172e-01, 3.2987e-03, 6.0205e-02, 4.7887e-07, 1.4315e-01, 3.2100e-01,
        1.0627e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.109

[Epoch: 132, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5162e-03, 4.7173e-02, 5.2273e-02, 3.3723e-08, 4.4633e-01, 3.2266e-01,
        1.2805e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 132, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2242, 0.3823, 0.2431, 0.0075, 0.1009, 0.0376],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.000

[Epoch: 132, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.6477e-01, 3.6449e-05, 1.2136e-02, 4.1723e-08, 3.2557e-07, 2.0620e-02,
        2.0243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.113

[Epoch: 132, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0112e-03, 2.0224e-03, 9.8892e-01, 3.0143e-07, 2.0586e-03, 2.4451e-03,
        2.5395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.015

[Epoch: 132, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4065e-01, 3.5496e-03, 5.2813e-02, 5.0952e-07, 1.6620e-01, 3.2780e-01,
        8.9881e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 133, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0573e-03, 4.9505e-02, 5.6336e-02, 3.4207e-08, 4.5726e-01, 3.2234e-01,
        1.1151e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 133, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0047, 0.2554, 0.3424, 0.2487, 0.0091, 0.1026, 0.0371],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 133, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9614e-01, 3.3707e-05, 1.1178e-02, 1.2128e-07, 3.8584e-07, 1.6439e-02,
        2.7621e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.097

[Epoch: 133, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2518e-03, 4.7434e-03, 9.8214e-01, 4.6364e-07, 3.2525e-03, 3.2397e-03,
        3.3704e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 133, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1473e-01, 3.2540e-03, 6.2747e-02, 6.5445e-07, 2.1310e-01, 2.9676e-01,
        9.3987e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.079

[Epoch: 134, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5917e-03, 4.1448e-02, 3.5975e-02, 1.0592e-08, 4.8244e-01, 2.9242e-01,
        1.4413e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.003

[Epoch: 134, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2621, 0.3288, 0.2429, 0.0057, 0.1241, 0.0325],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.003

[Epoch: 134, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9796e-01, 3.9475e-05, 9.8366e-03, 1.3899e-07, 5.5382e-07, 1.7373e-02,
        2.7479e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.150

[Epoch: 134, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.7630e-03, 3.5783e-03, 9.8067e-01, 7.3569e-07, 4.4045e-03, 3.8387e-03,
        3.7417e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 134, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3296e-01, 4.8758e-03, 6.3920e-02, 5.3576e-07, 1.7881e-01, 3.0895e-01,
        1.0482e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.106

[Epoch: 135, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0913e-03, 4.4790e-02, 6.3963e-02, 3.2167e-08, 4.3886e-01, 3.0396e-01,
        1.4533e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 135, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2740, 0.3695, 0.2166, 0.0057, 0.0947, 0.0363],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 135, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2104e-01, 2.8577e-05, 9.1057e-03, 7.4888e-08, 3.2220e-07, 1.6468e-02,
        2.5336e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.123

[Epoch: 135, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0142e-03, 3.2154e-03, 9.8416e-01, 3.1307e-07, 3.5222e-03, 3.2721e-03,
        2.8167e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 135, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3026e-01, 3.6889e-03, 5.8531e-02, 7.2784e-07, 1.8311e-01, 3.1513e-01,
        9.2882e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 136, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9087e-03, 5.2208e-02, 5.5515e-02, 4.3828e-08, 4.2872e-01, 3.2435e-01,
        1.3530e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 136, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0048, 0.2564, 0.3024, 0.2929, 0.0080, 0.0958, 0.0396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 136, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2092e-01, 2.6417e-05, 1.1330e-02, 8.1747e-08, 2.8936e-07, 1.7059e-02,
        2.5066e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.093

[Epoch: 136, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9604e-03, 3.6110e-03, 9.8394e-01, 2.8136e-07, 2.9001e-03, 3.1456e-03,
        3.4409e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 136, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3179e-01, 3.2588e-03, 5.3461e-02, 3.5718e-07, 1.7601e-01, 3.2723e-01,
        8.2483e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.105

[Epoch: 137, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8432e-03, 4.6600e-02, 5.2427e-02, 1.3474e-08, 4.7043e-01, 3.0750e-01,
        1.1920e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 137, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2684, 0.3488, 0.2151, 0.0071, 0.1189, 0.0379],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.002

[Epoch: 137, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8339e-01, 3.2021e-05, 9.3334e-03, 1.3463e-07, 3.5458e-07, 1.7443e-02,
        2.8980e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.128

[Epoch: 137, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.5010e-03, 4.4482e-03, 9.7916e-01, 2.4420e-07, 4.3007e-03, 3.7245e-03,
        3.8678e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 137, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3309e-01, 3.7870e-03, 6.4592e-02, 4.7343e-07, 1.8287e-01, 3.0474e-01,
        1.0920e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.084

[Epoch: 138, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9647e-03, 5.0163e-02, 5.4286e-02, 2.0242e-08, 4.6305e-01, 2.8851e-01,
        1.4103e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 138, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2427, 0.3536, 0.2519, 0.0071, 0.1075, 0.0339],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 138, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2742e-01, 2.9744e-05, 8.8228e-03, 4.2104e-08, 3.6983e-07, 1.3291e-02,
        2.5043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.116

[Epoch: 138, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9655e-03, 2.5309e-03, 9.8510e-01, 3.5393e-07, 3.4177e-03, 2.7883e-03,
        3.2013e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 138, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4127e-01, 4.9393e-03, 5.7555e-02, 3.5342e-07, 1.7249e-01, 3.1221e-01,
        1.1543e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.110

[Epoch: 139, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7711e-03, 4.1525e-02, 5.4188e-02, 3.1224e-08, 4.4071e-01, 3.3438e-01,
        1.2643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 139, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2984, 0.3147, 0.2478, 0.0057, 0.0986, 0.0313],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 139, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2052e-01, 2.1268e-05, 1.1535e-02, 6.1474e-08, 2.4279e-07, 1.9856e-02,
        2.4806e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.113

[Epoch: 139, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.1841e-03, 3.0973e-03, 9.8594e-01, 1.9002e-07, 2.7589e-03, 2.8529e-03,
        3.1620e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 139, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.5169e-01, 4.6370e-03, 6.0738e-02, 3.6881e-07, 1.7324e-01, 3.0075e-01,
        8.9451e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.089

[Epoch: 140, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8771e-03, 4.6083e-02, 4.5672e-02, 1.1482e-08, 4.3987e-01, 3.0416e-01,
        1.6034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 140, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0043, 0.2517, 0.3484, 0.2351, 0.0076, 0.1080, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.014

[Epoch: 140, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.6574e-01, 2.8905e-05, 1.0948e-02, 9.8468e-08, 3.6940e-07, 2.0744e-02,
        3.0254e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.119

[Epoch: 140, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.3444e-03, 4.9202e-03, 9.7804e-01, 3.3388e-07, 4.6266e-03, 3.7817e-03,
        4.2871e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 140, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2134e-01, 2.7564e-03, 5.6436e-02, 3.1960e-07, 1.8530e-01, 3.2582e-01,
        8.3411e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.089

[Epoch: 141, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9883e-03, 5.5731e-02, 6.7088e-02, 4.1341e-08, 4.6782e-01, 2.9961e-01,
        1.0575e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 141, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0043, 0.2455, 0.3471, 0.2468, 0.0080, 0.1069, 0.0415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.009

[Epoch: 141, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.6355e-01, 2.3539e-05, 8.6559e-03, 1.3285e-07, 4.4559e-07, 1.2196e-02,
        2.1557e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.142

[Epoch: 141, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8288e-03, 3.8520e-03, 9.8219e-01, 2.7270e-07, 3.4845e-03, 3.1178e-03,
        3.5222e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 141, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3879e-01, 3.8475e-03, 5.3621e-02, 5.6367e-07, 1.8574e-01, 3.0510e-01,
        1.2901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.106

[Epoch: 142, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.3374e-03, 2.6959e-02, 4.6177e-02, 7.0936e-09, 4.5043e-01, 3.3666e-01,
        1.3744e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 142, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2884, 0.3201, 0.2450, 0.0059, 0.1056, 0.0306],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 142, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9714e-01, 3.4940e-05, 1.4680e-02, 5.8450e-08, 3.4607e-07, 1.3443e-02,
        2.7470e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.085

[Epoch: 142, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1073e-03, 3.1991e-03, 9.8193e-01, 2.0548e-07, 4.2307e-03, 4.2024e-03,
        3.3347e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 142, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2917e-01, 5.5190e-03, 7.0087e-02, 4.9002e-07, 1.7553e-01, 3.0992e-01,
        9.7828e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 143, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5561e-03, 7.4400e-02, 5.7398e-02, 2.7359e-08, 4.2715e-01, 2.8629e-01,
        1.5121e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 143, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2463, 0.3708, 0.2366, 0.0067, 0.1010, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 143, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.8070e-01, 3.9843e-05, 7.8241e-03, 9.0713e-08, 4.6061e-07, 1.9388e-02,
        2.9204e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.118

[Epoch: 143, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.7052e-03, 2.8392e-03, 9.8311e-01, 3.6927e-07, 3.6540e-03, 4.1781e-03,
        3.5096e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 143, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2684e-01, 3.5000e-03, 6.5281e-02, 4.6478e-07, 1.6625e-01, 3.2999e-01,
        8.1386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 144, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9094e-03, 4.6314e-02, 5.0241e-02, 1.7298e-08, 4.5912e-01, 3.1979e-01,
        1.2063e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 144, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2784, 0.3128, 0.2387, 0.0075, 0.1240, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 144, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3486e-01, 2.3269e-05, 8.2143e-03, 9.0228e-08, 3.0937e-07, 2.0009e-02,
        2.3689e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.163

[Epoch: 144, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1957e-03, 3.4589e-03, 9.8703e-01, 1.4282e-07, 1.9992e-03, 1.7661e-03,
        2.5539e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.003

[Epoch: 144, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2099e-01, 3.3398e-03, 5.0879e-02, 4.0815e-07, 1.9941e-01, 3.1225e-01,
        1.3130e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.109

[Epoch: 145, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0558e-03, 3.5709e-02, 4.7347e-02, 1.5989e-08, 4.7906e-01, 2.9910e-01,
        1.3573e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 145, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2417, 0.3613, 0.2561, 0.0074, 0.0923, 0.0369],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.011

[Epoch: 145, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9079e-01, 2.7229e-05, 1.1198e-02, 6.5286e-08, 3.2676e-07, 1.3508e-02,
        2.8447e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.087

[Epoch: 145, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4771e-03, 3.3500e-03, 9.8239e-01, 2.3702e-07, 4.5885e-03, 3.5779e-03,
        3.6161e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.026

[Epoch: 145, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2948e-01, 3.7888e-03, 6.7633e-02, 5.8814e-07, 1.7884e-01, 3.1011e-01,
        1.0149e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.094

[Epoch: 146, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3236e-03, 4.5914e-02, 7.7877e-02, 3.3634e-08, 4.1012e-01, 3.2710e-01,
        1.3566e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 146, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2735, 0.3372, 0.2453, 0.0064, 0.0979, 0.0359],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.002

[Epoch: 146, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2493e-01, 3.0351e-05, 8.9799e-03, 9.5491e-08, 3.9350e-07, 1.7302e-02,
        2.4876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.151

[Epoch: 146, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([5.2011e-03, 4.7264e-03, 9.7407e-01, 8.4806e-07, 5.3877e-03, 5.8961e-03,
        4.7202e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 146, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1792e-01, 4.3508e-03, 6.8778e-02, 1.8566e-07, 1.8518e-01, 3.1410e-01,
        9.6789e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.068

[Epoch: 147, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.3215e-03, 5.3474e-02, 3.8669e-02, 3.7088e-08, 4.5905e-01, 3.0041e-01,
        1.4408e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 147, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2782, 0.3396, 0.2157, 0.0065, 0.1176, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 147, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9773e-01, 4.4903e-05, 1.0628e-02, 8.0410e-08, 2.0142e-07, 1.5064e-02,
        2.7653e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.101

[Epoch: 147, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2713e-03, 2.9160e-03, 9.8474e-01, 1.8512e-07, 2.6581e-03, 2.8988e-03,
        3.5198e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 147, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4386e-01, 4.3775e-03, 5.9450e-02, 7.3869e-07, 1.7874e-01, 3.0262e-01,
        1.0962e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.104

[Epoch: 148, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3266e-03, 4.2973e-02, 5.6217e-02, 6.8400e-08, 4.5422e-01, 3.1726e-01,
        1.2600e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.002

[Epoch: 148, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0044, 0.2369, 0.3305, 0.2697, 0.0070, 0.1108, 0.0407],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 148, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1247e-01, 4.5655e-05, 1.0395e-02, 1.8001e-07, 5.3825e-07, 1.7086e-02,
        2.6001e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.104

[Epoch: 148, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1880e-03, 2.7235e-03, 9.8537e-01, 3.2632e-07, 3.0599e-03, 2.7492e-03,
        2.9053e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 148, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3501e-01, 3.7530e-03, 4.8812e-02, 5.7534e-07, 1.7172e-01, 3.3113e-01,
        9.5735e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 149, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4832e-03, 5.3268e-02, 5.0372e-02, 3.8108e-08, 4.6846e-01, 3.0205e-01,
        1.2236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 149, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2802, 0.3326, 0.2521, 0.0076, 0.0886, 0.0353],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 149, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9293e-01, 6.1283e-05, 1.2259e-02, 1.3921e-07, 5.2613e-07, 1.6949e-02,
        2.7780e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.124

[Epoch: 149, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8731e-03, 3.4736e-03, 9.8254e-01, 3.8062e-07, 3.2016e-03, 3.5942e-03,
        3.3214e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 149, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3114e-01, 3.0215e-03, 6.3014e-02, 2.3593e-07, 1.8775e-01, 3.0473e-01,
        1.0344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.094

[Epoch: 150, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6177e-03, 4.0589e-02, 5.9479e-02, 1.3528e-08, 4.5209e-01, 3.0630e-01,
        1.3792e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 150, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2447, 0.3445, 0.2414, 0.0093, 0.1212, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 150, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4543e-01, 2.2817e-05, 8.7492e-03, 8.2475e-08, 5.2384e-07, 1.7848e-02,
        2.2795e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.130

[Epoch: 150, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2763e-03, 3.7800e-03, 9.8284e-01, 5.8620e-07, 3.5111e-03, 2.7124e-03,
        3.8776e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 150, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2439e-01, 5.5514e-03, 6.9701e-02, 7.1160e-07, 1.8678e-01, 3.0251e-01,
        1.1067e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.110

[Epoch: 151, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2821e-03, 4.1625e-02, 5.1150e-02, 2.4239e-08, 4.3689e-01, 3.2501e-01,
        1.4204e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 151, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2652, 0.3503, 0.2294, 0.0057, 0.1106, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 151, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9172e-01, 3.8507e-05, 8.8612e-03, 8.2923e-08, 3.6616e-07, 1.4217e-02,
        2.8516e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.122

[Epoch: 151, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3816e-03, 3.2830e-03, 9.8247e-01, 4.2823e-07, 4.0999e-03, 3.2044e-03,
        3.5618e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 151, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4539e-01, 3.3527e-03, 5.8831e-02, 4.1600e-07, 1.5915e-01, 3.2303e-01,
        1.0241e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 152, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.1568e-03, 6.0186e-02, 5.5648e-02, 2.8014e-08, 4.5711e-01, 2.8272e-01,
        1.4118e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 152, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2872, 0.3194, 0.2432, 0.0070, 0.0994, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 152, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2157e-01, 3.9633e-05, 1.0852e-02, 7.8873e-08, 3.4634e-07, 1.8749e-02,
        2.4879e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 152, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2633e-03, 3.2925e-03, 9.8460e-01, 3.1549e-07, 2.9108e-03, 2.9515e-03,
        2.9773e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 152, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3278e-01, 3.6621e-03, 5.6492e-02, 3.0474e-07, 1.8247e-01, 3.1458e-01,
        1.0011e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 153, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6073e-03, 4.4166e-02, 5.2691e-02, 2.4652e-08, 4.5211e-01, 3.1908e-01,
        1.2834e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 153, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2546, 0.3496, 0.2421, 0.0077, 0.1067, 0.0358],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 153, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0395e-01, 2.8958e-05, 9.1704e-03, 5.8175e-08, 2.8326e-07, 1.6194e-02,
        2.7065e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.117

[Epoch: 153, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4489e-03, 3.5699e-03, 9.8306e-01, 2.7623e-07, 3.2906e-03, 2.9459e-03,
        3.6810e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 153, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2874e-01, 3.8720e-03, 5.7518e-02, 3.3095e-07, 1.9342e-01, 3.0570e-01,
        1.0754e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 154, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0914e-03, 4.6500e-02, 4.9606e-02, 1.4447e-08, 4.5340e-01, 3.1517e-01,
        1.3223e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 154, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2583, 0.3398, 0.2473, 0.0067, 0.1098, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 154, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1250e-01, 2.8810e-05, 9.8451e-03, 6.1662e-08, 3.4029e-07, 1.6315e-02,
        2.6131e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.113

[Epoch: 154, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5069e-03, 3.4312e-03, 9.8235e-01, 2.7693e-07, 3.8159e-03, 3.2673e-03,
        3.6250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 154, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2968e-01, 3.8964e-03, 6.3197e-02, 2.8350e-07, 1.7627e-01, 3.1575e-01,
        1.1208e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 155, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3066e-03, 4.6176e-02, 5.7079e-02, 1.2747e-08, 4.6006e-01, 3.0449e-01,
        1.2889e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 155, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0032, 0.2618, 0.3480, 0.2439, 0.0065, 0.1017, 0.0349],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 155, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1563e-01, 2.8457e-05, 1.0331e-02, 5.7745e-08, 2.6328e-07, 1.6852e-02,
        2.5716e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.111

[Epoch: 155, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5024e-03, 3.4225e-03, 9.8285e-01, 2.4246e-07, 3.4841e-03, 3.1692e-03,
        3.5727e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 155, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4180e-01, 3.7113e-03, 6.0189e-02, 2.0004e-07, 1.7041e-01, 3.1424e-01,
        9.6527e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 156, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5799e-03, 4.6666e-02, 5.4404e-02, 1.5413e-08, 4.4637e-01, 3.1051e-01,
        1.3846e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 156, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2687, 0.3375, 0.2370, 0.0066, 0.1077, 0.0392],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 156, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9891e-01, 1.8636e-05, 9.7893e-03, 4.6296e-08, 1.8281e-07, 1.7681e-02,
        2.7360e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.130

[Epoch: 156, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0382e-03, 3.0923e-03, 9.8416e-01, 2.2152e-07, 3.4574e-03, 2.8431e-03,
        3.4094e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 156, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2684e-01, 3.3845e-03, 5.6988e-02, 1.7860e-07, 1.7980e-01, 3.2343e-01,
        9.5648e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 157, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7187e-03, 5.0995e-02, 5.5097e-02, 7.9841e-09, 4.4659e-01, 3.0909e-01,
        1.3451e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 157, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2485, 0.3396, 0.2550, 0.0071, 0.1107, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 157, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1925e-01, 2.0559e-05, 1.0629e-02, 3.3192e-08, 1.8421e-07, 1.5996e-02,
        2.5411e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 157, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8541e-03, 3.3262e-03, 9.8522e-01, 2.0536e-07, 2.6590e-03, 2.9885e-03,
        2.9498e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 157, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3242e-01, 3.1160e-03, 5.6964e-02, 2.0790e-07, 1.9001e-01, 3.0730e-01,
        1.0191e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 158, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3664e-03, 3.9071e-02, 4.4976e-02, 5.4295e-09, 4.6730e-01, 3.2208e-01,
        1.2321e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 158, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2763, 0.3266, 0.2518, 0.0070, 0.1001, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 158, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0741e-01, 1.4812e-05, 1.0315e-02, 3.0252e-08, 2.8978e-07, 1.5821e-02,
        2.6644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.114

[Epoch: 158, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.2331e-03, 3.8605e-03, 9.7957e-01, 2.0936e-07, 3.7743e-03, 4.5621e-03,
        3.9952e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.001

[Epoch: 158, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4429e-01, 4.2249e-03, 6.2906e-02, 2.4547e-07, 1.8149e-01, 2.9545e-01,
        1.1628e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

[Epoch: 159, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.9912e-03, 4.9576e-02, 6.0533e-02, 1.1626e-08, 4.5217e-01, 2.9150e-01,
        1.4323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 159, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2710, 0.3536, 0.2198, 0.0070, 0.1029, 0.0416],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 159, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0942e-01, 2.6816e-05, 9.2642e-03, 4.6702e-08, 2.6728e-07, 1.6101e-02,
        2.6518e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.123

[Epoch: 159, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.4419e-03, 4.3576e-03, 9.7788e-01, 2.9027e-07, 5.1499e-03, 3.5992e-03,
        4.5740e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.002

[Epoch: 159, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2490e-01, 4.1908e-03, 6.9072e-02, 1.5009e-07, 1.7629e-01, 3.1560e-01,
        9.9540e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 160, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4370e-03, 5.1363e-02, 5.0812e-02, 5.0889e-08, 4.2843e-01, 3.2866e-01,
        1.3730e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 160, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2542, 0.3422, 0.2404, 0.0060, 0.1208, 0.0328],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 160, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2603e-01, 3.0973e-05, 7.3469e-03, 3.6679e-08, 1.6988e-07, 2.0020e-02,
        2.4658e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 160, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.4799e-03, 2.7786e-03, 9.8541e-01, 3.1714e-07, 3.5994e-03, 2.4254e-03,
        3.3113e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.003

[Epoch: 160, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4019e-01, 3.2638e-03, 5.4032e-02, 2.6567e-07, 1.8148e-01, 3.1278e-01,
        8.2511e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 161, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.5192e-03, 5.1845e-02, 5.4785e-02, 1.0107e-08, 4.6494e-01, 2.8214e-01,
        1.4177e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 161, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2394, 0.3379, 0.2734, 0.0079, 0.1003, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.003

[Epoch: 161, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.7522e-01, 2.8287e-05, 1.4516e-02, 2.6531e-08, 2.8855e-07, 1.5465e-02,
        2.9477e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.145

[Epoch: 161, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.0402e-03, 2.5784e-03, 9.8803e-01, 2.8602e-07, 2.1757e-03, 2.2150e-03,
        2.9599e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.017

[Epoch: 161, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3159e-01, 4.0300e-03, 5.3406e-02, 5.9375e-07, 1.7526e-01, 3.2507e-01,
        1.0634e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 162, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3196e-03, 4.7137e-02, 4.1349e-02, 1.4347e-08, 5.0872e-01, 3.0149e-01,
        9.7985e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.001

[Epoch: 162, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2788, 0.3237, 0.2544, 0.0065, 0.1016, 0.0317],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 162, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4341e-01, 1.5081e-05, 8.9513e-03, 5.1026e-08, 5.5305e-07, 1.2617e-02,
        2.3501e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.072

[Epoch: 162, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2610e-03, 3.7212e-03, 9.8231e-01, 6.4388e-07, 3.6618e-03, 3.8497e-03,
        3.1991e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 162, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4108e-01, 4.1767e-03, 5.6933e-02, 6.3822e-07, 1.9245e-01, 2.9569e-01,
        9.6770e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 163, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.7586e-03, 3.3579e-02, 5.9315e-02, 2.3951e-08, 3.7725e-01, 3.5867e-01,
        1.6643e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.000

[Epoch: 163, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0042, 0.2680, 0.3516, 0.2295, 0.0070, 0.1095, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 163, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9324e-01, 1.4903e-05, 9.4314e-03, 3.3885e-08, 4.9196e-07, 1.8527e-02,
        2.7878e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.125

[Epoch: 163, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3323e-03, 3.4683e-03, 9.8318e-01, 7.3754e-07, 4.0147e-03, 2.6979e-03,
        3.3022e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.012

[Epoch: 163, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3346e-01, 3.2855e-03, 6.4656e-02, 8.8502e-07, 1.7254e-01, 3.1727e-01,
        8.7815e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 164, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7097e-03, 5.2933e-02, 5.5092e-02, 4.2657e-08, 4.6263e-01, 2.9355e-01,
        1.3208e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 164, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0040, 0.2509, 0.3538, 0.2342, 0.0078, 0.1052, 0.0441],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.011

[Epoch: 164, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0691e-01, 2.0468e-05, 1.1365e-02, 6.1433e-08, 4.9233e-07, 1.7857e-02,
        2.6385e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.123

[Epoch: 164, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3573e-03, 3.5444e-03, 9.8323e-01, 3.2382e-07, 3.3534e-03, 2.9021e-03,
        3.6126e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 164, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3057e-01, 3.9195e-03, 6.4821e-02, 9.4686e-07, 1.6988e-01, 3.2100e-01,
        9.8164e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 165, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4388e-03, 4.5142e-02, 5.8946e-02, 1.9919e-08, 4.5183e-01, 3.0626e-01,
        1.3438e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.000

[Epoch: 165, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0040, 0.2759, 0.3216, 0.2451, 0.0074, 0.1099, 0.0361],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 165, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2502e-01, 1.6399e-05, 9.4903e-03, 3.5678e-08, 2.9640e-07, 1.6487e-02,
        2.4898e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.118

[Epoch: 165, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4877e-03, 3.3476e-03, 9.8301e-01, 3.2030e-07, 3.1619e-03, 3.2999e-03,
        3.6884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 165, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3788e-01, 3.9694e-03, 6.2367e-02, 7.6724e-07, 1.8117e-01, 3.0433e-01,
        1.0282e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 166, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6784e-03, 4.7426e-02, 4.8666e-02, 1.4894e-08, 4.5159e-01, 3.1524e-01,
        1.3341e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 166, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2593, 0.3454, 0.2426, 0.0075, 0.1062, 0.0352],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 166, batch: 123/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9958e-01, 1.8659e-05, 9.6286e-03, 3.3431e-08, 3.1501e-07, 1.7519e-02,
        2.7325e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.120

[Epoch: 166, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2828e-03, 3.6249e-03, 9.8319e-01, 3.2100e-07, 3.2344e-03, 3.1187e-03,
        3.5455e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 166, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3441e-01, 3.7631e-03, 6.0554e-02, 7.2144e-07, 1.7802e-01, 3.1283e-01,
        1.0423e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 167, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6179e-03, 4.4222e-02, 5.4275e-02, 1.1010e-08, 4.5299e-01, 3.1068e-01,
        1.3422e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 167, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2691, 0.3455, 0.2360, 0.0067, 0.1025, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 167, batch: 123/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1700e-01, 1.4879e-05, 9.9961e-03, 2.6261e-08, 2.7905e-07, 1.5276e-02,
        2.5771e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.122

[Epoch: 167, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2334e-03, 3.6347e-03, 9.8305e-01, 3.4211e-07, 3.2777e-03, 3.1934e-03,
        3.6077e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 167, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3190e-01, 3.5433e-03, 5.8955e-02, 5.6537e-07, 1.7586e-01, 3.1960e-01,
        1.0144e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.104

[Epoch: 168, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3866e-03, 4.7571e-02, 5.3395e-02, 9.7104e-09, 4.5212e-01, 3.1059e-01,
        1.3294e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 168, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2612, 0.3392, 0.2448, 0.0064, 0.1071, 0.0374],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 168, batch: 123/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0610e-01, 1.1446e-05, 9.7592e-03, 1.9115e-08, 2.0509e-07, 1.8487e-02,
        2.6564e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.128

[Epoch: 168, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0545e-03, 3.4424e-03, 9.8316e-01, 2.7496e-07, 3.3991e-03, 3.2753e-03,
        3.6701e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 168, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3128e-01, 3.4724e-03, 5.9352e-02, 4.5719e-07, 1.8075e-01, 3.1512e-01,
        1.0017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 169, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3806e-03, 4.8341e-02, 5.2142e-02, 9.1381e-09, 4.5223e-01, 3.1116e-01,
        1.3275e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 169, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2579, 0.3396, 0.2438, 0.0072, 0.1123, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 169, batch: 123/208] total loss per batch: 0.541
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0474e-01, 1.1715e-05, 1.0390e-02, 1.7410e-08, 2.2052e-07, 1.6106e-02,
        2.6876e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.123

[Epoch: 169, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1086e-03, 3.1529e-03, 9.8401e-01, 1.9865e-07, 3.1777e-03, 3.2070e-03,
        3.3475e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 169, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2845e-01, 3.7083e-03, 6.3424e-02, 6.3836e-07, 1.8623e-01, 3.0806e-01,
        1.0126e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 170, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3631e-03, 4.7068e-02, 5.1486e-02, 8.3543e-09, 4.5600e-01, 3.1021e-01,
        1.3187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 170, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0030, 0.2762, 0.3337, 0.2422, 0.0062, 0.1037, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 170, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2131e-01, 1.3489e-05, 1.0556e-02, 1.6475e-08, 2.1214e-07, 1.5040e-02,
        2.5308e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.122

[Epoch: 170, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3374e-03, 3.2049e-03, 9.8360e-01, 2.4484e-07, 3.2224e-03, 2.9537e-03,
        3.6827e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 170, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3628e-01, 3.7801e-03, 6.2005e-02, 5.7790e-07, 1.7756e-01, 3.0997e-01,
        1.0401e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.092

[Epoch: 171, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0355e-03, 4.5058e-02, 5.5418e-02, 7.1710e-09, 4.5540e-01, 3.0750e-01,
        1.3359e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 171, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2566, 0.3474, 0.2424, 0.0068, 0.1050, 0.0381],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 171, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1827e-01, 9.9972e-06, 9.4527e-03, 1.7637e-08, 1.6361e-07, 1.7044e-02,
        2.5522e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.113

[Epoch: 171, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6602e-03, 3.7669e-03, 9.8152e-01, 2.1640e-07, 3.7569e-03, 3.9523e-03,
        3.3395e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 171, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4731e-01, 3.5022e-03, 6.2585e-02, 5.0416e-07, 1.7209e-01, 3.0531e-01,
        9.2010e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.096

[Epoch: 172, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6001e-03, 5.1793e-02, 5.7932e-02, 1.1088e-08, 4.4607e-01, 3.0088e-01,
        1.3972e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 172, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0041, 0.2696, 0.3327, 0.2539, 0.0068, 0.0974, 0.0356],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.009

[Epoch: 172, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.4696e-01, 2.3240e-05, 1.0439e-02, 2.4768e-08, 3.3090e-07, 2.3501e-02,
        3.1908e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.102

[Epoch: 172, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5386e-03, 3.1441e-03, 9.8381e-01, 2.6647e-07, 3.4437e-03, 3.2474e-03,
        3.8111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.010

[Epoch: 172, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3999e-01, 3.2285e-03, 5.1601e-02, 3.0736e-07, 1.8528e-01, 3.0910e-01,
        1.0798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 173, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.0317e-03, 3.4927e-02, 4.4807e-02, 7.8388e-09, 4.5172e-01, 3.3807e-01,
        1.2644e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 173, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0032, 0.2511, 0.3325, 0.2346, 0.0063, 0.1329, 0.0394],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 173, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.5960e-01, 2.1634e-05, 1.0578e-02, 2.7192e-08, 2.9665e-07, 1.4406e-02,
        2.1539e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.112

[Epoch: 173, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9425e-03, 3.5130e-03, 9.8596e-01, 2.8870e-07, 2.8021e-03, 1.8543e-03,
        2.9237e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 173, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.1723e-01, 3.5165e-03, 6.4658e-02, 2.2225e-06, 1.6693e-01, 3.3616e-01,
        1.1497e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.073

[Epoch: 174, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.7077e-03, 5.6867e-02, 6.9407e-02, 2.8309e-08, 4.4046e-01, 2.9359e-01,
        1.3596e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 174, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0043, 0.2580, 0.3658, 0.2404, 0.0069, 0.0918, 0.0328],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 -0.001

[Epoch: 174, batch: 123/208] total loss per batch: 0.544
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4216e-01, 1.4506e-05, 1.0372e-02, 4.5438e-08, 2.6390e-07, 1.6155e-02,
        2.3130e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.127

[Epoch: 174, batch: 164/208] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0622e-03, 5.3688e-03, 9.7424e-01, 6.2542e-07, 6.2592e-03, 5.6063e-03,
        4.4679e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 174, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0804e-01, 3.7965e-03, 6.6011e-02, 1.2506e-06, 1.9162e-01, 3.2097e-01,
        9.5671e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.102

[Epoch: 175, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4097e-03, 4.2850e-02, 4.5591e-02, 4.8624e-08, 4.6862e-01, 3.0374e-01,
        1.3579e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 175, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2702, 0.3247, 0.2691, 0.0069, 0.0903, 0.0354],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 175, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.3321e-01, 3.2230e-05, 1.1620e-02, 4.3212e-08, 3.9093e-07, 1.6366e-02,
        3.3877e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.105

[Epoch: 175, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4524e-03, 3.2300e-03, 9.8384e-01, 7.5503e-07, 3.6453e-03, 2.8913e-03,
        2.9433e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 175, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3125e-01, 3.7813e-03, 6.0568e-02, 6.9660e-07, 1.8124e-01, 3.1317e-01,
        9.9860e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.109

[Epoch: 176, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4207e-03, 4.8609e-02, 4.6698e-02, 2.8901e-08, 4.7908e-01, 2.9633e-01,
        1.2586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 176, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2696, 0.3348, 0.2266, 0.0070, 0.1235, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 176, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.4287e-01, 3.4078e-05, 1.0135e-02, 3.0691e-08, 3.3337e-07, 1.8048e-02,
        2.2892e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.131

[Epoch: 176, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.8347e-03, 3.4924e-03, 9.8408e-01, 8.7440e-07, 3.2281e-03, 3.1197e-03,
        3.2462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 176, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3164e-01, 3.5468e-03, 5.6264e-02, 1.0022e-06, 1.8254e-01, 3.1518e-01,
        1.0830e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 177, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5454e-03, 4.6863e-02, 5.9777e-02, 2.5194e-08, 4.3407e-01, 3.2202e-01,
        1.3373e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 177, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2556, 0.3416, 0.2448, 0.0070, 0.1083, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 177, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1387e-01, 2.9822e-05, 1.0975e-02, 3.6848e-08, 3.6370e-07, 1.9869e-02,
        2.5526e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.104

[Epoch: 177, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.0356e-03, 3.2525e-03, 9.8420e-01, 4.7450e-07, 3.0384e-03, 3.5608e-03,
        2.9142e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 177, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3517e-01, 3.5370e-03, 5.5937e-02, 4.9145e-07, 1.7910e-01, 3.1653e-01,
        9.7254e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 178, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5643e-03, 4.5953e-02, 5.1492e-02, 2.4736e-08, 4.5523e-01, 3.1356e-01,
        1.3020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 178, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2615, 0.3440, 0.2414, 0.0068, 0.1052, 0.0372],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 178, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0552e-01, 2.4287e-05, 1.0165e-02, 2.0392e-08, 2.2014e-07, 1.2627e-02,
        2.7167e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.130

[Epoch: 178, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4705e-03, 3.6849e-03, 9.8225e-01, 5.2438e-07, 3.3960e-03, 3.5079e-03,
        3.6940e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.002

[Epoch: 178, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4224e-01, 3.5524e-03, 5.9573e-02, 4.1450e-07, 1.7560e-01, 3.0940e-01,
        9.6382e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.081

[Epoch: 179, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3515e-03, 4.3972e-02, 5.8543e-02, 1.6866e-08, 4.4715e-01, 3.0440e-01,
        1.4258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 179, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2618, 0.3426, 0.2427, 0.0067, 0.1042, 0.0385],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 179, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1519e-01, 2.2378e-05, 1.0887e-02, 2.0480e-08, 2.4383e-07, 1.8162e-02,
        2.5574e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.108

[Epoch: 179, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6295e-03, 3.3542e-03, 9.8258e-01, 4.4723e-07, 3.6146e-03, 3.3505e-03,
        3.4697e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.011

[Epoch: 179, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2761e-01, 3.9275e-03, 6.0401e-02, 4.5114e-07, 1.9009e-01, 3.0683e-01,
        1.1148e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.087

[Epoch: 180, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5120e-03, 4.8220e-02, 5.0728e-02, 3.0127e-08, 4.5616e-01, 3.1543e-01,
        1.2595e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 180, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0031, 0.2652, 0.3402, 0.2462, 0.0061, 0.1057, 0.0335],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 180, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0292e-01, 2.9102e-05, 1.0236e-02, 2.5328e-08, 2.4844e-07, 1.4715e-02,
        2.7210e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.121

[Epoch: 180, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.6936e-03, 3.2236e-03, 9.8273e-01, 3.9149e-07, 3.4715e-03, 3.5977e-03,
        3.2829e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 180, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3355e-01, 3.8151e-03, 6.0202e-02, 2.8968e-07, 1.8078e-01, 3.1100e-01,
        1.0646e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.107

[Epoch: 181, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.6540e-03, 4.8061e-02, 5.3598e-02, 1.2914e-08, 4.6270e-01, 2.9977e-01,
        1.3222e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 181, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2584, 0.3280, 0.2558, 0.0072, 0.1074, 0.0398],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 181, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2535e-01, 2.1797e-05, 8.8422e-03, 2.4825e-08, 2.3475e-07, 1.7097e-02,
        2.4869e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.118

[Epoch: 181, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.5646e-03, 3.2530e-03, 9.8271e-01, 3.8972e-07, 3.4795e-03, 3.2580e-03,
        3.7303e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 181, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3393e-01, 3.1511e-03, 5.9578e-02, 2.6758e-07, 1.7161e-01, 3.2223e-01,
        9.5015e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.102

[Epoch: 182, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3754e-03, 4.6894e-02, 5.4631e-02, 7.9560e-09, 4.3807e-01, 3.1964e-01,
        1.3739e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 182, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2742, 0.3390, 0.2259, 0.0074, 0.1141, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 182, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0818e-01, 1.6339e-05, 1.0301e-02, 1.3606e-08, 1.5560e-07, 1.7582e-02,
        2.6392e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.117

[Epoch: 182, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9266e-03, 3.1189e-03, 9.8389e-01, 4.2180e-07, 3.3887e-03, 3.4019e-03,
        3.2747e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 182, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4040e-01, 3.1329e-03, 5.8172e-02, 2.5648e-07, 1.6822e-01, 3.2176e-01,
        8.3093e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 183, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.4800e-03, 4.9922e-02, 5.2555e-02, 8.1543e-09, 4.6546e-01, 2.9393e-01,
        1.3365e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 183, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2625, 0.3401, 0.2529, 0.0068, 0.0993, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 183, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9991e-01, 2.1035e-05, 1.2059e-02, 1.2090e-08, 1.7999e-07, 1.6504e-02,
        2.7150e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.119

[Epoch: 183, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.3290e-03, 3.2197e-03, 9.8663e-01, 3.3000e-07, 2.4184e-03, 2.4914e-03,
        2.9134e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 183, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3334e-01, 3.5528e-03, 5.5982e-02, 2.3528e-07, 1.9601e-01, 2.9919e-01,
        1.1921e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 184, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.8082e-03, 4.0540e-02, 4.2370e-02, 1.6212e-08, 4.5325e-01, 3.3108e-01,
        1.2996e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 184, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0040, 0.2424, 0.3525, 0.2452, 0.0068, 0.1106, 0.0385],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 184, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9784e-01, 2.1072e-05, 9.0195e-03, 1.3289e-08, 2.5803e-07, 1.2300e-02,
        2.8082e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.130

[Epoch: 184, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.5241e-03, 4.2624e-03, 9.7831e-01, 4.5189e-07, 4.3739e-03, 4.5110e-03,
        4.0139e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.000

[Epoch: 184, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2358e-01, 4.1051e-03, 6.1132e-02, 3.0186e-07, 1.9660e-01, 3.0313e-01,
        1.1454e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.088

[Epoch: 185, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([2.7955e-03, 4.1282e-02, 6.5394e-02, 8.9019e-09, 4.4633e-01, 3.0683e-01,
        1.3737e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 185, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2701, 0.3380, 0.2520, 0.0058, 0.0946, 0.0360],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 185, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3315e-01, 2.6727e-05, 9.9554e-03, 2.0924e-08, 2.6470e-07, 2.4473e-02,
        2.3239e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.097

[Epoch: 185, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.6774e-03, 2.9123e-03, 9.8500e-01, 8.1492e-07, 3.6049e-03, 2.8720e-03,
        2.9311e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.011

[Epoch: 185, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2275e-01, 4.9230e-03, 5.5681e-02, 4.6469e-07, 1.8034e-01, 3.2616e-01,
        1.0142e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.101

[Epoch: 186, batch: 41/208] total loss per batch: 0.543
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([4.4572e-03, 6.6954e-02, 5.1052e-02, 2.5801e-08, 4.4328e-01, 2.9684e-01,
        1.3741e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 186, batch: 82/208] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2891, 0.3314, 0.2127, 0.0070, 0.1168, 0.0396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 186, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0764e-01, 2.6650e-05, 7.1654e-03, 2.2094e-08, 1.8014e-07, 1.0516e-02,
        2.7465e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.163

[Epoch: 186, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2312e-03, 3.3952e-03, 9.8509e-01, 1.7635e-07, 2.4934e-03, 3.0440e-03,
        2.7452e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 186, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4359e-01, 3.1056e-03, 5.4000e-02, 5.1038e-07, 1.8729e-01, 3.0356e-01,
        8.4473e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.082

[Epoch: 187, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5124e-03, 4.7333e-02, 5.1810e-02, 1.9165e-08, 4.5979e-01, 3.1275e-01,
        1.2481e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 187, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2341, 0.3346, 0.2766, 0.0075, 0.1073, 0.0365],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.012

[Epoch: 187, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.2135e-01, 2.3340e-05, 1.0198e-02, 3.4335e-08, 4.7748e-07, 2.2308e-02,
        2.4612e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.097

[Epoch: 187, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.0330e-03, 3.5410e-03, 9.8145e-01, 4.2182e-07, 3.5046e-03, 3.5783e-03,
        3.8939e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 187, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3424e-01, 3.5235e-03, 6.6243e-02, 2.6812e-07, 1.8047e-01, 3.0589e-01,
        9.6368e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.093

[Epoch: 188, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5630e-03, 3.4411e-02, 5.6658e-02, 1.4692e-08, 4.4712e-01, 3.2075e-01,
        1.3750e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 -0.001

[Epoch: 188, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0030, 0.2626, 0.3474, 0.2473, 0.0063, 0.1012, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.009

[Epoch: 188, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0607e-01, 2.3416e-05, 9.0059e-03, 2.3060e-08, 3.6466e-07, 1.4706e-02,
        2.7020e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.109

[Epoch: 188, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4695e-03, 3.0481e-03, 9.8303e-01, 4.0576e-07, 3.7455e-03, 3.2498e-03,
        3.4586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 188, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3677e-01, 4.2509e-03, 6.4691e-02, 4.2683e-07, 1.8100e-01, 3.0331e-01,
        9.9865e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 189, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.9161e-03, 4.9779e-02, 5.6436e-02, 1.6456e-08, 4.6007e-01, 2.9613e-01,
        1.3367e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 189, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0033, 0.2648, 0.3391, 0.2389, 0.0064, 0.1051, 0.0424],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.009

[Epoch: 189, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0984e-01, 2.4809e-05, 1.2110e-02, 2.6044e-08, 3.0516e-07, 1.9278e-02,
        2.5875e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.134

[Epoch: 189, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4391e-03, 3.3978e-03, 9.8326e-01, 4.0553e-07, 3.3733e-03, 2.9626e-03,
        3.5717e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.007

[Epoch: 189, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3614e-01, 3.1503e-03, 6.0845e-02, 3.3965e-07, 1.7318e-01, 3.1722e-01,
        9.4596e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.106

[Epoch: 190, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2558e-03, 5.2225e-02, 4.7770e-02, 1.1455e-08, 4.4719e-01, 3.1918e-01,
        1.3038e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 190, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0038, 0.2567, 0.3407, 0.2446, 0.0070, 0.1111, 0.0362],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 190, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0294e-01, 2.0061e-05, 1.0749e-02, 1.8631e-08, 2.0440e-07, 1.5946e-02,
        2.7034e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.125

[Epoch: 190, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.4175e-03, 3.7945e-03, 9.8275e-01, 3.7902e-07, 3.5014e-03, 3.3113e-03,
        3.2251e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.009

[Epoch: 190, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3478e-01, 3.5002e-03, 5.6522e-02, 2.5025e-07, 1.7752e-01, 3.1710e-01,
        1.0580e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.098

[Epoch: 191, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.0699e-03, 4.4127e-02, 5.3334e-02, 8.3753e-09, 4.5669e-01, 3.0882e-01,
        1.3396e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 191, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0035, 0.2644, 0.3346, 0.2490, 0.0070, 0.1063, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 191, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1466e-01, 1.7058e-05, 1.0191e-02, 1.3501e-08, 2.2303e-07, 1.4562e-02,
        2.6057e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.107

[Epoch: 191, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2545e-03, 3.5160e-03, 9.8355e-01, 3.5532e-07, 3.0772e-03, 3.3762e-03,
        3.2232e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 191, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3051e-01, 3.8433e-03, 5.8097e-02, 4.0099e-07, 1.8157e-01, 3.1501e-01,
        1.0973e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.100

[Epoch: 192, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.3991e-03, 4.5991e-02, 5.3091e-02, 9.3327e-09, 4.5140e-01, 3.1456e-01,
        1.3156e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 192, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0032, 0.2776, 0.3351, 0.2391, 0.0061, 0.1031, 0.0359],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.010

[Epoch: 192, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1331e-01, 1.4353e-05, 7.7392e-03, 1.5201e-08, 1.9468e-07, 1.4713e-02,
        2.6423e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.136

[Epoch: 192, batch: 164/208] total loss per batch: 0.547
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.2244e-03, 2.9686e-03, 9.8426e-01, 2.4742e-07, 3.0911e-03, 2.9296e-03,
        3.5304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.005

[Epoch: 192, batch: 205/208] total loss per batch: 0.536
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2931e-01, 3.8848e-03, 6.2461e-02, 3.2201e-07, 1.7759e-01, 3.1705e-01,
        9.7088e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.095

[Epoch: 193, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4152e-03, 4.9072e-02, 5.3594e-02, 9.3787e-09, 4.4615e-01, 3.0906e-01,
        1.3870e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 193, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2459, 0.3638, 0.2354, 0.0071, 0.1068, 0.0377],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.008

[Epoch: 193, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.0813e-01, 1.7076e-05, 1.0379e-02, 1.4548e-08, 1.6953e-07, 2.2753e-02,
        2.5872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.104

[Epoch: 193, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.1301e-03, 3.4412e-03, 9.8479e-01, 1.8418e-07, 2.8961e-03, 2.4822e-03,
        3.2586e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.017

[Epoch: 193, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.2735e-01, 2.9643e-03, 6.2416e-02, 2.4424e-07, 1.7452e-01, 3.2308e-01,
        9.6726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.090

[Epoch: 194, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2247e-03, 4.3795e-02, 5.2283e-02, 6.8697e-09, 4.6458e-01, 3.0691e-01,
        1.2921e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 194, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2533, 0.3360, 0.2538, 0.0069, 0.1100, 0.0366],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 194, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1311e-01, 1.6401e-05, 1.0349e-02, 9.1619e-09, 1.4486e-07, 1.2281e-02,
        2.6424e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.149

[Epoch: 194, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([4.1102e-03, 3.5789e-03, 9.8113e-01, 5.7321e-07, 3.9119e-03, 3.5031e-03,
        3.7674e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.004

[Epoch: 194, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3928e-01, 3.5018e-03, 6.1512e-02, 1.9915e-07, 1.8218e-01, 3.0351e-01,
        1.0011e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.091

[Epoch: 195, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4308e-03, 4.7287e-02, 5.5339e-02, 1.3817e-08, 4.4495e-01, 3.1223e-01,
        1.3677e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 195, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0028, 0.2897, 0.3091, 0.2563, 0.0057, 0.1019, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 195, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9891e-01, 3.3627e-05, 1.3446e-02, 2.8492e-08, 3.5869e-07, 1.4454e-02,
        2.7316e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.091

[Epoch: 195, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.8558e-03, 4.0997e-03, 9.7938e-01, 7.4863e-07, 4.2640e-03, 4.7200e-03,
        3.6808e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 0.000

[Epoch: 195, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4324e-01, 4.2039e-03, 5.9152e-02, 5.9579e-07, 1.8417e-01, 2.9730e-01,
        1.1931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.083

[Epoch: 196, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5226e-03, 5.5534e-02, 4.8061e-02, 1.1800e-08, 4.5041e-01, 3.0797e-01,
        1.3450e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 196, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0034, 0.2539, 0.3648, 0.2208, 0.0076, 0.1121, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.006

[Epoch: 196, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.3132e-01, 1.6744e-05, 6.0459e-03, 3.3314e-08, 2.3752e-07, 2.5959e-02,
        2.3666e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.102

[Epoch: 196, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5247e-03, 2.9683e-03, 9.8455e-01, 1.3294e-07, 3.2351e-03, 3.5814e-03,
        3.1439e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.016

[Epoch: 196, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3370e-01, 3.2289e-03, 5.0792e-02, 2.3123e-07, 2.0032e-01, 3.0095e-01,
        1.1012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.126

[Epoch: 197, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.8960e-03, 3.8011e-02, 6.9884e-02, 1.6856e-08, 4.4848e-01, 3.0798e-01,
        1.3174e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 197, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0047, 0.2374, 0.3368, 0.2689, 0.0077, 0.1016, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 197, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9943e-01, 9.1461e-06, 9.4070e-03, 2.3477e-08, 2.2469e-07, 1.1338e-02,
        2.7982e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.121

[Epoch: 197, batch: 164/208] total loss per batch: 0.549
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.3384e-03, 3.0354e-03, 9.8249e-01, 3.1880e-07, 4.1711e-03, 3.1884e-03,
        3.7791e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.013

[Epoch: 197, batch: 205/208] total loss per batch: 0.538
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.0731e-01, 2.5272e-03, 5.2490e-02, 3.0418e-07, 1.8251e-01, 3.4443e-01,
        1.0735e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.075

[Epoch: 198, batch: 41/208] total loss per batch: 0.542
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.5186e-03, 4.9909e-02, 4.2057e-02, 7.6202e-09, 4.6920e-01, 3.0920e-01,
        1.2611e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 198, batch: 82/208] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0039, 0.2926, 0.3425, 0.2296, 0.0067, 0.0932, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.004

[Epoch: 198, batch: 123/208] total loss per batch: 0.543
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1528e-01, 2.0903e-05, 1.6100e-02, 3.3126e-08, 4.3113e-07, 1.5901e-02,
        2.5270e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.116

[Epoch: 198, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([3.9804e-03, 3.2623e-03, 9.8241e-01, 5.2963e-07, 3.8926e-03, 3.1924e-03,
        3.2578e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.006

[Epoch: 198, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4116e-01, 3.8839e-03, 7.0112e-02, 7.1682e-07, 1.6000e-01, 3.1314e-01,
        1.1706e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.099

[Epoch: 199, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.2984e-03, 4.5551e-02, 5.7590e-02, 2.0180e-08, 4.4073e-01, 3.0921e-01,
        1.4362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 199, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0036, 0.2606, 0.3414, 0.2429, 0.0065, 0.1100, 0.0350],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.005

[Epoch: 199, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([7.1493e-01, 1.6908e-05, 8.4676e-03, 3.3110e-08, 3.0387e-07, 1.4932e-02,
        2.6165e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.147

[Epoch: 199, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.9475e-03, 3.4786e-03, 9.8477e-01, 4.3868e-07, 3.0474e-03, 2.7948e-03,
        2.9655e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.008

[Epoch: 199, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.4296e-01, 3.8705e-03, 6.3366e-02, 5.6321e-07, 1.7034e-01, 3.0867e-01,
        1.0796e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.111

[Epoch: 200, batch: 41/208] total loss per batch: 0.541
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0467, 0.0533, 0.0000, 0.4533, 0.3100, 0.1333])
Policy pred: tensor([3.4674e-03, 4.8627e-02, 4.9761e-02, 1.1276e-08, 4.4682e-01, 3.1039e-01,
        1.4093e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 200, batch: 82/208] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.2633, 0.3400, 0.2433, 0.0067, 0.1067, 0.0367])
Policy pred: tensor([0.0037, 0.2587, 0.3371, 0.2359, 0.0070, 0.1157, 0.0419],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.007 0.007

[Epoch: 200, batch: 123/208] total loss per batch: 0.542
Policy (actual, predicted): 0 0
Policy data: tensor([0.7100, 0.0000, 0.0100, 0.0000, 0.0000, 0.0167, 0.2633])
Policy pred: tensor([6.9365e-01, 1.3880e-05, 9.1838e-03, 1.9862e-08, 2.1212e-07, 1.7423e-02,
        2.7973e-01], grad_fn=<SelectBackward>)Value (actual, predicted): -0.119 -0.127

[Epoch: 200, batch: 164/208] total loss per batch: 0.548
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.9833, 0.0000, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([2.5569e-03, 2.9525e-03, 9.8594e-01, 3.6078e-07, 2.8332e-03, 2.6639e-03,
        3.0552e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.007 -0.011

[Epoch: 200, batch: 205/208] total loss per batch: 0.537
Policy (actual, predicted): 0 0
Policy data: tensor([0.4333, 0.0033, 0.0600, 0.0000, 0.1800, 0.3133, 0.0100])
Policy pred: tensor([4.3750e-01, 3.6806e-03, 5.8118e-02, 5.1585e-07, 1.8026e-01, 3.0912e-01,
        1.1320e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.097 0.097

